Agile manufacturing, fast-response micromarketing, and the rise of the virtual organization have led managers to focus on cross-functional business processes that link various divisions and organizations. These processes may be realized as one or more workflows, each of which is an instantiation of a process under certain conditions. Because an ability to adapt processes to workflow conditions is essential for organizational responsiveness, identifying and analyzing significant workflows is an important activity for managers, organization designers, and information systems specialists. A variety of software systems have been developed to aid in the structuring and implementation of workflow systems, but they are mostly visualization tools with few analytical capabilities. For example, they do not allow their users to easily determine which information elements are needed to compute other information elements, whether certain tasks depend on other tasks, and how resource availability affects information and tasks. Analyses of this type can be performed by inspection, but this gives rise to the possibility of error, especially in large systems. In this paper, we show how a mathematical construct called a metagraph can be used to represent workflows, so that such questions can be addressed through formal operations, leading to more effective design of organizational processes.
As more business is being conducted internationally and corporations establish themselves globally, the impact of cross-cultural aspects becomes an important research issue. The need to conduct cross-cultural research is perhaps even more important in the relatively newly emerging and quickly changing information systems (IS) field. This article presents issues relating to qualitative research, emic versus etic approaches, and describes a structured, yet flexible, qualitative research interviewing technique, which decreases the potential for bias on the part of the researcher. The grounded theory technique presented in this article is based on Kelly's Repertory Grid (RepGrid), which concentrates on laddering, or the further elaboration of elicited constructs, to obtain detailed research participant comments about an aspect within the domain of discourse. The technique provides structure to a one-to-one interview. But, at the same time, RepGrids allow sufficient flexibility for the research participants to be able to express their own interpretation about a particular topic. This article includes a brief outline of a series of research projects that employed the RepGrid technique to examine similarities and differences in the way in which excellent systems analysts are viewed in two different cultures. Also included is a discussion of the technique's applicability for qualitative research in general and cross-cultural studies specifically. The article concludes by suggesting ways in which the RepGrid technique addresses some of the major methodological issues in cross-cultural research.
Organizations today face great pressure to maximize the benefits from their investments in information technology (IT). They are challenged not just to use IT, but to use it as effectively as possible. Understanding how to assess the competence of users is critical in maximizing the effectiveness of IT use. Yet the user competence construct is largely absent from prominent technology acceptance and fit models, poorly conceptualized, and inconsistently measured. We begin by presenting a conceptual model of the assessment of user competence to organize and clarify the diverse literature regarding what user competence means and the problems of assessment. As an illustrative study, we then report the findings from an experiment involving 66 participants. The experiment was conducted to compare empirically two methods (paper and pencil tests versus self-report questionnaire), across two different types of software, or domains of knowledge (word processing versus spreadsheet packages), and two different conceptualizations of competence (software knowledge versus self-efficacy). The analysis shows statistical significance in all three main effects. How user competence is measured, what is measured, what measurement context is employed: all influence the measurement outcome. Furthermore, significant interaction effects indicate that different combinations of measurement methods, conceptualization, and knowledge domains produce different results. The concept of frame of reference, and its anchoring effect on subjects' responses, explains a number of these findings. The study demonstrates the need for clarity in both defining what type of competence is being assessed and in drawing conclusions regarding competence, based upon the types of measures used. Since the results suggest that definition and measurement of the user competence construct can change the ability score being captured, the existing information system (IS) models of usage must contain the concept of an ability rating. We conclude by discussing how user competence can be incorporated into the Task-Technology Fit model, as well as additional theoretical and practical implications of our research.
Recent theoretical work suggests that network externalities are a determinant of network adoption. However, few empirical studies have reported the impact of network externalities on the adoption of networks. As a result, little is known about the extent to which network externalities may influence network adoption and diffusion. Using electronic banking as a context and an econometric technique called hazard modeling, this research examines empirically the impact of network externalities and other influences that combine to determine network membership. The results support the network externalities hypothesis. We find that banks in markets that can generate a larger effective network size and a higher level of externalities tend to adopt early, while the size of a bank's own branch network (a proxy for the opportunity cost of adoption) decreases the probability of early adoption.
The inherent uncertainty pervasive over the real world often forces business decisions to be made using uncertain data. The conventional relational model does not have the ability to handle uncertain data. In recent years, several approaches have been proposed in the literature for representing uncertain data by extending the relational model, primarily using probability theory. The aspect of database modification, however, has not been addressed in prior research. It is clear that any modification of existing probabilistic data, based on new information, amounts to the revision of one's belief about real-world objects. In this paper, we examine the aspect of belief revision and develop a generalized algorithm that can be used for the modification of existing data in a probabilistic relational database. The belief revision scheme is shown to be closed, consistent, and complete.
This research paper analyzes the impact of information technology (IT) in a healthcare setting using a longitudinal sample of hospital data from 1976 to 1994. We classify production inputs into labor and capital categories. Capital is classified into three components-medical IT capital, medical capital, and IT capital-and labor is classified into two components, medical labor and IT labor. Results provide evidence that IT contributes positively to the production of services in the healthcare industry.
First impression bias refers to a limitation of human information processing in which people are strongly influenced by the first piece of information that they are exposed to, and that they are biased in evaluating subsequent information in the direction of the initial influence. The psychology literature has portrayed first impression bias as a virtually inherent human bias. Drawing from multimedia literature, this study identifies several characteristics of multimedia presentations that have the potential to alleviate first impression bias. Based on this literature, a set of predictions was generated and tested through a laboratory experiment using a simulated multimedia intranet. Half of the 80 subjects were provided with a biased cue. Subjects were randomly assigned to four groups: (1) text with first impression bias cue, (2) multimedia with first impression bias cue, (3) text without biased cue, and (4) multimedia without biased cue. The experimental task involved conducting a five-year performance appraisal of a department head. The first impression bias cue was designed to provide incomplete and unfavorable information about the department head, but the information provided subsequently was intended to be favorable of his performance. Results show that the appraisal score of the text with biased cue group was significantly lower than the text only (without biased cue) group. On the other hand, the appraisal score of the multimedia with biased cue group was not significantly different from the multimedia only (without biased cue) group. As a whole, the results suggest that multimedia presentations, but not text-based presentations, reduce the influence of first impression bias.
For more than a decade NEC dominated the Japanese PC market with its PC-98 architecture, which was incompatible both with its major Japanese rivals and the global PC standard. However, NEC was powerless to prevent the introduction of Japanese versions of Windows 3.1 and 95 that ran on its competitors' architectures as well as on the PC-98, unifying the Japanese PC market and creating a common set of application programming interfaces for all Intel-based Japanese PCs. The introduction of Windows rendered obsolete the large DOS-based software library that had provided strong positive externalities for the NEC architecture. Absent those advantages, the market share of the PC-98 standard fell from 60% to 33% in five years, and NEC finally abandoned the PC-98 in favor of the global standard. An examination of the unusual rise and fall of the PC-98 shows how victory in a standards competition can be negated by the introduction of a new architectural layer that spans two or more previously incompatible architectures.
Notably absent in previous research on inductive expert systems is the study of mean-risk trade-offs. Such trade-offs may be significant when there are asymmetries such as unequal classification costs, and uncertainties in classification and information acquisition costs. The objective of this research is to develop models to evaluate mean-risk trade-offs in value-based inductive approaches. We develop a combined mean-risk measure and incorporate it into the Risk-Based induction algorithm. The mean-risk measure has desirable theoretical properties (consistency and separability) and is supported by empirical results on decision making under risk. Simulation results using the Risk-Based algorithm demonstrate: (i) an order of magnitude performance difference between mean-based and risk-based algorithms and (ii) an increase in the performance difference between these algorithms as either risk aversion, uncertainty, or asymmetry increases given modest thresholds of the other two factors.
Previous research has demonstrated that the production of information services can be characterized at the aggregate economy-wide level by the Cobb-Douglas production function. However, the underlying production process at the firm level has not yet been ascertained. The objective of this paper is to determine the form of the production process for information systems services at the firm level by conducting an empirical analysis of IS budget data. The production of information services is modeled using a production function with two inputs, hardware and personnel. We estimate various econometric specifications to determine several characteristics of the provision of information services, including the allocation of the information systems budget to its two largest components--hardware and personnel--and its implications for the form of the production function. After controlling for industry sector, we find that the ratio of personnel to hardware is independent of scale, which indicates a homothetic production function. We also find that the ratio of factor shares is constant with time, consistent with the Cobb-Douglas production function. We conclude that the underlying form of the production function is the same at the level of both the firm and the economy. Our analysis demonstrates how the application of production theory to the production of information services can yield useful insights from both a theoretical and managerial perspective.
Prior research has generated considerable knowledge about the design of effective IT organizational architectures. Today, however, increasing signs have accumulated that this wisdom might be inadequate in shaping appropriate insights for contemporary practice. This essay seeks to direct research attention toward the following question: How should firms organize their IT activities in order to manage the imperatives of the business and technological environments in the digital economy? We articulate the platform logic as a conceptual framework for both viewing the organizing of IT management activities as well as for framing important questions for future research. In articulating this logic, we aim to shift thinking away from the traditional focus on governance structures (i.e., choice of centralized, decentralized, or federal forms) and sourcing structures (i.e., insourcing, outsourcing) and toward more complex structures that are reflective of contemporary practice. These structures are designed around important IT capabilities and network architectures.
We propose priority pricing as an on-line adaptive resource scheduling mechanism to manage real-time databases within organizations. These databases provide timely information for delay sensitive users. The proposed approach allows diverse users to optimize their own objectives while collectively maximizing organizational benefits. We rely on economic principles to derive priority prices by modeling the fixed-capacity real-time database environment as an economic system. Each priority is associated with a price and a delay, and the price is the premium (congestion toll resulting from negative externalities) for accessing the database. At optimality, the prices are equal to the aggregate delay cost imposed on all other users of the database. These priority prices are used to control admission and to schedule user jobs in the database system. The database monitors the arrival processes and the state of the system, and incrementally adjusts the prices to regulate the flow. Because our model ignores the operational intricacies of the real-time databases (e.g., intermediate queues at the CPU and disks, memory size, etc.) to maintain analytical tractability, we evaluate the performance of our pricing approach through simulation. We evaluate the database performance using both the traditional real-time database performance metrics (e.g., the number of jobs serviced on time, average tardiness) and the economic benefits (e.g., benefits to the organization). The simulation results, under various database workload parameters, show that our priority pricing mechanism not only maximizes organizational benefits but also outperforms in all aspects of traditional performance measures compared to frequently used database scheduling techniques, such as first-come-first-served, earliest deadline first and least slack first.
Error search and correction are major contributors to software development cost, yet typically uncover only a small fraction of software errors. Postrelease errors, i.e., those that are only observed after a system is released, threaten a variety of potential failures and consequences, each with low individual probability of occurrence. The combined effect of postrelease errors can and often does result in a significant rate of occurrence of these potential failures, with unpredictable consequences and severity. One particular source of postrelease errors that has received extensive publicity is the year 2000, or Y2K, error. The modeling in this research report suggests that testing probably needs to be conducted over more than half of the useful life of a system in order to discover even one-third of the total errors in the system. It suggests that short product lifecycles, lifetime testing, and effective feedback loops for error reporting are necessary to assure reliable software.
In order to understand diagrammatic reasoning with multiple diagrams, this study proposes a theoretical framework that focuses on the cognitive processes of perceptual and conceptual integration. The perceptual integration process involves establishing interdependence between relevant system elements that have been dispersed across multiple diagrams, while the conceptual integration process involves generating and refining hypotheses about a system by combining higher-level information inferred from the diagrams. This study applies a diagrammatic reasoning framework of a single diagram to assess the usability of multiple diagrams as an integral part of a system development methodology. Our experiment evaluated the effectiveness and usability of design guidelines to aid problem solving with multiple diagrams. The results of our experiment revealed that understanding a system represented by multiple diagrams involves a process of searching for related information and of developing hypotheses about the target system. The results also showed that these perceptual and conceptual integration processes were facilitated by incorporating visual cues and contextual information in the multiple diagrams as representation aids. Visual cues indicate which elements in a diagram are related to elements in other diagrams; the contextual information indicates how the individual datum in one diagram is related to the overall hypothesis about the entire system.
The cost of enhancing software applications to accommodate new and evolving user requirements is significant. Many enhancement cost-reduction initiatives have focused on increasing software structure in applications. However, while software structure can decrease enhancement effort by localizing data processing, increased effort is also required to comprehend structure. Thus, it is not clear whether high levels of software structure are economically efficient in all situations. In this study, we develop a model of the relationship between software structure and software enhancement costs and errors. We introduce the notion of software structure as a moderator of the relationship between software volatility, total data complexity, and software enhancement outcomes. We posit that it is efficient to more highly structure the more volatile applications, because increased familiarity with the application structure through frequent enhancement enables localization of maintenance effort. For more complex applications, software structure is more beneficial than for less complex applications because it facilitates the comprehension process where it is most needed. Given the downstream enhancement benefits of structure for more volatile and complex applications, we expect that the optimal level of structure is higher for these applications. We empirically evaluate our model using data collected on the business applications of a major mass merchandiser and a large commercial bank. We find that structure moderates the relationship between complexity, volatility, and enhancement outcomes, such that higher levels of structure are more advantageous for the more complex and more volatile applications in terms of reduced enhancement costs and errors. We also find that more structure is designed in for volatile applications and for applications with higher levels of complexity. Finally, we identify application type as a significant factor in predicting which applications are more volatile and more complex at our research sites. That is, applications with induction-based algorithms such as those that support planning, forecasting, and management decision-making activities are more complex and more volatile than applications with rule-based algorithms that support operational and transaction-processing activities. Our results indicate that high investment in software quality practices such as structured design is not economically efficient in all situations. Our findings also suggest the importance of organizational mechanisms in promoting efficient design choices that lead to reduced enhancement costs and errors.
The most difficult challenge facing a market leader is maintaining its leading position. This is especially true in information technology and telecommunications industries, where multiple product generations and rapid technological evolution continually test the ability of the incumbent to stay ahead of potential entrants. In these industries, an incumbent often protects its position by launching prematurely to retain its leadership. Entry, however, happens relatively frequently. We identify conditions under which an entrant will launch a next generation product thereby preventing the incumbent from employing a protection strategy. We define a capabilities advantage as the ability to develop and launch a next generation product at a lower cost than a competitor, and a product with a greater market response is one with greater profit flows. Using these definitions, we find that an incumbent with a capabilities advantage in one next generation product can be overtaken by an entrant with a capabilities advantage in another next generation product only if the entrant's capabilities advantage is in a disruptive technology that yields a product with a greater market response. This can occur even though both next generation products are available to both firms. We also show that the competition may require the launching firm to lose money at the margin on the next generation product.
The Quick Response (QR) program is a hierarchical suite of information technologies (IT) and applications designed to improve the performance of retailers. Consultants advise retailers to adopt the program wholesale, implying that more and higher levels of technology are better than less technology and lower levels. Academicians, on the other hand, argue that good technology is appropriate technology. That is, firms should adopt only those technologies that suit the specific strategic directions pursued by the firm. Who is right? Which approach to investing in IT yields better performance results? Surprisingly, this cross-sectional survey of 80 specialty retailers found more support for the practitioners' claims than for the academicians'. Adoption of the QR program at a minimal level was associated with higher performance, although there was no performance impact due to higher levels of QR use. Firms did appear to match their IT usage to their business strategies, but there was no linkage between strategic alignment and firm performance, and there was surprisingly little variation in business or IT strategy. In short, the findings of our study suggest that both practitioners and academicians need to refine their theories and advice about what makes IT investments pay off.
We examine the verification of large knowledge-based systems. When knowledge bases are large, the verification process poses several problems that are usually not significant for small systems. We focus on decompositions that allow verification of such systems to be performed in a modular fashion. We identify a graphical framework, that we call an ordered polytree, for decomposing systems in a manner that enables modular verification. We also determine the nature of information that needs to be available for performing local checks to ensure accurate detection of anomalies. We illustrate the modular verification process using examples, and provide a formal proof of its accuracy. Next, we discuss a meta-verification procedure that enables us to check if decompositions under consideration do indeed satisfy the requirements for an ordered polytree structure. Finally, we show how the modular verification algorithm leads to considerable improvements in the computational effort required for verification as compared to the traditional approach.
Application-driven, technology-intensive research is critically needed to meet the challenges of globalization, interactivity, high productivity, and rapid adaptation faced by business organizations. Information systems researchers are uniquely positioned to conduct such research, combining computer science, mathematical modeling, systems thinking, management science, cognitive science, and knowledge of organizations and theft functions. We present an agenda for addressing these challenges as they affect organizations in heterogeneous and distributed environments. We focus on three major capabilities enabled by such environments: Mobile Computing, Intelligent Agents, and Net-Centric Computing. We identify and define important unresolved problems in each of these areas and propose research strategies to address them.
In this paper we first present an empirical study of groupware use illustrating problems that users faced with restricted feedback about others' activities. Awareness can aid users in learning interdependencies, and in forming conventions to regulate system use and information-sharing. As a solution to providing awareness, we integrate the framework of organizational memory with intelligent agent technology to provide a coordination mechanism that enables the structuring of awareness events and gives information about the users' feedback control. In the proposed model, feedback control relationships are captured into a multilayered model of organizational memory and transferred to users by agents-facilitators. The approach is based on a system dynamics approach to organizational learning.
Intelligent user interfaces, particularly in interactive group settings, can be based on system explanations that guide model building, application, and interpretation. Here we extend Silver's (1990,1991) conceptualization of decisional guidance and the theory of breakpoints in group interaction to operationalize feedback and feedforward for a complex multicriteria modeling system operating within a group decision support system context. We outline a design approach for providing decisional guidance in GDSS and then test the feasibility of the design in a preliminary laboratory experiment. Findings show how decisional guidance that provides system explanations at breakpoints in group interaction can improve MCDM GDSS usability. Our findings support Dhaliwal and Benbasat's (1996) conjecture that system explanations can improve decisional outcomes due to improvement in user understanding of decision models. Further research on intelligent agents, particularly in interactive group settings, can build on the concepts of decisional guidance outlined in this paper.
Much previous research has established that perceived ease of use is an important factor influencing user acceptance and. usage behavior of information technologies. However, very little research has been conducted to understand how that perception forms and changes over time. The current work presents and tests an anchoring and adjustment-based theoretical model of the determinants of system-specific perceived ease of use. The model proposes control (internal and external-conceptualized as computer sell-efficacy and facilitating conditions, respectively), intrinsic motivation. (conceptualized as computer playfulness), and emotion (conceptualized as computer anxiety) as anchors that determine early perceptions about the ease of use of a new system. With increasing experience, it is expected that system-specific perceived ease of use, while still anchored to the general beliefs regarding computers and computer use, will adjust to reflect objective usability, perceptions of external control specific to the new system environment, and system-specific perceived enjoyment. The proposed model was tested in three different organizations among 246 employees using three measurements taken over a three-month period. The proposed model was strongly supported at all points of measurement, and. explained up to 60% of the variance in system-specific perceived ease of use, which is twice as much as our current understanding. important theoretical and practical implications of these findings are discussed.
Recent empirical work by Compeau and Higgins (1995) investigated the role of behavioral IX modeling training in the development of computer skills. Their efforts have provided insight into our understanding of the role of computer self-efficacy (CSE) and behavioral modeling (BM) techniques with regard to training effectiveness. Contrary to their expectations, however, several of the hypothesized relationships were not supported, especially those relating to outcome expectancy. In this paper, an empirically derived model of the (SE construct proposed by Marakas, Yi, and Johnson (1998) is offered to highlight potential theoretical, methodological, and measurement issues which may have contributed to or exacerbated the unexpected results obtained in the Compeau and Higgins study. The empirical work contained herein is intended to both replicate and extend the work of Compeau and Higgins and to assist in resolving several key issues left unsettled by their seminal work in this area.
The concept of computer self-efficacy (CSE) recently has been proposed as important to the study of individual, behavior toward information technology. This paper extends current understanding about the concept of self-efficacy in the context of computer software. We describe how two broad types of computer self-efficacy beliefs, general self-efficacy and task- specific self-efficacy, are constructed across different computing tasks by suggesting that initial general CSE beliefs will strongly predict subsequent specific CSE beliefs. The theorized causal relationships illustrate the malleability and development of CSE beliefs over time, within a training environment where individuals are progressively provided with greater opportunity for hands-on experience and practice with different software. Consistent with the findings of prior research, judgments of self-efficacy then serve as key antecedents of the perceived cognitive effort (ease of use) associated with technology usage. Further, we theorize that self-efficacy judgments in the task domain of computing are strongly influenced by the extent to which individuals believe that they are personally innovative with respect to information technology. Panel data were collected using a longitudinal research design within a training context where 186 subjects were taught two software packages in a sequential manner over a 14-week period. The emergent patterns of the hypothesized relationships are examined using structural equation modeling techniques. Results largely support the relationships posited.
In general, OLAP applications are characterized by the rendering of enterprise data into multidimensional perspectives. This is achieved through complex, adhoc queries that frequently aggregate and consolidate data, often using statistical formulae. For example, a retail organization is often interested in comparing the total sales for the current year with the total sales for the previous year, or identifying sequences of 5 years or more when sales has increased within a 50-year envelope. It has been conjectured that relational database technology is well suited to fulfilling the needs of OLAP. This situation is somewhat analogous to the situation in the mid 1970s, when data processing experts would suggest special purpose algorithms to perform operations such as selection, projection, and join. warehouses. However, unrealistic restrictions are placed in these models, restrictions are imposed on either the number of attributes per dimension or the number of total measures representable in the cube. Moreover dimensions and measures are treated asymmetrically, leading to the inability of these models to answer particular types of queries with out requiring expensive redesign.
It has been argued that the intangible benefits of IT, in areas such as improved quality, variety, timeliness, and customization have not been appropriately measured. Many IT productivity studies that use conventional productivity measurement techniques fail to consider many of the improvements in economic output brought- about by IT. To complement these productivity studies, a powerful argument can he made for the use of the event study methodology that has become popular in the accounting and finance literatures.' The event study methodology is a powerful tool that can help IS researchers assess the business performance of IT investments using such marker-based measures as stock once or trading volume, it obviates the need to analyze accounting-based measures of IT investments' benefits, which have been criticized because they are often not adequate indicators of the performance of investments. This method enables researchers to measure stock price changes that can serve as estimates for the effectiveness of the firm in foreseeing and rapidly adapting to its changing environment.
Recent developments an information technology have changed the balance between the relative costs of producing and consuming information. With current technologies, information is generated faster than individuals and organizations cart make sense of it. As these technologies have become more powerful, organizations are able to collect detailed information about realtime events even as the events unfold. For example, UPS can track in real-time the movements of every package from pickup to delivery. The first level of situational awareness is the ability to perceive the status, attributes, and dynamics of relevant elements in the environment, that is, to monitor the environment effectively. For example, automobile drivers learn, through time, decision strategies for allocating attentional resources to those cues in the road that are most important. At this level of situational awareness poor monitoring alone may result in poor performance. A consideration of the first level of situational awareness led people to ask how increasing decision-makers attentional resources might permit them to better monitor important features of the decision situation.
The impact of information systems and technology on business performance has increased noticeably during the last decade. This paper seeks to contribute to the literature on strategic IS management by pursuing three specific goals. First, it seeks to provide further insights into the performance implications of the alignment between business and IS strategies. Thus, in viewing alignment and its performance implications, this paper employs a theory-driven approach, which incorporates prior knowledge and is therefore able to contribute to the cumulative stream of research in this area. IS strategy is directly concerned with business applications, and there have been previous suggestions that it should be aligned with the business strategy. IS strategy attributes are mapped to IS strategy types in a similar manners The IS strategy best aligned with each business strategy is examined in terms of four IS strategy attributes. Three of these--operational support systems, market information systems, and strategic decision support systems--reflect the traditional classification of information systems into transaction processing systems, management information systems, and decision support systems, respectively.
The past decade has witnessed a dramatic increase in the development of technology-based teaching and learning. For example, in the university landscape a prominent change has been the increase in virtual course offerings, otherwise referred to as distance learning. Since 1998, the number of universities offering some form of distance learning has increased by 33%. The objective of this essay is to motivate future research and dialogue on technology-mediated learning (TML) by suggesting some potentially productive research venues in this area TML is defined as an environment in which the learner's interactions with learning materials, peers, and instructors are mediated through advanced information technologies. The term Information technology broadly refers to computing, communication, and data management technologies, and their convergence. The focus of the essay is on learning from instruction in the context of post secondary educational environments, although the issues are also relevant to corporate training.
With the current stampede toward electronic commerce, businesses no longer have the luxury of setting up trading-partner-specific communication systems. Businesses want to communicate and do business with people and companies with whom they have not had any prior contact. They also want to build more extensive linkages with their best trading partners and in their outsourcing relationships. Companies have built these links using electronic data interchange (EDI). EDI cannot be implemented easily, quickly, or inexpensively, and is therefore inappropriate for many companies and people to adopt. An application based on the technologies proposed in this paper would have, already built in, a certain capability for understanding messages plus a flexibility that would allow the system to grow with few restrictions provided by the underlying communication system. The benefit is that almost any message that is structured correctly can be handled to some extent further, more complex message handling for new messages can be defined incrementally.
An agency framework is used to model the behavior of software developers as they weigh concerns about product quality against concerns about missing individual task dead- lines. Developers who care about quality but fear the career impact of missed deadlines may take shortcuts. Managers sometimes attempt to reduce this risk via their deadline-setting policies; a common method involves adding slack to best estimates when setting deadlines to partially alleviate the time pressures believed to encourage shortcut-taking. This paper derives a formal relationship between deadline-setting policies and software product quality. It shows that: (1) adding slack does not always preserve quality, thus, systematically adding slack is an incomplete policy for minimizing costs; (2) costs can be minimized by adopting policies that permit estimates of completion dates and deadlines that arc different and; (3) contrary to casual intuition, shortcut-taking can be eliminated by setting deadlines aggressively, thereby maintaining or even increasing the time pressures under which developers work.
As organizations implement more and more distributed work arrangements such as telecommuting, there is a need to understand the determinants of success of this new work setting. This research investigated three variables believed to impact outcomes in telecommuting: the availability of information system technology, the availability of communication technologies, and the communication patterns of telecommuters within their work groups. Two perspectives are used in this study. The direct effects of these three variables on perceived productivity, performance, and satisfaction were tested. A second perspective, based on the concept of fit and contingency theory, posits that successful telecommuting outcomes, measured by perceived productivity, performance, and satisfaction, are predicted by interactions between these independent variables. The study was conducted by surveying multiple respondents from different organizations who were members of work groups in which some or all employees were telecommuters. The results indicate that technology variables positively impact productivity, performance, and satisfaction of telecommuters, while the interaction between the technology variables is significant in predicting perceived productivity. Work group communication, as measured by the centrality of individuals, negatively affects perceived productivity and performance. The paper presents a discussion of the theoretical significance of these findings, and offers recommendations for future research.
Firms are undertaking growing numbers of e-commerce initiatives and increasingly making significant investments required to participate in the growing online market. However, empirical support for the benefits to firms from e-commerce is weaker than glowing accounts in the popular press, based on anecdotal evidence, would lead us to believe. In this paper, we explore the following questions: What are the returns to shareholders in firms engaging in e-commerce? How do the returns to conventional, brick and mortar firms from e-commerce initiatives compare with returns to the new breed of net firms? How do returns from business-to-business e-commerce compare with returns from business-to-consumer e-commerce? How do the returns to e-commerce initiatives involving digital goods compare to initiatives involving tangible goods? We examine these issues using event study methodology and assess the cumulative abnormal returns to shareholders (CARs) for 251 e-commerce initiatives announced by firms between October and December 1998. The results suggest that e-commerce initiatives do indeed lead to significant positive CARs for firms' shareholders. While the CARs for conventional firms are not significantly different from those for net firms, the CARs for business-to-consumer (B2C) announcements are higher than those for business-to-business (B2B) announcements. Also, the CARs with respect to e-commerce initiatives involving tangible goods are higher than for those involving digital goods. Our data were collected in the last quarter of 1998 during a unique bull market period and the magnitudes of CARs (between 4.9 and 23.4% for different subsamples) in response to e-commerce announcements are larger than those reported for a variety of other firm actions in prior event studies. This paper presents the first empirical test of the dot coin effect validating popular anticipations of significant future benefits to firms entering into e-commerce arrangements.
Kohonen's self organizing map (SOM) network is one of the most important nctwork architectures developed during the 1980s. The main function of SOM networks is to map the input data from an n-dimensional space to a lower dimensional (usually one- or two-dimensional) plot while maintaining the original topological relations. Therefore, it can be viewed as an analog of factor analysis. In this research, we evaluate the feasibility of using SOM networks as a robust alternative to factor analysis and clustering for data mining applications. Specifically, we compare SOM network solutions to factor analytic and K-Means clustering solutions on simulated data sets with known underlying factor and cluster structures. The comparisons indicate that the SOM networks provide solutions superior to unrotated factor solutions in general and provide more accurate recovery of underlying cluster structures when the input data are skewed. Our findings suggest that SOM networks can provide robust alternatives to traditional factor analysis and clustering techniques in data mining applications.
The field of information systems is premised on the centrality of information technology in everyday socio-economic life. Yet, drawing on a review of the full set of articles published in Information Systems Research (ISR) over the past ten years, we argue that the field has not deeply engaged its core subject matter-the information technology (IT) artifact. Instead, we find that IS researchers tend to give central theoretical significance to the context (within which some usually unspecified technology is seen to operate), the discrete processing capabilities of the artifact (as separable from its context or use), or the dependent variable (that which is posited to be affected or changed as technology is developed, implemented, and used). The IT artifact itself tends to disappear from view, be taken for granted, or is presumed to be unproblematic once it is built and installed. After discussing the implications of our findings, we propose a research direction for the IS field that begins to take technology as seriously as its effects, context, and capabilities. In particular, we propose that IS researchers begin to theorize specifically about IT artifacts, and then incorporate these theories explicitly into their studies. We believe that such a research direction is critical if IS research is to make a significant Contribution to the understanding of a world increasingly suffused with ubiquitous, interdependent, and emergent information technologies.
The Technology Acceptance Model (TAM) has received considerable research attention in the IS field over the past decade, placing an emphasis on the roles played by perceived ease-of-use and perceived usefulness in influencing technology adoption decisions. Mean- while, alternative sets of antecedents to adoption have received less attention. In this paper, sets of antecedent constructs drawn from both TAM and the Perceived Characteristics of Innovating (PCI) inventory are tested and subsequently compared with one another. The comparison is done in the context of a large-scale market trial of a smart card-based electronic payment system being evaluated by a group of retailers arid merchants. The PCI set of ante-cedents explains substantially more variance than does TAM, while also providing managers with more detailed information regarding the antecedents driving technology innovation adoption.
As demand for online network services continues to grow, service providers are looking to meet this need and avail themselves of business opportunities. However, despite strong growth in demand,providers continue to have dif .culty achieving profitability, customer churn remains high,and network performance continues to draw complaints. We suggest that strategicbusiness planning for network services would benefit from a systems thinking approach that analyzes the feedback effects present in the underlying business process. These feedback loops can be complex and have significant impact n business performance. For instance, while the size of a providers customer base depends on price and network performance, network performance is itself dependent on the size of the customer base. In this paper, we develop a planning model that represents these feedback effects using the finite difference equations methodology of systems dynamics.The model is validated by showing its fit with essential characteristics of the underlying problem domain,and by showing its ability to rep- licate observed reference mode behaviors.Simulations are then carried out under a variety of scenarios to examine issues important to service providers.Among other findings,the simulations suggest that (a) under flat-rate pricing, lowering price to increase customer base can hurt profitability as well as network performance; (b) under usage-based pricing, lowering price need not necessarily lead to a larger customer base; and (c) in addition to price, the customers of threshold of tolerance for performance degradation plays a significant role in balancing market share with profitability. We briefly present a prototype decision support system based on the systems thinking approach, and suggest ways in which it could be used to help business planning for network services.
In today's networked economy, ideas that challenge existing business models and paradigms are becoming more important. This study investigated how individual differences, groupware-based creativity techniques, and ideas from others influenced the type of ideas that individuals generated. While individual differences were important (in that some individuals were inherently more likely to generate ideas that followed the existing problem paradigm while others were more likely to generate paradigm-modifying ideas that attempted to change the problem paradigm), the exposure to paradigm-modifying ideas from others and the use of intuitive groupware-based creativity techniques rather than analytical groupware-based creativity techniques were found to increase the number of paradigm-modifying ideas produced.
Prior research has generated considerable knowledge on information systems design from software engineering and user-acceptance perspectives. As organizational processes are increasingly embedded within information systems, one of the key considerations of many business processes--organizational incentives--should become an important dimension of any information systems design and evaluation, which we categorize as the third dimension: incentive alignment.Incentive issues have become important in many IS areas,including distributed decision support systems (DSS), knowledge management, and e-business supply chain coordination. In this paper we outline why incentives are important in each of these areas and specify requirements for designing incentive-aligned information systems. We identify and define important unresolved problems along the incentive-alignment dimension of information systems and present a research agenda to address them.
Retrieving information from heterogeneous database systems involves a complex process and remains a challenging research area.We propose a cognitively guided approach for developing an information-retrieval agent that takes the user's information request, identifies relevant information sources,and generates a multidatabase access plan. Our work is distinctive in that the agent design is based on an empirical study of how human experts retrieve information from multiple, heterogeneous database systems. To improve on empirically observed information-retrieval capabilities, the design incorporates mathematical models and algorithmic components. These components optimize the set of information sources that need to be considered to respond to a user query and are used to develop efficient multidatabase-access plans. This agent design, which integrates cognitive and mathematical models, has been implemented using Soar, a knowledge-based architecture.
T his paper puts forward arguments in favor of a pluralist approach t IS research.Rather than advocating a single paradigm,be it interpretive or positivist,or even a plurality of paradigms within the discipline as a whole,it suggests that research results will be richer and more reliable if different research methods,preferably from different (existing)paradigms,are routinely combined together.The paper is organized into three sections after the Introduction. In 2,the main arguments for the desirability of multimethod research are put forward,while 3 discusses its feasibility in theory and practice. 4 outlines two frameworks that are helpful in designing mixed-method research studies.These are illustrated with a critical evaluation of three examples of empirical research.
This paper is the first test of a parsimonious model that posits three factors as determinants of the adoption of electronic data interchange (EDI): readiness ,perceived benefits, and external pressure. To construct the model, we identified and organized the factors that were found to be influential in prior EDI research. By testing all these factors together in one model, we are able to investigate their relative contributions to EDI adoption decisions. Senior purchasing managers, chosen for their experience with EDI and proximity to the EDI adoption decision, were surveyed and their responses analyzed using structural equation modeling. All three determinants were found t be significant predictors of intent to adopt EDI, with external pressure and readiness being considerably more important than perceived benefits. We show that the constructs inthis model canbe categorized into three levels: technological, organizational, and interorganizational. We hypothesize that these categories of influence will also be determinants of the adoption of other emerging forms of interorganizational systems (IOS).
An important feature of some conceptual modelling grammars is the features they provide to allow database designers to show real-world things may or may not possess a particular attribute or relationship. In the entity-relationship model, for example, the fact that a thing may not possess an attribute can be represented by using a special symbol to indicate that the attribute is optional. Similarly, the fact that a thing may or may not be involved in a relationship can be represented by showing the minimum cardinality of the relationship as zero. Whether these practices should be followed, however, is a contentious issue. An alternative approach is to eliminate optional attributes and relationships from conceptual schema diagrams by using subtypes that have only mandatory attributes and relationships. In this paper, we first present a theory that led us to predict that optional attributes and relationships should be used in conceptual schema diagrams only when users of the diagrams require a surface-level understanding of the domain being represented by the diagrams. When users require a deep-level understanding, however, optional attributes and relationships should not be used because they undermine users' abilities to grasp important domain semantics. We describe three experiments which we then undertook to test our predictions. The results of the experiments support our predictions.
As telecommunication networks become more common, there is an increasing interest in the factors underlying the development of online social structures. It has been proposed that these structures are new forms of organizing which are not subject to the same constraints as traditional social structures. However, from anecdotal evidence and case studies it is difficult to evaluate whether online social structures are subject to the same problems as traditional social structures. Drawing from prior studies of traditional social structures and empirical analyses of longitudinal data from a sample of Internet-based groups, this exploratory work considers the role of size and communication activity in sustainable online social structures. A resource-based theory of sustainable social structures is presented. Members contribute time, energy, and other resources, enabling a social structure to provide benefits for individuals. These benefits, which include information, influence, and social support, are the basis for a social structure's ability to attract and retain members. This model focuses on the system of opposing forces that link membership size as a component of resource availability and communication activity as an aspect of benefit provision to the sustainability of an online social structure. Analyses of data from a random sample of e-mail-based Internet social structures (listservs) indicate that communication activity and size have both positive and negative effects on a structure's sustainability. These results suggest that while the use of networked communication technologies may alter the form of communication, balancing the opposing impacts of membership size and communication activity in order to maintain resource availability and provide benefits for current members remains a fundamental problem underlying the development of sustainable online social structures.
Electronic data interchange (EDI), used traditionally to exchange business documents, has recently been extended to facilitate interorganizational collaborative processes such as the continuous replenishment program (CRP). The key characteristics of CRP are the sharing of real-time inventory data by retailers with manufacturers and continuous replenishment of retailer inventory by manufacturers. Prior research on EDI has focused on the transaction efficiency of EDI. We analyze the impact of information sharing and continuous replenishment in the CRP context and study the factors that affect the value of CRP. The study quantifies the value derived from CRP and the optimal number of retailers a manufacturer should partner with.
The conduct of net-enabled business, known variously as electronic commerce (EC) or e-Business, has changed the landscape and opportunities for IS research by shifting the focus from internal to customer/partnering systems. The article examines the two primary dyadic net-enabled relationships in the marketplace: B2C and B2B. It also considers issues that extend beyond these two relationships. B2C practice and research are analyzed from: (1) consumer, (2) service, and (3) risk perspectives. Three central issues of B2B or supply chain practice and research are next considered: (1) beyond simple efficiencies, (2) innovations in B2B technology, and (3) information visibility. Finally, four overarching research issues are examined: (1) strategy, (2) organizational design, (3) metrics, and (4) managing IS. Not all research on the net-enabled organization (NEO) is IS research, and it is critical that IS journals maintain their distinctive focus. Within the bounds of the net-enabled revolution, though, the IS field has an opportunity to shape the phenomenon with timely, theory-based work that will disseminate beyond the IS academic and practitioner communities.
The facilitation and analytical support of argumentation-based collaborative decision making is the focus of this research. We model collaborative decision making as an argumentation process. We develop a connectionist modeling framework, a network representation formalism for argument structures, connectionist network mechanisms, and their models of computations to extract the behavior of argument structures. We use two examples from the case study literature to illustrate the concepts. Several interesting properties of the connectionist network models are observed from our computational results. We find that although the length of the computation is affected by parametric values, the final activation levels of the units are largely unaffected. We observe that the initial activation levels of the defeasible units seem to have no effect on their final activation levels. The proposed modeling approach generates valuable insights into the characteristics of specific argumentative discussions. While the intention of this work is not to introduce the connectionist paradigm as a means to bring arguments to a closure (resolution), we show that certain resolution mechanisms can be easily implemented under the connectionist framework.
Decision support systems (DSS) typically contain data and models to facilitate decision making. DSS users, in response to a particular decision-making situation, often execute a sequence of models, in which inputs to a model in the sequence are obtained from outputs of other models upstream in the sequence and from database retrievals. The problem of generating a sequence of models from the set of available models is known as the model composition problem. In this paper, we propose a new construct called filter spaces to support model composition. We show how filter spaces can significantly facilitate automation of model composition and execution process, and provide effective means to integrate partial solutions from multiple composite models and databases.
Group polarization is the tendency of people to become more extreme in their thinking following group discussion. It may be beneficial to some, but detrimental to other, organizational decisions. This study examines how computer-mediated communication (CMC) may be associated with group polarization. Two laboratory experiments were carried out. The first experiment, conducted in an identified setting, demonstrated that removal of verbal cues might not have reduced social presence sufficiently to impact group polarization, but removal of visual cues might have reduced social presence sufficiently to raise group polarization. Besides confirming the results of the first experiment, the second experiment showed that the provision of anonymity might also have reduced social presence sufficiently to raise group polarization. Analyses of process data from both experiments indicated that the reduction in social presence might have increased group polarization by causing people to generate more novel arguments and engage in more one-upmanship behavior. Collectively, process and outcome data from both experiments reveal how group polarization might be affected by level of social presence. Specifically, group discussion carried out in an unsupported setting or an identified face-to-face CMC setting tends to result in weaker group polarization. Conversely, group discussion conducted in an anonymous face-to-face CMC setting or a dispersed CMC setting (with or without anonymity) tends to lead to stronger group polarization. Implications of these results for further research and practice are provided.
Proper measurement is critical to the advancement of theory (Blalock 1979). Adaptive Structuration theory (AST) is rapidly becoming an important theoretical paradigm for comprehending the impacts of advanced information technologies (DeSanctis and Poole 1994). Intended as a complement to the faithfulness of appropriation scale developed by Chin et al. (1997), this research note describes the development of an instrument to capture the AST construct of consensus on appropriation. Consensus on appropriation (COA) is the extent to which group participants perceive that they have agreed on how to adopt and use a technology. While consensus on appropriation is an important component of AST, no scale is currently available to capture this construct. This research note develops a COA instrument in the context of electronic meeting systems use. Initial item development, statistical analyses, and validity assessment (convergent, discriminant, and nomological) are described here in detail. The contribution of this effort is twofold: First, a scale is provided for an important construct from AST. Second, this report serves as an example of rigorous scale development using structural equation modeling. Employing rigorous procedures in the development of instruments to capture AST constructs is critical if the sound theoretical base provided by AST is to be fully exploited in understanding phenomena related to the use of advanced information technologies.
The arrival of the information age holds great promise in terms of providing organizations with access to a wealth of information stores. However, the free exchange of electronic information also brings the threat of providing easy, and many times unwanted, access to personal information. Given the potential backlash of consumers, it is imperative that both researchers and practitioners understand the nature of consumers' concern for information privacy and accurately model the construct within evolving research and business contexts. Drawing upon a sample of 355 consumers and working within the framework of confirmatory factor analysis, this study examines the factor structure of the concern for information privacy (CFIP) instrument posited by Smith et al. (1996), Consistent with prior findings, the results suggest that each dimension of this instrument is reliable and distinct. However, the results also suggest that CFIP may be more parsimoniously represented as a higher-order factor structure rather than a correlated set of first-order factors. The implication of these results is that each dimension of CFIP as well as the supra dimension derived from the associations among dimensions are important in capturing CFIP and associating the construct to other important antecedents and consequences.
The purpose of the present study is to empirically and theoretically assess DeLone and McLean's (1992) and Seddon's (1997) models of information systems (IS) success in a quasi-voluntary IS use context. Structural modeling techniques were applied to data collected by questionnaire from 274 system users of an integrated student information system at a midwestern university. The Seddon structural model and the DeLone and McLean structural model each contained five variables (system quality, information quality, perceived usefulness, user satisfaction, and IS use). Both models exhibit reasonable fit with the collected data. The empirical findings are assessed in the broader theoretical context of the IS success literature, including the Technology Acceptance Model and the theory of Planned Behavior. Our results support DeLone and McLean's focus on integrated IS success models and their observation that IS success models need to be carefully specified in a given context. The Seddon model conceptually elaborates and clarifies aspects of the DeLone and McLean model, thereby effectively integrating core theoretical relationships espoused in the IS success literature. Our study also supports Seddon's three construct categories (system and information quality, general perceptual measures about net benefits about IS use, and IS behavior), as defining IS success and its impact on nature of IS use.
This work analyzes firm-level investment in information technology and corresponding productivity through the use of a production function over the period from 19951997. The results are then compared to previous studies that utilized similar data and methodologies to compare productivity estimates over time. The analysis indicates that investment in IT enhances productivity over the period in question and has illustrated increasing returns over time. These findings are supported by the corresponding empirical analysis which yielded IT capital coefficients in a production function of (0.12, 0.16, 0.18) and IT flow coefficients in a similar function of (0.17, 0.24, 0.22) for the years 1995, 1996, and 1997, respectively. These results reflect the change in firm output given a one-percent change in the natural log of dollars invested in IT capital and flow, and are statistically significant.
Trends towards increased business process automation, e-commerce, and e-business have led to increasing interest in the field of workflow management. In this paper, we provide a perspective on the state of research in workflow management systems, and discuss possible future research directions in this area, with a particular emphasis on workflow systems in integrating interorganizational processes and enabling e-commerce solutions.
Clear and precise metrics are essential for evaluating phenomena such as e-commerce ('Net'-enablement) and the organizational use of networks and the Internet for commercial activities. Researchers require them for theory building and testing; practitioners require them for improving organizational processes. But for IS professionals to engage in serious creation of metrics, it is critical to recognize: (1) that the phenomenon of net-enablement is an enduring change, probably led in the future by 'brick-cum-click' firms, (2) that some new and old measures need to be differentially applied, and (3) that the papers in this special issue are not the end of metrics creation, but just the beginning.
Efforts to develop measures of Internet commerce success have been hampered by (1) the rapid development and use of Internet technologies and (2) the lack of conceptual bases necessary to develop success measures. In a recent study, Keeney (1999) proposed two sets of variables labeled as means objectives and fundamental objectives that influence Internet shopping. Means objectives, he argues, help businesses achieve what is important for their customersfundamental objectives. Based on Keeney's work, this paper describes the development of two instruments that together measure the factors that influence Internet commerce success. One instrument measures the means objectives that influence online purchase (e.g., Internet vendor trust) and the other measures the fundamental objectives that customers perceive to be important for Internet commerce (e.g., Internet product value). In phase one of the instrument development process, we generated 125 items for means and fundamental objectives. Using a sample of 199 responses by individuals with Internet shopping experience, these constructs were examined for reliability and validity. The Phase 1 results suggested a 4-factor, 21-item instrument to measure means objectives and a 4-factor, 17-item instrument to measure fundamental objectives. In Phase 2 of the instrument development process, we gathered a sample of 421 responses to further explore the 2 instruments. With minor modifications, the Phase 2 data support the 2 models. The Phase 2 results suggest a 5-factor, 21-item instrument that measures means objectives in terms of Internet product choice, online payment, Internet vendor trust, shopping travel, and Internet shipping errors. Results also suggest a 4-factor, 16-item instrument that measures fundamental objectives in terms of Internet shopping convenience, Internet ecology, Internet customer relation, and Internet product value. Evidence of reliability and discriminant, construct, and content validity is presented for the hypothesized measurement models. The paper concludes with discussions on the usefulness of these measures and future research ideas.
In this study, we consider the online consumer as both a shopper and a computer user. We test constructs from information systems (Technology Acceptance Model), marketing (Consumer Behavior), and psychology (Flow and Environmental Psychology) in an integrated theoretical framework of online consumer behavior. Specifically, we examine how emotional and cognitive responses to visiting a Web-based store for the first time can influence online consumers' intention to return and their likelihood to make unplanned purchases. The instrumentation shows reasonably good measurement properties and the constructs are validated as a nomological network. A questionnaire-based empirical study is used to test this nomological network. Results confirm the double identity of the online consumer as a shopper and a computer user because both shopping enjoyment and perceived usefulness of the site strongly predict intention to return. Our results on unplanned purchases are not conclusive. We also test some individual and Web site factors that can affect the consumer's emotional and cognitive responses. Product involvement, Web skills, challenges, and use of value-added search mechanisms all have a significant impact on the Web consumer. The study provides a more rounded, albeit partial, view of the online consumer and is a significant step towards a better understanding of consumer behavior on the Web. The validated metrics should be of use to researchers and practitioners alike.
We propose the Net-Enabled Business Innovation Cycle (NEBIC) as an applied dynamic capabilities theory for measuring, predicting, and understanding a firm's ability to create customer value through the business use of digital networks. The theory incorporates both a variance and process view of net-enabled business innovation. It identifies four sequenced constructs: Choosing new IT, Matching Economic Opportunities with technology, Executing Business Innovation for Growth, and Assessing Customer Value, along with the processes and events that interrelate them as a cycle. The sequence of these theorized relationships for net-enablement (NE) asserts that choosing IT precedes rather than aligns with corporate strategy. The theory offers a logically consistent and falsifiable basis for grounding research programs on metrics of net-enabled business innovation.
Web site usability is a critical metric for assessing the quality of a firm's Web presence. A measure of usability must not only provide a global rating for a specific Web site, ideally it should also illuminate specific strengths and weaknesses associated with site design. In this paper, we describe a heuristic evaluation procedure for examining the usability guidelines developed by Microsoft. We present the categories and subcategories comprising these guidelines, and discuss the development of an instrument that operationalizes the measurement of usability. The proposed instrument was tested in a heuristic evaluation study where 1,475 users rated multiple Web sites from four different industry sectors: airlines, online bookstores, automobile manufacturers, and car rental agencies. To enhance the external validity of the study, users were asked to assume the role of a consumer or an investor when assessing usability. Empirical results suggest that the evaluation procedure, the instrument, as well as the usability metric exhibit good properties. Implications of the findings for researchers, for Web site designers, and for heuristic evaluation methods in usability testing are offered.
Web sites provide the key interface for consumer use of the Internet. This research reports on a series of three studies that develop and validate Web site usability, design and performance metrics, including download delay, navigability, site content, interactivity, and responsiveness. The performance metric that was developed includes the subconstructs user satisfaction, the likelihood of return, and the frequency of use. Data was collected in 1997, 1999, and 2000 from corporate Web sites via three methods, namely, a jury, third-party ratings, and a software agent. Significant associations between Web site design elements and Web site performance indicate that the constructs demonstrate good nomological validity. Together, the three studies provide a set of measures with acceptable validity and reliability. The findings also suggest lack of significant common methods biases across the jury-collected data, third-party data, and agent-collected data. Results suggest that Web site success is a first-order construct. Moreover, Web site success is significantly associated with Web site download delay (speed of access and display rate within the Web site, navigation (organization, arrangement, layout, and sequencing), content (amount and variety of product information), interactivity (customization and interactivity), and responsiveness (feedback options and FAQs).
Wheeler's Net-Enabled Business Innovation Cycle (NEBIC) integrates IS and strategy research to offer an interesting and timely perspective on value creation. We extend Wheeler's theoretical propositions, highlighting the interplay between strategy, IS and entrepreneurship in a quest for competitive advantage. This interplay is crucial to the creation of the dynamic capabilities that enable companies to gain an advantage through NEBIC. The importance of opportunity recognition and absorptive capacity in bringing about the changes that make NEBIC viable is also highlighted.
Although electronic commerce (EC) has created new opportunities for businesses as well as consumers, questions about consumer attitudes toward Business-to-Consumer (B2C) e-commerce vis-a-vis the conventional shopping channels continue to persist. This paper reports results of a study that measured consumer satisfaction with the EC channel through constructs prescribed by three established frameworks, namely the Technology Acceptance Model (TAM), Transaction Cost Analysis (TCA), and Service Quality (SERVQUAL). Subjects purchased similar products through conventional as well as EC channels and reported their experiences in a survey after each transaction. Using constructs from the three frameworks, a model was constructed and tested to examine the determinants of the EC channel satisfaction and preference using the survey data. Structural equation model analyses indicate that metrics tested through each model provide a statistically significant explanation of the variation in the EC consumers' satisfaction and channel preference. The study found that TAM components--perceived ease of use and usefulness--are important in forming consumer attitudes and satisfaction with the EC channel. Ease of use also was found to be a significant determinant of satisfaction in TCA. The study found empirical support for the assurance dimension of SERVQUAL as determinant in EC channel satisfaction. Further, the study also found general support for consumer satisfaction as a determinant of channel preference.
Evidence suggests that consumers often hesitate to transact with Web-based vendors because of uncertainty about vendor behavior or the perceived risk of having personal information stolen by hackers. Trust plays a central role in helping consumers overcome perceptions of risk and insecurity. Trust makes consumers comfortable sharing personal information, making purchases, and acting on Web vendor advice--behaviors essential to wide-spread adoption of e-commerce. Therefore, trust is critical to both researchers and practitioners. Prior research on e-commerce trust has used diverse, incomplete, and inconsistent definitions of trust, making it difficult to compare results across studies. This paper contributes by proposing and validating measures for a multidisciplinary, multidimensional model of trust in e-commerce. The model includes four high-level constructs--disposition to trust, institution-based trust, trusting beliefs, and trusting intentions--which are further delineated into 16 measurable, literature-grounded subconstructs. The psychometric properties of the measures are demonstrated through use of a hypothetical, legal advice Web site. The results show that trust is indeed a multidimensional concept. Proposed relationships among the trust constructs are tested (for internal nomological validity), as are relationships between the trust constructs and three other e-commerce constructs (for external nomological validity)--Web experience, personal innovativeness, and Web site quality. Suggestions for future research as well as implications for practice are discussed.
In this study, we developed a set of constructs to measure e-commerce capability in Internet-enhanced organizations. The e-commerce capability metrics consist of four dimensions: information, transaction, customization, and supplier connection. These measures were empirically validated for reliability, content, and construct validity. Then we examined the nomological validity of these e-commerce metrics in terms of their relationships to firm performance, with data from 260 manufacturing companies divided into high IT-intensity and low IT-intensity sectors. Grounded in the dynamic capabilities perspective and the resource-based theory of the firm, a series of hypotheses were developed. After controlling for variations of industry effects and firm size, our empirical analysis found a significant relationship between e-commerce capability and some measures of firm performance (e.g., inventory turnover), indicating that the proposed metrics have demonstrated value for capturing e-commerce effects. However, our analysis showed that e-commerce tends to be associated with the increased cost of goods sold for traditional manufacturing companies, but there is an opposite relationship for technology companies. This result seems to highlight the role of resource complementarity for the business value of e-commerce--traditional companies need enhanced alignment between e-commerce capability and their existing IT infrastructure to reap the benefits of e-commerce.
Metrics are sine qua non for solid research, and scientific metrics have now been advanced with new approaches in the arena of Net-enablement (NE), otherwise known as e-commerce. Questions that likely require additional attention include: (1) Where/what is the real value in substituting information for physical processes?, (2) which NE systems effectively support end-to-end fulfillment?, and (3) when should a Net-enabled organization share information? With respect to extant studies in Net-enhancement, the field has been advanced in three methodological dimensions. Multiple methods have been used to validate measures. Approaches to metrics using archival/secondary data have also been initiated. Finally, strong external validity has been established through large scale data gathering.
Metrics for the architectural quality of Internet businesses are essential in gauging the success and failure of e-commerce. This study proposes six dimensions of architectural metrics for Internet businesses: internal stability, external security, information gathering, order processing, system interface, and communication interface. The metrics are based on the three constructs that have been used to evaluate buildings in the real world. The structural construct indicates that Internet businesses need to be stable internally and secure externally. The functional construct implies that Internet businesses should provide convenient functions in the information-gathering and order-processing phases. Finally, the representational construct indicates that they need to provide a pleasant interface both to the system and to those using it. For each of the six metrics, we have constructed questionnaires to measure the perceived level of architectural quality and identified feature lists that may be closely related to the perceived quality level. Large-scale empirical studies were conducted both to validate the proposed metrics and to explore their relevance across four Internet business domains. The validity of the metrics has been obtained in three ways. First, the content validity of the metrics was assured by pretests and pilot survey. Second, the results from the confirmatory factor analysis showed that the metrics had high convergent and discriminant validities. Finally, the reliability coefficients were found to be high enough to establish the reliability of the proposed metrics. The relevance of the metrics has been explored in two ways. Structural equation models were used to test the causal relations between the three constructs and user satisfaction, as well as customer loyalty, in four domains. Correlation analyses were used to explore the relations between the perceived architectural quality and objective design features in four domains. This paper ends with the implications and limitations of the study results.
Online shopping provides convenience to Web shoppers, yet its electronic format changes information-gathering methods traditionally used by customers. This change raises questions concerning customer satisfaction with the online purchasing process. Web shopping involves a number of phases, including the information phase, in which customers search for information regarding their intended purchases. The purpose of this paper is to develop theoretically justifiable constructs for measuring Web-customer satisfaction during the information phase. By synthesizing the expectation-disconfirmation paradigm with empirical theories in user satisfaction, we separate Web site quality into information quality (IQ) and system quality (SQ), and propose nine key constructs for Web-customer satisfaction. The measurements for these constructs are developed and tested in a two-phase study. In the first phase, the IQ and SQ dimensions are identified, and instruments for measuring them are developed and tested. In the second phase, using the salient dimensions of Web-IQ and Web-SQ as the basis for formulating first-order factors, we develop and empirically test instruments for measuring IQ-and SQ-satisfaction. Moreover, this phase involves the design and test of second-order factors for measuring Web-customer expectations, disconfirmation, and perceived performance regarding IQ and SQ. The analysis of the measurement model indicates that the proposed metrics have a relatively high degree of validity and reliability. The results of the study provide reliable instruments for operationalizing the key constructs in the analysis of Web-customer satisfaction within the expectation-disconfirmation paradigm.
The ability to retain and lock in customers in the face of competition is a major concern for online businesses, especially those that invest heavily in advertising and customer acquisition. In this paper, we develop and implement an approach for measuring the magnitudes of switching costs and brand loyalty for online service providers based on the random utility modeling framework. We then examine how systems usage, service design, and other firm-and individual-level factors affect switching and retention. Using data on the online brokerage industry, we find significant variation (as much as a factor of two) in measured switching costs. We find that customer demographic characteristics have little effect on switching, but that systems usage measures and systems quality are associated with reduced switching. We also find that firm characteristics such as product line breadth and quality reduce switching and may also reduce customer attrition. Overall, we conclude that online brokerage firms appear to have different abilities in retaining customers and have considerable control over their switching costs.
Replications are an important component of scientific method in that they convert tentative belief to accepted knowledge. Given the espoused importance of replications to the extraction of knowledge from research, there is surprisingly little evidence of its practice or discussion of its importance in the management information systems literature. In this article we develop a framework within which to systematize the conceptualization of replications; we review and illustrate how some key information systems research fits into the framework and examine the factors that influence the selection of a research strategy. Our framework includes a conceptualization of the relationship among replication, extension, and generation in IS research. The concept of 'research space ' is defined and a framework is developed that delineates eight possible research strategies. Finally, the benefits of our framework to salient stakeholders in the research process are outlined.
Within the information systems field, the task of conceptual modeling involves building a representation of selected phenomena in some domain. High-quality conceptual- modeling work is important because it facilitates early detection and correction of system development errors. It also plays an increasingly important role in activities like business process reengineering and documentation of best-practice data and process models in enterprise resource planning systems. Yet little research has been undertaken on many aspects of conceptual modeling. In this paper, we propose a framework to motivate research that addresses the following fundamental question: How can we model the world to better facilitate our developing, implementing, using, and maintaining more valuable information systems? The framework comprises four elements: conceptual-modeling grammars, conceptual-modeling methods, conceptual-modeling scripts, and conceptual-modeling contexts. We provide examples of the types of research that have already been undertaken on each element and illustrate research opportunities that exist.
Advances in information and communication technologies have fueled rapid growth in the popularity of technology-supported distributed learning (DL). Many educational institutions, both academic and corporate, have undertaken initiatives that leverage the myriad of available DL technologies. Despite their rapid growth in popularity, however, alternative technologies for DL are seldom systematically evaluated for learning efficacy. Considering the increasing range of information and communication technologies available for the development of DL environments, we believe it is paramount for studies to compare the relative learning outcomes of various technologies. In this research, we employed a quasi-experimental field study approach to investigate the relative learning effectiveness of two collaborative DL environments in the context of an executive development program. We also adopted a framework of hierarchical characteristics of group support system (GSS) technologies, outlined by DeSanctis and Gallupe (1987), as the basis for characterizing the two DL environments. One DL environment employed a simple e-mail and listserv capability while the other used a sophisticated GSS (herein referred to as Beta system).Interestingly, the learning outcome of the e-mail environment was higher than the learning outcome of the more sophisticated GSS environment. The post-hoc analysis of the electronic messages indicated that the students in groups using the e-mail system exchanged a higher percentage of messages related to the learning task. The Beta system users exchanged a higher level of technology sense-making messages. No significant difference was observed in the students' satisfaction with the learning process under the two DL environments.; Jenkins, A. Milton2
Organizations are storing large amounts of data in databases for data mining and other types of analysis. Some of this data is considered confidential and has to be protected from disclosure. When access to individual values of confidential numerical data in the database is prevented, disclosure may occur when a snooper uses linear models to predict individual values of confidential attributes using nonconfidential numerical and categorical attributes. Hence, it is important for the database administrator to have the ability to evaluate security for snoopers using linear models. In this study we provide a methodology based on Canonical Correlation Analysis that is both appropriate and adequate for evaluating security. The methodology can also be used to evaluate the security provided by different security mechanisms such as query restrictions and data perturbation. In situations where the level of security is inadequate, the methodology provided in this study can also be used to select appropriate inference control mechanisms. The application of the methodology is illustrated using a simulated database.; Saunders, Carol S.2 carol.saunders@bus.ucf.edu
We examine the intrafirm resource allocation problem with the following characteristics. The resource exhibits negative externalities, and the benefit of using the resource is known only to the user department and not to top management or other user departments. In addition, the consumption of the resource depends upon the choice of the mechanism for allocating the resource. For this problem, we derive a two-stage mechanism, and show that this proposed mechanism leads to optimal allocation.
The full potential of the Web as a medium for electronic commerce can be realized only when multiple partners in a supply chain can route information among themselves in a seamless way. Commerce on the Internet is still far from being 'friction free,' because business partners cannot exchange information about their business processes in an automated manner. In this paper, we propose the design for an exchangeable Routing Language (XRL) using extensible Markup Language (XML) syntax. XML is a means for trading partners to exchange business data electronically. The novel contribution of our work is to show how XML can also be used to describe workflow process schemas to support flexible routing of documents in the Internet environment. The design of XRL is grounded in Petri nets, which is a well-known formalism. By using this formalism, it is possible to analyze correctness and performance of workflows described in XRL. Architectures to facilitate interoperation through loose and tight integration are also discussed. Examples illustrate how this approach can be used for implementing interorganizational electronic commerce applications. As a proof of concept, we have also developed XRL/flower, a prototype implementation of a workflow management system based on XRL.
This paper investigates the persistence of managerial expectations in an IT outsourcing context where the traditional relationship between supervisor and subordinate changes to one of client-manager and contractor. A mixed-method approach was used, in which a qualitative methodology preceded a large-scale quantitative survey. Data were collected from 147 survivors of a government IT organization which had undergone IT outsourcing in the previous year. Findings show that role overload, the presence of strong ties between manager and contractor, and the lack of prior outsourcing experience increased the persistence of managerial expectations. In turn, persistence of expectations had a distinct influence on managerial perceptions of contractor performance.
This research investigates how knowledge workers are influenced to adopt the advice that they receive in mediated contexts. The research integrates the Technology Acceptance Model (Davis 1989) with dual-process models of information influence (e.g., Petty and Cacioppo 1986, Chaiken and Eagly 1976) to build a theoretical model of information adoption. This model highlights the assessment of information usefulness as a mediator of the information adoption process. Importantly, the model draws on the dual-process models to make predictions about the antecedents of information usefulness under different processing conditions. The model is investigated qualitatively first, using interviews of a sample of 40 consultants, and then quantitatively on another sample of 63 consultants from the same international consulting organization. Data reflect participants' perceptions of actual e-mails they received from colleagues consisting of advice or recommendations. Results support the model, suggesting that the process models used to understand information adoption can be generalized to the field of knowledge management, and that usefulness serves a mediating role between influence processes and information adoption. Organizational knowledge work is becoming increasingly global. This research offers a model for understanding knowledge transfer using computer-mediated communication.
Traditional development of large-scale information systems is based on centralized information processing and decision making. With increasing competition, shorter product life-cycle, and growing uncertainties in the marketplace, centralized systems are inadequate in processing information that grows at an explosive rate and are unable to make quick responses to real-world situations. Introducing a decentralized information system in an organization is a challenging task. It is often intertwined with other organizational processes. The goal of this research is to outline a new approach in developing a supply chain information system with a decentralized decision making process. Particularly, we study the incentive structure in the decentralized organization and design a market-based coordination system that is incentive aligned, i.e., it gives the participants the incentives to act in a manner that is beneficial to the overall system. We also prove that the system monotonically improves the overall organizational performance and is goal congruent.
We consider how the government should set the fine for copying, tax on copying medium, and subsidy on legitimate purchases, whereas a monopoly publisher sets price and spending on detection. There are two segments of potential software users--ethical users who will not copy, and unethical users who would copy if the benefit outweighs the cost. In deciding on policy, the government must consider how the publisher adjusts price and detection to changes in the fine, tax, and subsidy. Our key welfare result is that increases in detection affect welfare more negatively than price cuts. We also show that the tax is welfare superior to the fine, and that a subsidy is optimal. Generally, government policies that focus on penalties alone will miss the social welfare optimum.
Research on information sharing has viewed this activity as essential for informing groups on content relevant to a decision. We propose and examine an alternate function of information sharing, i.e., the social construction of meaning. To accomplish this goal, we turn to social construction, social presence, and task closure theories. Drawing from these theories, we hypothesize relationships among the meeting environment, breadth and depth of information shared during a meeting, and decision quality. We explore these relationships in terms of the effects of both the media environment in which the group is situated and the medium that group members choose to utilize for their communication. Our study of 32, 5- and 6-person groups supports our belief that interpretation underlies information sharing and is necessary for favorable decision outcomes. It also supports the proposed negative effect of low social presence media on interpretation in terms of depth of information sharing; a low social presence medium, however, promotes information sharing breadth. Finally, the findings indicate that when in multimedia environments and faced with a relatively complex task, choosing to utilize an electronic medium facilitates closure and, therefore, favorable outcomes.
Computer skills are key to organizational performance, and past research indicates that behavior modeling is a highly effective form of computer skill training. The present research develops and tests a new theoretical model of the underlying observational learning processes by which modeling-based training interventions influence computer task performance. Observational learning processes are represented as a second-order construct with four dimensions (attention, retention, production, and motivation). New measures for these dimensions were developed and shown to have strong psychometric properties. The proposed model controls for two pretraining individual differences (motivation to learn and self-efficacy) and specifies the relationships among three training outcomes (declarative knowledge, post-training self-efficacy, and task performance). The model was tested using PLS on data from an experiment (N=95) on computer spreadsheet training. As hypothesized, observational learning processes significantly influenced training outcomes. A representative modeling-based training intervention (retention enhancement) significantly improved task performance through its specific effects on the retention processes dimension of observational learning. The new model provides a more complete theoretical account of the mechanisms by which modeling-based interventions affect training outcomes, which should enable future research to systematically evaluate the effectiveness of a wide range of modeling-based training interventions. Further, the new instruments can be used by practitioners to refine ongoing training programs.
Data Quality Information (DQI) is metadata that can be included with data to provide the user with information regarding the quality of that data. As users are increasingly removed from any personal experience with data, knowledge that would be beneficial in judging the appropriateness of the data for the decision to be made has been lost. Data tags could provide this missing information. However, it would be expensive in general to generate and maintain such information. Doing so would be worthwhile only if DQI is used and affects the decision made. This work focuses on how the experience of the decision maker and the available processing time influence the use of DQI in decision making. It also explores other potential issues regarding use of DQI, such as task complexity and demographic characteristics. Our results indicate increasing use of DQI when experience levels progress through the stages from novice to professional. The overall conclusion is that DQI should be made available to managers without domain-specific experience. From this it would follow that DQI should be incorporated into data warehouses used on an ad hoc basis by managers.
Payoffs from information technology (IT) continue to generate interest and debate both among academicians and practitioners. The extant literature cites inadequate sample size, lack of process orientation, and analysis methods among the reasons some studies have shown mixed results in establishing a relationship between IT investment and firm performance. In this paper we examine the structural variables that affect IT payoff through a meta-analysis of 66 firm-level empirical studies between 1990 and 2000. Employing logistic regression and discriminant analyses, we present statistical evidence of the characteristics that discriminate between IT payoff studies that observed a positive effect and those that did not. In addition, we conduct ordinary least squares (OLS) regression on a continuous measure of IT payoff to examine the influence of structural variables on the result of IT payoff studies. The results indicate that the sample size, data source (firm-level or secondary), and industry in which the study is conducted influence the likelihood of the study finding greater improvements on firm performance. The choice of the dependent variable(s) also appears to influence the outcome (although we did not find support for process-oriented measurement), the type of statistical analysis conducted, and whether the study adopted a cross-sectional or longitudinal design. Finally, we present implications of the findings and recommendations for future research.
The ability to detect and accurately estimate the strength of interaction effects are critical issues that are fundamental to social science research in general and IS research in particular. Within the IS discipline, a significant percentage of research has been devoted to examining the conditions and contexts under which relationships may vary, often under the general umbrella of contingency theory (cf. McKeen et al. 1994, Weill and Olson 1989). In our survey of such studies, the majority failed to either detect or provide an estimate of the effect size. In cases where effect sizes are estimated, the numbers are generally small. These results have led some researchers to question both the usefulness of contingency theory and the need to detect interaction effects (e.g., Weill and Olson 1989). This paper addresses this issue by providing a new latent variable modeling approach that can give more accurate estimates of interaction effects by accounting for the measurement error that attenuates the estimated relationships. The capacity of this approach at recovering true effects in comparison to summated regression is demonstrated in a Monte Carlo study that creates a simulated data set in which the underlying true effects are known. Analysis of a second, empirical data set is included to demonstrate the technique's use within IS theory. In this second analysis, substantial direct and interaction effects of enjoyment on electronic-mail adoption are shown to exist.
Conceptual design is an important, but difficult, phase of systems development. Analysis patterns can greatly benefit this phase because they capture abstractions of situations that occur frequently in conceptual modeling. Nave approaches to automate conceptual design with reuse of analysis patterns have had limited success because they do not emulate the learning that occurs over time. This research develops learning mechanisms for improving analysis pattern reuse in conceptual design. The learning mechanisms employ supervised learning techniques to support the generic reuse tasks of retrieval, adaptation, and integration, and emulate expert behaviors of analogy making and designing by assembly. They are added to a nave approach and the augmented methodology implemented as an intelligent assistant to a designer for generating an initial conceptual design that a developer may refine. To assess the potential of the methodology to benefit practice, empirical testing is carried out on multiple domains and tasks of different sizes. The results suggest that the methodology has the potential to benefit practice.; Dos Santos, Brian L.2
Generalizability is a major concern to those who do, and use, research. Statistical, sampling-based generalizability is well known, but methodologists have long been aware of conceptions of generalizability beyond the statistical. The purpose of this essay is to clarify the concept of generalizability by critically examining its nature, illustrating its use and misuse, and presenting a framework for classifying its different forms. The framework organizes the different forms into four types, which are defined by the distinction between empirical and theoretical kinds of statements. On the one hand, the framework of firms the bounds within which statistical, sampling-based generalizability is legitimate. On the other hand, the framework indicates ways in which researchers in information systems and other fields may properly lay claim to generalizability, and thereby broader relevance, even when their inquiry falls outside the bounds of sampling-based research.; Davis, Randall2
This paper examines the evolution of portfolio of controls over the duration of outsourced information systems development (ISD) projects. Drawing on five cases, it concludes that many findings from research on control of internal ISD projects apply to the outsourced context as well, but with some interesting differences. The portfolios of control in outsourced projects are dominated by outcome controls, especially at the start of the project; although the precision and frequency of these controls varies across projects. Behavior controls are often added later in the project, as are controls aimed to encourage and enable vendor self-control. Clan controls were used in only two of the cases--when the client and vendor had shared goals, and when frequent interactions led to shared values. In general, the outsourced projects we studied began with relatively simple controls but often required significant additional controls after experiencing performance problems. Factors influencing the choice and evolution of controls are also examined.; Pitt, Leyland pitt@curtin.edu.au; Ewing, Michael mike.ewing@buseco.monash.edu.au; Carr, Christopher L. clcarr@katz.pitt.edu
We present a simulation approach that provides a relatively risk-free and cost-effective environment to examine the decision space for both bid takers and bid makers in web-based dynamic price setting processes. The applicability of the simulation platform is demonstrated for Yankee auctions in particular. We focus on the optimization of bid takers' revenue, as well as on examining the welfare implications of a range of consumer-bidding strategies--some observed, some hypothetical. While these progressive open discriminatory multi-unit auctions with discrete bid increments are made feasible by Internet technologies, little is known about their structural characteristics, or their allocative efficiency. The multi-unit and discrete nature of these mechanisms renders the traditional analytic framework of game theory intractable (Nautz and Wolfstetter 1997). The simulation is based on theoretical revenue generating properties of these auctions. We use empirical data from real online auctions to instantiate the simulation's parameters. For example, the bidding strategies of the bidders are specified based on three broad bidding strategies observed in real online auctions. The validity of the simulation model is established and subsequently the simulation model is configured to change the values of key control factors, such as the bid increment. Our analysis indicates that the auctioneers are, most of the time, far away from the optimal choice of bid increment, resulting in substantial losses in a market with already tight margins. The simulation tool provides a test bed for jointly exploring the combinatorial space of design choices made by the auctioneer's and the bidding strategies adopted by the bidders. For instance, a multinomial logit model reveals that endogenous factors, such as the bid increment and the absolute magnitude of the...; Weber, Ron weber@commerce.uq.edu.au
Measurement is perhaps the most difficult aspect of behavioral research. In a recent edition of ISR, a scale for consensus on appropriation was developed. Consensus on appropriation is one of three global constructs incorporated in adaptive structuration theory (Poole and DeSanctis 1990). The principal components analysis on the initial questionnaire revealed two factors with eigenvalues greater than one. While the methods used to develop the scale were thorough, the weaker factor was excluded from the rest of the analysis with little justification. We suggest that this finding has two possible explanations, multidimensionality or response bias. This research note suggests that in addition to the convergent and discriminant validity that Salisbury et al. (2002) provided for the consensus on appropriation scale, we may have an opportunity to further refine the measurement of this construct. By further exploring this principal component finding, consensus on appropriation scale, we may have an opportunity to further refine the measurement of this construct. By further exploring this principal component finding, consensus on appropriation may be better understood and measured.; :; Business Source Complete
Organizations today face increasing pressures to integrate their processes across disparate divisions and functional units, in order to remove inefficiencies as well as to enhance manageability. Process integration involves two major types of changes to process structure: (1) synthesizing processes from separate but interdependent subprocesses, and (2) decomposing aggregate processes into distinct subprocesses that are more manageable. We present an approach to facilitate this type of synthesis and decomposition through formal analysis of process structure using a mathematical structure called a metagraph.; Cummings, Larry L.2; Straub, Detmar W.3; Earley, P. Christopher4
The article presents a reply to questions raised by Allport and Kerler (A&K) in their research note about theory and data in scale development. With the objective of creating a single scale consistent with an a priori construct definition, we choose principal components analysis as a means for initial data reduction. However, the study was indeed designed to have an initial set of items useful for data reduction or scale purification, as opposed to running tests to immediately suggest valid measures. An author suggested that the only way to evaluate the psychometric properties of the responses to rating scales with both positively and negatively worded items would be to use confirmatory factor analysis and structural equation model methods. Besides wording effects, A&K suggested that a response bias effect based on positive or negative framing might well be another possibility. To aid model improvement, the modification index for a parameter is an estimate of the amount by which the discrepancy function would decrease if the analysis were repeated with the constraints on that parameter removed.
With the increased importance of IT in organizations, business managers are now expected to show stronger leadership in regard to its deployment of IT in organizations. This requires greater focus on their capability to understand and use IT resources effectively. This paper explores the concept of IT competence of business managers as a contributor to their intention to champion IT within their organizations. Based on the knowledge literature, IT competence is defined as the set of IT-related knowledge and experience that a business manager possesses. The relationship between IT knowledge, IT experience, and championing IT is tested empirically using Structural Equation Modeling with LISREL. Four hundred and four business managers from two large insurance organizations were surveyed. Specific areas of IT knowledge and IT experience were first identified and the first half of the data set was utilized to assess the measurement properties of the instrument in a confirmatory analysis. The contribution of IT knowledge and IT experience to their intention to champion IT was assessed using the second half of the data set. The results show that IT knowledge and IT experience together explain 34% of the variance in managers' intentions to champion IT. Recommendations are given as to how organizations can enhance their business managers' IT knowledge and experience to achieve stronger IT leadership from line people.; Jarvenpaa, Sirkka L.2
Embedded relationships with customers have been key in generating repeat business and economic advantage, especially in business-to-business settings. Such relationships are typically maintained through interpersonal interactions between customers and their providers. Lately, however, firms have been seeking to make their service operations more scalable by offering customers access to Internet-based, self-serve technology. This raises questions about the implications of inserting self-serve technology into embedded relationships. Recent research on the role of information technology (IT) within interfirm network relations suggests that relationships and the use of IT are complementary. However, most of this research focuses on the organizational level and fails to consider the instantiation of these interfirm relations by the actions and interactions of individual actors (e.g., customers and salespeople) representing their respective firms. In this paper, we explore the implications of using IT within interfirm relations through an analysis of customers' and sales representatives' (reps) work activities and interpersonal relationships. We apply a practice perspective that highlights how macrolevel phenomena such as interfirm relations are created and recreated through the microlevel actions taken by firm members. This analysis reveals that managing the complementarity between relationships and IT in practice is fraught with considerable tension. This study of WebGA, a bricks-and-clicks dotcom, highlights how the use of the self-serve technology made it more difficult for sales reps to build and maintain embedded relationships with their customers. The use of IT altered the nature and quality of information shared by the participants, undermined the ability of sales reps to provide consulting services to customers, reduced the frequency of their interaction, and prompted sales reps to expend social capital to promote customers' technology adoption. These chang...
Institution-based trust is a buyer's perception that effective third-party institutional mechanisms are in place to facilitate transaction success. This paper integrates sociological and economic theories about institution-based trust to propose that the perceived effectiveness of three IT-enabled institutional mechanisms--specifically feedback mechanisms, third-party escrow services, and credit card guarantees--engender buyer trust in the community of online auction sellers. Trust in the marketplace intermediary that provides the overarching institutional context also builds buyer's trust in the community of sellers. In addition, buyers' trust in the community of sellers (as a group) facilitates online transactions by reducing perceived risk. Data collected from 274 buyers in Amazon's online auction marketplace provide support for the proposed structural model. Longitudinal data collected a year later show that transaction intentions are correlated with actual and self-reported buyer behavior. The study shows that the perceived effectiveness of institutional mechanisms encompasses both weak (market-driven)and strong (legally binding) mechanisms. These mechanisms engender trust, not only in a few reputable sellers, but also in the entire community of sellers, which contributes to an effective online marketplace. The results thus help explain why, despite the inherent uncertainty that arises when buyers and sellers are separated in time and in space, online marketplaces are proliferating. Implications for theory are discussed, and suggestions for future research on improving IT-enabled trust-building mechanisms are suggested.
The proliferation of information on the Internet poses a significant challenge on humans' limited attentional resources. To attract online users' attention, various kinds of animation are widely used on websites. Despite the ubiquitous use of animation, there is an inadequate understanding of its effect on attention. Focusing on flash animation, this study examines its effects on online users' performance and perceptions in both task-relevant and task-irrelevant information search contexts by drawing on the visual search literature and two theories from cognitive psychology. In the task-relevant context, flash is applied on the search target; while in the task-irrelevant context, flash is applied on a nontarget item. The results of this study confirm that flash does attract users' attention and facilitates quicker location of the flashed target item in tightly packed screen displays. However, there is no evidence that attracting attention increases recall of the flashed item, as is generally presumed in practice, and may even decrease the overall recall. One explanation is that when users have to use their limited attentional resources on suppressing the distraction of flash, they will have less mental resources to process information. Moreover, the results suggest that processing information about an item depends not only on the attention it attracts per se, but also on the attention that other items on the same screen attract. While flashing an item may not increase the recall of that item, it can reduce the recall of other items (especially the nontarget items) on the screen. Finally, flash has negative effects on users' focused attention and attitude towards using the website. These results have implications for website interface design, online product promotion, online advertising, and multimedia training systems, among others.
The widespread use of the Internet has led to the emergence of numerous information intermediaries that bring buyers and sellers together and leverage their knowledge of the marketplace to provide value-added services. Infomediaries offer matching services that facilitate establishment of a buyer-seller agreement, and value-added services that either provide a standalone benefit or enhance benefits from matching services. This paper develops and analyzes economic models of intermediaries to examine their pricing and product line design strategies. Intermediaries provide aggregation benefits: Buyers find an intermediary's service more valuable if it provides access to more sellers, and sellers value it more if it provides access to more buyers, but also when they compete with fewer sellers. Due to this unique combination of network effects, we find that an intermediary has stronger incentives to provide quality-differentiated versions of its service relative to other information goods sellers. When buyers have constant marginal valuations for service quality, the intermediary should offer only two levels of service. While it is optimal for the intermediary to offer two levels of service, increasing the quality of the low-level service reduces the intermediary's profits due to increased cannibalization of the premium service. Hence, the optimal menu consists of a basic matching service and a premium service that includes matching and value-added services. The intermediary's profits are larger when positive network effects are stronger, and lower when negative network effects are stronger.
This paper presents a project management policy in which the appearance of software faults during system construction is used to determine the timing of system integration activities (e.g., team meetings, analyzing modules for interface inconsistencies, system fault correction, and so on). System integration is performed only if a threshold fault count has been exceeded; otherwise, module development is allowed to continue. We derive an expression for calculating fault thresholds and analyze the policy to reveal the presence of three operating regions: (1) a region in which development should continue with no system integration, (2) a region in which system integration occurs if a threshold fault count has been exceeded, and (3) a region in which system integration should always take place. Analytical and numerical results demonstrate how the fault thresholds change with system complexity, team skill, development environment, and project schedule. We also show how learning that occurs during each round of system integration leads to less frequent integration in the future, and lower total construction effort. Simulation experiments reveal that the fault threshold policy can be applied even if several homogeneity assumptions in the model are relaxed, allowing for differences in the propensity among modules to accumulate faults and the effort needed to correct these faults. Finally, the fault threshold policy outperforms a fixed-release policy in which system integration occurs whenever a fixed number of modules has been released.
Focus on individual outsourcing decisions in IT research has often yielded contradictory findings and recommendations. To address these contradictions, we investigate a holistic, configurational approach with the prevailing universalistic or contingency perspectives in exploring the effects of IT outsourcing strategies on outsourcing success. Based on residual rights theory, we begin by identifying three dimensions of IT out-sourcing strategies: degree of integration, allocation of control, and performance period. We then develop a model of fit-as-gestalt, drawing from literatures on strategy, governance, interorganizational relationships, and outsourcing. Next, based on data from 311 firms in South Korea, we test universalistic and contingency perspectives in explaining the relationship between IT outsourcing strategies and outsourcing success. We then identify three congruent patterns, or gestalts, of IT outsourcing strategies. We term these strategies independent, arm's-length, and embedded strategies. To establish the predictive validity of these gestalts and the viability of a configurational perspective, we then explore the effects of these congruent gestalts vis--vis noncongruent patterns on three dimensions of outsourcing success: strategic competence, cost efficiency, and technology catalysis. We also contrast the effects of each of the three gestalts on each of the three dimensions of outsourcing success. Our findings indicate the superiority of the configurational approach over universalistic and contingency perspectives in explaining outsourcing success.
Online spaces that enable shared public interpersonal communications are of significant social, organizational, and economic importance. In this paper, a theoretical model and associated unobtrusive method are proposed for researching the relationship between online spaces and the behavior they host. The model focuses on the collective impact that individual information-overload coping strategies have on the dynamics of open, interactive public online group discourse. Empirical research was undertaken to assess the validity of both the method and the model, based on the analysis of over 2.65 million postings to 600 Usenet newsgroups over a 6-month period. Our findings support the assertion that individual strategies for coping with information over-load have an observable impact on large-scale online group discourse. Evidence was found for the hypotheses that: (1) users are more likely to respond to simpler messages in overloaded mass interaction; (2) users are more likely to end active participation as the overloading of mass interaction increases; and (3) users are more likely to generate simpler responses as the overloading of mass interaction grows. The theoretical model outlined offers insight into aspects of computer-mediated communication tool usability, technology design, and provides a road map for future empirical research.
Peer-to-peer (P2P) file sharing networks are an important medium for the distribution of information goods. However, there is little empirical research into the optimal design of these networks under real-world conditions. Early speculation about the behavior of P2P networks has focused on the role that positive network externalities play in improving performance as the network grows. However, negative network externalities also arise in P2P networks because of the consumption of scarce network resources or an increased propensity of users to free ride in larger networks, and the impact of these negative network externalities--while potentially important--has received far less attention. Our research addresses this gap in understanding by measuring the impact of both positive and negative network externalities on the optimal size of P2P networks. Our research uses a unique dataset collected from the six most popular OpenNap P2P networks between December 19, 2000, and April 22, 2001. W find that users contribute additional value to the network at a decreasing rate and impose costs on the network at an increasing rate, while the network increases in size. Our results also suggest that users are less likely to contribute resources to the network as the network size increases. Together, these results suggest that the optimal size of these centralized P2P networks is bounded--at some point the costs that a marginal user imposes on the network will exceed the value they provide to the network. This finding is in contrast to early predictions that larger P2P networks would always provide more value to users than smaller networks. Finally, these results also highlight the importance of considering user incentives--an important determinant of resource sharing in P2P networks--in network design.
The decision processes surrounding investments in innovative information technology (IT) platforms are complicated by uncertainty about expected payoffs and irreversibilities in the costs of implementation. When uncertainty and irreversibility are high, concepts from real options should be used to properly structure the evaluation and management of investment opportunities, and thereby capture the value of managerial flexibility. However, while innovation researchers have posited that option value can influence the motivations of early adopters, and options researchers have identified emerging IT as a promising area for application of options valuation techniques, there has yet to be a systematic theoretical integration of work on IT innovation and real options. This paper seeks to all this gap by developing a model of the determinants of option value associated with investments in innovative IT platforms. In so doing, the model addresses a central question in the innovation field: When should a firm take a lead role in innovation with emerging technologies? The analysis begins with an explanation of real options analysis and how it differs from conventional approaches for evaluating new technologies. Then a set of 12 factors--drawn from 4 complementary perspectives on organizational innovation (technology strategy, organizational learning, innovation bandwagons, and technology adaptation)--is synthesized into a model of the option value of IT platform investments. Rationales are provided to explain the direct effects of these factors on option value, and selected interactions among the factors are also considered. Finally, the implications of the model are presented in three areas: predicting IT platform initiation and adoption, valuing IT platform options, and managing IT platform implementation.
Today, more than ever before, organizations are faced with the task of processing volumes of information under more uncertain and more competitive environments. This study investigates the impact of environmental uncertainty and task characteristics on user satisfaction with data by using IS and organizational theories. Responses were matched from 77 CEOs and 166 senior managers, who were end users of IS. The partial least squares technique indicated that environmental uncertainty has a positive impact on task characteristics. Task characteristics have a direct and mediating impact on user satisfaction with data. Our findings also demonstrated that user satisfaction with data could be better understood by overlapping IS and organizational theories, rather than by treating the subject matter in disjoint fields. The paper concludes with discussions and implications for researchers and practitioners.
Although trust has received much attention in many streams of information systems research, there has been little theorizing to explain how trust evokes sentiments and affects task performance in IT-enabled relationships. Many studies unquestionably assume that trust is intrinsically beneficial, and dismiss the possibility that the effects of trust may be dependent on the situation (or conditions) at present. This paper theoretically and empirically examines outcomes of an individual's trust in global virtual teams under differing situations (or conditions). In Study 1, we find that early in a team's existence, a member's trusting beliefs have a direct positive effect on his or her trust in the team and perceptions of team cohesiveness. Later on, however, a member's trust in his team operates as a moderator, indirectly affecting the relationships between team communication and perceptual outcomes. Study 2 similarly suggests that trust effects are sensitive to the particular situation or condition. Combined, the studies find that trust affects virtual teams differently in different situations. Future studies on trust will need to consider situational contingencies. This paper contributes to the literature on IT-enabled relationships by theorizing and empirically testing how trust affects attitudes and behaviors.
This paper analyzes the optimal choice of pricing schedules and technological deterrence levels in a market with digital piracy where sellers can influence the degree of piracy by implementing digital rights management (DRM) systems. It is shown that a monopolist's optimal pricing schedule can be characterized as a simple combination of the zero-piracy pricing schedule and a piracy-indifferent pricing schedule that makes all customers indifferent between legal usage and piracy. An increase in the quality of pirated goods, while lowering prices and profits, increases total surplus by expanding both the fraction of legal users and the volume of legal usage. In the absence of price discrimination, a seller's optimal level of technology-based protection against piracy is shown to be at the technologically maximal level, which maximizes the difference between the quality of the legal and pirated goods. However, when a seller can price discriminate, its optimal choice is always a strictly lower level of technology-based protection. These results are based on the following digital rights conjecture: that granting digital rights increases the incidence of digital piracy, and that managing digital rights therefore involves restricting the rights of usage that contribute to customer value. Moreover, if a digital rights management system weakens over time due to the underlying technology being progressively hacked, a seller's optimal strategic response may involve either increasing or decreasing its level of technology-based protection. This direction of change is related to whether the DRM technology implementing each marginal reduction in piracy is increasingly less or more vulnerable to hacking. Pricing and technology choice guidelines are presented, and some welfare implications are discussed.
We study the process by which model-based decision support systems (DSSs) influence managerial decision making in the context of marketing budgeting and resource allocation. We focus on identifying whether and how DSSs influence the decision process (e.g., cognitive effort deployed, discussion quality, and decision alternatives considered) and, as a result, how these DSSs influence decision outcomes (e.g., profit and satisfaction both with the decision process and the outcome). We study two specific marketing resource allocation decisions in a laboratory context: sales effort allocation and customer targeting. We find that decision makers who use high-quality, model-based DSSs make objectively better decisions than do decision makers who only have access to a generic decision tool (Microsoft Excel). However, their subjective evaluations (perceptions) of both their decisions and the processes that lead to those decisions do not necessarily improve as a result of DSS use. And expert judges, serving as surrogates for top management, have a difficult time assessing the objective quality of those decisions. Our results suggest that what managers get from a high-quality DSS may be substantially better than what they see. To increase the inclination for managerial adoption and use of DSS, we must get users to see the benefits of using a DSS. Our results also suggest two ways to bridge the perception-reality gap: (1) improve the perceived value of the decision process by designing DSSs both to encourage discussion (e.g., by providing explanation and support for alternative recommendations) as well as to reduce the perceived complexity of the problem so that managers invest more cognitive effort in exploring additional options and (2) provide feedback on the likely market/business outcomes of various decision options.
We use an economic model to formalize the complex relationships among IT investments, intermediate performance measures (e.g., product quality and output levels), and economic performance (e.g., productivity, profits, and consumer surplus). We demonstrate that a profit-maximizing monopolist invests in IT (modeled as changes in parametric characteristics of the firm) to design a better-quality product and charge a higher price. While this profit-maximizing adjustment generates more consumer surplus, it also increases production costs in a way that adversely affects productivity. In contrast, a simple model extension shows that when a firm is unwilling or unable to improve product quality, then IT investments result in suboptimal improvements in profits, an increase in consumer surplus, and an increase in productivity. Together, these models highlight the way in which product quality moderates the relationship between IT investments and economic performance. We also demonstrate that these relationships are robust to the socially optimal case in which a social planner chooses price and quality to maximize social welfare. In addition, we demonstrate that the results of the monopoly model hold when considering the design and development of products offered free of charge (e.g., free online content), but that provide indirect benefits to the firm (e.g., more advertising revenues).
In this paper we address the problem of increasing software maintenance costs in a custom software development environment, and develop a stochastic decision model for the maintenance of information systems. Based on this modeling framework, we derive an optimal decision rule for software systems maintenance, and present sensitivity analysis of the optimal policy. We illustrate an application of this model to a large telecommunications switching software system, and present sensitivity analysis of the optimal state for major upgrade derived from our model. Our modeling framework also allows for computing the expected time to perform major upgrade to software systems.
In today's competitive environment, an increasing number of firms are building common information systems, which will be deployed globally, to support their strategic globalization initiatives. These systems are designed to meet the requirements of a diverse set of stakeholders with different business needs, priorities, and objectives. One managerial tool for addressing and reconciling such differences is control, which encompasses all attempts to motivate individuals to act in a manner that is consistent with organizational objectives. This paper examines two research questions. How do stakeholders exercise control during different phases of large IS projects? Why do control choices change across project phases? Results of two case studies suggest control is exercised differently for each phase. During the initial phase of a project, control is exercised as collective sensemaking, in which both IS and business stakeholders utilize mostly informal mechanisms of control. During development, technical winnowing of mechanisms occurs such that control is vested primarily in IS managers, who structure hierarchical relationships with subordinates and who rely extensively on formal control mechanisms. Both IS and business stakeholders employ formal and informal mechanisms during implementation to exercise control as collaborative coordinating. The results also suggest that changes in control choices from one project phase to another are triggered by factors in the project, stakeholder, and global contexts. As factors change across phases, so too do control choices. Further, problems that surface in one project phase trigger changes to controls in subsequent phases. These findings are integrated into a model of the dynamics of control. Implications of these results are drawn, and directions for future research are suggested.
Information technology (IT) outsourcing success requires careful management of customer-supplier relationships. However, there are few published studies on the ongoing relationships, and most of these adopt a customer perspective, de-emphasizing suppliers. In this study, we look at both customer and supplier perspectives, by means of the psychological contract of customer and supplier project managers. We apply the concept of psychological contract to perceived mutual obligations, and to how such fulfillment of obligations can predict success. Our research questions are (1) What are the critical customer-supplier obligations in an IT outsourcing relationship? and (2) What is the impact of fulfilling these obligations on success? We use a sequential, qualitative-quantitative approach to develop and test our model. In the qualitative study, we probe the nature of customer-supplier obligations using in-depth interviews. Content analysis of interview transcripts show that both customers and suppliers identify six obligations that are critical to success. Customers perceive supplier obligations to be accurate project scoping, clear authority structures, taking charge, effective human capital management, effective knowledge transfer, and effective interorganizational teams. Suppliers perceive customer obligations as clear specifications, prompt payment, close project monitoring, dedicated project staffing, knowledge sharing, and project ownership. In the second quantitative study, we assess the impact of fulfilling these obligations on success through a field study of 370 managers. Results show that fulfilled obligations predict success over and above the effects of contract type, duration, and size.
The lack of consumer confidence in information privacy has been identified as a major problem hampering the growth of e-commerce. Despite the importance of understanding the nature of online consumers' concerns for information privacy, this topic has received little attention in the information systems community. To fill the gap in the literature, this article focuses on three distinct, yet closely related, issues. First, drawing on social contract theory, we offer a theoretical framework on the dimensionality of Internet users' information privacy concerns (IUIPC). Second, we attempt to operationalize the multidimensional notion of IUIPC using a second-order construct, and we develop a scale for it. Third, we propose and test a causal model on the relationship between IUIPC and behavioral intention toward releasing personal information at the request of a marketer. We conducted two separate field surveys and collected data from 742 household respondents in one-on-one, face-to-face interviews. The results of this study indicate that the second-order IUIPC factor, which consists of three first-order dimensions -- namely, collection, control, and awareness -- exhibited desirable psychometric properties in the context of online privacy. In addition, we found that the causal model centering on IUIPC fits the data satisfactorily and explains a large amount of variance in behavioral intention, suggesting that the proposed model will serve as a useful tool for analyzing online consumers' reactions to various privacy threats on the Internet.
This paper presents an approach to organizational modeling that combines both agent-centric and activity-centric approaches. Activity-centric approaches to process modeling capture the mechanistic components of a process (including aspects of work flow, decision, and information), but agent-centric approaches capture specific aspects of the human component. In this paper, we explore an integrative viewpoint in which the transactional aspects of agent-centric concerns--for example, economic incentives for agents to perform--are integrated with decision and informational aspects of a process. To illustrate issues in this approach, we focus on modeling incentive mechanisms in a specific sales process and present results from an extensive simulation experiment. Our results highlight the importance of considering the effects of incentives when decision and informational aspects of a process undergo changes.
Grounded in the innovation diffusion literature and the resource-based theory, this paper develops an integrative research model for assessing the diffusion and consequence of e-business at the firm level. Unlike the typical focus on adoption as found in the literature, we focus on postadoption stages, that is, actual usage and value creation. The model thus moves beyond dichotomous adoption versus nonadoption and accounts for the :missing link-actual usage-as a critical stage of value creation. The model links technological, organizational, and environmental factors to e-business use and value, based on which a series of hypotheses are developed. The theoretical model is tested by using structural equation modeling on a dataset of 624 firms across 10 countries in the retail industry. To probe deeper into whether e-business use and value are influenced by economic environments, two subsamples from developed and developing countries are compared. The study finds that technology competence, firm size, financial commitment, competitive pressure, and regulatory support are important antecedents of e-business use. In addition, the study finds that, while both front-end and back-end capabilities contribute to e-business value, back-end integration has a much stronger impact. While front-end functionalities are becoming commodities, e-businesses are more differentiated by back-end integration. This is consistent with the resource-based theory because back-end integration possesses the value-creating characteristics of resources (e.g., firm specific, difficult to imitate), which are strengthened by the Internet-enabled connectivity. Our study also adds an international dimension to the innovation diffusion literature, showing that careful attention must be paid to the economic and regulatory factors that may affect technology diffusion across different countries.
In general, perceptions of information systems (IS) success have been investigated within two primary research streams-the user satisfaction literature and the technology acceptance literature. These two approaches have been developed in parallel and have not been reconciled or integrated. This paper develops an integrated research model that distinguishes beliefs and attitudes about the system (i.e., object-based beliefs and attitudes) from beliefs and attitudes about using the system (i.e., behavioral beliefs and attitudes) to build the theoretical logic that links the user satisfaction and technology acceptance literature. The model is then tested using a sample of 465 users from seven different organizations who completed a survey regarding their use of data warehousing software. The proposed model was supported, providing preliminary evidence that the two perspectives can and should be integrated. The integrated model helps build the bridge from design and implementation decisions to system characteristics (a core strength of the user satisfaction literature) to the prediction of usage (a core strength of the technology acceptance literature).
In today's global market environment, enterprises are increasingly turning to use of distributed teams to leverage their resources and address diverse markets. Individual members of structurally diverse distributed teams need to develop their collaboration know-how to work effectively with others on their team. The lack of face-to-face cues creates challenges in developing the collaboration know-how-challenges that can be overcome by communicating not just content, but also context. We derive a theoretical model from Te'eni's (2001) cognitive-affective model of communication to elaborate how information technology (IT) can support an individual's communication of context to develop collaboration know-how. Two hundred and sixty-three individuals working in structurally diverse distributed teams using a variety of virtual workspace technologies to support their communication needs were surveyed to test the model. Results indicate that when individuals perceive their task as nonroutine, there is a positive relationship between an individual's perceived degree of IT support for communicating context information and his collaboration know-how development. However, when individuals perceive their task as routine, partial IT support for contextualization is associated with lower levels of collaboration know-how development. This finding is attributed to individuals' misunderstanding of the conveyed context, or their struggling to utilize the context conveyed with partial IT support, making a routine task more prone to misunderstanding and leaving the user worse than if she had no IT support for contextualization. We end the paper by drawing theoretical and practical implications based on these findings.
The increasing significance of information technology (IT) security to firms is evident from their growing IT security budgets. Firms rely on security technologies such as firewalls and intrusion detection systems (IDSs) to manage IT security risks. Although the literature on the technical aspects of IT security is proliferating, a debate exists in the IT security community about the value of these technologies. In this paper, we seek to assess the value of IDSs in a firm's IT security architecture. We find that the IDS configuration, represented by detection (true positive) and false alarm (false positive) rates, determines whether a firm realizes a positive or negative value from the IDS. Specifically, we show that a firm realizes a positive value from an IDS only when the detection rate is higher than a critical value, which is determined by the hacker's benefit and cost parameters. When the firm realizes a positive (negative) value, the IDS deters (sustains) hackers. However, irrespective of whether the firm realizes a positive or negative value from the IDS, the IDS enables the firm to better target its investigation of users, while keeping the detection rate the same. Our results suggest that the positive value of an IDS results not from improved detection per se, but from an increased deterrence enabled by improved detection. Finally, we show that the firm realizes a strictly nonnegative value if the firm configures the IDS optimally based on the hacking environment.
With the advent of the Internet, and the minimal information technology requirements of a trading partner to join an exchange, the number of sellers who can qualify and participate in online exchanges is greatly increased. We model the competition between two sellers with different unit costs and production capacities responding to a buyer demand. The resulting mixed-strategy equilibrium shows that one of the sellers has a normal high price with random sales, while the other seller continuously randomizes its prices. It also brings out the inherent advantages that sellers with lower marginal costs or higher capacities have in joining these exchanges, and provides a theoretical basis for understanding the relative advantages of various types of sellers in such exchanges.
Given that information technology (IT) security has emerged as an important issue in the last few years, the subject of security information sharing among firms, as a tool to minimize security breaches, has gained the interest of practitioners and academics. To promote the disclosure and sharing of cyber security information among firms, the U.S. federal government has encouraged the establishment of many industry-based Information Sharing and Analysis Centers (ISACs) under Presidential Decision Directive (PDD) 63. Sharing security vulnerabilities and technological solutions related to methods for preventing, detecting, and correcting security breaches is the fundamental goal of the ISACs. However, there are a number of interesting economic issues that will affect the achievement of this goal. Using game theory, we develop an analytical framework to investigate the competitive implications of sharing security information and investments in security technologies. We find that security technology investments and security information sharing act as strategic complements in equilibrium. Our results suggest that information sharing is more valuable when product substitutability is higher, implying that such sharing alliances yield greater benefits in more competitive industries. We also highlight that the benefits from such information-sharing alliances increase with the size of the firm. We compare the levels of information sharing and technology investments obtained when firms behave independently (Bertrand-Nash) to those selected by an ISAC, which maximizes social welfare or joint industry profits. Our results help us predict the consequences of establishing organizations such as ISACs, Computer Emergency Response Team (CERT), or InfraGard by the federal government.
Growth of Web-based applications has drawn a great number of diverse stakeholders and specialists into the information systems development (ISD) practice. Marketing, strategy, and graphic design professionals have joined technical developers, business managers, and users in the development of Web-based applications. Often, these specialists work for different organizations with distinct histories and cultures. A longitudinal, qualitative field study of a Web-based application development project was undertaken to develop an in-depth understanding of the collaborative practices that unfold among diverse professionals on ISD projects. The paper proposes that multiparty collaborative practice can be understood as constituting a collective reflection-in- action cycle through which an information systems (IS) design emerges as a result of agents producing, sharing, and reflecting on explicit objects. Depending on their control over the various economic and cultural (intellectual) resources brought to the project and developed on the project, agents influence the design in distinctive ways. They use this control to either add to, ignore, or challenge the work produced by others. Which of these modes of collective reflection-in-action are enacted on the project influences whose expertise will be reflected in the final design. Implications for the study of boundary objects, multiparty collaboration, and organizational learning in contemporary ISD are drawn.
Many auctions involve selling several distinct items simultaneously, where bidders can bid on the whole or any part of the lot. Such auctions are referred to as combinatorial auctions. Examples of such auctions include truck delivery routes,industrial procurement, and FCC spectrum. Determining winners in such auctions is an NP-hard problem, and significant research is being conducted in this area. However, multiple- round (iterative) combinatorial auctions present significant challenges in bid formulations as well. Because the combinatorial dynamics in iterative auctions can make a given bid part of a winning and nonwinning set of bids without any changes in the bid, bidders are usually not able to evaluate whether they should revise their bid at a given point in time or not. Therefore, in this paper we address various computational problems that are relevant from the bidder's perspective. In particular, we introduce two bid evaluation metrics that can be used by bidders to determine whether any given bid can be a part of the winning allocation and explore their theoretical properties. Based on these metrics, we also develop efficient data structures and algorithms that provide comprehensive information about the current state of the auction at any time, which can help bidders in evaluating their bids and bidding strategies. Our approach uses exponential memory storage but provides fast incremental update for new bids, thereby facilitating bidder support for real-time iterative combinatorial auctions.
Research on group behavior has identified social loafing, i.e., the tendency of members to do less than their potential, as a particularly serious problem plaguing groups. Social Impact theory (SIT) helps explain social loafing in terms of two theoretical dimensions--the dilution effect (where an individual feels submerged in the group) and the immediacy gap (where an individual feels isolated from the group). In this study, which employed a controlled experiment, we investigated these dimensions of social loafing in the context of group decision making, using collocated and distributed teams of varying sizes. Our results--in line with SIT--indicate that small groups, signifying a small dilution effect, had increased individual contributions and better group outcomes compared to their larger counterparts. However,support for SIT's arguments about the immediacy gap was mixed: Members contributed visibly more when they were collocated, but no significant differences in group outcomes were evident. Regardless of dimension, the quality of the input (ideas generated) determined the quality of the output (decisions made). Also, contrary to the literature on brainstorming, having more ideas to work with resulted in poorer-quality decisions. This apparent paradox is explained using the notion of integrative complexity, which challenges conventional wisdom regarding the relationship between individual inputs and group outputs. The implications of these results for practice and research are examined.
This paper offers a systematic exploration of reputation mechanism design in trading environments with opportunistic sellers of commonly known cost and ability parameters, imperfect monitoring of a seller's actions, and two possible seller effort levels, one of which has no value to buyers. The objective of reputation mechanisms in such pure moral hazard settings is to induce sellers to exert high effort as often as possible. I study the impact of various mechanism parameters (such as the granularity of solicited feedback, the format of the public reputation profile, the policy regarding missing feedback, and the rules for admitting new sellers) on the resulting market efficiency. I find that maximum efficiency is bounded away from the hypothetical first- best case where sellers can credibly precommit to full cooperation by a factor that is related to the probability that cooperating sellers may receive unfair bad ratings. Furthermore, maximum efficiency is independent of the length of past history summarized in a seller's public reputation profile. I apply my framework to a simplified model of eBay's feedback mechanism and conclude that, in pure moral hazard settings, eBay's simple mechanism is capable of inducing the maximum theoretical efficiency independently of the number of recent ratings that are being summarized in a seller's profile. I derive optimal policies for dealing with missing feedback and easy online identity changes. Finally, I show that if the number of buyers is large, the results obtained in the monopoly case are also approximately valid in settings where multiple sellers of different reputations simultaneously offer auctions for identical goods.
We consider a new variety of sequential information gathering problems that are applicable for Web-based applications in which data provided as input may be distorted by the system user, such as an applicant for a credit card. We propose two methods to compensate for input distortion. The first method, termed knowledge base modification, considers redesigning the knowledge base of an expert system to best account for distortion in the input provided by the user. The second method, termed input modification, modifies the input directly to account for distortion and uses the modified input in the existing (unmodified) knowledge base of the system. These methods are compared with an approach where input noise is ignored. Experimental results indicate that both types of modification substantially improve the accuracy of recommendations, with knowledge base modification outperforming input modification in most cases. Knowledge base modification is, however, more computationally intensive than input modification. Therefore, when computational resources are adequate, the knowledge base modification approach is preferred; when such resources are very limited, input modification may be the only viable alternative.
Increasingly, scholars and practitioners acknowledge that information technology (IT) human capital is a strategic resource and that its effective management represents a significant organizational capability. We use configurational theory to examine organizational practices related to the management of IT human capital. In contrast to much prior work in IT human resource management (HRM) that is focused at the individual level, our inquiry is focused at the organizational level of analysis. Building on strategic human resource management (SHRM) research in general and research on the management of IT professionals in particular, we examine the broad question: Are different configurations of IT HRM practices associated with different IT staff turnover rates? A multidimensional view of IT HRM practices is presented, based on prior IT and SHRM literature. We formalize hypotheses regarding the relationship of turnover with configurations of IT HRM practices grounded in prior theory and empirical research. Based on survey responses from 106 organizations, IT HRM dimensions and configurations are derived and the hypotheses are tested. A five-configuration solution, obtained via cluster analysis, includes two contrasting configurations consistent with two archetypes found in the prior literature. Specifically, the configuration with a human capital focus has lower turnover than the task-focused configuration, providing support for our first hypothesis. Although the hypothesis on intermediate configurations and their relationship with turnover is not supported, we discover and interpret three additional configurations that embody patterns of practices with unique emphases. Theoretical and practical implications of the findings are discussed.
The need to ensure reliability of data in information systems has long been recognized. However, recent accounting scandals and the subsequent requirements enacted in the Sarbanes-Oxley Act have made data reliability assessment of critical importance to organizations, particularly for accounting data. Using the accounting functions of management information systems as a context, this paper develops an interdisciplinary approach to data reliability assessment. Our work builds on the literature in accounting and auditing, where reliability assessment has been a topic of study for a number of years. While formal probabilistic approaches have been developed in this literature, they are rarely used in practice. The research reported in this paper attempts to strike a balance between the informal, heuristic-based approaches used by auditors and formal, probabilistic reliability assessment methods. We develop a formal, process-oriented ontology of an accounting information system that defines its components and semantic constraints. We use the ontology to specify data reliability assessment requirements and develop mathematical-model-based decision support methods to implement these requirements. We provide preliminary empirical evidence that the use of our approach improves the efficiency and effectiveness of reliability assessments. Finally, given the recent trend toward specifying information systems using executable business process models (e.g., business process execution language), we discuss opportunities for integrating our process-oriented data reliability assessment approach--developed in the accounting context--in other IS application contexts.
The sharing of databases either within or across organizations raises the possibility of unintentionally revealing sensitive relationships contained in them. Recent advances in data-mining technology have increased the chances of such disclosure. Consequently, firms that share their databases might choose to hide these sensitive relationships prior to sharing. Ideally, the approach used to hide relationships should be impervious to as many data-mining techniques as possible, while minimizing the resulting distortion to the database. This paper focuses on frequent item sets, the identification of which forms a critical initial step in a variety of data-mining tasks. It presents an optimal approach for hiding sensitive item sets, while keeping the number of modified transactions to a minimum. The approach is particularly attractive as it easily handles databases with millions of transactions. Results from extensive tests conducted on publicly available real data and data generated using IBM's synthetic data generator indicate that the approach presented is very effective, optimally solving problems involving millions of transactions in a few seconds.
We study the problem of optimally allocating effort between software construction and debugging. As construction proceeds, new errors are introduced into the system. The objective is to deliver a system of the highest possible quality (fewest number of errors) subject to the constraint that N system modules are constructed in a specified duration T. If errors are not corrected during construction, then further construction can produce errors at a faster rate. To curb the growth of errors, some of the effort must be taken away from construction and assigned to testing and debugging. A key finding of this model is that the practice of alternating between pure construction and pure debugging is suboptimal. Instead, it is desirable to concurrently construct and debug the system. We extend the above model to integrate decisions traditionally considered external such as the time to release the product to the market with those that are typically treated as internal such as the division of effort between construction and debugging. Results show that integrating these decisions can yield significant reduction in the overall cost. Also, when competitive forces are strong, it may be better to release a product early (with more errors) than late (with fewer errors). Thus, underestimating the cost of errors in the product may be better than overestimating the cost.
With advances in tracking and database technologies, firms are increasingly able to understand their customers and translate this understanding into products and services that appeal to them. Technologies such as collaborative filtering, data mining, and click-stream analysis enable firms to customize their offerings at the individual level. While there has been a lot of hype about web personalization recently, our understanding of its effectiveness is far from conclusive. Drawing on the elaboration likelihood model (ELM) literature, this research takes the view that the interaction between a firm and its customers is one of communicating a persuasive message to the customers driven by business objectives. In particular, we examine three major elements of a web personalization strategy: level of preference matching, recommendation set size, and sorting cue. These elements can be manipulated by a firm in implementing its personalization strategy. This research also investigates a personal disposition, need for cognition, which plays a role in assessing the effectiveness of web personalization. Research hypotheses are tested using 1,000 subjects in three field experiments based on a ring-tone download website. Our findings indicate the saliency of these variables in different stages of the persuasion process. Theoretical and practical implications of the findings are discussed.
Although much research has examined conscious use, which involves deliberate evaluation and decision making, we know less about automatic use, which occurs spontaneously with little conscious effort. The objective of this study is to compare two contrasting views in the literature on the nature of automatic use, namely, the habit/automaticity perspective (HAP) and the instant activation perspective (IAP). According to HAP, automatic use occurs because of the force of habit/automaticity without the formation of evaluations and intention; thus, past use--which is a proxy for habit/automaticity--is believed to weaken the evaluations-intention-usage relationship. In contrast, IAP posits that automatic use is simply an expedited form of conscious use; accordingly, as with conscious use, automatic use is still a function of evaluations/intention, so past use will not weaken the evaluations-intention-usage relationship. We tested the competing hypotheses using 2,075 cross-sectional and 990 longitudinal responses from actual users of two online news sites. Our results show that the evaluations-intention-usage relationship is generally weaker among heavier users than among lighter users. These findings suggest that with an increase in past use, user behavior becomes less evaluative and less intentional, in support of the argument that automatic use is driven more by habit/automaticity than by instant activation of cognitions. Overall, this research shows an initial piece of evidence of the moderating role of past use in postadoption phenomena, and it is expected to help the information systems community systematically investigate the important yet underexplored subject of habit/automaticity.
Digital goods lend themselves to versioning but also suffer from piracy losses. This paper develops a pricing model for digital experience goods in a segmented market and explores the optimality of sampling as a piracy-mitigating strategy. Consumers are aware of the true fit of an experience good to their tastes only after consumption, and as piracy offers an additional (albeit illegal) consumption opportunity, traditional segmentation findings from economics and sampling recommendations from marketing, need to be revisited. We develop a two-stage model of piracy for a market where consumers are heterogeneous in their marginal valuation for quality and their moral costs. In our model, some consumers pirate the product in the first stage allowing them to update their fit-perception that may result in re-evaluation of their buying/pirating decision in the second stage. We recommend distinct pricing and sampling strategies for underestimated and overestimated products and suggest that any potential benefits of piracy can be internalized through product sampling. Two counterintuitive results stand out. First, piracy losses are more severe for products that do not live up to their hype rather than for those that have been undervalued in the market, thus requiring a greater deterrence investment for the former, and second, unlike physical goods where sampling is always beneficial for underestimated products, sampling for digital goods is optimal only under narrowly defined circumstances due to the price boundaries created by both piracy and segmentation.
Internet firms frequently employ a two-stage approach to promotional activities. In Stage 1, they attract customers to their websites through advertising. In Stage 2, firms generate sales transactions or sales leads through their website. Comprehensive assessment of the promotional performance of pure online firms requires the study of Stage 1 and of Stage 2 jointly. In this paper we develop a joint two-stage conceptual and econometric model for assessing website promotion on three important dimensions: (1) how advertising response can be measured by linking media schedules to website log files; (2) how advertising and website characteristics jointly affect the desired system outcome of the promotion; and (3) whether the joint investigation of advertising response and desired system outcomes is essential to assess the results of website promotion. Three general findings follow from application of our model to a pure online firm's campaign to generate sales leads through print advertising. First, advertising and website characteristics affect sales leads in different ways. A characteristic may influence sales leads directly, or indirectly, or both. Second, assessing advertising effectiveness in an online environment may not require costly survey research data. Instead, secondary data available from website log files may be used for such assessment. Third, the interaction between the first and second stages of our two-stage model can lead to misspecifications that produce misleading inferences. This occurs because the unobserved characteristics in generating website visits and sales leads may be correlated.
This study examines the nature and role of Psychological Contract Violation (PCV) in online marketplaces, a critical factor that has been largely overlooked by previous research. Applied to buyer-seller relationships, PCV is defined as a buyer's perception of having being treated wrongly regarding the terms of an exchange agreement with an individual seller. PCV with individual sellers is proposed as a formative first-order construct driven by the occurrence of fraud, product misrepresentation, contract default, delivery delay, and failure to follow product guarantees and payment policies. PCV with an individual seller is proposed to prompt a generalized perception of PCV with the entire community of sellers in a marketplace. PCV with the community of sellers is hypothesized to negatively affect buyer transaction behavior in a marketplace by directly impacting transaction intentions, price premiums, trust, perceived risk, and the perceived effectiveness of institutional structures. PCV is also hypothesized to act as a moderator, transforming the buyers' initial trust-based mindset to one more centered on perceived risk. Finally, PCV is hypothesized to attenuate the positive impact of trust on transaction intentions, while reinforcing the negative impact of perceived risk on transaction intentions. It is also proposed to attenuate the impact of the perceived effectiveness of institutional structures on trust, while strengthening its negative effect on perceived risk. As a means of preventing PCV, the buyers' positive experience and the sellers' favorable past performance are hypothesized to make PCV with the community of sellers less likely. A combination of primary and secondary longitudinal data from 404 buyers in eBay's and Amazon's online auctions support the proposed hypotheses, validating PCV as a central element of buyer-seller relationships in online marketplaces. Interestingly, ex post facto results show that buyers with higher perceptions of PCV with the community of sellers are less likely to experience PCV with an individual seller in the future. Implications for buyer-seller relationships in online marketplaces and the PCV literature are discussed. Also discussed is how the increasing number of buyers who experience PCV in online marketplaces extends the literature that has been largely developed based on buyers who had not experienced PCV.
Although information systems (IS) problem solving involves knowledge of both the IS and application domains, little attention has been paid to the role of application domain knowledge. In this study, which is set in the context of conceptual modeling, we examine the effects of both IS and application domain knowledge on different types of schema understanding tasks: syntactic and semantic comprehension tasks and schema-based problem-solving tasks. Our thesis was that while IS domain knowledge is important in solving all such tasks, the role of application domain knowledge is contingent upon the type of understanding task under investigation.We use the theory of cognitive fit to establish theoretical differences in the role of application domain knowledge among the different types of schema understanding tasks. We hypothesize that application domain knowledge does not influence the solution of syntactic and semantic comprehension tasks for which cognitive fit exists, but does influence the solution of schema-based problem-solving tasks for which cognitive fit does not exist.To assess performance on different types of conceptual schema understanding tasks, we conducted a laboratory experiment in which participants with high- and low-IS domain knowledge responded to two equivalent conceptual schemas that represented high and low levels of application knowledge (familiar and unfamiliarapplication domains). As expected, we found that IS domain knowledge is important in the solution of all types of conceptual schema understanding tasks in both familiar and unfamiliar applications domains, and that the effect of application domain knowledge is contingent on task type. Our findings for the EER model were similar to those for the ER model. Given the differential effects of application domain knowledge on different types of tasks, this study highlights the importance of considering more than one application domain in designing future studies on conceptual modeling.
Although its popularity is widespread, the Web is well known for one particular drawback: its frequent delay when moving from one page to another. This experimental study examined whether delay and two other website design variables (site breadth and content familiarity) have interaction effects on user performance,attitudes, and behavioral intentions. The three experimental factors (delay, familiarity, and breadth) collectively impact the cognitive costs and penalties that users incur when making choices in their search for target information. An experiment was conducted with 160 undergraduate business majors in a completely counterbalanced, fully factorial design that exposed them to two websites and asked them to browse the sites for nine pieces of information. Results showed that all three factors have strong direct impacts on performance and user attitudes,in turn affecting behavioral intentions to return to the site, as might be expected. A significant three-way interaction was found between all three factors indicating that these factors not only individually impact a user's experiences with a website, but also act in combination to either increase or decrease the costs a user incurs. Two separate analyses support an assertion that attitudes mediate the relationship of the three factors on behavioral intentions. The implications of these results for both researchers and practitioners are discussed. Additional research is needed to discover other factors that mitigate or accentuate the effects of delay, other effects of delay, and under what amounts of delay these effects occur.
During the early phase of systems development, systems analysts often conceptualize the domain under study and represent it in one or more conceptual models. One of the most important, yet elusive roles of conceptual models is to increase analyst understanding of a domain. In this paper, we evaluate the ability of the good decomposition model (GDM) (Wand and Weber 1990) to explain the degree to which conceptual models communicate meaning about a domain to analysts. We address the question, Do unified modeling language (UML) analysis diagrams that manifest better decompositions increase analysts understanding of a domain? GDM defines five conditions (minimality, determinism, losslessness, weak coupling, and strong cohesion) deemed necessary to decompose a domain in such a way that the resulting model communicates meaning about the domain effectively. In our evaluation, we operationalized each of these conditions in a set of UML diagrams and tested participant understanding of those diagrams. Our results lend support to GDM across measures of actual understanding. However, the impact on participant perceptions of their understanding was equivocal.
Information systems and the Internet have facilitated the creation of used-product markets that feature a dramatically wider selection, lower search costs, and lower prices than their brick-and-mortar counterparts do. The increased viability of these used-product markets has caused concern among content creators and distributors, notably the Association of American Publishers and Author's Guild, who believe that used-product markets will significantly cannibalize new product sales.This proposition, while theoretically possible, is based on speculation as opposed to empirical evidence. In this paper, we empirically analyze the degree to which used products cannibalize new-product sales for books'one of the most prominent used-product categories sold online. To do this, we use a unique data set collected from Amazon.com's new and used book marketplaces to measure the degree to which used products cannibalize new-product sales. We then use these estimates to measure the resulting first-order changes in publisher welfare and consumer surplus.Our analysis suggests that used books are poor substitutes for new books for most of Amazon's customers.The cross-price elasticity of new-book demand with respect to used-book prices is only 0.088. As a result, only16% of used-book sales at Amazon cannibalize new-book purchases. The remaining 84% of used-book sales apparently would not have occurred at Amazon's new-book prices. Further, our estimates suggest that this increase in book readership from Amazon's used-book marketplace increases consumer surplus by approximately $67.21 million annually. This increase in consumer surplus, together with an estimated $45.05 million loss in publisher welfare and a $65.76 million increase in Amazon's profits, leads to an increase in total welfare to society of approximately $87.92 million annually from the introduction of used-book markets at Amazon.com.
While privacy is a highly cherished value, few would argue with the notion that absolute privacy is unattainable. Individuals make choices in which they surrender a certain degree of privacy in exchange for outcomes that are perceived to be worth the risk of information disclosure. This research attempts to better understand the delicate balance between privacy risk beliefs and confidence and enticement beliefs that influence the intention to provide personal information necessary to conduct transactions on the Internet. A theoretical model that incorporated contrary factors representing elements of a privacy calculus was tested using data gathered from 369 respondents. Structural equations modeling (SEM) using LISREL validated the instrument and the proposed model. The results suggest that although Internet privacy concerns inhibit e-commerce transactions, the cumulative influence of Internet trust and personal Internet interest are important factors that can outweigh privacy risk perceptions in the decision to disclose personal information when an individual uses the Internet. These findings provide empirical support for an extended privacy calculus model.
The theory of incomplete contracts has been used to study the relationship between buyers and suppliers following the deployment of modern information technology to facilitate coordination between them. Previous research has sought to explain anecdotal evidence from some industries on the recent reduction in the number of suppliers selected to do business with buyers by appealing to relationship-specific costs that suppliers may incur. In contrast, this paper emphasizes that information technology enables greater completeness of buyer-supplier contracts through more economical monitoring of additional dimensions of supplier performance. The number of terms included in the contract is an imperfect substitute for the number of suppliers. Based on this idea, alternative conditions are identified under which increased use of information technology leads to a reduction in the number of suppliers without invoking relationship-specific costs. We find that a substantial increase in contract completeness due to reduced cost of information technology could increase the cost per supplier even though the cost of coordination and the cost per term monitored decrease. Such an increase in the cost per supplier leads to a reduction in the number of suppliers with whom the buyer chooses to do business. Similarly, we find that if coordination cost is reduced when more information technology is deployed so that the number of suppliers in the buyer's pool increases substantially, the buyer might choose to make the supplier contracts less complete, instead relying on a more market-oriented approach to finding a supplier with good fit.
This paper reports an analysis of the proportion of faculty publishing articles in premier business journals (i.e., the ratio of authors of premier business journal articles to total faculty of a discipline) across the disciplines of accounting, finance, management, marketing, and information systems (IS) for the years 19942003. This analysis revealed that over this period the management discipline had on average the highest proportion of faculty publishing in premier journals (12.7 authors per 100 management faculty), followed by finance (9.4 authors per 100 faculty), marketing (9.2 authors per 100 faculty), IS (5.5 authors per 100 faculty), and accounting (4.8 authors per 100 faculty). A further analysis examined these ratios for the different disciplines over time, finding that the ratios of authors to faculty have actually decreased for the disciplines of marketing and IS over this time period but have remained stable for the disciplines of accounting, management, and finance. Given steady growth in faculty size of all disciplines, the proportion of faculty publishing articles in premier journals in 2003 for all disciplines is lower than their 10-year averages, with IS having the lowest proportion in 2003. A sensitivity analysis reveals that without substantial changes that would allow more IS faculty to publish in the premier journals (e.g., by increasing publication cycles, number of premier outlets, and so on), IS will continue to lag far below the average of other disciplines. The implications of these findings for IS researchers, for institutions and administrators of IS programs, and for the IS academic discipline are examined. Based on these implications, recommendations for the IS discipline are presented.
This study contributes to the growing body of literature on the value of enterprise resource planning (ERP)investments at the firm level. Using an organization integration lens that takes into account investments in complementary resources as well as an options thinking logic about the value of an ERP platform, we argue that not all ERP purchases have the same potential impact at the firm level due to ERP project decisions made at the time of purchase. Based on a sample of 116 investment announcements in United State based firms between 1997 and 2001, we find support for our hypotheses that ERP projects with greater functional scope (two or more value-chain modules) or greater physical scope (multiple sites) result in positive, higher shareholder returns. Furthermore, the highest increases in returns (3.29%) are found for ERP purchases with greater functional scope and greater physical scope; negative returns are found for projects with lesser functional scope and lesser physical scope. These findings provide empirical support for prior theory about the organizational integration benefits of ERP systems, the contribution of complementary resource investments to the business value of IT investments, and the growth options associated with IT platform investments. The article concludes with implications of our firm-level findings for this first wave of enterprise systems.
We have come to a stage when information technology (IT) innovations have permeated every walk of life. Many new technologies can be used for many different purposes and in different contexts other than the workplace. The current study attempts to understand individual adoption of IT innovations that are used beyond work settings. We define a new class of IT innovations called multipurpose information appliances, which are personal, universally accessible, and multipurpose. The ubiquitous nature of these appliances has led to a constant permeability between the separate contexts of social life. An adoption model that reflects the unique characteristics and usage contexts of multipurpose information appliances was developed. The model consists of five sets of adoption factors and was tested using data collected on mobile data services adoption.Our findings show that the determinants of multipurpose information appliance adoption decisions are not only different from those in the workplace, but are also dependent on the nature of the target technology and its usage context. Theoretical and practical implications of the findings are discussed.
What differentiates successful from unsuccessful open source software projects? This paper develops and tests a model of the impacts of license restrictiveness and organizational sponsorship on two indicators of success: user interest in, and development activity on, open source software development projects. Using data gathered from Freshmeat.net and project home pages, the main conclusions derived from the analysis are that (1) license restrictiveness and organizational sponsorship interact to influence user perceptions of the likely utility of open source software in such a way that users are most attracted to projects that are sponsored by nonmarket organizations and that employ nonrestrictive licenses, and (2) licensing and sponsorship address complementary developer motivations such that the influence of licensing on development activity depends on what kind of organizational sponsor a project has. Theoretical and practical implications are discussed, and the paper outlines several avenues for future research.
A trust-assuring argument refers to "a claim and its supporting statements used in an Internet store to address trust-related issues." Although trust-assuring arguments often appear in Internet stores, little research has been conducted to understand their effects on consumer trust in an Internet store. The goals of this study are (1) to investigate whether or not the provision of trust-assuring arguments on the Web site of an Internet store increase consumer trust in that Internet store and (2) to identify the most effective form of trust-assuring arguments to provide guidelines for their implementation. Toulmin's (1958) model of argumentation is proposed as a basis to identify the elements of an argument and to strengthen the effects of trust-assuring arguments on consumer trust in an Internet store. Based on Toulmin's (1958) model of argumentation, three elements of arguments that commonly appear in daily communication; namely, claim, data, and backing, are identified. Data refers to the grounds for a claim, while backing is used for providing reasons for why the data should be accepted. By combining these three elements, three forms of trust assuring arguments (claim only, claim plus data, and claim plus data and backing) are developed. The effects of these three forms of trust-assuring arguments on consumer trust in an Internet store are tested by comparing them to a no trust-assuring argument condition in a laboratory experiment with 112 participants. The results indicate (1) providing trust-assuring arguments that consist of claim plus data or claim plus data and backing increases consumers' trusting belief but displaying arguments that contain claim only does not and (2) trust-assuring arguments that include claim plus data and backing lead to the highest level of trusting belief among the three forms of arguments examined in this study. Based on the results, we argue that Toulmin's (1958) model of argumentation is an effective basis for Web site designers to develop convincing trust-assuring arguments and to improve existing trust-assuring arguments in Internet stores.
A burning question for information systems (IS) researchers and practitioners is whether and how IT can build a competitive advantage in turbulent environments. To address this question, this study focuses on the business process level of analysis and introduces the construct of IT leveraging competence-the ability to effectively use IT functionalities. This construct is conceptualized in the context of new product development (NPD). IT leveraging competence is shown to indirectly influence competitive advantage in NPD through two key mediating links: functional competencies (the ability to effectively execute operational NPD processes) and dynamic capabilities (the ability to reconfigure functional competencies to address turbulent environments). Environmental turbulence is also shown to moderate the process by which IT leveraging competence influences competitive advantage in NPD. Empirical data were collected from 180 NPD managers. Through the construct of IT leveraging competence, the study shows that the effective use of IT functionalities, even generic functionalities, by business units can help build a competitive advantage. The study also shows that the strategic effect of IT leveraging competence is more pronounced in higher levels of environmental turbulence. This effect is not direct: It is fully mediated by both dynamic capabilities and functional competencies. Taken together, these findings suggest that IS researchers should look beyond the direct effects of firm-level IT infrastructures and focus their attention on how business units can leverage IT functionalities to better reconfigure and execute business processes. In turbulent environments, focusing on these aspects is even more vital.
A tariff is the total charge payable by a customer for services provided. We study the design of tariffs for a telecommunications service provider. We develop an economic model that captures the negative externalities of the network and the diversity of customers. The tariff is designed so that it reflects the expected response of different customers and the system congestion it would induce. We study a simple tariff structure in wide use by mobile phone carriers-a menu of fixed-up-to (FUT) plans like fixed access fee $35 up to 300 minutes, and $0.40 per minute beyond the limit. We derive the optimal menu of FUT plans and show that such a simple FUT menu structure delivers as good performance to the monopolistic carrier as any nonlinear pricing schedule.; Sin, Raymond G.2 rsin@ust.hk; Siddarth, S.3 siddarth@usc.edu
Although DeLone, McLean, and others insist that system usage is a key variable in information systems research, the system usage construct has received little theoretical scrutiny, boasts no widely accepted definition, and has been operationalized by a diverse set of unsystematized measures. In this article, we present a systematic approach for reconceptualizing the system usage construct in particular nomological contexts. Comprising two stages, definition and selection, the approach enables researchers to develop clear and valid measures of system usage for a given theoretical and substantive context. The definition stage requires that researchers define system usage and explicate its underlying assumptions. In the selection stage, we suggest that system usage be conceptualized in terms of its structure and function. The structure of system usage is tripartite, comprising a user, system, and task, and researchers need to justify which elements of usage are most relevant for their study. In terms of function, researchers should choose measures for each element (i.e., user, system, and/or task) that tie closely to the other constructs in the researcher's nomological network. To provide evidence of the viability of the approach, we undertook an empirical investigation of the relationship between system usage and short-run task performance in cognitively engaging tasks. The results support the benefits of the approach and show how an inappropriate choice of usage measures can lead researchers to draw opposite conclusions in an empirical study. Together, the approach and the results of the empirical investigation suggest new directions for research into the nature of system usage, its antecedents, and its consequences.; Tuzhilin, Alexander2 atuzhili@stern.nyu.edu; Rong Zheng3 rzheng@ust.hk
Because of challenges often experienced when deploying software, many firms have embarked on software process improvement (SPI) initiatives. Critical to the success of these initiatives is the transfer of knowledge across individuals who occupy a range of roles in various organizational units involved in software production. Prior research suggests that a portfolio of different mechanisms, employed frequently, can be required for effective knowledge transfer. However, little research exists that examines under what situations differing portfolios of mechanisms are selected. Further, it is not clear how effective different portfolio designs are. In this study, we conceptualize knowledge transfer portfolios in terms of their composition (the types of mechanisms used) and their intensity (the frequency with which the mechanisms are utilized). We hypothesize the influence of organizational design decisions on the composition and intensity of knowledge transfer portfolios for SPI. We then posit how the composition and intensity of knowledge transfer portfolios affect performance improvement. Our findings indicate that a more intense portfolio of knowledge transfer mechanisms is used when the source and recipient are proximate, when they are in a hierarchical relationship, or when they work in different units. Further, a source and recipient select direction-based portfolios when they are farther apart, in a hierarchical relationship, or work in different units. In terms of performance, our results reveal that the fit between the composition and intensity of the knowledge transfer portfolio influences the recipient's performance improvement. At lower levels of intensity direction-based portfolios are more effective, while at higher levels of intensity routine-based portfolios yield the highest performance improvement. We discuss the implications of our findings for researchers and for managers who want to promote knowledge transfer to improve software processes in their organizations.
To respond to growing concerns about privacy of personal information, organizations that use their customers' records in data-mining activities are forced to take actions to protect the privacy of the individuals involved. A common practice for many organizations today is to remove identity-related attributes from the customer records before releasing them to data miners or analysts. We investigate the effect of this practice and demonstrate that many records in a data set could be uniquely identified even after identity-related attributes are removed. We propose a perturbation method for categorical data that can be used by organizations to prevent or limit disclosure of confidential data for identifiable records when the data are provided to analysts for classification, a common data-mining task. The proposed method attempts to preserve the statistical properties of the data based on privacy protection parameters specified by the organization. We show that the problem can be solved in two phases, with a linear programming formulation in Phase I (to preserve the first-order marginal distribution), followed by a simple Bayes-based swapping procedure in Phase II (to preserve the joint distribution). Experiments conducted on several real-world data sets demonstrate the effectiveness of the proposed method.; Dennis, Alan R.2 ardennis@indiana.edu
For online marketplaces to succeed and prevent a market of lemons, their feedback mechanism (reputation system) must differentiate among sellers and create price premiums for trustworthy sellers as returns to their reputation. However, the literature has solely focused on numerical (positive and negative) feedback ratings, alas ignoring the role of feedback text comments. These text comments are proposed to convey useful reputation information about a seller's prior transactions that cannot be fully captured with crude numerical ratings. Building on the economics and trust literatures, this study examines the rich content of feedback text comments and their role in building a buyer's trust in a seller's benevolence and credibility. In turn, benevolence and credibility are proposed to differentiate among sellers by influencing the price premiums that a seller receives from buyers. This paper utilizes content analysis to quantify over 10,000 publicly available feedback text comments of 420 sellers in eBay's online auction marketplace, and to match them with primary data from 420 buyers that recently transacted with these 420 sellers. These dyadic data show that evidence of extraordinary past seller behavior contained in the sellers' feedback text comments creates price premiums for reputable sellers by engendering buyer's trust in the sellers' benevolence and credibility (controlling for the impact of numerical ratings). The addition of text comments and benevolence helps explain a greater variance in price premiums (R = 50%) compared to the existing literature (R =20%-30%). By showing the economic value of feedback text comments through trust in a seller's benevolence and credibility, this study helps explain the success of online marketplaces that primarily rely on the text comments (versus crude numerical ratings) to differentiate among sellers and prevent a market of lemon sellers. By integrating the economics and trust literatures, the paper has theoretical and practical implications for better understanding the nature and role of feedback mechanisms, trust building, price premiums, and seller differentiation in online marketplaces.
Previous research shows that synchronous text discussion through group support systems (GSS) can improve the exchange of information within teams, but this improved information exchange usually does not improve decisions because participants fail to process the new information they receive. This study examined one potential cause for this failure: Dual-task interference caused by the need to concurrently process new information from others while also contributing one's own information to the discussion. Although prior research argues that dual-task interference should be minimal, we found that it significantly reduced participants' information processing and led to lower decision quality. The effect sizes were large, suggesting that dual-task interference is one of a handful of major factors that exert the greatest influence on information processing and decision-making performance. We believe that these results call for an increased emphasis on and understanding of the cognitive underpinnings of GSS and virtual team decision making.
Prior research suggests that supply chain collaboration has enabled companies to compete more efficiently in a global economy. We investigate a class of collaboration software for product design and development called collaborative product commerce (CPC). Drawing on prior research in media richness theory and organizational science, we develop a theoretical framework to study the impact of CPC on product development. Based on data collected from 71 firms, we test our research hypotheses on the impact of CPC on product design quality, design cycle time, and development cost. We find that CPC implementation is associated with greater collaboration among product design teams. This collaboration has a significant, positive impact on product quality and reduces cycle time and product development cost. Further analyses reveal that CPC implementation is associated with substantial cost savings that can be attributed to improvements in product design quality, design turnaround time, greater design reuse, and lower product design documentation and rework costs.
Recommendations and consumer reviews are universally acknowledged as significant features of a business-to-consumer website. However, because of the well-documented obstacles to measuring the causal impact of these artifacts, there is still a lack of empirical evidence demonstrating their influence on two important outcome variables in the shopping context: perceived usefulness and social presence. To test the existence of a causal link between information technology (IT)-enabled support for the provision of recommendations and consumer reviews on the usefulness and social presence of the website, this study employs a novel approach to generate the experimental conditions by filtering the content of Amazon.com in real time. The results show that the provision of recommendations and consumer reviews increases both the usefulness and social presence of the website.
This study examines the role of information quality in the success of initial phase interorganizational (I-O) data exchanges. We propose perceived information quality (PIQ) as a factor of perceived risk and trusting beliefs, which will directly affect intention to use the exchange. The study also proposes that two important system design factors--control transparency and outcome feedback--will incrementally influence PIQ. An empirical test of the model demonstrated that PIQ predicts trusting beliefs and perceived risk, which mediate the effects of PIQ on intention to use the exchange. Thus, PIQ constitutes an important indirect factor influencing exchange adoption. Furthermore, control transparency had a significant influence on PIQ, while outcome feedback had no significant incremental effect over that of control transparency. The study contributes to the literature by demonstrating the important role of PIQ in I-O systems adoption and by showing that information cues available to a user during an initial exchange session can help build trusting beliefs and mitigate perceived exchange risk. For managers of I-O exchanges, the study implies that building into the system appropriate control transparency mechanisms can increase the likelihood of exchange success.
Workflow technology has become a standard solution for managing increasingly complex business processes. Successful business process management depends on effective workflow modeling and analysis. One of the important aspects of workflow analysis is the data-flow perspective because, given a syntactically correct process sequence, errors can still occur during workflow execution due to incorrect data-flow specifications. However, there have been only scant treatments of the data-flow perspective in the literature and no formal methodologies are available for systematically discovering data-flow errors in a workflow model. As an indication of this research gap, existing commercial workflow management systems do not provide tools for data-flow analysis at design time. In this paper, we provide a data-flow perspective for detecting data-flow anomalies such as missing data, redundant data, and potential data conflicts. Our data-flow framework includes two basic components: data-flow specification and data-flow analysis; these components add more analytical rigor to business process management.
A variety of information technology (IT) artifacts, such as those supporting reputation management and digital archives of past interactions, are commonly deployed to support online communities. Despite their ubiquity, theoretical and empirical research investigating the impact of such IT-based features on online community communication and interaction is limited. Drawing on the social psychology literature, we describe an identity-based view to understand how the use of IT-based features in online communities is associated with online knowledge contribution. Specifically, the use of four categories of IT artifacts--those supporting virtual co-presence, persistent labeling, self-presentation, and deep profiling--is proposed to enhance perceived identity verification, which thereafter promotes satisfaction and knowledge contribution. To test the theoretical model, we surveyed more than 650 members of two online communities. In addition to the positive effects of community IT artifacts on perceived identity verification, we also find that perceived identity verification is strongly linked to member satisfaction and knowledge contribution. This paper offers a new perspective on the mechanisms through which IT features facilitate computer-mediated knowledge sharing, and it yields important implications for the design of the supporting IT infrastructure.
It can be expensive to acquire the data required for businesses to employ data-driven predictive modeling--for example, to model consumer preferences to optimize targeting. Prior research has introduced active-learning policies for identifying data that are particularly useful for model induction, with the goal of decreasing the statistical error for a given acquisition cost (error-centric approaches). However, predictive models are used as part of a decision-making process, and costly improvements in model accuracy do not always result in better decisions. This paper introduces a new approach for active data acquisition that specifically targets decision making. The new decision-centric approach departs from traditional active learning by placing emphasis on acquisitions that are more likely to affect decision making. We describe two different types of decision-centric techniques. Next, using direct-marketing data, we compare various data-acquisition techniques. We demonstrate that strategies for reducing statistical error can be wasteful in a decision-making context, and show that one decision-centric technique in particular can improve targeting decisions significantly. We also show that this method is robust in the face of decreasing quality of utility estimations, eventually converging to uniform random sampling, and that it can be extended to situations where different data acquisitions have different costs. The results suggest that businesses should consider modifying their strategies for acquiring information through normal business transactions. For example, a firm such as Amazon.com that models consumer preferences for customized marketing may accelerate learning by proactively offering recommendations--not merely to induce immediate sales, but for improving recommendations in the future.
Researchers have widely postulated that the adoption of information technology (IT) products enhances global competitiveness and production efficiency as successful technological innovation replaces and improves traditional inputs and modes of production. This study suggests that when IT products are traded across borders, IT investment in an economy has a positive influence on the productivity of its import partner country. We provide empirical evidence for the positive effect of global IT diffusion on productivity through international trading of IT products. The results show a positive effect of foreign IT transfer on the recipient country's productivity. In addition, we find that the effect of transferred IT is only significant when the source country is an IT-intensive or hi-tech export country. The results and implications are robust, even controlling for other important factors such as openness, innovative capacity, and IT infrastructure in addition to the transferred IT. Finally, a panel cointegration test--a recently developed advanced econometric method--is used to address the common problems of spurious relations that arise in regressions with nonstationary time-series data.
The ability to collect and disseminate individually identifiable microdata is becoming increasingly important in a number of arenas. This is especially true in health care and national security, where this data is considered vital for a number of public health and safety initiatives. In some cases legislation has been used to establish some standards for limiting the collection of and access to such data. However, all such legislative efforts contain many provisions that allow for access to individually identifiable microdata without the consent of the data subject. Furthermore, although legislation is useful in that penalties are levied for violating the law, these penalties occur after an individual's privacy has been compromised. Such deterrent measures can only serve as disincentives and offer no true protection. This paper considers security issues involved in releasing microdata, including individual identifiers. The threats to the confidentiality of the data subjects come from the users possessing statistical information that relates the revealed microdata to suppressed confidential information. The general strategy is to recode the initial data, in which some subjects are safe and some are at risk, into a data set in which no subjects are at risk. We develop a technique that enables the release of individually identifiable microdata in a manner that maximizes the utility of the released data while providing preventive protection of confidential data. Extensive computational results show that the proposed method is practical and viable and that useful data can be released even when the level of risk in the data is high.
This paper examines the antecedents and consequences of Internet use in the procurement process. Drawing upon the resource-based view (RBV) of the firm and the technology, organization, and environment framework, we develop an integrative model that examines the antecedents and consequences of Internet use in two stages--the search stage and the order initiation and completion (OIC) stage--of the procurement process. The model enables us to deconstruct both the usage and the performance aspects of information technology (IT) in business processes, and to provide insights into the enablers of use and business value. The model is estimated with survey data from 412 firms. Our results suggest that while some resources, such as procurement-process digitization, influence Internet use in both the procurement stages, other resources, such as the diversity of organizational procurement knowledge, impact Internet use in only one stage. We also find that Internet use in the OIC stage has a more significant impact on procurement-process performance than use in search. This study extends the digital capabilities and firm performance literature in the context of electronic procurement. This study also contributes to the small but emerging stream of literature that investigates antecedents, the extent, and implications of IT use holistically.
Virtual communities are a significant source of information for consumers and businesses. This research examines how users value virtual communities and how virtual communities differ in their value propositions. In particular, this research examines the nature of trade-offs between information quantity and quality, and explores the sources of positive and negative externalities in virtual communities. The analyses are based on more than 500,000 postings collected from three large virtual investing-related communities (VICs) for 14 different stocks over a period of four years. The findings suggest that the VICs engage in differentiated competition as they face trade-offs between information quantity and quality. This differentiation among VICs, in turn, attracts users with different characteristics. We find both positive and negative externalities at work in virtual communities. We propose and validate that the key factor that determines the direction of network externalities is posting quality. The contributions of the study include the extension of our understanding of the virtual community evaluation by users, the exposition of competition between virtual communities, the role of network externalities in virtual communities, and the development of an algorithmic methodology to evaluate the quality (noise or signal) of textual data. The insights from the study provide useful guidance for design and management of VICs.
A significant amount of information systems (IS) research involves hypothesizing and testing for interaction effects. Chin et al. (2003) completed an extensive experiment using Monte Carlo simulation that compared two different techniques for detecting and estimating such interaction effects: partial least squares (PLS) with a product indicator approach versus multiple regression with summated indicators. By varying the number of indicators for each construct and the sample size, they concluded that PLS using product indicators was better (at providing higher and presumably more accurate path estimates) than multiple regression using summated indicators. Although we view the Chin et al. (2003) study as an important step in using Monte Carlo analysis to investigate such issues, we believe their results give a misleading picture of the efficacy of the product indicator approach with PLS. By expanding the scope of the investigation to include statistical power, and by replicating and then extending their work, we reach a different conclusion--that although PLS with the product indicator approach provides higher point estimates of interaction paths, it also produces wider confidence intervals, and thus provides less statistical power than multiple regression. This disadvantage increases with the number of indicators and (up to a point) with sample size. We explore the possibility that these surprising results can be explained by capitalization on chance. Regardless of the explanation, our analysis leads us to recommend that if sample size or statistical significance is a concern, regression or PLS with product of the sums should be used instead of PLS with product indicators for testing interaction effects.
As the IT workforce becomes global, it is increasingly important to understand the factors affecting IT workers' compensation in labor markets distributed across the globe. Ang et al. (2002) published the first in-depth analysis of compensation for IT professionals juxtaposing human capital endowment (education and experience) and institutional determinants (firm's size, industry, and sector) of compensation in the Singaporean economy. In this paper, we explore the influence of particular national economies on IT workers' compensation. We draw on research into the roots of wage differentials in labor economics and build on the Ang et al. research to present a multilevel analysis of IT workers' compensation in the United States, analyzing the U.S. Bureau of Labor Statistics' Current Population Survey (CPS) data for 1997, 2001, and 2003. We find that, while institutional differences in Singapore mattered only in conjunction with individual factors, in the U.S. institutional differences had a direct effect on IT workers' wages. As tightness of IT labor supply decreased in the United States in the early 2000s, the influence of a firm's size on wages became more pronounced. Also, female IT workers and workers without a college degree fared worse than their male and college-educated counterparts as the IT job market slowed down. We suggest that factors such as presence of job search friction, diversity in the educational system, geographical differences in cost of living, labor mobility, and shortages in IT labor supply vis--vis demand help explain the differences among countries. We conclude by outlining the implications of these findings for IT workers, firms, and policy makers.
Despite calls for improving current approaches to conceptualizing and measuring the construct of information system use, theoretical advances in this regard are still insufficient. The present paper proposes to expand the focus of existing conceptualizations that exclusively focus on technology interaction behaviors via the construct of IS use-related activity. Based on task-technology fit and activity theory, IS use-related activity is conceptualized as a second-order aggregate construct that comprises both technology interaction behaviors, as well as activities users undertake to adapt the task-technology-individual system. A multiple-indicators and multiple-causes analysis of data collected from 190 users in 21 organizations is found to support the proposed conceptualization.
How are business schools thinking about developing leaders for the emerging digital economy? Is there a set of core principles we can apply to thinking about the enabling potential of information technologies and their consequences for business and society? We present a business-centric framework and a technology-centric framework that together form a blueprint for answering these questions. The business-centric framework articulates three compelling reasons why information technology (IT) matters in business: (1) IT continually transform industry and society, (2) executive decisions about IT investments, governance, and strategy are critical to organizational success, and (3) deriving value from increasingly available data trails defines effective decision making in the digital economy. However, our conversations with the leadership of 45 business schools and our subsequent data indicate that business schools are challenged by effectively training future executives to think about these reasons and act on them as part of a forward-looking program of business education that is grounded in stable concepts. In response, the technology-centric framework provides a set of grounding concepts and stable principles about IT that have emerged over the last four decades, and leads to a natural set of consequences that can inform thinking about IT in business. We illustrate how these complementary frameworks--business and technology--can be combined to frame an educational program by outlining a set of key questions, by placing these questions in the context suggested by our frameworks, and by providing guidelines toward answering them. These questions also define a natural path for future research about IT in business and society that will lead to stronger intellectual foundations for the field and define future education that is better grounded in concepts and theories that emerge from academic research.
This paper considers a two-stage development problem for information goods with costless quality degradation. In our model, a seller of information goods faces customers that are heterogeneous with regard to both the marginal willingness to pay for quality and the outside opportunity. In the development stage, the seller determines the quality limit of the product. In the second stage, the seller's problem is to design the price schedule corresponding to different quality levels, taking into account production and distribution costs. We show that versioning is optimal for the seller when customers have multiple outside options or, more generally, convex reservation utilities. In addition, we show that in the optimal solution the seller discards both low-end and high-end customers. Among those that are served, the seller offers a continuum of (inferior) versions to customers with relatively low willingness to pay, and extracts full information rent from each of them. A common version with the quality limit is offered to the rest. We further prove that the seller should offer a single version when reservation utilities are either concave or linear. Through numerical experiments, we study the sensitivity of our results to changes in the cost structure and customer utilities.
This paper presents quantitative and qualitative results from a conference on IT teaching held in May of 2006 in Boston. Participants completed a survey in advance, and the conference consisted of presentations and interactive panel discussions. The conference revealed both heterogeneity and convergence across participants' course offerings, and grounds for both optimism and concern about the health and future of IT curricula within business schools. This paper highlights these tensions, synthesizes and extends data and discussions from the conference, and suggests open questions for faculty who teach IT.
Adaptive supply chain partnerships are a key factor in driving the ability of extended enterprise partners to achieve long-term goals in an environment characterized by disruptive environmental shifts. Adaptive extended enterprise arrangements allow participating enterprises to leverage their combined assets for collective exploration and exploitation. In the context of extended enterprises, where significant investments have been directed toward instituting common interfaces, this study examines the question: How does the use of standard electronic business interfaces (SEBIs) enable supply chain partnerships to become more adaptive? This study conceptualizes the use of SEBIs as a boundary-spanning mechanism that helps overcome boundaries that impede knowledge transfer between enterprises in supply chains. SEBIs enables partners to gain insight into their broader environments, enriching each partner's perspective (enhanced bridging). SEBIs also help strengthen the cooperative ties between partners, motivating each partner to adapt for collective gain (enhanced bonding). Our research model is empirically tested using data collected from 41 demand-side supply chain partnerships (between original equipment manufacturers (OEMs), distributors, and retailers) in the information technology (IT) industry The results show that collaborative information exchange (CIE) between supply chain partners mediates the relationship between use of SEBIs and mutual adaptation (MA) and adaptive knowledge creation between supply chain partners. Interestingly, the use of SEBIs is found to be directly associated with MA but only indirectly associated with adaptive knowledge creation. The study points out that the strategic impacts of SEBIs go well beyond the exchange of transaction information and process integration. It also shows that multilateral, quasi-open, and information exchange-and process linkage-oriented SEBIs can result in both bonding and bridging across supply chain partners without binding them inflexibly to specific partners. Based on the model and results, the study offers practical implications for how SEBIs should be developed, adopted, and used.
In many industries, Internet referral services, hosted either by independent third-party infomediaries or by manufacturers, serve as digitally enabled lead generators in electronic markets, directing consumer traffic to downstream retailers in a distribution network. This reshapes the extended enterprise from the traditional network of upstream manufacturers and downstream retailers to include midstream third-party and manufacturer-owned referral services in the supply chain. We model competition between retailers in a supply chain with such digitally enabled institutions and consider their impact on the optimal contracts among the manufacturer, referral intermediary, and the retailers. Offline, retailers face a higher customer discovery cost. In return, they can engage in price discrimination based on consumer valuations. Online, they save on the discovery costs but lose the ability to identify consumer valuations. This critical trade-off drives firms' equilibrium strategies. We derive the optimal contracts for different entities in the supply chain and highlight how these contracts change with the entry of independent and manufacturer-owned referral services. The establishment of a referral service is a strategic decision by the manufacturer. It leads to diversion of supply chain profit from a third-party infomediary to the manufacturer. Further, it enables the manufacturer to respond to an infomediary, by giving itself greater flexibility in setting the unit wholesale fee to the profit-maximizing level. Both third-party and manufacturer-sponsored referral services play a critical role in enabling retailers to discriminate across consumers' different valuations. Retailers use online referral services to screen out low-valuation consumers and sell only to high-valuation consumers in the online channel. Our model thus endogenously derives a correlation between consumer valuation and online purchase behavior. Finally, we show that under some circumstances, it is too costly for the manufacturer to eliminate the referral infomediary.
Organizations have not fully realized the benefits of interorganizational relationships (IORs) due to the lack of cross-enterprise process integration capabilities. Recently, interorganizational business process standards (IBPS) enabled by information technology (IT) have been suggested as a solution to help organizations overcome this problem. Drawing on three theoretical perspectives, i.e., the relational view of the firm, institutional theory, and organizational inertia theory, we propose three mechanismsrelational, influence, and inertialto explain the assimilation of IBPS in organizations. We theorize that these mechanisms will have differential effects on the assimilation of IBPS in dominant and nondominant firms. Using a cross-case analysis based on data from 11 firms in the high-tech industry, we found evidence to support our propositions that relational depth, relationship extendability, and normative pressure were important for dominant firms while relational specificity and influence mechanisms (coercive, mimetic, and normative pressures) were important for nondominant firms. Inertial mechanisms, i.e., ability and willingness to overcome resource and routine rigidities, were important for both dominant and nondominant firms.
There is unprecedented interest in digitally enabled extended enterprises that enable firms to gain access to specialized skills and capabilities globally Given this motivation, firms are unbundling their value chain processes and exploring new sourcing mechanisms. With the emergence of world-class skills and capabilities in offshore locations, new sourcing mechanisms have become available beyond traditional domestic insourcing and outsourcing. However, there is little systematic research examining how firms choose sourcing mechanisms for their business processes. This study views the digitally enabled extended enterprise as a complex system of business processes and examines how sourcing choices are made in such enterprises. It builds on the modular systems theory to posit that modularization of business processes and their underlying information technology (IT) support infrastructures are associated with the choice of sourcing mechanisms for the processes. The study tests this proposition in a sample of business process sourcing choices made by 93 medium and large U.S. firms. The results show that firms tend to choose domestic outsourcing for processes that are high in modularity and offshore outsourcing for processes that are low in modularity Further, when processes can be detached from a firm's IT infrastructure, firms tend to use offshore outsourcing. However, when processes are tightly coupled with underlying IT infrastructure, it may be infeasible to detach processes and execute them in remote locations. Implications for theory and practice are also discussed.
This special issue of Information Systems Research includes six papers that investigate the role of information technology for the management of the extended enterprise in the global economy. These papers contribute to our theoretical and practical understanding of how IT is restructuring occupations and industries and how firms can leverage IT for the management of their supply, production, and logistics and distribution networks.
This study examines how capabilities of information systems (IS) applications deployed in the context of interfirm relationships contribute to business performance. We propose that these capabilities augment the relational value that a firm derives from its business partnerschannel partners and customer enterprisesin the context of the distribution channel. Two cospecialized relational assets are considered as key to realization of relational valueknowledge sharing and process coupling. Hypotheses linking two IS capabilities (IS flexibility and IS integration) to the relational asset dimensions, and ultimately to firm performance, are proposed. The research model is tested based on data collected through a survey of business units of enterprises embedded in customer and channel partner ties in the high-tech and financial services industries. We find that IS integration with channel partners and customers contributes to both knowledge sharing and process coupling with both types of enterprise partners, whereas IS flexibility is a foundational capability that indirectly contributes to value creation in interfirm relationships by enabling greater IS integration with partner firms. We find that two types of relational assets are significantly associated with business performanceknowledge sharing with channel partners and process coupling with customerspointing to underlying mechanisms that differentially leverage resources of different types of channel partners. Implications for theory development and practice based on these findings are proposed.
Which service occupations are the most susceptible to global disaggregation? What are the factors and mechanisms that make service occupations amenable to global disaggregation? This research addresses these questions by building on previous work by Apte and Mason (1995) and Rai et al. (2006) that focuses on the unbundling of information and physical flows. We propose a theory of service disaggregation and argue that high information intensity makes an occupation more amenable to disaggregation because the activities in such occupations can be codified, standardized, and modularized. We empirically validate our theoretical model using data on more than 300 service occupations. We find that at the mean skill level, the information intensity of an occupation is positively associated with the disaggregation potential of that occupation, and the effect of information intensity on disaggregation potential is mediated by the modularizability of an occupation. We also find that skills moderate the effect of information intensity on service disaggregation. Furthermore, we study the patterns in U.S. employment and salary growth from 2000 to 2004. Contrary to popular perception, we do not find any adverse effect in terms of employment growth or salary growth for high information-intensity occupations at the mean skill level. Our findings show that high-skill occupations have experienced higher employment and salary growth than low-skill occupations at the mean level of information intensity Notably, high information-intensity occupations that require higher skill levels have experienced higher employment growth, though this employment growth is accompanied by a decline in salary growth. Occupations with a higher need for physical presence have also experienced higher employment growth and lower salary growth. Overall, these results imply that firms and managers need to consider the modularizability of occupations as they reallocate global resources to pursue cost and innovation opportunities. For individual workers, our results highlight the importance of continuous investments in human capital and skill acquisition because high information-intensity and high-skill occupations appear to be relatively less vulnerable to global disaggregation.
Because information technologies are often characterized by network effects, compatibility is an important issue. Although total network value is maximized when everyone operates in one compatible network, we find that the technology benefits for the users depend on vendor incentives, which are driven by the existence of de facto or de jure standards. In head-to-head competition, customers are better off letting a thousand flowers bloom, fostering fierce competition that results in a de facto standard if users prefer compatibility over individual fit, or a split market if fit is more important. In contrast, firms that sponsor these products are better off establishing an up-front, de jure standard to lessen the competitive effects of a network market. However, if a firm is able to enter the market first by choosing a proprietary/incompatible technology, it can use a divide-and-conquer strategy to increase its profit compared with head-to-head competition, even when there are no switching costs. When there is a first mover, the early adopters, who are locked in because of switching costs, never regret their decision to adopt, whereas the late adopters, who are not subject to switching costs, are exploited by the incumbent firm. In head-to-head competition, customers are unified in their preference for incompatibility when there is a first mover; late adopters prefer de jure compatibility because they bear the brunt of the first-mover advantage. This again underscores the interdependence of user net benefits and vendor strategies.
In this paper we pursue three main objectives: (1) to develop a model of an intermediated search market in which matching between consumers and firms takes place primarily via paid referrals; (2) to address the question of designing a suitable mechanism for selling referrals to firms; and (3) to characterize and analyze the firms' bidding strategies given consumers' equilibrium search behavior. To achieve these objectives we develop a two-stage model of search intermediaries in a vertically differentiated product market. In the first stage an intermediary chooses a search engine design that specifies to which extent a firm's search rank is determined by its bid and to which extent it is determined by the product offering's performance. In the second stage, based on the search engine design, competing firms place their open bids to be paid for each referral by the search engine. We find that the revenue-maximizing search engine design bases rankings on a weighted average of product performance and bid amount. Nonzero pure-strategy equilibria of the underlying discontinuous bidding game generally exist but are not robust with respect to noisy clicks in the system. We determine a unique nondegenerate mixed-strategy Nash equilibrium that is robust to noisy clicks, In this equilibrium firms of low product performance fully dissipate their rents, which are appropriated by the search intermediary and the firm with the better product. The firms' expected bid amounts are generally nonmonotonic in product performance and depend on the search engine design parameter. The intermediary's profit-maximizing design choice, by attributing a positive weight to the firms' bids, tends to obfuscate search results and reduce overall consumer surplus compared to the socially optimal design of fully transparent results ranked purely on product performance.
Internet-based interactive multimedia technologies enable online firms to employ a variety of formats to present and promote their products: They can use pictures, videos, and sounds to depict products, as well as give consumers the opportunity to try out products virtually. Despite the several previous endeavors that studied the effects of different product presentation formats, the functional mechanisms underlying these presentation methods have not been investigated in a comprehensive way. This paper investigates a model showing how these functional mechanisms (namely, vividness and interactivity) influence consumers' intentions to return to a website and their intentions to purchase products. A study conducted to test this model has largely confirmed our expectations: (1) both vividness and interactivity of product presentations are the primary design features that influence the efficacy of the presentations; (2) consumers' perceptions of the diagnosticity of websites, their perceptions of the compatibility between online shopping and physical shopping, and their shopping enjoyment derived from a particular online shopping experience jointly influence consumers' attitudes toward shopping at a website; and (3) both consumers' attitudes toward products and their attitudes toward shopping at a website contribute to their intentions to purchase the products displayed on the website.
Manufacturing firms are increasingly using advanced enterprise-level information systems to coordinate and synchronize externally oriented functions such as marketing and supply chain and internally oriented activities such as manufacturing. In this paper, we present a model of manufacturing performance that simultaneously considers the effects of a firm's integrated IS capability in conjunction with interfunctional and interorganizational coordination mechanisms. Consistent with the complementarity perspective, we view this specific form of IS capability as enhancing manufacturing's coordination with marketing and supply chain functions to drive manufacturing performance. Additionally, the theoretical model presented here introduces manufacturing-IS coordination, a form of coordination not considered in past research, as a key antecedent to integrated IS capability. The research thus provides a comprehensive framework for examining manufacturing performance in contexts that have been transformed by the use of advanced information systems. The theoretical model is tested using primary data collected from manufacturing firms and matched with objective manufacturing performance data from secondary sources. Results show that a firm's integrated IS capability, as well as the complementary effects of IS capability with manufacturing, marketing, and supply chain processes, are significant predictors of manufacturing performance. These findings are robust to concerns of endogeneity, unobserved heterogeneity and alternative model specification.
This paper takes an event study approach to jointly examine the wealth and risk effects associated with electronic commerce announcements, contributing to the emerging research on the riskiness of IT investments and the trade-off between risk and return in the information systems literature. We estimate a generalized event study model that allows for both systematic and unsystematic risk changes on data collected for electronic commerce announcements in the 1996-2002 time frame. A striking result emerging from our analysis is that wealth effects are not significant after controlling for contemporaneous risk changes. Both total and unsystematic risk show a significant postevent increase in 1998 and 2000, whereas systematic risk adjusts downward in 1996 and 2002. Put together, our results contribute to our nascent understanding of how IT initiatives affect the risk-return profile of the firm.
A major impediment to accurate information retrieval from the World Wide Web is the inability of search engines to incorporate semantics in the search process. This research presents a methodology, CONQUER (CONtext-aware QUERy processing), that enhances the semantic content of Web queries using two complementary knowledge sources: lexicons and ontologies. The methodology constructs a semantic net using the original query as a seed, and refines the net with terms from the two knowledge sources. The enhanced query, represented by the refined semantic net, can be executed by search engines. This paper describes the methodology and its implementation in a prototype. An empirical evaluation shows that queries suggested by the prototype produce more relevant results than those obtained by the original queries. The research, thus, provides a successful demonstration of the use of existing knowledge sources to enhance the semantic content of Web queries. The paper concludes by identifying potential uses of such enhancements of search technology in organizational contexts.
Information security investment has been getting increasing attention in recent years. Various methods have been proposed to determine the effective level of security investment. However, traditional expected value methods (such as annual loss expectancy) cannot fully characterize the information security risk confronted by organizations, considering some extremal yet perhaps relatively rare cases in which a security failure may be critical and cause high losses. In this research note we introduce the concept of value-at-risk to measure the risk of daily losses an organization faces due to security exploits and use extreme value analysis to quantitatively estimate the value at risk. We collect a set of internal daily activity data from a large financial institution in the northeast United States and then simulate its daily losses with information based on data snapshots and interviews with security managers at the institution. We illustrate our methods using these simulated daily losses. With this approach, decision makers can make a proper investment choice based on their own risk preference instead of pursuing a solution that minimizes only the expected cost.
Technology-mediated learning methods are widely used by organizations and educational institutions to deliver information technology training. One form of technology-mediated learning, e-learning, in which the platform is the tutor, is quickly becoming the cost-effective solution of choice for many corporations. Unfortunately, the learning outcomes have been very disappointing. E-learning training makes an implicit assumption that learners can apply a high level of self-directed learning to assimilate the training content. In contrast, based on perspectives from social cognitive theory, we propose that instructional strategies need to persuade learners to follow self-regulated learning strategies. We test our ideas with participants who were trained through e-learning to design a website. Our findings indicate that participants who were induced to follow self-regulated learning strategies scored significantly higher on learning outcomes than those who were not persuaded to do so. We discuss our findings, and suggest that the interaction among information technology features, instructional strategies, and psychological learning processes offers a fruitful avenue for future information systems training research.
The five-factor model (FFM) of personality has been used to great effect in management and psychology research to predict attitudes, cognitions, and behaviors, but has largely been ignored in the IS field. We demonstrate the potential utility of incorporating this model into IS research by using the FFM personality factors in the context of technology acceptance. We propose a dispositional perspective to understanding user attitudes and beliefs, and examine the effect of user personalitycaptured using the FFM's big five factorson both the perceived usefulness of and subjective norms toward the acceptance and use of technology. Using logged usage data from 180 new users of a collaborative technology, we found general support for our hypotheses that the FFM personality dimensions can be useful predictors of users' attitudes and beliefs. We also found strong support for the relationships between intention to use and system use.
This study compares the performances of pair development (an approach in which a pair of developers jointly work on the same piece of code), solo development, and mixed development under two separate objectives: effort minimization and time minimization. To this end, we develop analytical models to optimize module-developer assignments in each of these approaches. These models are shown to be strongly NP-hard and solved using a genetic algorithm. The solo and pair development approaches are compared for a variety of problem instances to highlight project characteristics that favor one of the two practices. We also propose a simple criterion that can reliably recommend the appropriate approach for a given problem instance. Typically, for efficient knowledge sharing between developers or for highly connected systems, the pair programming approach is preferable. Also, the pair approach is better at leveraging expertise by pairing experts with less skilled partners. Solo programming is usually desirable if the system is large or the effort needed either to form a pair or to code efficiently in pairs is high. Solo programming is also appropriate for projects with a tight deadline, whereas the reverse is true for projects with a lenient deadline. The mixed approach (i.e., an approach where both the solo and pair practices are used in the same project) is only indicated when the system consists of groups of modules that are sufficiently different from one another.
We study the question of whether a software vendor should allow users of unlicensed (pirated) copies of a software product to apply security patches. We present a joint model of network software security and software piracy and contrast two policies that a software vendor can enforce: (i) restriction of security patches only to legitimate users or (ii) provision of access to security patches to all users whether their copies are licensed or not. We find that when the software security risk is high and the piracy enforcement level is low, or when tendency for piracy in the consumer population is high, it is optimal for the vendor to restrict unlicensed users from applying security patches. When piracy tendency in the consumer population is low, applying software security patch restrictions is optimal for the vendor only when the piracy enforcement level is high. If patching costs are sufficiently low, however, an unrestricted patch release policy maximizes vendor profits. We also show that the vendor can use security patch restrictions as a substitute to investment in software security, and this effect can significantly reduce welfare. Furthermore, in certain cases, increased piracy enforcement levels can actually hurt vendor profits. We also show that governments can increase social surplus and intellectual property protection simultaneously by increasing piracy enforcement and utilizing the strategic interaction of piracy patch restrictions and network security. Finally, we demonstrate that, although unrestricted patching can maximize welfare when the piracy enforcement level is low, contrary to what one might expect, when the piracy enforcement level is high, restricting security patches only to licensed users can be socially optimal.
Client control over the vendor has been identified as a critical factor in successfully managing information technology outsourcing relationships. Though prior studies have suggested that how much control is exercised has significant ramifications for individuals and firms, relatively few studies have operationalized and studied this important concept. In this study, we define the amount of formal control as the variety of mechanisms used by a client to exercise control over a vendor and the extent to which the mechanisms are used. We use literature on transaction cost economics and organizational control to build a model of the antecedents of the amount of formal control. The study uses data from 138 client-vendor matched pairs working in eight large, long-term, ongoing outsourcing arrangements to test specific hypotheses. The results suggest that clients who have technical or relationship management knowledge, or have high levels of trust in their vendors, use formal control mechanisms to a lesser extent. On the other hand, task uncertainty was found to be positively associated with the amount of formal control, and the degree of core competency involved in the outsourced activity was not found to be related to the amount of formal control. These results are discussed, and implications for research and practice are drawn.
Prior research has indicated that, on average, offshore vendors have higher profits associated with time and materials (T&M) contracts than fixed price (FP) contracts. This research raises two questions. First, Is the relative importance of various profit drivers different across two contractual regimes? Second, Does it follow that vendors unconditionally prefer T&M contracts for all projects? We address these questions by using data on 93 offshore projects completed by a leading Indian vendor. We use an endogenous switching regression framework and the program evaluation methodology to show that profit equations are distinctly different for the two contractual regimes. Using these two profit equations, we also identify contingencies under which the vendor prefers an FP contract to a T&M contract. We hypothesize that the vendor's ability leverage information asymmetry about capabilities and experiences translates into the vendor preferring FP contract to secure larger information rents. Our results support this hypothesis and suggest that the vendor would prefer the FP contract for larger and longer projects with larger teams. However, vendors would prefer a T&M contract when the risk of employee attrition from the project team is high. In addition, we discuss managerial implications of these results in the paper.
Researchers and designers have been building awareness displays to improve the coordination of communication between distributed co-workers since the early 1990s. Awareness displays are technology designed to provide contextual information about the activities of group members. Most researchers have assumed that these displays improve the coordination of communication regardless of the relationship between the communicating parties. This article examines the conditions under which awareness displays improve coordination and the types of designs that most effectively support communication timing without overwhelming people with irrelevant information. Results from a pair of laboratory experiments indicate that awareness displays containing information about a remote collaborator's workload lead to communication attempts that are less disruptive, but only when the interrupter has incentives to be concerned about the collaborator's welfare. High-information awareness displays harmed interrupters' task performance, while abstract displays did not. We conclude that a display with an abstract representation of a collaborator's workload is optimal; it leads to better timing of interruptions without overwhelming the person viewing the display.
The growth of the Internet has spawned an increasing number of online information sources (OISs). The effect of OISs on consumer information search processes has been particularly striking in sectors such as auto retailing, where the typical consumer has conventionally been confronted with an unpleasant and inefficient purchase process. However, the relationships between the information found in the online marketspace, consumer search in the offline marketplace, and other aspects of the multichannel shopping process are not well understood. This study examines the differential impact of price and product information found in the marketspace, relating consumers' information needs and information retrieval from OISs to three shoppingrelated outcomespurchase based on online infomediary referral (i.e., referred purchase), intensity of search in the marketplace, and online search satisfaction. We draw on a large data set of more than 16,000 new vehicle purchasers who reported using the Web for search related to their new vehicle purchase. We find that OISs offer different levels of price and product information and consumers are differentiated in their ability to retrieve this information. Further, the retrieval of price versus product information online has important implications for whether consumers consummate their online search through referred purchase or extend their search into the physical marketplace. Our results suggest different business models for infomediaries providing price and product information and underscore the need for designing information provisioning systems of OISs to facilitate transition between the marketspace and the marketplace.
With the continued growth of business-to-consumer (B2C) e-business, online vendors are providing an increasing array of services that support and enhance their core products or services. For example, Amazon.com does not just sell books; it also enhances that core product with automated product recommendations, wish list tracking, order status updates, customer reviews, and many other valuable supporting services. These supporting services are made possible exclusively through the design and deployment of information technology (IT) to provide website supporting services functionality (SSF). In this paper, we define and develop the concept of B2C SSF and investigate how IT can support core products or services. We theorize the role that SSF plays in an environment where individuals who visit B2C websites are not only customers but also technology users. Given the unique online environment that amalgamates vendor services with information systems (IS), our model integrates theories from both services marketing and technology acceptance to help explain the behavior of these customers/users. In doing so, we investigate the role of the extensively researched concept of service quality in relation to SSF. Although service quality provides guidance for how supporting services should be provided (e.g., responsively and reliably), it does not address what those services are (e.g., product recommendations). SSF addresses this deficiency, thus providing both theoretical and practical benefits through a focus on IT design and deployment. The results of a field study support that SSF is an important predictor of customer beliefs and behavior, beyond that predicted by service quality alone. SSF is an important concept to considertheoretically and practicallyin IT-mediated B2C service.
With the rapid growth of rich-media content over the Internet, content and service providers (SP) are increasingly facing the problem of managing their service resources cost-effectively while ensuring a high quality of service (QoS) delivery at the same time. In this research we conceptualize and model an Internetbased storage provisioning network for rich-media content delivery. This is modeled as a capacity provision network (CPN) where participants possess service infrastructures and leverage their topographies to effectively serve specific customer segments. A CPN is a network of SPs coordinated through an allocation hub. We first develop the notion of discounted QoS capabilities of storage resources. We then investigate the stability of the discount factors over time and the network topography using a test-bed on the Internet through a longitudinal empirical study. Finally, we develop a market maker mechanism for optimal multilateral allocation and surplus sharing in a network. The proposed CPN is closely tied to two fundamental properties of Internet service technology: positive network externality among cooperating SPs and the property of effective multiplication of capacity allocation among several distributed service sites. We show that there exist significant incentives for SPs to engage in cooperative allocation and surplus sharing. We further demonstrate that intermediation can enhance the allocation effectiveness and that the opportunity to allocation and surplus sharing can play an important role in infrastructure planning. In conclusion, this study demonstrates the practical business viability of a cooperative CPN market.
Consumer-generated product reviews have proliferated online, driven by the notion that consumers' decision to purchase or not purchase a product is based on the positive or negative information about that product they obtain from fellow consumers. Using research on information processing as a foundation, we suggest that in the context of an online community, reviewer disclosure of identity-descriptive information is used by consumers to supplement or replace product information when making purchase decisions and evaluating the helpfulness of online reviews. Using a unique data set based on both chronologically compiled ratings as well as reviewer characteristics for a given set of products and geographical location-based purchasing behavior from Amazon, we provide evidence that community norms are an antecedent to reviewer disclosure of identity-descriptive information. Online community members rate reviews containing identity-descriptive information more positively, and the prevalence of reviewer disclosure of identity information is associated with increases in subsequent online product sales. In addition, we show that shared geographical location increases the relationship between disclosure and product sales, thus highlighting the important role of geography in electronic commerce. Taken together, our results suggest that identity-relevant information about reviewers shapes community members' judgment of products and reviews. Implications for research on the relationship between online word-of-mouth (WOM) and sales, peer recognition and reputation systems, and conformity to online community norms are discussed.
Information systems (IS) researchers have typically examined the user-system relationship as an isolated dyad between a single, independent user and an individual, freestanding information system. We argue that this conceptualization does not adequately represent most organizations today, in which multiple users interact with multiple information systems within a group. Relying heavily on the theory and methods behind social network analysis, we introduce the concept of multimodal networks to assess both users and information systems as equivalent nodes in a single social network. This perspective allows us to examine the influence of information systems on organizational outcomes as a function of all of the user-system and interpersonal interactions in a group. We explore two different possible mechanisms for this influence: (1) direct user-system interactions by aggregating the strength of all the dyadic user-system interactions in a group, and (2) indirect user-system interactions by assessing the centrality of the information systems within the social network. We survey approximately 600 individuals in 40 healthcare groups to test whether either or both of these mechanisms are associated with two types of organizational performance outcomes-efficiency and quality of care. We find that the centrality of the information systems within the network is significantly and positively associated with both efficiency and quality outcomes, but that the average strength of the user-system interactions is not. Implications are that managers and researchers should examine the wider multimodal network of multiple users and multiple systems when assessing the role of IS in organizations in relation to organizational performance outcomes.
Recent years have witnessed a surge in self-organizing voluntary teams collaborating online to produce goods and services. Motivated by this phenomenon, this research investigates how these teams are formed and how individuals make decisions about which teams to join in the context of open source software development (OSSD). The focus of this paper is to explore how the collaborative network affects developers' choice of newly initiated OSS projects to participate in. More specifically, by analyzing software project data from real-world OSSD projects, we empirically test the impact of past collaborative ties with and perceived status of project members in the network on the self-assembly of OSSD teams. Overall, we find that a developer is more likely to join a project when he has strong collaborative ties with its initiator. We also find that perceived status of the noninitiator members of a project influences its probability of attracting developers. We discuss the implications of our results with respect to self-organizing teams and OSSD.
Social networks constructed on digital platforms are becoming increasingly pervasive in all aspects of individual and organizational life. This special issue of Information Systems Research includes 10 papers that focus on the interplay between digital and social networks. The interplay draws attention to the fact that digital interaction among individuals and organizations is almost always embedded in, influenced by, and in turn influences a social network. The papers in this special issue collectively shed light on the technical, behavioral, and economic challenges and implications of such networks and contribute to our understanding of how the power of such networks can be harnessed.
The capabilities offered by digital communication are leading to the evolution of new network structures that are grounded in communication patterns. As these structures are significant for organizations, much research has been devoted to understanding network dynamics in ongoing processes of electronic communication. A valuable method for this objective is Social Network Analysis. However, its current focus on quantifying and interpreting aggregated static relationship structures suffers from some limitations for the domain of analyzing online communication with high volatility and massive exchange of timed messages. To overcome these limitations, this paper presents a method for event-based dynamic network visualization and analysis together with its exploratory social network intelligence software Commetrix. Based on longitudinal data of corporate email communication, the paper demonstrates how exploration of animated graphs combined with measuring temporal network changes identifies measurement artifacts of static network analysis, describes community formation processes and network lifecycles, bridges actor level with network level analysis by analyzing the structural impact of actor activities, and measures how network structures react to external events. The methods and findings improve our understanding of dynamic phenomena in online communication and motivate novel metrics that complement Social Network Analysis.
The interactive nature of the Internet promotes collaborative business models (e.g., auctions) and facilitates information-sharing via social networks. In Internet auctions, an important design option for sellers is the setting of a secret reserve price that has to be met by a buyer's bid for a successful purchase. Bidders have strong incentives to learn more about the secret reserve price in these auctions, thereby relying on their own network of friends or digital networks of users with similar interests and information needs. Information-sharing and flow in digital networks, both person-to-person and via communities, can change bidding behavior and thus can have important implications for buyers and sellers in secret reserve price auctions. This paper uses a multiparadigm approach to analyze the impact of information diffusion in social networks on bidding behavior in secret reserve price auctions. We first develop an analytical model for the effect of shared information on individual bidding behavior in a secret reserve price auction with a single buyer facing a single seller similar to eBay's Best Offer and some variants of NYOP. Next, we combine the implications from our analytical model with relational data that describe the individual's position in social networks. We empirically test the implications of our analytical model in a laboratory experiment, and examine the impact of information diffusion in social networks on bidding behavior in a field study with real purchases where we use a virtual world as proxy for the real world. We find that the amount and dispersion of information in the individualized context, and betweenness centrality in the social network context, have a significant impact on bidding behavior. Finally, we discuss the implications of our results for buyers and sellers.
To understand the impact of social capital on knowledge integration and performance within digitally enabled teams, we studied 46 teams who had a history and a future working together. All three dimensions of their social capital (structural, relational, and cognitive) were measured prior to the team performing two tasks in a controlled setting, one face-to-face and the other through a lean digital network. Structural and cognitive capital were more important to knowledge integration when teams communicated through lean digital networks than when they communicated face-to-face; relational capital directly impacted knowledge integration equally, regardless of the communication media used by the team. Knowledge integration, in turn, impacted team decision quality, suggesting that social capital influences team performance in part by increasing a team's ability to integrate knowledge. These results suggest that team history may be necessary but not sufficient for teams to overcome the problems with the use of lean digital networks as a communication environment. However, team history may present a window of opportunity for social capital to develop, which in turn allows teams to perform just as well as in either communication environment.
Viral marketing is a form of peer-to-peer communication in which individuals are encouraged to pass on promotional messages within their social networks. Conventional wisdom holds that the viral marketing process is both random and unmanageable. In this paper, we deconstruct the process and investigate the formation of the activated digital network as distinct from the underlying social network. We then consider the impact of the social structure of digital networks (random, scale free, and small world) and of the transmission behavior of individuals on campaign performance. Specifically, we identify alternative social network models to understand the mediating effects of the social structures of these models on viral marketing campaigns. Next, we analyse an actual viral marketing campaign and use the empirical data to develop and validate a computer simulation model for viral marketing. Finally, we conduct a number of simulation experiments to predict the spread of a viral message within different types of social network structures under different assumptions and scenarios. Our findings confirm that the social structure of digital networks play a critical role in the spread of a viral message. Managers seeking to optimize campaign performance should give consideration to these findings before designing and implementing viral marketing campaigns. We also demonstrate how a simulation model is used to quantify the impact of campaign management inputs and how these learnings can support managerial decision making.
Despite the growing research interest in Internet auctions, particularly those on eBay, little is known about quantifiable consumer surplus levels in such mechanisms. Using an ongoing novel field experiment that involves real bidders participating in real auctions, and voting with real dollars, we collect and examine a unique data set to estimate consumer surplus in eBay auctions. The estimation procedure relies mainly on knowing the highest bid, which is not disclosed by eBay but is available to us from our experiment. At the outset we assume a private value second-price sealed-bid auction setting, as well as a lack of alternative buying options within or outside eBay. Our analysis, based on a sample of 4,514 eBay auctions, indicates that consumers extract a median surplus of at least $4 per eBay auction. This estimate is unbiased under the above assumptions; otherwise it is a lower bound. The surplus distribution is highly skewed given the diverse nature of the data. We find that eBay's auctions generated at least $7.05 billion in total consumer surplus in 2003 and could generate up to $7.68 billion if the private value sealed-bid assumption does not hold. We check for the validity of our assumptions and the robustness of our estimates using an additional data set from 2005 and a randomly sampled validation data set from eBay.
Online product reviews may be subject to self-selection biases that impact consumer purchase behavior, online ratings' time series, and consumer surplus. This occurs if early buyers hold different preferences than do later consumers about the quality of a given product. Readers of early product reviews may not successfully correct for these preference differences when interpreting ratings and making purchases. In this study, we develop a model that examines how idiosyncratic preferences of early buyers can affect long-term consumer purchase behavior as well as the social welfare created by review systems. Our model provides an explanation for the structure of product ratings over time, which we empirically test using online book reviews posted on Amazon.com. Our analysis suggests that firms could benefit from altering their marketing strategies such as pricing, advertising, or product design to encourage consumers likely to yield positive reports to self-select into the market early and generate positive word-of-mouth for new products. On the other hand, self-selection bias, if not corrected, decreases consumer surplus.
Organizations in many different industries employ virtual teams in a variety of contexts, including research and development, customer support, software development, and product design. Many virtual teams are geographically and culturally dispersed in order to facilitate around-the-clock work and to allow the most qualified individuals to be assigned to a project team. As such dispersion increases, virtual teams tend to experience greater and more diverse conflict compared to co-located teams. Since the dynamics of virtual team leadership are not yet well understood, research that examines how team leaders alleviate threats to team cohesion and provide strategies for conflict resolution makes significant contributions to the literature. Our study uses a survey-based methodology to examine the perceptions of 159 virtual team members employed by a large U.S. telecommunications corporation and five Korean firms involved in construction, finance, business consulting, sales, and distribution. The study integrates the dynamic model of conflict in distributed teams with the behavioral complexity in leadership theory to investigate the roles that virtual team leaders must effectively employ to reduce various forms of virtual team conflict. Our findings indicate that communication technologies are effective in reducing task conflict; however, the team leader may also mitigate task conflict by assuming the role of monitor. Likewise, process conflict may be abated in the virtual team as the leader performs coordinator activities. An effective virtual team leader exhibits specific roles to manage different types of conflict and the leader's response to conflict plays an important part in virtual team success.
Peer production phenomena such as open source software (OSS) have been posited as a viable alternative to traditional production models. However, community-based development often falls short of creating software products in the sense that consumers understand. Our research identifies an emerging business network archetype in the OSS sector, the open source service network (OSSN), which seeks to address the productization challenge. To do so, OSSNs must overcome the problems associated with exchanging resources between firms. We demonstrate that OSSNs overcome exchange problems by primarily relying on social, rather than legal, mechanisms; similar to the OSS communities from which they emerged. This is made possible because OSSNs use IT infrastructures that provide high visibility for primary value-creating activities. The research utilizes a multimethod theory-building approach, deriving a model from extant research, refining the model through qualitative case study analysis, and further refining the model through quantitative analysis of survey data. The paper reveals the manifestation of social mechanisms in OSSNs and how these are used for coordinating and safeguarding exchanges between firms. Specifically, we illustrate the primary importance of a shared macroculture (goals and norms) and collective sanctions for punishing firms who violate these goals/norms. Furthermore, our research highlights the interplay between digital and social networks within OSSNs, demonstrating that the use of social mechanisms is inherently dependent upon the underlying IT infrastructure.
The research reported in this paper studies the phenomenon of technostress, that is, stress experienced by end users of Information and Communication Technologies (ICTs), and examines its influence on their job satisfaction, commitment to the organization, and intention to stay. Drawing from the Transaction-Based Model of stress and prior research on the effects of ICTs on end users, we first conceptually build a nomological net for technostress to understand the influence of technostress on three variables relating to end users of ICTs: job satisfaction, and organizational and continuance commitment. Because there are no prior instruments to measure constructs related to technostress, we develop and empirically validate two second order constructs: technostress creators (i.e., factors that create stress from the use of ICTs) and technostress inhibitors (i.e., organizational mechanisms that reduce stress from the use of ICTs). We test our conceptual model using data from the responses of 608 end users of ICTs from multiple organizations to a survey questionnaire. Our results, based on structural equation modeling (SEM), show that technostress creators decrease job satisfaction, leading to decreased organizational and continuance commitment, while Technostress inhibitors increase job satisfaction and organizational and continuance commitment. We also find that age, gender, education, and computer confidence influence technostress. The implications of these results and future research directions are discussed.
This paper explores a new phenomenon at the intersection of digital networks and organizationsthe Internet-based volunteer work forcepeople who use Internet applications to pursue a personal interest through volunteering contributions of time and talent that may create value for organizations and their customers or members. This work force is not centrally organized, managed, or measured. It is an emergent phenomenon resulting from discretionary small actions taken by large numbers of people, enabled by technology and human initiative. This paper proposes a general framework for understanding the phenomenon and offers an empirical investigation of one component of itthe role of feedback in producing and sustaining high-quality contributions from this work force. In a comparative study of Internet-based voluntary technical support groups for software problems, we found that in groups who implement systematic quality feedback systems (compared to those that do not), question askers return over a longer duration, answer providers contribute more often, and technical problem resolution is more effective. We also found that with systematic feedback, volunteers who produce higher quality contributions have longer participation duration, and participation duration is positively associated with community maintenance contributions.
In this study, we seek to better understand the value of information technology (IT) in supply chain contexts. Grounded in the resource-based theory in conjunction with transaction cost economics, we develop a conceptual model that links three IT-related resources (backend integration, managerial skills, and partner support) to firm performance improvement. The model differs from previous studies by proposing a moderating effect of competition on the resource-performance relationships. Using data of 743 manufacturing firms, our analysis indicates significant contribution of IT to supply chains, which is generated through development of the digitally enabled integration capability and manifested at the process level along the supply chain. The technological resource alone, however, does not hold the answer to IT value creation. In fact, managerial skills, which enable adaptations on supply chain processes and corporate strategy to accommodate the use of IT, are shown to play the strongest role in IT value creation. Furthermore, backend integration and managerial skills are found to be more valuable in more competitive environments. While commodity-like resources have diminishing value under competition, integrational and managerial resources become even stronger. Overall, our results shed light on the key drivers of IT-enabled supply chains, and provide insights into how competition shapes IT value.
Information derived from relational databases is routinely used for decision making. However, little thought is usually given to the quality of the source data, its impact on the quality of the derived information, and how this in turn affects decisions. To assess quality, one needs a framework that defines relevant metrics that constitute the quality profile of a relation, and provides mechanisms for their evaluation. We build on a quality framework proposed in prior work, and develop quality profiles for the result of the primitive relational operations Difference and Union. These operations have nuances that make both the classification of the resulting records as well as the estimation of the different classes quite difficult to address, and very different from that for other operations. We first determine how tuples appearing in the results of these operations should be classified as accurate, inaccurate or mismember, and when tuples that should appear do not (called incomplete) in the result. Although estimating the cardinalities of these subsets directly is difficult, we resolve this by decomposing the problem into a sequence of drawing processes, each of which follows a hyper-geometric distribution. Finally, we discuss how decisions would be influenced based on the resulting quality profiles.
Intentional insider misuse of information systems resources (i.e., IS misuse) represents a significant threat to organizations. For example, industry statistics suggest that between 50%-75% of security incidents originate from within an organization. Because of the large number of misuse incidents, it has become important to understand how to reduce such behavior. General deterrence theory suggests that certain controls can serve as deterrent mechanisms by increasing the perceived threat of punishment for IS misuse. This paper presents an extended deterrence theory model that combines work from criminology, social psychology, and information systems. The model posits that user awareness of security countermeasures directly influences the perceived certainty and severity of organizational sanctions associated with IS misuse, which leads to reduced IS misuse intention. The model is then tested on 269 computer users from eight different companies. The results suggest that three practices deter IS misuse: user awareness of security policies; security education, training, and awareness (SETA) programs; and computer monitoring. The results also suggest that perceived severity of sanctions is more effective in reducing IS misuse than certainty of sanctions. Further, there is evidence that the impact of sanction perceptions vary based on one's level of morality. Implications for the research and practice of IS security are discussed.
Iterative combinatorial auctions (ICAs) are IT-based economic mechanisms where bidders submit bundle bids in a sequence and an auctioneer computes allocations and ask prices in each auction round. The literature in this field provides equilibrium analysis for ICAs with nonlinear personalized prices under strong assumptions on bidders' strategies. Linear pricing has performed very well in the lab and in the field. In this paper, we compare three selected linear price ICA formats based on allocative efficiency and revenue distribution using different bidding strategies and bidder valuations. The goal of this research is to benchmark different ICA formats and design and analyze new auction rules for auctions with pseudodual linear prices. The multi-item and discrete nature of linear price iterative combinatorial auctions and the complex price calculation schemes defy much of the traditional game theoretical analysis in this field. Computational methods can be of great help in exploring potential auction designs and analyzing the virtues of various design options. In our simulations, we found that ICA designs with linear prices performed very well for different valuation models even in cases of high synergies among the valuations. There were, however, significant differences in efficiency and in the revenue distributions of the three ICA formats. Heuristic bidding strategies using only a few of the best bundles also led to high levels of efficiency. We have also identified a number of auction rules for ask price calculation and auction termination that have shown to perform very well in the simulations.
Many studies measure the value of information technology (IT) by focusing on how much value is added rather than on the mechanisms that drive value addition. We argue that value from IT arises not only directly through changes in the factor input mix but also indirectly through IT-enabled augmentation of non-IT inputs and changes in the underlying production technology. We develop an augmented form of the Cobb- Douglas production function to separate and measure different productivity-enhancing effects of IT. Using industry-level data from the manufacturing sector, we find evidence that both direct and indirect effects of IT are significant. Partitioning industries into IT-intensive and non-IT-intensive, we find that the indirect effects of IT predominate in the IT-intensive sector. In contrast, the direct effects of IT predominate in the non-IT intensive sector. These results indicate structural differences in the role of IT in production between industries that are IT-intensive and those that are not. The implication for decision-makers is that for IT-intensive industries the gains from IT come primarily through indirect effects such as the augmentation of non-IT capital and labor.
No longer the exclusive domain of technology experts, information security is now a management issue. Through a grounded approach using interviews, observations, and secondary data, we advance a model of the information security compromise process from the perspective of the attacked organization. We distinguish between deliberate and opportunistic paths of compromise through the Internet, labeled choice and chance, and include the role of countermeasures, the Internet presence of the firm, and the attractiveness of the firm for information security compromise. Further, using one year of alert data from intrusion detection devices, we find empirical support for the key contributions of the model. We discuss the implications of the model for the emerging research stream on information security in the information systems literature.
Prior research on technology and team performance concludes that the fit of the technology to tasks influences team performance. It also suggests that the way teams appropriate technology influences performance. This research examines how fit and appropriation (from the Fit Appropriation Model) influence performance over time. Initially, the results show that fit better predicted performance; teams using poor-fitting technology performed worse than teams with better fitting technology. However, over a short time period (two days in this study), this initial fit no longer predicted performance; performance of teams using better fitting technology remained constant while teams using poor-fitting technology innovated and adapted, improving performance. There are two key findings from this study. First, fit can predict team performance soon after technology adoption, but initial assessments of fit are temporary as teams innovate and adapt; thus, our current theoretical models of fitting technology to a task likely will not be useful beyond the first use. Second, teams should understand how to better adapt existing technology and work structures. Because our current theories of tasktechnology fit failed to predict performance beyond the first use of technology, we believe that this calls for a reconsideration of what fit means for teams using technology.
With the proliferation of e-commerce, there is growing evidence that online impulse buying is occurring, yet relatively few researchers have studied this phenomenon. This paper reports on two studies that examine how variations in a website influence online impulse buying. The results reveal some relevant insights about this phenomenon. Specifically, although many participants had the urge to buy impulsively, regardless of website quality, this behavior's likelihood and magnitude was directly influenced by varying the quality of taskrelevant and mood-relevant cues. Task-relevant cues include characteristics, such as navigability, that help in the attainment of the online consumer's shopping goal. Conversely, mood-relevant cues refer to the characteristics, such as visual appeal, that affect the degree to which a user enjoys browsing a website but that do not directly support a particular shopping goal. The implications of the results for both future research and the design of human-computer interfaces are discussed.
A key feature of service-oriented models of information technology is the promise of prespecified quality levels enforceable via service level agreements (SLAs). This poses difficult management problems when considerable variability exists in user preferences and service demand within any organization. Because variance in expectations impact service levels, effective pricing and resource allocation mechanisms are needed to deliver services at the promised quality level. In this paper, we propose a mechanism for SLA formulation that is responsive to demand fluctuations and user preference variance, with the objective of maximizing organizational welfare of the participants. This formulation features a dynamic priority based price-penalty scheme targeted to individual users. An analytical model is presented and evaluated for effectiveness of a proposed dynamic priority-based pricing scheme vis--vis a baseline fixed-price single-quality level SLA. Simulations using data from an existing SLA is used to provide evidence that the proposed dynamic pricing scheme is likely to be more effective than a fixed-price approach from a system welfare perspective.
Alignment of information systems (IS) strategy with business strategy is a top concern of both the chief information officer (CIO) and the top management team (TMT) of organizations. Even though researchers and key decision makers in organizations recognize the importance of IS strategic alignment, they often struggle to understand how this alignment is created. In this paper, we develop a nomological network in which shared understanding between the CIO and TMT about the role of IS in the organization (which represents the social dimension of IS strategic alignment) is posited to be a proximal antecedent of the intellectual dimension of IS strategic alignment. We further posit that shared language, shared domain knowledge manifest in the CIO's business knowledge and the TMT's strategic IS knowledge, systems of knowing (structural and social), and CIO-TMT experiential similarity are important determinants of this shared understanding. Data were collected from 243 matched CIO-TMT pairs. Results largely support the proposed nomological network. Specifically, shared understanding between the CIO and TMT is a significant antecedent of IS strategic alignment. Furthermore, shared language, shared domain knowledge, and structural systems of knowing influence the development of shared understanding between the CIO and the TMT. Contrary to expectations and to findings of prior research, social systems of knowing, representing informal social interactions between the CIO and TMT, and experiential similarity did not have a significant effect on shared understanding.
Competition from open source software and free software (OSS/FS) alternatives is causing proprietary software producers to reevaluate product strategies. OSS/FS alternatives complicate an already complex information goods market plagued by piracy concerns. Although producer perspectives on software pricing and piracy controls have been addressed extensively, consumers' perspective and willingness to pay for commercial software is not very well understood. This paper empirically determines willingness to pay for a leading commercial software application (Microsoft Office) in the presence of an OSS/FS alternative. A contingent valuation approach is used to elicit willingness to pay for the application. The research design employs a 2  2  2 experiment to investigate the impact of preventive control, deterrence control, and OSS/FS alternative. The results indicate that the availability of an OSS/FS alternative has little impact on willingness to pay for Microsoft Office. However, piracy controls significantly increase willingness to pay for Microsoft Office, even in the presence of OSS/FS alternatives.
Despite the importance of causal analysis in building a valid knowledge base and in answering managerial questions, the issue of causality rarely receives the attention it deserves in information systems (IS) and management research that uses observational data. In this paper, we discuss a potential outcomes framework for estimating causal effects and illustrate the application of the framework in the context of a phenomenon that is also of substantive interest to IS researchers. We use a matching technique based on propensity scores to estimate the causal effect of an MBA on information technology (IT) professionals' salary in the United States. We demonstrate the utility of this counterfactual or potential outcomes-based framework in providing an estimate of the sensitivity of the estimated causal effects because of selection on unobservables. We also discuss issues related to the heterogeneity of treatment effects that typically do not receive as much attention in alternative methods of estimation, and show how the potential outcomes approach can provide several new insights into who benefits the most from the interventions and treatments that are likely to be of interest to IS researchers. We discuss the usefulness of the matching technique in IS and management research and provide directions to move from establishing association to assessing causation.
Given the increasingly important role of the Internet in education, healthcare, and other essential services, it is important that we develop an understanding of the digital divide. Despite the widespread diffusion of the Web and related technologies, pockets remain where the Internet is used sparingly, if at all. There are large geographic variations, as well as variations across ethnic and racial lines. Prior research suggests that individual, household, and regional differences are responsible for this disparity. We argue for an alternative explanation: Individual choice is subject to social influence (peer effects) that emanates from geographic proximity; this influence is the cause of the excess variation. We test this assertion with empirical analysis of a data set compiled from a number of sources. We find, first, that widespread Internet use among people who live in proximity has a direct effect on an individual's propensity to go online. Using data on residential segregation, we test the proposition that the Internet usage patterns of people who live in more ethnically isolated regions will more closely resemble usage patterns of their ethnic group. Finally, we examine the moderating impact of housing density and directly measured social interactions on the relationship between Internet use and peer effects. Results are consistent across analyses and provide strong evidence of peer effects, suggesting that individual Internet use is influenced by local patterns of usage. Implications for public policy and the diffusion of the Internet are discussed.
Proper configuration of security technologies is critical to balance the needs for access and protection of information. The common practice of using a layered security architecture that has multiple technologies amplifies the need for proper configuration because the configuration decision about one security technology has ramifications for the configuration decisions about others. Furthermore, security technologies rely on each other for their operations, thereby affecting each other's contribution. In this paper we study configuration of and interaction between a firewall and intrusion detection systems (IDS). We show that deploying a technology, whether it is the firewall or the IDS, could hurt the firm if the configuration is not optimized for the firm's environment. A more serious consequence of deploying the two technologies with suboptimal configurations is that even if the firm could benefit when each is deployed alone, the firm could be hurt by deploying both. Configuring the IDS and the firewall optimally eliminates the conflict between them, ensuring that if the firm benefits from deploying each of these technologies when deployed alone, it will always benefit from deploying both. When optimally configured, we find that these technologies complement or substitute each other. Furthermore, we find that while the optimal configuration of an IDS does not change whether it is deployed alone or together with a firewall, the optimal configuration of a firewall has a lower detection rate (i.e., allowing more access) when it is deployed with an IDS than when deployed alone. Our results highlight the complex interactions between firewall and IDS technologies when they are used together in a security architecture, and, hence, the need for proper configuration to benefit from these technologies.
This study addresses the theoretically underexplored question of how fit between project governance configurations, and the knowledge of specialized information technology (IT) and client departments, influences information systems development (ISD) performance. It conceptualizes project governance configurations using two classes of project decisions rightsdecision control rights and decision management rights. The paper then develops a middle-range theory of how governance-knowledge fit shapes ISD performance by influencing the effective exercise of these decision rights during the development process. Further, the two dimensions of ISD performanceefficiency and effectivenessare shaped by different classes of project decision rights. Data from 89 projects in 89 firms strongly support the proposed ideas. Implications for theory and practice are also discussed.
Trust and satisfaction are essential ingredients for successful business relationships in business-to-consumer electronic commerce. Yet there is little research on trust and satisfaction in e-commerce that takes a longitudinal approach. Drawing on three primary bodies of literature, the theory of reasoned action, the extended valence framework, and expectation-confirmation theory, this study synthesizes a model of consumer trust and satisfaction in the context of e-commerce. The model considers not only how consumers formulate their prepurchase decisions, but also how they form their long-term relationships with the same website vendor by comparing their prepurchase expectations to their actual purchase outcome. The results indicate that trust directly and indirectly affects a consumer's purchase decision in combination with perceived risk and perceived benefit, and also that trust has a longer term impact on consumer e-loyalty through satisfaction. Thus, this study extends our understanding of consumer Internet transaction behavior as a three-fold (prepurchase, purchase, and postpurchase) process, and it recognizes the crucial, multiple roles that trust plays in this process. Implications for theory and practice as well as limitations and future directions are discussed.
Agility is increasingly being seen as an essential element underlying the effectiveness of globally distributed information systems development (ISD) teams today However, for a variety of reasons, such teams are often unable develop and enact agility in dealing with changing situations. This paper seeks to provide a deeper understanding of agility through an intensive study of the distributed ISD experience in TECHCOM, an organization widely recognized for its excellence in IT development and use. The study reveals that agility should be viewed as a multifaceted concept having three dimensions: resource, process, and linkage. Resource agility is based on the distributed development team's access to necessary human and technological resources. Process agility pertains to the agility that originates in the team's systems development method guiding the project, its environmental scanning, and sense-making routines to anticipate possible crises, and its work practices enabling collaboration across time zones. Linkage agility arises from the nature of interactional relationships within the distributed team and with relevant project stakeholders, and is composed of cultural and communicative elements. The paper highlights some of the difficulties in developing agility in distributed ISD settings, provides actionable tactics, and suggests contingencies wherein different facets of agility may become more (or less) critical.
Awareness and use of agile methods has grown rapidly among the information systems development (ISD) community in recent years. Like most previous methods, the development and promotion of these methods have been almost entirely driven by practitioners and consultants, with little participation from the research community during the early stages of evolution. While these methods are now the focus of more and more research efforts, most studies are still based on XP, Scrum, and other industry-driven foundations, with little or no conceptual studies of ISD agility in existence. As a result, this study proposes that there are a number of significant conceptual shortcomings with agile methods and the associated literature in its current state, including a lack of clarity, theoretical glue, parsimony, limited applicability, and naivety regarding the evolution of the concept of agility in fields outside systems development. Furthermore, this has significant implications for practitioners, rendering agile method comparison and many other activities very difficult, especially in instances such as distributed development and large teams that are not conducive to many of the commercial agile methods. This study develops a definition and formative taxonomy of agility in an ISD context, based on a structured literature review of agility across a number of disciplines, including manufacturing and management where the concept originated, matured, and has been applied and tested thoroughly over time. The application of the texonomy in practice is then demonstrated through a series of thought trials conducted in a large multinational organization. The intention is that the definition and taxonomy can then be used as a starting point to study ISD method agility regardless of whether the method is XP or Scrum, agile or traditional, complete or fragmented, out-of-the-box or in-house, used as is or tailored to suit the project context.
Despite the popularity of agile methods in software development and increasing adoption by organizations there is debate about what agility is and how it is achieved. The debate suffers from a lack of understanding of agile concepts and how agile software development is practiced. This paper develops a framework for the organization of agile software development that identifies enablers and inhibitors of agility and the emergent capabilities of agile teams. The work is grounded in complex adaptive systems (CAS) and draws on three principles of coevolving systems: match coevolutionary change rate, maximize self-organizing, and synchronize exploitation and exploration. These principles are used to study the processes of two software development teams, one a team using eXtreme Programming (XP) and the other a team using a more traditional, waterfall-based development cycle. From the cases a framework for the organization of agile software development is developed. Time pacing, self-management with discipline and routinization of exploration are among the agile enablers found in the cases studies while event pacing, centralized management, and lack of resources allocated to exploration are found to be inhibitors to agility. Emergent capabilities of agile teams that are identified from the research include coevolution of business value, sustainable working with rhythm, sharing and team learning, and collective mindfulness.
In globally distributed projects, members have to deal with spatial boundaries (different cities) and temporal boundaries (different work hours) because other members are often in cities within and across time zones. For pairs of members with spatial boundaries and no temporal boundaries (those in different cities with overlapping work hours), synchronous communication technologies such as the telephone, instant messaging (TM), and Web conferencing provide a means for real-time interaction. However, for pairs of members with spatial and temporal boundaries (those in different cities with nonoverlapping work hours), asynchronous communication technologies, such as e-mail, provide a way to interact intermittently Using survey data from 675 project members (representing 5,674 pairs of members) across 108 projects in a multinational semiconductor firm, we develop and empirically test a relational model of coordination delay In our model, the likelihood of delay for pairs of members is a function of the spatial and temporal boundaries that separate them, as well as the communication technologies they use to coordinate their work. As expected, greater use of synchronous web conferencing reduces coordination delay for pairs of members in different cities with overlapping work hours relative to pairs of members with nonoverlapping work hours. Unexpectedly, greater use of asynchronous e-mail does not reduce coordination delay for pairs of members in different cities with nonoverlapping work hours, but rather reduces coordination delay for those with overlapping work hours. We discuss the implications of our findings that temporal boundaries are more difficult to cross with communication technologies than spatial boundaries.
In recent years, flexibility has emerged as a divisive issue in discussions about the appropriate design of processes for making software. Partisans in both research and practice argue for and against plan-based (allegedly inflexible) and agile (allegedly too flexible) approaches. The stakes in this debate are high; questions raised about plan-based approaches undermine longstanding claims that those approaches, when realized, represent maturity of practice. In this commentary, we call for research programs that will move beyond partisan disagreement to a more nuanced discussion, one that takes into account both benefits and costs of flexibility Key to such programs will be the development of a robust contingency framework for deciding when (in what conditions) plan-based and agile methods should be used. We develop a basic contingency framework in this paper, one that models the benefit/cost economics described in narratives about the transition from craft to industrial production of physical products. We use this framework to demonstrate the power of even a simple model to help us accomplish three objectives: (1) to refocus discussions about the appropriate design of software development processes, concentrating on when to use particular approaches and how they might be usefully combined; (2) to suggest and guide a trajectory of research that can support and enrich this discussion; and (3) to suggest a technology-based explanation for the emergence of agile development at this point in history. Although we are not the first to argue in favor of a contingency perspective, we show that there remain many opportunities for information systems (IS) research to have a major impact on practice in this area.
In this paper, we draw on control theory to understand the conditions under which the use of agile practices is most effective in improving software project quality. Although agile development methodologies offer the potential of improving software development outcomes, limited research has examined how project managers can structure the software development environment to maximize the benefits of agile methodology use during a project. As a result, project managers have little guidance on how to manage teams who are using agile methodologies. Arguing that the most effective control modes are those that provide teams with autonomy in determining the methods for achieving project objectives, we propose hypotheses related to the interaction between control modes, agile methodology use, and requirements change. We test the model in a field study of 862 software developers in 110 teams. The model explains substantial variance in four objective measures of project qualitybug severity, component complexity, coordinative complexity, and dynamic complexity. Results largely support our hypotheses, highlighting the interplay between project control, agile methodology use, and requirements change. The findings contribute to extant literature by integrating control theory into the growing literature on agile methodology use and by identifying specific contingencies affecting the efficacy of different control modes. We discuss the theoretical and practical implications of our results.
The article discusses various reports published within the issue, including one by Kieran Conboy on the concept of agility in information systems development, one by Richard Vidgen and Xiaofeng Wang on the enablers and inhibitors of agility, and one by Likoebe M. Maruping, Viswanath Venkatesh, and Ritu Agarwal on the effectiveness of agility methods.
When should software development teams have the flexibility to modify their directions and how do we balance that flexibility with controls essential to produce acceptable outcomes? We use dynamic capabilities theory and an extension of control theory to understand these questions. This work is examined in a case study. Our results demonstrate that flexibility may be needed when the starting conditions are uncertain and that effective control in these situations requires use of traditional controls plus a new type of control we term emergent outcome control.
Data models provide a map of the components of an information system. Prior research has indicated that more expressive conceptual data models (despite their increased size) result in better performance for problem solving tasks. An initial experiment using logical data models indicated that more expressive logical data models also enhanced end-user performance for information retrieval tasks. However, the principles of parsimony and bounded rationality imply that, past some point, increases in size lead to a level of complexity that results in impaired performance. The results of this study support these principles. For a logical data model of increased but still modest size, users composing queries for the more expressive logical data model did not perform as well as users composing queries for the corresponding less expressive but more parsimonious logical data model. These results indicate that, when constructing logical data models, data modelers should consider tradeoffs between parsimony and expressiveness.
Model-based decision support systems (DSS) improve performance in many contexts that are data-rich, uncertain, and require repetitive decisions. But such DSS are often not designed to help users understand and internalize the underlying factors driving DSS recommendations. Users then feel uncertain about DSS recommendations, leading them to possibly avoid using the system. We argue that a DSS must be designed to induce an alignment of a decision maker's mental model with the decision model embedded in the DSS. Such an alignment requires effort from the decision maker and guidance from the DSS. We experimentally evaluate two DSS design characteristics that facilitate such alignment: (i) feedback on the upside potential for performance improvement and (ii) feedback on corrective actions to improve decisions. We show that, in tandem, these two types of DSS feedback induce decision makers to align their mental models with the decision model, a process we call deep learning, whereas individually these two types of feedback have little effect on deep learning. We also show that deep learning, in turn, improves user evaluations of the DSS. We discuss how our findings could lead to DSS design improvements and better returns on DSS investments.
Outsourcing of information technology (IT) services has received much attention in the information systems (IS) literature. However, considerably less attention has been paid to actual contract structures used in IT outsourcing (ITO). Examining contract structures yields important insights into how the contracting parties structure the governance provisions and the factors or transaction risks that influence them. Based on insights from prior literature, from practicing legal experts, and through in-depth content analysis of actual contracts, we develop a comprehensive coding scheme to capture contract provisions across four major dimensions: monitoring, dispute resolution, property rights protection, and contingency provisions. We then develop an empirical data set describing the contract structures across these distinct dimensions, using a sample of 112 ITO contracts from the Securities and Exchange Commission (SEC) database from 1993 to 2003. Drawing on transaction cost, agency, and relational exchange theories, we hypothesize the effects of transaction and relational characteristics on the specific contractual provisions, as well as on overall contract extensiveness. Furthermore, we examine how these associations vary under conditions of fixed price and time and materials pricing structures. The results provide good support for the main hypotheses of the study and yield interesting insights about contractual governance of ITO arrangements.
The information systems (IS) literature suggests that by lowering coordination costs, information technology (IT) will lead to an overall shift towards more use of markets. Empirical work in this area provides evidence that IT is associated with a decrease in vertical integration (VI). Economy-wide data, however, suggests that over the last 25 years the average level of VI has, in fact, increased. This paper studies this empirical anomaly by explicating the moderating impact of two measures of competitive environment, demand uncertainty, and industry concentration, on the relationship between IT and VI. We examine firms included in 1995 to 1997 InformationWeek 500 and the COMPUSTAT database. Consistent with the IS literature, the analysis suggests that IT is associated with a decrease in VI when demand uncertainty is high or industry concentration is low. However, contrary to the IS literature, IT is found to be associated with an increase in VI when industry concentration is high or demand uncertainty is low. Furthermore, as demand uncertainty increases, less vertically integrated firms invest more in IT, while as industry concentration increases, more vertically integrated firms invest more in IT. The analysis also suggests that firms' choice of the level of VI and IT investment, under different levels of demand uncertainty and industry concentration, are rational. When demand uncertainty is high or industry concentration is low, increase in VI may increase coordination and production costs. Thus, less VI is rational. However, when industry concentration is high or demand uncertainty is low, increase in VI may decrease coordination and production costs. Thus, firms choose more VI in such industries. The implications for research and practice are discussed.
This paper investigates the practice-based learning dynamics that emerge among peers who share occupational practices but do not necessarily work with each other or even know each other because of geographical or organizational distance. To do so, it draws on the literatures on situated learning, networks of practice, and information infrastructures, and on insights from a longitudinal case study of the implementation of a Web-based information system used by people working in the field of environmental health. The system was deeply involved in the transformations of local practices as well as relationships between peers. Based on a dialogue between existing literatures and observations from the case study, this research extends the practice-based perspective on learning to the computer-mediated context of a network of practice. To that effect, it proposes a model of what we call trans-situated learning that is supported by the local universality of an information infrastructure whose use becomes embedded with other infrastructures.
This paper presents analytical, computational, and empirical analyses of strategies for intelligent bid formulations in online auctions. We present results related to a weighted-average ascending price auction mechanism that is designed to provide opaque feedback information to bidders and presents a challenge in formulating appropriate bids. Using limited information provided by the mechanism, we design strategies for software agents to make bids intelligently. In particular, we derive analytical results for the important characteristics of the auction, which allow estimation of the key parameters; we then use these theoretical results to design several bidding strategies. We demonstrate the validity of designed strategies using a discrete event simulation model that resembles the mechanisms used in treasury bills auctions, business-to-consumer (B2C) auctions, and auctions for environmental emission allowances. In addition, using the data generated by the simulation model, we show that intelligent strategies can provide a high probability of winning an auction without significant loss in surplus.
A key aspect of better and more secure software is timely patch release by software vendors for the vulnerabilities in their products. Software vulnerability disclosure, which refers to the publication of vulnerability information, has generated intense debate. An important consideration in this debate is the behavior of software vendors. How quickly do vendors patch vulnerabilities and how does disclosure affect patch release time? This paper compiles a unique data set from the Computer Emergency Response Team/Coordination Center (CERT) and SecurityFocus to answer this question. Our results suggest that disclosure accelerates patch release. The instantaneous probability of releasing the patch rises by nearly two and a half times because of disclosure. Open source vendors release patches more quickly than closed source vendors. Vendors are more responsive to more severe vulnerabilities. We also find that vendors respond more slowly to vulnerabilities not disclosed by CERT. We verify our results by using another publicly available data set and find that results are consistent. We also show how our estimates can aid policy makers in their decision making.
Each market session in a reverse electronic marketplace features a procurer and many suppliers. An important attribute of a market session chosen by the procurer is its information revelation policy. The revelation policy determines the information (such as the number of competitors, the winning bids, etc.) that will be revealed to participating suppliers at the conclusion of each market session. Suppliers participating in multiple market sessions use strategic bidding and fake their own cost structure to obtain information revealed at the end of each market session. The information helps to reduce two types of uncertainties encountered in future market sessions, namely, their opponents' cost structure and an estimate of the number of their competitors. Whereas the first type of uncertainty is present in physical and e-marketplaces, the second type of uncertainty naturally arises in IT-enabled marketplaces. Through their effect on the uncertainty faced by suppliers, information revelation policies influence the bidding behavior of suppliers which, in turn, determines the expected price paid by the procurer. Therefore, the choice of information revelation policy has important consequences for the procurer. This paper develops a partially observable Markov decision process model of supplier bidding behavior and uses a multiagent e-marketplace simulation to analyze the effect that two commonly used information revelation policiescomplete information policy and incomplete information policyhave on the expected price paid by the procurer. We find that the expected price under the complete information policy is lower than that under the incomplete information policy. The integration of ideas from the multiagents literature, the machinelearning literature, and the economics literature to develop a method to evaluate information revelation policies in e-marketplaces is a novel feature of this paper.
Information goods vendors offer different pricing schemes such as per user pricing and site licensing. Why do competing sellers adopt different pricing schemes for the same information good? Pricing schemes affect buyers' usage levels and thus the revenue generated from different segments of buyers. This can allow competing firms in a duopoly to differentiate themselves by offering different pricing schemes. Such strategic use of pricing schemes can allow undifferentiated sellers to earn substantial profits in a friction-free market for a commoditized information good. These conditions would otherwise lead to the Bertrand equilibrium and zero profits. We show that adopting asymmetric pricing schemes can be a Nash equilibrium for information goods with negligible marginal cost of production. We extend our model to the case of information goods that are horizontally differentiated and show that sellers will offer a single-pricing scheme that is different from competitors when the sellers are weakly differentiated. When the sellers are strongly differentiated, each seller will offer multiple pricing schemes. We show that it can be optimal for a seller to offer multiple pricing schemesmetered and flat fee pricing schemes, even in the absence of transactions costs.
Keyword advertising, including sponsored links and contextual advertising, powers many of today's online information services such as search engines and Internet-based emails. This paper examines the design of keyword auctions, a novel mechanism that keyword advertising providers such as Google and Yahoo! use to allocate advertising slots. In our keyword auction model, advertisers bid their willingness-to-pay per click on their advertisements, and the advertising provider can weight advertisers' bids differently and require different minimum bids based on advertisers' click-generating potential. We study the impact and design of such weighting schemes and minimum-bid policies. We find that weighting scheme determines how advertisers with different click-generating potential match in equilibrium. Minimum bids exclude low-valuation advertisers and at the same time may distort the equilibrium matching. The efficient design of keyword auctions requires weighting advertisers' bids by their expected click-through-rates, and requires the same minimum weighted bids. The revenue-maximizing weighting scheme may or may not favor advertisers with low click-generating potential. The revenue-maximizing minimum-bid policy differs from those prescribed in the standard auction design literature. Keyword auctions that employ the revenue-maximizing weighting scheme and differentiated minimum bid policy can generate higher revenue than standard fixed-payment auctions. We draw managerial implications for pay-per-click and other pay-for-performance auctions and discuss potential applications to other areas.
Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.
Outsourcing of software development allows a business to focus on its core competency and take advantage of vendors' technical expertise, economies of scale and scope, and their ability to smooth labor demand fluctuations across several clients. However, contracting a software project to an outside developer is often quite challenging because of information asymmetry and incentive divergence. A typical software development contract must deal with a variety of interrelated issues such as the quality of the developed system, the timeliness of delivery, the effort and cost associated with the project, the contract payment, and the postdelivery software support. This paper presents a contract-theoretic model that incorporates these factors to analyze how software outsourcing contracts can be designed. We find that despite their relative inefficiency, fixed-price contracts are often appropriate for simple software projects that require short development time. Time-and-materials contracts work well for more complex projects when the auditing process is efficient and effective. We also examine a type of performance-based contract called quality-level agreement and find that the first-best solution can be reached with such a contract. Finally, we consider profit-sharing contracts that are useful in situations where the developer has more bargaining power.
Reusing database queries by adapting them to satisfy new information requests is an attractive strategy for extracting information from databases without involving database specialists. However, the reuse of information systems artifacts has been shown to be susceptible to the phenomenon of anchoring and adjustment. Anchoring often leads to a systematic adjustment bias in which people fail to make sufficient changes to an anchor in response to the needs of a new task. In a study involving 157 novice query writers from six universities, we examined the effect of this phenomenon on the reuse of Structured Query Language (SQL) queries under varying levels of domain familiarity and for different types of anchors. Participants developed SQL queries to respond to four information requests in a familiar domain and four information requests in an unfamiliar domain. For two information requests in each domain, participants were also provided with sample queries (anchors) that answered similar information requests. We found evidence that the opportunity to reuse sample queries resulted in an adjustment bias leading to poorer quality query results and greater overconfidence in the correctness of results. The results also indicate that the strength of the adjustment bias depends on a combination of domain familiarity and type of anchor. This study demonstrates that anchoring and adjustment during query reuse can lead to queries that are less accurate than those written from scratch. We also extend the concept of anchoring and adjustment by distinguishing between surface-structure and deep-structure anchors and by considering the impact of domain familiarity on the adjustment bias.
The complexity and scope of outsourced information technology (IT) demands relationship-specific investments from vendors, which, when combined with contract incompleteness, may result in underinvestment and inefficient bargaining, referred to as the holdup problem. Using a unique data set of over 100 IT outsourcing contracts, we examine whether contract extensiveness, i.e., the extent to which firms and vendors can foresee contingencies when designing contracts for outsourced IT services, can alleviate holdup. While extensively detailed contracts are likely to include a greater breadth of activities outsourced to a vendor, task complexity makes it difficult to draft extensive contracts. Furthermore, extensive contracts may still be incomplete with respect to enforcement. We then examine the role of nonprice contractual provisions, contract duration, and extendibility terms, which give firms an option to extend the contract to limit the likelihood of holdup. We also validate the ex post efficiency of contract design choices by examining renewals of contracting agreements.
This paper reports on a specific promotional initiative designed to spur enrollment in IT-related fieldsan IT Careers Camp aimed at high school students. The camp was different from most prior computer camps in that it was not aimed at building skills such as programming or Web development. Rather, it was specifically designed to convince participants that (1) job prospects in the field are strong, and (2) IT/IS work is interesting and creative. To this end, the camp was designed in partnership with a number of corporations, and included as a central element a series of experiential opportunities for participants. Each day of the camp featured a visit to a corporation where the students took part in a hands-on activity that involved solving a business problem with IT. Qualitative and quantitative evaluations indicate that the camp was very successful in changing students' perceptions about the nature of IT work and the IT job market. We believe the camp can be a useful tool to create a pipeline of well-informed students interested in IT careers. We present here details of the design and execution of the camp in the hope that others may wish to replicate our efforts.
We study how interorganizational systems (IOS) such as electronic markets and other enabling information technologies that facilitate broader interfirm transactions affect the extent of outsourcing in firms. We do so by modeling firms in a three-tier value chain consisting of buyers, intermediaries, and suppliers, who can interact using IOS that lower the procurement search costs associated with finding appropriate trading partners. In the context of complex business-to-business (B2B) search, we study how decreasing search costs affect a firm's decision to insource or outsource the procurement function, depending on whether the search process is information intensive or communication intensive. Variation in search costs changes the transaction costs of interaction between firms, as well as the contracting costs associated with outsourcing, owing to changes in the costs of moral hazard for delegated search. We study these effects in a new model that integrates search theory into the principal-agent framework, and establish that the optimal outsourcing contract has a simple all or nothing performance-based structure under fairly general assumptions. Our model predicts that when B2B search is information intensive, IOS will facilitate an increase in outsourcing, market-based transactions, and a reduction in the vertical scope of extended enterprises. In contrast, when B2B search is primarily communication intensive, IOS will lead to tighter integration and an increase in the vertical scope of the extended enterprise. Our research suggests that the nature of the information technologies and of the business activities supported by IOS are crucial determinants of the organizational and industry changes they induce, and our results have important implications for a variety of industries in which both technological and agency issues will influence the eventual success of global IT-facilitated extended enterprise initiatives.
We study the problem where a decision maker needs to discover a classification rule to classify intelligent, self-interested agents. Agents may engage in strategic behavior to alter their characteristics for a favorable classification. We show how the decision maker can induce a classification rule that anticipates such behavior while still satisfying an important risk minimization principle.
As networks of all forms become ubiquitous, the network-based information they generate is increasingly being used in a wide variety of analysis tasks. In organizations, social network analysis techniques are being applied to a number of domains, particularly the understanding of knowledge stocks and flows. Because this information is generated from large data sets, computerized visualizations of it are very helpful for accomplishing these complex tasks. This paper presents a model for evaluating the effectiveness of network visualizations based on theories of cognitive fit, working memory capacity, and information load. The model was empirically tested in two experiments using two types of data visualizations from two different social networks. Results support the theoretical model, illustrating that variations in cognitive fit and working memory interact. Findings suggest that visualizations can enable superior outcomes when they are designed to support this interaction.
Because a fundamental attribute of a good theory is causality, the information systems (IS) literature has strived to infer causality from empirical data, typically seeking causal interpretations from longitudinal, experimental, and panel data that include time precedence. However, such data are not always obtainable and observational (cross-sectional, nonexperimental) data are often the only data available. To infer causality from observational data that are common in empirical IS research, this study develops a new data analysis method that integrates the Bayesian networks (BN) and structural equation modeling (SEM) literatures. Similar to SEM techniques (e.g., LISREL and PLS), the proposed Bayesian networks for latent variables (BN-LV) method tests both the measurement model and the structural model. The method operates in two stages: First, it inductively identifies the most likely LVs from measurement items without prespecifying a measurement model. Second, it compares all the possible structural models among the identified LVs in an exploratory (automated) fashion and it discovers the most likely causal structure. By exploring the causal structural model that is not restricted to linear relationships, BN-LV contributes to the empirical IS literature by overcoming three SEM limitations (Lee, B., A. Barua, A. B. Whinston. 1997. Discovery and representation of causal relationships in MIS research: A methodological framework. MIS Quart. 21(1) 109-136)-lack of causality inference, restrictive model structure, and lack of nonlinearities. Moreover, BN-LV extends the BN literature by (1) overcoming the problem of latent variable identification using observed (raw) measurement items as the only inputs, and (2) enabling the use of ordinal and discrete (Likert-type) data, which are commonly used in empirical IS studies. The BN-LV method is first illustrated and tested with actual empirical data to demonstrate how it can help reconcile competing hypotheses in terms of the direction of causality in a structural model. Second, we conduct a comprehensive simulation study to demonstrate the effectiveness of BN-LV compared to existing techniques in the SEM and BN literatures. The advantages of BN-LV in terms of measurement model construction and structural model discovery are discussed.
In peer-to-peer (P2P) media distribution, users obtain content from other users who already have it. This form of decentralized product distribution demonstrates several unique features. Only a small fraction of users in the network are queried when a potential adopter seeks a file, and many of these users might even free-ride, i.e., not distribute the content to others. As a result, generated demand might not always be fulfilled immediately. We present mixing models for product diffusion in P2P networks that capture decentralized product distribution by current adopters, incomplete demand fulfillment and other unique aspects of P2P product diffusion. The models serve to demonstrate the important role that P2P search process and distribution referrals-payments made to users that distribute files-play in efficient P2P media distribution. We demonstrate the ability of our diffusion models to derive normative insights for P2P media distributors by studying the effectiveness of distribution referrals in speeding product diffusion and determining optimal referral policies for fully decentralized and hierarchical P2P networks.
One of the distinctive features of sites on the Internet is their ability to gather enormous amounts of information about their visitors and to use this information to enhance a visitor's experience by providing personalized information or recommendations. In providing personalized services, a website is typically faced with the following trade-off: When serving a visitor's request, it can deliver an optimally personalized version of the content to the visitor, possibly with a long delay because of the computational effort needed, or it can deliver a suboptimal version of the content more quickly. This problem becomes more complex when several requests are waiting for information from a server. The website then needs to trade off the benefit from providing more personalized content to each user with the negative externalities associated with higher waiting costs for all other visitors that have requests pending. We examine several deterministic resource allocation policies in such personalization contexts. We identify an optimal policy for the above problem when requests to be scheduled are batched, and show that the policy can be very efficiently implemented in practice. We provide an experimental approach to determine optimal batch lengths, and demonstrate that it performs favorably when compared with viable queueing approaches.;
Prior research at the firm level finds information technology (IT) to be a net substitute for both labor and non-IT capital inputs. However, it is unclear whether these results hold, given recent IT innovations and continued price declines. In this study we extend prior research to examine whether these input relationships have evolved over time. First, we introduce new price indexes to account for varying technological progress across different types of IT hardware. Second, we use the rental price methodology to measure capital in terms of the flow of services provided. Finally, we use hedonic methods to extend our IT measures to 1998, enabling analysis spanning the emergence of the Internet. Analyzing approximately 9,800 observations from over 800 Fortune 1,000 firms for the years 1987-1998, we find firm demand for IT to be elastic for decentralized IT and inelastic for centralized IT. Moreover, Allen Elasticity of Substitution estimates confirm that through labor substitution, the increasing factor share of IT comes at the expense of labor. Last, we identify a complementary relationship between IT and ordinary capital, suggesting an evolution in this relationship as firms have shifted to more decentralized organizational forms. We discuss these results in terms of prior research, suggest areas of future research, and discuss managerial implications.
The Internet has brought about significant changes in the availability of market information in many industries. E-commerce technologies provide sellers with opportunities to design electronic mercantile mechanisms that reveal, conceal, bias, and distort market information, depending on their goals and market position (e.g., suppliers versus intermediaries). In particular, in information-intensive industries where electronic markets play an important role, many firms are using advanced technologies to put innovative strategies into play that are based on the provision of differential information to their customers. We examine the role of information transparency in electronic markets. We contend that there is an opportunity to develop research on sellers' strategies regarding information disclosure to customers and competitors. For that purpose, we develop a set of concepts and a framework to guide future research. We then propose an interdisciplinary agenda for research on the emerging and increasingly important topic of transparency strategy, which we define as the set of policies and decisions that a firm makes to disclose, conceal, bias, or distort market information.
The process by which organizations incorporate technological innovations into existing routines and use them on a regular basis persists as a central concern in the literature. Although we now have a fairly robust understanding of the drivers of innovation adoption, the use of innovations is less understood. In this paper, we draw on two streams of literature, managerial and organizational sensemaking, and organizational capabilities that have hitherto been used independently, to investigate organizational use of information technology (IT)-based innovations. Building on and extending prior work, we posit that organizational capabilities serve as complements to managers' technological frames related to an innovation. We focus on the use of an important technological innovation-business-to-business (B2B) electronic markets for procurement. We examine interactions between three technological frames-benefits frame, threat frame, and adjustment frame, and two organizational capabilities-technological opportunism and technological sophistication, and their relationship with the use of B2B electronic markets in firms. We test our research model using survey data collected from 292 firms. Results largely support the proposed conceptualization and shed new light on the key factors associated with firms' use of B2B electronic markets. Theoretical and practical implications of the findings are discussed.
The World Wide Web has become a key intermediary between producers and consumers of information. Web's linkage structure has been exploited by contemporary search engines to decrease the search cost for consumers while usually also rewarding the producers of higher status Web pages. In addition to influencing visibility and accessibility, in-links, as marks of recognition, accord status to a Web page. In this paper we show how Web page status may be predicted at least in part by page location and topic specificity. Moreover, we observe that the philanthropic contributions of a Web page-specifically, contributions of information brokerage function-are also good predictors of in-links. The observations are made in the presence of domainand topic-specific effects. Interestingly, all of these features that may predict status are local to a given Web page and within the control of the owner/author of the page. This is in contrast to the global nature of Web linkage-based metrics such as in-link count that are derived as a result of downloading and indexing billions of pages. Because the linkage structure of the Web affects browsing, crawling, and retrieval, our results have implications for vertical and general search, business intelligence, and content management.
This research explores how consumers use online decision aids with screening and evaluation support functionalities under varying product attribute-load conditions. Drawing on resource-matching theory, we conducted a 3  2 factorial experiment to test the interaction between decision aid features (i.e., low versus high-screening support, and aids with weight assignment and computation decision tools) and attribute load (i.e., large versus small number of product attributes) on decision performance. The findings reveal that: (1) where the decision aids render cognitive resources that match those demanded for the task environment, consumers will process more information and decision performance will be enhanced; (2) where the decision aids render cognitive resources that exceed those demanded for the task environment, consumers will engage in less task-related elaboration of decision-making issues to the detriment of decision performance; and (3) where the decision aids render cognitive resources that fall short of those demanded for the task environment, consumers will use simplistic heuristic decision strategies to the detriment of decision performance or invest additional effort in information processing to attain a better decision performance if they perceive the additional investments in effort to be manageable.
This study addresses the theoretically neglected interplay between organizational information technology (IT) architecture and IT governance structure in shaping IT alignment. We theoretically develop the idea that IT architecture modularity helps sustain IT alignment by increasing IT agility, and that decentralization of IT governance strengthens this relationship. IT architecture therefore complements IT governance structure. Tests of the proposed mediated-moderation model using data from 223 organizations support these ideas. Implications for theory and practice are also discussed.
As more and more firms seek to digitize their business processes and develop new digital capabilities, the enterprise systems software (ESS) has emerged as a significant industry. ESS firms offer software components (e.g., ERP, CRM, Marketing analytics) to shape their clients' digitization strategies. With rapid rates of technological and market innovation, the ESS industry consists of several horizontal markets that form around these components. As numerous vendors compete with each other within and across these markets, many of these horizontal markets appear to be crowded with rivals. In fact, multimarket contact and presence in crowded markets appear to be the pathways through which a majority of the ESS firms compete. Though the strategy literature has demonstrated the virtues of multimarket contact, paradoxically, the same literature argues that operating in crowded markets is not wise. In particular, crowded markets increase a firm's exposure to the whirlwinds of intense competition and have deleterious consequences for financial performance. Thus, the behavior of ESS firms raises an interesting anomaly and research question: Why do ESS firms continue to compete in crowded markets if they are deemed to be bad for financial performance? We argue that the effects of rivalry in crowded markets are counteracted by a different force, in the form of the economics of demand externalities. Demand externalities occur because the customers of ESS firms expect that software components from one market will be easily integrated with those that they buy from other markets. However, with rapid rates of technological innovation and market formation and dissolution, customers experience significant ambiguity in deciding which markets and components suit their needs. Therefore, they look at crowded markets as an important signal about the legitimacy and viability of specific components for their needs. Through their presence in crowded markets, ESS firms can signal their commitment to many of the components that customers might need for their digital platforms. Customers might find that such firms are attractive because their commitments to crowded markets can mitigate concerns about compatibilities between the components purchased across several markets. This unique potential for demand externality across markets suggests that ESS vendors might, in fact, benefit from competing in many crowded markets. We test our explanations through data across three time periods from a set of ESS firms that account for more than 95% of the revenue in this market. We find that ESS firms do reap performance benefits by competing in crowded markets. More importantly, we find that they can enhance their benefits from crowded markets if they face the same competitors in multiple markets, thereby increasing their multimarket contact with rivals. These results have interesting implications not just for understanding competitive conduct in the ESS industry but also in many of the emerging digital goods industries where the markets have similar competitive characteristics to the ESS industry. Our ideas complement emerging ideas about platform models of competition in the digital goods industry and provide important directions for future research.
Firms are increasingly dependent on external resources and are establishing portfolios of interorganizational relationships (IRs) to leverage external resources for competitive advantage. However, the systems of information technology (IT) and process capabilities that firms should develop to manage IR portfolios dynamically are not well-understood. In order to theorize how key structural IT capabilities (IT integration and IT reconfiguration) and competitive process capabilities (process alignment, partnering flexibility, and offering flexibility) operate as systems of complements, we draw on the competitive dynamics perspective and resource dependency theory and on the literature for IT business value, interorganizational systems, and interorganizational relationship management. We also theorize how a firm's IR portfolio moderates the effects of structural IT capabilities on competitive process capabilities and why a firm's environmental turbulence moderates the effects of complementary process capabilities on competitive performance. We test our model using survey data from 318 firms in 4 industries. Our results provide broad support for the following: (1) structural IT capabilities and process capabilities operating as a system of complements, (2) the effects of structural IT capabilities on competitive process capabilities being contingent on IR portfolio concentration, and (3) the effects of complementary process capabilities on competitive performance being contingent on environmental turbulence. We discuss the theoretical and practical implications of how firms should develop complementary systems of structural IT capabilities and competitive process capabilities to manage IR portfolios dynamically and leverage external resources.
Organizations are increasingly engaged in competitive dynamics that are enabled or induced by information technology (IT). A key competitive dynamics question for many organizations is how to build a competitive advantage in turbulence with digital IT systems. The literature has focused mostly on developing and exercising dynamic capabilities for planned reconfiguration of existing operational capabilities in fairly stable environments with patterned waves, but this may not always be possible, or even appropriate, in highly turbulent environments with unexpected storms. We introduce improvisational capabilities as an alternative means for managing highly turbulent environments; we define this as the ability to spontaneously reconfigure existing resources to build new operational capabilities to address urgent, unpredictable, and novel environmental situations. In contrast to the planned role of dynamic and operational capabilities and the ambidexterity that they jointly offer, improvisational capabilities are proposed to operate distinctly as a third hand that facilitates reconfiguration and change in highly turbulent environments. First, the paper develops the notion of improvisational capabilities and articulates the key differences between the two reconfigurationimprovisational and dynamiccapabilities. Second, the paper compares the relative effects of improvisational and dynamic capabilities in the context of new product development in different levels of environmental turbulence. Third, the paper shows how IT-leveraging capability in new product development is decomposed into its three digital IT systems: project and resource management systems, organizational memory systems (OMS), and cooperative work systemsand how each of these IT systems enhances improvisational capabilities, an effect that is accentuated in highly turbulent environments. The results show that although dynamic capabilities are the primary predictor of competitive advantage in moderately turbulent environments, improvisational capabilities fully dominate in highly turbulent environments. Besides discriminant validity, the distinction between improvisational and dynamic capabilities is evidenced by the differential effects of IT-leveraging capability on improvisational and dynamic capabilities. The results show that the more the IT-leveraging capability is catered toward managing resources (through project and resource management systems) and team collaboration (through cooperative work systems) rather than relying on past knowledge and procedures (through organizational memory systems), the more it is positively associated with improvisational capabilities, particularly in more turbulent environments. The paper draws implications for how different IT systems can influence improvisational capabilities and competitive advantage in turbulent environments, thereby enhancing our understanding of the role of IT systems on reconfiguration capabilities. The paper discusses the theoretical and practical implications of building and exercising the third hand of improvisational capabilities for IT-enabled competitive dynamics in turbulence.
Researchers in competitive dynamics have demonstrated that firms that carry out intense, complex, and heterogeneous competitive actions exhibit better performance. However, there is a need to understand factors that enable firms to undertake competitive actions. In this study, we focus on two antecedents of competitive behavior of firms: (1) access to network resources and (2) use of information technology (IT). We argue that while network structure provides firms with the opportunity to tap into external resources, the extent to which they are actually exploited depends on firms' IT-enabled capability. We develop a theoretical model that examines the relationships between IT-enabled capability, network structure, and competitive action. We test the model using secondary data, about 12 major automakers over 16 years from 1988 to 2003. We find that network structure rich in structural holes has a positive direct effect on firms' ability to introduce a greater number and a wider range of competitive actions. However, the effect of dense network structure is contingent on firms' IT-enabled capability. Firms benefit from dense network structure only when they develop a strong IT-enabled capability. Our results suggest that IT-enabled capability plays both a substitutive role, when firms do not have advantageous access to brokerage opportunities, and a complementary role, when firms are embedded in dense network structure, in the relationship between network structure and competitive actions.
This paper examines two important questions in the context of the social networking services (SNS) firms: what kind of competitive moves do SNS firms undertake and to what extent do the competitive moves impact firm performance?We blend the literature streams on information systems (IS) and strategic management and argue that given the unique characteristics of this nascent industry, SNS firms' competitive moves are likely to focus on value cocreation, as well as enhancement of the repertoire of their moves. We propose a conceptual model by blending value cocreation perspectives from the IS literature and repertoire of competitive actions from the competitive dynamics literature, and test our hypotheses using archival data. Results show that firms that emphasize value cocreation actions through the engagement of codevelopers in their technology platform and formation of strategic alliances enhance their performance. Furthermore, firms that undertake complex action repertoires achieve better performance. This study provides unique insights about the ways in which firms compete in the industry and has several implications for future research.
This study examines why firms fail or survive in the volatile software industry. We provide a novel perspective by considering how software firms' capabilities and their competitive actions affect their ultimate survival. Drawing on the resource-based view (RBV), we conceptualize capabilities as a firm's ability to efficiently transform input resources into outputs, relative to its peers. We define three critical capabilities of software-producing firmsresearch and development (RD), marketing (MK), and operations (OP)and hypothesize that in the dynamic, high-technology software industry, RD and MK capabilities are most important for firm survival. We then draw on the competitive dynamics literature to theorize that competitive actions distinguished by a greater emphasis on innovation-related moves will increase firm survival more than actions emphasizing resource-related moves. Finally, we postulate that firms' capabilities will complement their competitive actions in affecting firm survival. Our empirical evaluation examines a cross-sectional, time series panel of 5,827 observations on 870 software companies from 1995 to 2007. We use a stochastic frontier production function to measure the capability for each software firm in each time period. We then use the Cox proportional hazard regression technique to relate capabilities and competitive actions to software firms' failure rates. Unexpectedly, our results reveal that higher OP capability increases software firm survival more than higher MK and RD capabilities. Further, firms with a greater emphasis on innovation-related than resource-related competitive actions have a greater likelihood of survival, and this likelihood increases even further when these firms have higher MK and OP capabilities. Additional analyses of subsectors within the software industry reveal that firms producing visual applications (e.g., graphical and video game software) have the highest MK capability but the lowest OP and RD capabilities and make twice as many innovation-related as resource-related moves. These firms have the highest market values but the worst Altman Z scores, suggesting that they are valued highly but also are at high risk for failure, and indeed the firms in this sector fail at a greater rate than expected. In contrast, firms producing traditional decision-support applications and infrastructure software have different capabilities and make different competitive moves. Our findings suggest that the firms that persist and survive over the long term in the dynamic software industry are able to capitalize on their competitive actions because of their greater capabilities, and particularly OP capabilities.
To cut costs, companies have chosen to deliver a variety of service offerings online. However, the digital systems providing such services (e-service) have always been complemented with or supported by humanbased service (h-service). Whereas h-service has total costs that increase with the demand for services, e-service mainly requires a fixed investment upfront, which can be amortized over the totality of customers served. Considering the different nature of the costs of h-service and e-service and the heterogeneity of customer preferences for services, we derive the optimal mix of h-service and e-service for a service-providing company vis--vis its competitor. Our theoretical analysis finds the subgame-perfect Nash equilibria that determines the optimal positions in a duopoly setting. We further study the competitive dynamics of the system to examine how firms stay on the equilibrium paths. Using simulation, we investigate the effects of starting positions, small adjustments in h-service and/or e-service, and monotonic expansions of e-service on the final positioning and profits of the firms. Our results demonstrate that when firms follow a local best-reply strategy, they may end up in a position of low profitability, and when only monotonic expansions of e-service are allowed, both firms may end up overinvesting in e-service.
Using agent-based simulation experiments, we investigate the outcome of SAs between two smaller online search engine companies in competition with a dominant market leader in settings where an advertiser's decision making is the consequence of a combination of NI (e.g., an individual's willingness to follow others' decisions) and IP. In particular, we focus on a context in which the combined search engine company competes with a market leader holding a larger share of the market than the two runner-up underdogs combined. Our results indicate that, with the presence of NI and cascading effects, an alliance with only 35%-40% combined market share could compete with a leader whose market share, at the time of an alliance, is 60%-65%. Although important, size alone might be insufficient to build the market as suggested by the vanilla network effect theory. Another noteworthy finding is that a nonlinear association exists between NI and an alliance outcome; the combined runner-up companies have the best chance of success when the extent of NI is midrange, rather than on the high or low end of continuum. Contrary to the conventional view, this finding might also stimulate discussions among network science researchers. Furthermore, our results suggest that NI substantially moderates the relationship between the combined market share at the time of an alliance and the likelihood of resulting alliance success.
Using an interpretive grounded theory research approach, we investigate the utilization of organization-wide information systems in the competitive actions and responses undertaken by top managers to sustain their firms' leading competitive position. Our central contribution is a model that explicates the role of information systems in the process by which competitive actions or responses are conceived, enacted, and executed, and resulting impacts on firm performanceissues that have been largely missing from contemporary research in both the information systems and competitive dynamics domains. This study has important implications for both research and practice. Specifically, researchers should consider organizational context; the intentions and actions of key players; and the process of conceiving, enacting, and executing competitive actions or responses carried out by the organization to account for the impact of information systems on firm performance. Findings suggest that when managers envision information systems as a resource that provides opportunities for competitive actions rather than viewing information systems in a service role, competitive advantages will evolve. Furthermore, practitioners will be better able to leverage information systems investments if they recognize the embedded role of information systems within the competitive actions or responses a firm undertakes to maintain or improve relative performance.
We theoretically and empirically investigate the relationship between information technology (IT) and firm innovation. Invoking absorptive capacity (ACAP) theory, we introduce and develop the concepts of three types of IT-enabled knowledge capabilities. Firm innovation is examined through two observable innovation outcomes: patents, and new product and service introductions. These innovation outcomes are often labeled as competitive actions aggressively undertaken by firms to gain market share or to achieve profitability. We use secondary data about IT-enabled knowledge capabilities and innovation outcomes of 110 firms. Our data results provide strong support for our main assertion that knowledge capabilities that are enhanced through the use of IT contribute to firm innovation. The study's findings suggest that the three types of IT-enabled knowledge capabilities have differential effects on firm innovation. This study substantially contributes to the information systems (IS) research, methodology, and practice in multiple ways.
A growing number of vendors are using a sequence of online auctions to sell large inventories of identical items. Although bidding strategies and bidder behavior in single auctions have been extensively studied, limited research exists on bidding in sequential auctions. We seek to explain how bidders in such an environment learn from the information, and form and update their willingness to pay (WTP). Using a large data set from an online auction retailer, we analyze the evolution of the bidders' WTP as well as the effect of auction design on bidders' WTP in sequential auctions. We see our study in the context of a longitudinal field experiment, in which we were able to track actions of repeat bidders over an extended period of time. Our results show that bidders' WTP in sequential auctions can be explained from their demand characteristics, their participation experience in previous auctions, outcomes in previous auctions, and auction design parameters. We also observe, characterize, and measure what we call a modified demand reduction effect exhibited across different auctions, over time, by multiunit demand bidders. Our findings are important to enable better auction mechanism design, and more sophisticated bidding tools that explore the rich information environment of sequential auctions.
As the United States expends extraordinary efforts toward the digitization of its health-care system, and as policy makers across the globe look to information technology (IT) as a means of making health-care systems safer, more affordable, and more accessible, a rare and remarkable opportunity has emerged for the information systems research community to leverage its in-depth knowledge to both advance theory and influence practice and policy. Although health IT (HIT) has tremendous potential to improve quality and reduce costs in healthcare, significant challenges need to be overcome to fully realize this potential. In this commentary, we survey the landscape of existing studies on HIT to provide an overview of the current status of HIT research. We then identify three major areas that warrant further research: (1) HIT design, implementation, and meaningful use; (2) measurement and quantification of HIT payoff and impact; and (3) extending the traditional realm of HIT. We discuss specific research questions in each domain and suggest appropriate methods to approach them. We encourage information systems scholars to become active participants in the global discourse on health-care transformation through IT.
Current knowledge management (KM) technologies and strategies advocate two different approaches: knowledge codification and knowledge-sharing networks. However, the extant literature has paid limited attention to the interaction between them. This research draws on the literature on formal modeling of networks to examine the interaction between knowledge codification and knowledge-sharing networks. The analysis suggests that an increase in codification may damage existing network-sharing ties. Anticipating that, individuals may hoard their knowledge to protect their network ties, even when there are nontrivial rewards for codification. We find that despite the aforementioned tension between the codification and the network approach, a firm may still benefit from combining the two approaches. Specifically, when the future sharing potential between knowledge workers is high, a combination of the two approaches may outperform a codification-only or a network-only approach because the codification reward causes fewer network ties to break down, and the benefit from increased codification can offset the loss of some network ties. However, when the future sharing potential is low, an increase in codification reward can quickly break down the whole network. Thus, firms may be better off by pursuing a codification-only or a network-only strategy.
Electronic markets have been a core topic of information systems (IS) research for last three decades. We focus on a more recent phenomenon: smart markets. This phenomenon is starting to draw considerable interdisciplinary attention from the researchers in computer science, operations research, and economics communities. The objective of this commentary is to identify and outline fruitful research areas where IS researchers can provide valuable contributions. The idea of smart markets revolves around using theoretically supported computational tools to both understand the characteristics of complex trading environments and multiechelon markets and help human decision makers make real-time decisions in these complex environments. We outline the research opportunities for complex trading environments primarily from the perspective of design of computational tools to analyze individual market organization and provide decision support in these complex environments. In addition, we present broad research opportunities that computational platforms can provide, including implications for policy and regulatory research.
Enterprise systems software (ESS) is a multibillion dollar industry that produces systems components to support a variety of business functions for a widerange of vertical industry segments. Even if it forms the core of an organization's information systems (IS) infrastructure, there is little prior IS research on the competitive dynamics in this industry. Whereas economic modeling has generally provided the methodological framework for studying standards-driven industries, our research employs social network methods to empirically examine ESS firm competition. Although component compatibility is critical to organizational end users, there is an absence of industry-wide ESS standards and compatibility is ensured through interfirm alliances. First, our research observes that this alliance network does not conform to the equilibrium structures predicted by economics of network evolution supporting the view that it is difficult to identify dominant standards and leaders in this industry. This state of flux combined with the multifirm multicomponent nature of the industry limits the direct applicability of extant analytical models. Instead, we propose that the relative structural position acquired by a firm in its alliance network is a reasonable proxy for its standards dominance and is an indicator of its performance. In lieu of structural measures developed mainly for interpersonal networks, we develop a measure of relative firm prominence specifically for the business software network where benefits of alliances may accrue through indirect connections even if attenuated. Panel data analyses of ESS firms that account for over 95% of the industry revenues, show that our measure provides a superior model fit to extant social network measures. Two interesting counterintuitive findings emerge from our research. First, unlike other software industries compatibility considerations can trump rivalry concerns. We employ quadratic assignment procedure to show that firms freely form alliances even with their rivals. Second, we find that smaller firms enjoy a greater value from acquiring a higher structural position as compared to larger firms.
In this essay, we argue that pervasive digitization gives birth to a new type of product architecture: the layered modular architecture. The layered modular architecture extends the modular architecture of physical products by incorporating four loosely coupled layers of devices, networks, services, and contents created by digital technology. We posit that this new architecture instigates profound changes in the ways that firms organize for innovation in the future. We develop (1) a conceptual framework to describe the emerging organizing logic of digital innovation and (2) an information systems research agenda for digital strategy and the creation and management of corporate information technology infrastructures.
Interest in the area of virtual work continues to increase with articles being written from different disciplinary perspectives-e.g., information systems (IS), management, psychology, and transportation. In this paper, we map research on virtual work to (a) understand the intellectual base from which this field has emerged, (b) explore how this field has evolved over time, and (c) identify clusters of research themes that have emerged over time and the relationships between them. Specifically, we use cocitation analysis of research published in all social science disciplines to map the field at three points in time-1995, 2000, and 2006. Our results show that the field has grown from 9 research clusters in 1995 to 16 in 2006. A comparison across these maps suggests that research in the cluster of virtual teams has gained significance even as research in some earlier clusters such as urban planning and transportation has lost ground. Our longitudinal analysis identifies relevant concepts, theories, and methodologies that have emerged in the field of virtual work. This analysis can help interested researchers identify how they may want to contribute to the field of virtual work-by adding to popular clusters, by enriching emerging smaller clusters, or by acting as bridges across clusters.
Most information systems research until now has focused on information systems in organizations and their use by digital immigrants. Digital immigrants are those who were not born into the digital world-they learnt to use information systems at some stage in their adult lives. An underlying assumption of much of this research is that users resist technology or at least have some difficulty in accepting it. Digital natives, conversely, are those who have grown up in a world where the use of information and communications technology is pervasive and ubiquitous. These ubiquitous technologies, networks, and associated systems have proliferated and have woven themselves into the very fabric of everyday life. This article suggests that the rise of the digital native, along with the growth of ubiquitous information systems (UIS), potentially represents a fundamental shift in our paradigm for IS research. We propose a research agenda that focuses on digital natives and UIS.
Multisourcing, the practice of stitching together best-of-breed IT services from multiple, geographically dispersed service providers, represents the leading edge of modern organizational forms. While major strides have been achieved in the last decade in the information systems (IS) and strategic management literature in improving our understanding of outsourcing, the focus has been on a dyadic relationship between a client and a vendor. We demonstrate that a straightforward extrapolation of such a dyadic relationship falls short of addressing the nuanced incentive-effort-output linkages that arise when multiple vendors, who are competitors, have to cooperate and coordinate to achieve the client's business objectives. We suggest that when multiple vendors have to work together to deliver end-to-end services to a client, the choice of formal incentives and relational governance mechanisms depends on the degree of interdependence between the various tasks as well as the observability and verifiability of output. With respect to cooperation, we find that a vendor must not only put effort in a primary task it is responsible for but also cooperate through helping effort in enabling other vendors perform their primary tasks. In the context of coordination, we find that task redesign for modularity, OLAs, and governance structures such as the guardian vendor model represent important avenues for further research. Based on the analysis of actual multisourcing contract details over the last decade, interviews with leading practitioners, and a review of the single-sourcing literature, we lay a foundation for normative theories of multisourcing and present a research agenda in this domain.
Online participation engenders both the benefits of knowledge sharing and the risks of harm. Vigilant interaction in knowledge collaboration refers to an interactive emergent dialogue in which knowledge is shared while it is protected, requiring deep appraisals of each others' actions in order to determine how each action may influence the outcomes of the collaboration. Vigilant interactions are critical in online knowledge collaborations under ambivalent relationships where users collaborate to gain benefits but at the same time protect to avoid harm from perceived vulnerabilities. Vigilant interactions can take place on discussion boards, open source development, wiki sites, social media sites, and online knowledge management systems and thus is a rich research area for information systems researchers. Three elements of vigilant interactions are described: trust asymmetry, deception and novelty. Each of these elements challenges prevailing theory-based assumptions about how people collaborate online. The study of vigilant interaction, then, has the potential to provide insight on how these elements can be managed by participants in a manner that allows knowledge sharing to proceed without harm.
Past research has studied how the selection and use of control portfolios in software projects is based on environmental and task characteristics. However, little research has examined the consequences of control mode choices on project performance. This paper reports on a study that addresses this issue in the context of outsourced software projects. In addition, we propose that boundary-spanning activities between the vendor and the client enable knowledge sharing across organizational and knowledge domain boundaries. This is expected to lead to facilitation of control through specific incentives and performance norms that are suited to client needs as well as the vendor context. Therefore, we argue that boundary spanning between the vendor and client moderates the relationship between formal controls instituted by the vendor on the development team and project performance. We also hypothesize the effect of collaboration as a clan control on project performance. We examine project performance in terms of software quality and project efficiency. The research model is empirically tested in the Indian software industry setting on a sample of 96 projects. The results suggest that formal and informal control modes have a significant impact on software project outcomes, but need to be finely tuned and directed toward appropriate objectives. In addition, boundary-spanning activities significantly improve the effectiveness of formal controls. Finally, we find that collaborative culture has provided mixed benefits by enhancing quality but reducing efficiency.
Prior studies investigating business-to-consumer e-commerce have focused predominantly on online shopping by individuals on their own, although consumers often desire to conduct their shopping activities with others. This study explores the important, but seldom studied, topic of collaborative online shopping. It investigates two design components that are pertinent to collaborative online shopping support tools, namely, navigation support and communication support. Results from a laboratory experiment indicate that compared to separate navigation, shared navigation effectively reduces uncoupling (i.e., the loss of coordination with one's shopping partner) incidents per product discussed and leads to fewer communication exchanges dedicated to resolving each uncoupling incident, thereby enhancing coordination performance. Compared to text chat, voice chat does not help reduce the occurrence of uncoupling, but likely increases the efficiency in resolving uncoupling. The results further show that shared navigation and voice chat can significantly enhance the collaborative shoppers' perceptions of social presence derived from their online shopping experiences. The interaction effect on social presence implies that the benefit of shared navigation is higher in the presence of text chat than in the presence of voice chat.
Process virtualization occurs when a process that relies upon physical interaction between people and/or objects is transitioned to a virtual environment. Process virtualization is having profound effects on society, as an increasing number of both business and nonbusiness processes such as those related to education, medicine, and dating are being migrated to virtual environments. There is a vast literature that relates to process virtualization topics, but it is fragmented across different domains. The purpose of this paper is to propose a research agenda to develop high-level theories and frameworks that inform the general process virtualization phenomenon. Developing these theories and frameworks will synthesize existing knowledge and provide a theoretical foundation upon which to add new knowledge as it is created. This will help policy makers maximize the substantial benefits of virtual processes while minimizing the risks. Given the background, interests, and skills of IS scholars, the IS discipline is well suited to lead in this endeavor.
Research on software piracy often relies on self-reports by individual users and thus suffers from possible response distortion attributable to a variety of human motivations. Conclusions drawn directly from distorted self-reports may misguide managerial and policy decisions. The randomized response technique (RRT) was proposed as a remedy to response distortion. In this paper, a model based on RRT was used to illustrate how truthful responses to sensitive questions can be empirically estimated. The model was tested in two empirical studies on software piracy. Consistent with our expectations, respondents responding to RRT were more willing to disclose sensitive information about their attitudes, intentions, and behaviors on software piracy. Nontrivial distortions were demonstrated in causal relationships involving sensitive and nonsensitive variables. The study extends RRT to multivariate analysis and illustrates the feasibility and usefulness of the method in studying sensitive behavioral issues in the information systems (IS) domain.
Since the inauguration of information systems research (ISR) two decades ago, the information systems (IS) field's attention has moved beyond administrative systems and individual tools. Millions of users log onto Facebook, download iPhone applications, and use mobile services to create decentralized work organizations. Understanding these new dynamics will necessitate the field paying attention to digital infrastructures as a category of IT artifacts. A state-of-the-art review of the literature reveals a growing interest in digital infrastructures but also confirms that the field has yet to put infrastructure at the centre of its research endeavor. To assist this shift we propose three new directions for IS research: (1) theories of the nature of digital infrastructure as a separate type of IT artifact, sui generis; (2) digital infrastructures as relational constructs shaping all traditional IS research areas; (3) paradoxes of change and control as salient IS phenomena. We conclude with suggestions for how to study longitudinal, large-scale sociotechnical phenomena while striving to remain attentive to the limitations of the traditional categories that have guided IS research.
Sponsored search is the mechanism whereby advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) web search results. Based on prior literature, we draw an analogy between these markets and financial markets. We use the analogy as well as the key differences to present a theoretical framework consisting of a set of research questions about the pricing of keywords and design choices available to firms in sponsored search markets. These questions define an agenda for future research in sponsored search markets. They also have practical implications for advertisers and online marketplaces such as search engines and social media sites that support advertising.
The article offers information on the development and the changes of the editorship of the journal Information Systems Research (IRS) in the U.S. It states that E. Burton Swanson is the first appointed editor-in-chief of the journal in 1987, where editorial policy and accomplishments are being highlights. Moreover, the second editorship is passed to John Leslie King in 1992 and resolve two major issues such as work submission of top researchers in the field and quality of work being published. Furthermore, the third editorship is passed down to Izak Benbasat in 1999, where he established a Senior Editor Board.
The emergence of software-based platforms is shifting competition toward platform-centric ecosystems, although this phenomenon has not received much attention in information systems research. Our premise is that the coevolution of the design, governance, and environmental dynamics of such ecosystems influences how they evolve. We present a framework for understanding platform-based ecosystems and discuss five broad research questions that present significant research opportunities for contributing homegrown theory about their evolutionary dynamics to the information systems discipline and distinctive information technology-artifact-centric contributions to the strategy, economics, and software engineering reference disciplines.
The Internet and related information technologies are transforming the distribution of product sales across products, and these effects are likely to grow in coming years. Both the Long Tail and the Superstar effect are manifestations of these changes, yet researchers lack consistent metrics or models for integrating and extending their insights and predictions. In this paper, we begin with a taxonomy of the technological and nontechnological drivers of both Long Tails and Superstars and then define and compare the key metrics for analyzing these phenomena. The core of the paper describes a large and promising set of questions forming a research agenda. Important opportunities exist for understanding future changes in sales concentration patterns; the impact on supply chains (including cross-channel competition, competition within the Internet channel, implications for the growth of firms, and the balance of power within the supply chain); implications for pricing, promotion, and product design; and, ultimately, the potential effects on society in general. Our approach provides an introduction to some of the relevant research findings and allows us to identify opportunities for cross-pollination of methods and insights from related research topics.
This paper starts from the premise that the simultaneous increase in environmental turbulence, the requisite speed of organizational change, and the intensified ubiquity of digital technologies are spawning a phenomenon that is messy, complex, and chaotic. Accordingly, we need to change the way we examine how information technology (IT) can help organizations build a strategic advantage in turbulent environments. We propose a more systemic and holistic perspective to theory building and testing in the information system (IS) strategy area and correspondingly appropriate methods that capture the complexity of this phenomenon. We term this phenomenon digital ecodynamics, defined as the holistic confluence among environmental turbulence, dynamic capabilities, and IT systems-and their fused dynamic interactions unfolding as an ecosystem. We believe that a more holistic understanding of digital ecodynamics will fuel the next leap in knowledge in the IS strategy area. First, extending the strategic management literature that has mainly focused on two-way interactions between environmental turbulence and dynamic capabilities, we foreground IT systems as a third central element. We use a threesome tango analogy with strong mutual interdependence to accentuate our view of digital ecodynamics-while also stressing the emerging role of IT systems in triggering environmental turbulence and shaping dynamic capabilities to build a strategic advantage. Second, we propose a different paradigmatic lens (configuration theories) as an appropriate inquiring system to better understand the complexity of digital ecodynamics. The paper articulates the key aspects of configuration theories as inquiring systems, compares them with the more common variance theories and process theories, and illustrates the power of recent advances in configurational methods. Third, we create a preliminary roadmap for IS researchers to better examine digital ecodynamics using novel structural properties afforded by configuration theories (i.e., mutual causality, discontinuity, punctuated equilibria, nonlinear change). Fourth, we reflect on the broader opportunities that the configurational perspective of digital ecodynamics can create for IS strategy research. The paper ends by highlighting the double-barreled opportunity that digital ecodynamics renders, both as an energizing vision for IS strategy research and also as a reshaper of strategic management research and practice in a turbulent and digitized world.
This paper studies the cross-country diffusion of personal computers (PCs) and the Internet, and examines how the diffusive interactions across these technologies affect the evolution of the global digital divide. We adopt a generalized diffusion model that incorporates the impact of one technology's installed base on the diffusion of the other technology. We estimate the model on data from 26 developing and developed countries between 1991 and 2005. We find that the codiffusion effects between PCs and the Internet are complementary in nature and the impact of PCs on Internet diffusion is substantially stronger in developing countries as compared to developed ones. Furthermore, our results suggest that these codiffusive effects are a significant driver of the narrowing of the digital divide. We also examine the policy implications of our results, especially with respect to how complementarities in the diffusion of PC and Internet technologies might be harnessed to further accelerate the narrowing of the global digital divide.
Virtual worlds are immersive, simulated, persistent, and dynamic environments that include rich graphical three dimensional spaces, high fidelity audio, motion, viewpoint, and interactivity. Initially dismissed as environments of play, virtual worlds have gained legitimacy in business and educational settings for their application in globally distributed work, project management, online learning, and real-time simulation. Understanding the emergent aspects of these virtual worlds and their implications for organizations will require both new theories and new methods. We propose that a performative perspective may be particularly useful as it challenges the existence of independent objects with fixed or given properties and boundaries, and focuses instead on situated and relational practices that enact entangled and contingent boundaries, entities, identities, and effects.
We review and reframe three main quests of research on information systems (IS) strategy: (1) the strategic alignment quest, (2) the integration quest, and (3) the sustained competitive advantage quest. The assumptions and logic of these quests have become less relevant in increasingly complex adaptive business systems (CABS), where the competitive performance landscapes of products and services are highly dynamic and co-evolve. We revise the strategic alignment quest to propose a co-evolution quest that addresses not only competitive strategy questions of a firm but also corporate strategy questions. The co-evolution quest seeks to increase a firm's agility and dynamism in repositioning itself, identifying profitable product-market positions as the evolving competitive landscape erodes the profitability of the firm's existing positions. To support the co-evolution quest, we revise the integration quest and propose a reconfiguration quest that encompasses not only business processes but also products and services, as well as the contracts, resources, and transactions associated with them. As the firm makes repositioning moves to co-evolve with the competitive landscape, the reconfiguration quest seeks to increase the firm's agility in disintegrating its existing nexus of contracts, resources, and transactions that support the old positions and in reconfiguring new ones that support the new positions. Finally, we revise the sustained competitive advantage quest to propose a renewal quest that recognizes the temporary nature of competitive advantage in CABS. The renewal quest seeks to destabilize the firm's old sources of competitive advantage when competitive dynamics erode their utility, rapidly create new sources of competitive advantage, and concatenate a series of temporary advantages over time. The three reframed quests provide the foundation for a research agenda on IS strategy in CABS.
Although efficiency-enhancing features of online markets have been well studied, much less is known about firms' differentiation strategies in these competitive markets or the outcomes of such differentiation. This study examines competition among firms in online sponsored search marketsone of the fastest growing and most competitive of online markets. We develop and test a model that predicts the clickthrough rate (CTR) of a seller's listing in a sponsored search setting. Drawing on consumer search theory and competitive positioning strategies, we theorize that CTR is jointly driven by a seller's positioning strategy as reflected by the unique selling proposition (USP) in its ad creative, by its rank in a sponsored search listing, and by the nature of competition around the focal firm's listing. We use data from a field experiment conducted by a leading firm in the mortgage industry where the firm varied its rank and USP dynamically. Results suggest that sponsored search listings can act as effective customer segmentation mechanisms, consistent with a model of consumer search in directional markets. We further find that the effect on CTR of a firm's positioning strategy and its rank in a listing is strongly moderated by its ability to differentiate itself from adjacent rivals. We discuss the implications of our findings for sellers' strategies in sponsored search markets and for extending the understanding of consumer search behavior in directional markets.
Despite much hype about electronic payments systems (EPSs), a 2004 survey establishes that close to 80% of between-business payments are still made using paper-based formats. We present a finite mixture logit model to predict likelihood of EPS adoption in business-to-business (B2B) settings. Our model simultaneously classifies firms into homogeneous segments based on firm-specific characteristics and estimates the model's coefficients relating predictor variables to EPS adoption decisions for each respective segment. While such models are increasingly making their presence felt in the marketing literature, we demonstrate their applicability to traditional information systems (IS) problems such as technology adoption. Using the finite mixture approach, we predict the likelihood of EPS adoption using a unique data set from a Fortune 100 company. We compare the finite mixture model with a variety of traditional approaches. We find that the finite mixture model fits the data better, controlling for the number of parameters estimated; that our explicit model-based segmentation leads to a better delineation of segments; and that it significantly improves the predictive accuracy in holdout samples. Practically, the proposed methodology can help business managers develop actionable segment-specific strategies for increasing EPS adoption by their business partners. We discuss how the methodology is potentially applicable to a wide variety of IS research.
In markets that exhibit network effects, the presence of digital conversion technologies provides an alternative mechanism to achieve compatibility. This study examines the impact of conversion technologies on market equilibrium in the context of sequential duopoly competition and proprietary technology standards. We analyze this question by departing from the extant literature to endogenize the decision to provide a converter and incorporate explicit negotiations between firms concerning the extent of conversion. We argue that these choices better reflect the environment facing firms in digital goods industries and find that these decisions change some of the established results in the literature. Specifically, we find that unless network effects are very large, the subgame-perfect equilibrium (SPNE) involves firms' agreeing to provide digital converters at a sufficiently low price to all consumers. At this equilibrium, both the entrant and the incumbent are better off because the provision of converters alleviates price competition in the market and leads to both higher product revenues and higher proceeds from the sale of converters. Moreover, under some circumstances, the provision of converters is welfare enhancing. These findings have important implications for research and practice in the adoption of new digital goods as the introduction of conversion technologies can reduce the social costs of standardization without compromising the benefits of network effects.
Many successful open-source projects have been developed by programmers who were employed by firms but worked on open-source projects on the side because of economic incentives like career improvement benefits. Such side work may be a good thing for the employing firms, too, if they get some strategic value from the open-source software and if the productivity of the programmers on these projects improves through learning-by-doing effects. However, the programmers may work more or less on these projects than what is best for the firms. To manage the programmers' efforts, the firms set appropriate employment policies and incentives. These policies and career concerns then together govern the programmers' effort allocation between the open-source and proprietary projects. We examine this relationship using a variant of the principal/agent model. We derive and characterize optimal employment contracts and show that firms either offer a bonus for only one of the two projects or do not offer any bonuses. However, if attractive alternate employment opportunities are available, they change their strategy and may offer bonuses for both projects simultaneously.
The digital divide has loomed as a public policy issue for over a decade. Yet, a theoretical account for the effects of the digital divide is currently lacking. This study examines three levels of the digital divide. The digital access divide (the first-level digital divide) is the inequality of access to information technology (IT) in homes and schools. The digital capability divide (the second-level digital divide) is the inequality of the capability to exploit IT arising from the first-level digital divide and other contextual factors. The digital outcome divide (the third-level digital divide) is the inequality of outcomes (e.g., learning and productivity) of exploiting IT arising from the second-level digital divide and other contextual factors. Drawing on social cognitive theory and computer self-efficacy literature, we developed a model to show how the digital access divide affects the digital capability divide and the digital outcome divide among students. The digital access divide focuses on computer ownership and usage in homes and schools. The digital capability divide and the digital outcome divide focus on computer self-efficacy and learning outcomes, respectively. This model was tested using data collected from over 4,000 students in Singapore. The results generate insights into the relationships among the three levels of the digital divide and provide a theoretical account for the effects of the digital divide. While school computing environments help to increase computer self-efficacy for all students, these factors do not eliminate knowledge the gap between students with and without home computers. Implications for theory and practice are discussed.
The status quo of prohibiting broadband service providers from charging websites for preferential access to their customers-the bedrock principle of net neutrality (NN)is under fierce debate. We develop a game-theoretic model to address two critical issues of NN: (1) Who are gainers and losers of abandoning NN? (2) Will broadband service providers have greater incentive to expand their capacity without NN? We find that if the principle of NN is abolished, the broadband service provider stands to gain from the arrangement, as a result of extracting the preferential access fees from content providers. Content providers are thus left worse off, mirroring the stances of the two sides in the debate. Depending on parameter values in our framework, consumer surplus either does not change or is higher in the short run. When compared to the baseline case under NN, social welfare in the short run increases if one content provider pays for preferential treatment but remains unchanged if both content providers pay. Finally, we find that the incentive to expand infrastructure capacity for the broadband service provider and its optimal capacity choice under NN are higher than those under the no-net-neutrality (NNN) regime, except in some specific cases. Under NN, the broadband service provider always invests in broadband infrastructure at the socially optimal level but either under- or overinvests in infrastructure capacity in the absence of NN.
Two important gaps exist in the information systems (IS) alignment research. First, there is scant research on the potential of organizational culture, and specifically subcultures to influence the strategic alignment of IS and organizations. Second, there is a dearth of literature that considers the relationship between alignment and implementation success. In this paper, we address both of these gaps by considering the influence of organizational subcultures on the alignment of a specific ISa knowledge management system (KMS)with organizational strategy. Our analysis demonstrates the important roles played by three different subculturesenhancing, countercultural, and chameleonin the alignment of the KMS. The analysis also underscores the complementary nature of the alignment and implementation literatures and suggests that they should be used in concert to explain the success of an IS. Drawing on our analysis, we build a subculture model, which depicts the intersection of alignment and implementation. From a managerial perspective, the subculture model highlights three different approaches to managing alignment and implementation. From a theoretical perspective, our paper highlights the need for IS alignment models to be modified, so that subunit-level analyses are incorporated. It also illustrates that organizations confront challenges of alignment and implementation simultaneously rather than sequentially.
A large body of research in economics, information systems, and marketing has sought to understand sources of price dispersion. Previous empirical work has mainly offered consumer- and/or product-based explanations for this phenomenon. In contrast, our research explores the key role played by vendors' price-format adoption in explaining price dispersion. We empirically analyze over a half-million online and offline prices offered by major U.S. airlines in the top 500 domestic markets. Our study shows that a vendor's price format remains an important source of price dispersion in both channels even after accounting for other factors known to impact dispersion in airline ticket prices. Importantly, this finding is true for both transacted and posted tickets. We document several other interesting empirical findings. First, the lower variance in the prices of "everyday low price" (EDLP) firms serves to reduce the market-level dispersion in prices when such firms are present. Moreover, the price variance of non-EDLP firms in these markets is also lower than in those markets in which EDLP competitors are absent. Second, we also find that dispersion in offered prices increases closer to the departure date, which is consistent with theoretical assertion that price dispersion increases with reservation prices. Finally, we continue to observe dispersion of online prices even after accounting for vendor strategy and other known sources of dispersion, suggesting that the prices are unlikely to converge even in the presence of sophisticated online search mechanisms
Initially popularized by Amazon.com, recommendation technologies have become widespread over the past several years. However, the types of recommendations available to the users in these recommender systems are typically determined by the vendor and therefore are not flexible. In this paper, we address this problem by presenting the recommendation query language REQUEST that allows users to customize recommendations by formulating them in the ways satisfying personalized needs of the users. REQUEST is based on the multidimensional model of recommender systems that supports additional contextual dimensions besides traditional User and Item dimensions and also OLAP-type aggregation and filtering capabilities. This paper also presents the recommendation algebra RA, shows how REQUEST recommendations can be mapped into this algebra, and analyzes the expressive power of the query language and the algebra. This paper also shows how users can customize their recommendations using REQUEST queries through a series of examples.
Although many organizations are implementing knowledge management systems (KMS), there is little empirical evidence about whether KMS use can improve individual performance, and how time and experience influence the value derived from KMS use. Using hierarchical linear modeling (HLM) statistical analysis, we examined the impact of using a codification-based KMS on the sales performance of 2,154 sales representatives in a pharmaceutical firm over a 24-month period. We found that KMS had significant positive impacts on individual performance and that these performance benefits grew over time. Moreover, experience moderated the relationship between KMS use and individual performance. Knowledge workers with more experience were able to more quickly absorb and apply the knowledge from the KMS than were those with less experience, who took longer to benefit from KMS use. However, over time experience played a diminishing role in leveraging performance gains from KMS use, and knowledge workers with less experience eventually derived similar performance benefits as those of their more experienced counterparts.
Although online retailers detail their privacy practices in online privacy policies, this information often remains invisible to consumers, who seldom make the effort to read and understand those policies. This paper reports on research undertaken to determine whether a more prominent display of privacy information will cause consumers to incorporate privacy considerations into their online purchasing decisions. We designed an experiment in which a shopping search engine interface clearly and compactly displays privacy policy information. When such information is made available, consumers tend to purchase from online retailers who better protect their privacy. In fact, our study indicates that when privacy information is made more salient and accessible, some consumers are willing to pay a premium to purchase from privacy protective websites. This result suggests that businesses may be able to leverage privacy protection as a selling point.
Application service providers (ASP), who host and maintain information technology (IT) applications across the Internet, emerged as an innovation in the way IT services are delivered to client firms. In spite of many potential benefits of this model, ASPs experienced business failure and high rates of exit. Drawing on agency theory, we argue that the efficiency of contracting arrangements between ASPs and client organizations is an important determinant of ASP survival. We test this prediction using a unique data set combining multiple sources that allows us to track an ASP from the year of founding through the beginning of 2006. Contractual misalignment, or adopting contracts mismatched with the underlying agency costs, significantly lowers the probability of survival of service providers in the ASP marketplace. The impact of misalignment is particularly severe when coupled with adjustment costs that impede the transition to aligned contracts. To account for potential heterogeneity in ASPs' knowledge of contracting, we test for endogenous self-selection of ASPs in the relationship between contractual misalignment and survival. Our results are robust to a variety of model specifications as well as alternate explanations of survival from multiple theoretical domains.
Combinatorial auctions are used for the efficient allocation of heterogeneous goods and services. They require appropriate software platforms that provide automated winner determination and decision support for bidders. Several promising ascending combinatorial auction formats have been developed throughout the past few years based on primal-dual algorithms and linear programming theory. The ascending proxy auction and iBundle result in Vickrey payoffs when the coalitional value function satisfies buyer submodularity conditions and bidders bid their best responses. These auction formats are based on nonlinear and personalized ask prices. In addition, there are a number of designs with linear prices that have performed well in experiments, the approximate linear prices auction, and the combinatorial clock auction. In this paper, we provide the results of lab experiments that tested these different auction formats in the same setting. We analyze aggregate metrics such as efficiency and auctioneer revenue for small- and medium-sized value models. In addition, we provide a detailed analysis not only of aggregate performance metrics but also of individual bidding behaviour under alternative combinatorial auction formats.
Digital inequality, or unequal access to and use of information and communication technologies (ICT), is a severe problem preventing the socioeconomically disadvantaged (SED) from participating in a digital society. To understand the critical resources that contribute to digital inequality and inform public policy for stimulating initial and continued ICT usage by the SED, we drew on capital theories and conducted a field study to investigate: (1) the forms of capital for using ICT and how they differ across potential adopters who are SED and socioeconomically advantaged (SEA); (2) how these forms of capitals are relatively impacted for the SEA and the SED through public policy for ICT access; and (3) how each form of capital influences the SED's intentions to use initially and to continue to use ICT. The context for our study involved a city in the southeastern United States that offered its citizens free ICT access for Internet connectivity. Our results show that SED potential adopters exhibited lower cultural capital but higher social capital relative to the SEA. Moreover, the SED who participated in the city's initiative realized greater positive gains in cultural capital, social capital, and habitus than the SEA. In addition, we find that the SED's initial intention to use ICT was influenced by intrinsic motivation for habitus, self-efficacy for cultural capital, and important referents' expectations and support from acquaintances for social capital. Cultural capital and social cultural capital also complemented each other in driving the SED's initial use intention. The SED's continued use intention was affected by both intrinsic and extrinsic motivations for habitus and both knowledge and self-efficacy for cultural capital but was not affected by social capital. We also make several recommendations for future research on digital inequality and ICT acceptance to extend and apply the proposed capital framework.
Extant research considers the IT governance choice to be a trade-off between the cost-efficiency of centralization and the responsiveness provided by local information processing. This view predicts that firms tend to decentralize IT governance in more uncertain environments. We investigate this issue by studying the relationship between environmental uncertainty and IT infrastructure governance in a sample of business units from Fortune 1000 companies. The key proposition in this paper is that the relationship between environmental uncertainty and decentralization in IT infrastructure governance is best characterized as a curvilinear relationship. That is, when environmental uncertainty increases from low to high, firms tend to first decentralize their IT infrastructure decisions to the business units to enhance their responsiveness; and then centralize their IT infrastructure decisions to the headquarters as uncertainty increases further, to achieve the benefits of coordination and to mitigate the potential agency problem in uncertain environments. Moreover, the study proposes that business unrelatedness between business units and their headquarters moderates the curvilinear relationship between environmental uncertainty and IT infrastructure governance. We find that both the propositions are supported by the data.
Historically, the use of peer-to-peer (P2P) networks has been limited primarily to user-initiated exchanges of (mostly music) files over the Internet. This traditional view of P2P networks is changing, however, and the use of P2P networks has been suggested for delivering general-purpose content over the Web (or corporate intranets), even in real time. We analyze sharing in a P2P community in this new context under three different congestion measures: delay, jitter, and packet loss. Sharing is important to study in the presence of congestion because most existing research on P2P networks views congestion in the network as a relatively insignificant criterion. However, when delivering general-purpose content, congestion and its relationship to sharing is a critical factor that influences end-user performance. This paper looks at P2P networks from this new perspective by explicitly considering the effects of congestion on user incentives for sharing. We also propose a simple incentive mechanism that induces socially optimal sharing.
In this paper, we empirically investigate the impact of information technology (IT) investment on firm return and risk financial performance, emphasizing the moderating role of the firm boundary strategies of diversification and vertical integration. Our results indicate a sharp contrast between the direct and interactive effects of IT on both the return (profitability) and risk (variability of returns) dimensions. Although the direct effect of IT capital is to increase firm risk for a given level of return, we find that suitable boundary strategies can moderate the impact of IT on firm performance in a way that increases return and decreases risk, at the margin. This interaction effect is strongest in service firms, in firms with high levels of IT investment intensity, and in more recent time periods. Our results are robust to alternative proxies for firm risk, including an ex ante risk measure (variability of analysts' earnings estimates), and alternative risk-return specifications. Put together, our results provide new insights into how IT and firm boundary strategies interact to affect the risk and return performance of firms.
Price dispersion is an important indicator of market efficiency. Internet-based electronic markets have the potential to reduce transaction and search costs, thereby creating more efficient, frictionless markets, as predicted by theories in information economics. However, earlier work has reported significant levels of price dispersion on the Internet, which is in contrast to theoretical predictions. A key feature of the existing stream of work has been its use of posted prices to estimate price dispersion. In theory, this can lead to an overestimation of price dispersion because a sale may not have occurred at the posted price. In this research, we use a unique data set of actual transaction prices collected from both the electronic and offline markets of buyers in a business-to-business market to evaluate the extent of price dispersion. We find that price dispersion in the electronic market is as low as 0.22%, which is substantially less than that reported in the existing literature. This near-zero price dispersion suggests that in some electronic markets the law of one price can prevail when we consider transaction prices, instead of posted prices. We further develop a theoretical framework that identifies several new drivers of price dispersion using transaction data. In particular, we focus on four product-level and market-level attributes-product cost, order cycle time, own price elasticity, and transaction quantity, and we estimate their impact on price dispersion. We also examine the electronic market's moderating role in the relationship between these drivers and price dispersion. Finally, we estimate the efficiency gains that accrue from transactions in the relatively friction-free market and find that the electronic market can enhance consumer surplus by as much as $97.92 million per year.
The Internet is making a significant transition from primarily a network of desktop computers to a network variety of connected information devices such as personal digital assistants and global positioning system-based devices. On the other hand, new paradigms such as overlay networks are defining service-based logical architecture for the network services that make locating content and routing more efficient. Along with Internet2' s proposed service-based routing, overlay networks will create a new set of challenges in the provision and management of content over the network. However, a lack of proper infrastructure investment incentive may lead to an environment where network growth may not keep pace with the service requirements. In this paper, we present an analysis of investment incentives for network infrastructure owners under two different pricing strategies: congestion-based negative externality pricing and the prevalent flat-rate pricing. We develop a theoretically motivated gradient-based heuristic to compute maximum capacity that a network provider will be willing to invest in under different pricing schemes. The heuristic appropriates different capacities to different network components based on demand for these components. We then use a simulation model to compare the impact of dynamic congestion-based pricing with flat-rate pricing on the choice of capacity level by the infrastructure provider. The simulation model implements the heuristic and ensures that near-optimal level of capacity is allocated to each network component by checking theoretical optimality conditions. We investigate the impact of a variety of factors, including the per unit cost of capacity of a network resource, average value of the users' requests, average level of users' tolerance for delay, and the level of exogenous demand for services on the network. Our results indicate that relationships between these factors are crucial in determining which of the two pricing schemes results in a higher level of socially optimal network capacity. The simulation results provide a possible explanation for the evolution of the Internet pricing from time-based to flat-rate pricing. The results also indicate that regardless of how these factors are related, the average stream of the net benefits realized under congestion-based pricing tends to be higher than the average net benefits realized under flat-rate pricing. These central results point to the fallacy of the arguments presented by the supporters of net neutrality that do not consider the incentives for private investment in network capacity.
This paper aims to understand the influence of punishment and perceived justice on user compliance with mandatory information technology (IT) policies. Drawing on punishment research and justice theory, a research model is developed. Data collected from a field survey of enterprise resource planning (ERP) users are analyzed to test the proposed hypotheses. The results indicate that IT compliance intention is strongly influenced by perceived justice of punishment, which is negatively influenced by actual punishment. When perceived justice of punishment is considered, the effect of satisfaction on compliance intention decreases and that of perceived usefulness becomes insignificant. This paper contributes to information systems (IS) research and practice by drawing attention to the importance of punishment, particularly perceived justice of punishment, in mandatory IT settings. It delineates the relationships among actual punishment, punishment expectancy, perceived justice of punishment, and IT compliance intention, and thus provides a better understanding of user compliance behavior in mandatory IT settings.
We study agency problems that arise when prototypes are used for requirements assessment. The precision with which the prototype helps a client assess his requirements depends on (a) the type of prototype provided by the vendor and (b) the client's feedback effort. The vendor can provide either a neutral or nonneutral prototype: The nonneutral prototype influences the client towards one particular set of requirements that may not be the true requirement, and the neutral prototype allows the client to assess his true requirements. This leads to the vendor's moral hazard problem. The client chooses to exert either the high or low feedback effort after the vendor provides the prototype. Because the effort is unobservable to the vendor, it can lead to the client exerting the low feedback effort: the client's commitment problem. In this paper we develop and discuss the role of the contract payment to provide the vendor with incentives to supply the neutral prototype, as well as for the client to commit to the high feedback effort. In this setting, we also examine the anchoring effect, wherein even a high-feedback effort can influence the client more toward a particular set of requirements with the nonneutral prototype. Our results highlight the interplay among the feedback effort, anchoring, and vendor payments.
Although companies have spent a great deal of money to adopt CRM (customer relationship management) technologies, many have not seen satisfactory returns on their CRM implementations. We study optimal CRM implementation strategies and the impact of CRM investments on profitability. For our analysis, we classify CRM technologies into two broad categories: targeting-related and support-related technologies. While targeting CRM improves the success rate of distinguishing between nonloyal and loyal customers, support CRM increases the probability of retaining the loyalty of existing customers. We also consider the costs of implementing each CRM type separately as well as both types simultaneously. We show that the optimal CRM implementation strategy depends on the initial mass of loyal customers and diseconomies of scale in simultaneous implementation. We also find that the two types of CRM technologies are substitutive rather than complementary in generating revenue. We discuss why it is difficult to avoid overinvestments in CRM when the nature of the investments is misunderstood. We study the optimal CRM implementation scope and the impact of different types of CRM on customers. We develop a model that not only considers both the revenue and costs sides but is also helpful in determining the deployment of right CRM technology in the right scope.
As healthcare becomes increasingly digitized, the promise of improved care enabled by technological advances inevitably must be traded off against any unintended negative consequences. There is little else that is as consequential to an individual as his or her health. In this context, the privacy of one's personal health information has escalated as a matter of significant concern for the public. We pose the question: under what circumstances will individuals be willing to disclose identified personal health information and permit it to be digitized? Using privacy boundary theory and recent developments in the literature related to risk-as-feelings as the core conceptual foundation, we propose and test a model explicating the role played by type of information requested (general health, mental health, genetic), the purpose for which it is to be used (patient care, research, marketing), and the requesting stakeholder (doctors/hospitals, the government, pharmaceutical companies) in an individual's willingness to disclose personal health information. Furthermore, we explore the impact of emotion linked to one's health condition on willingness to disclose. Results from a nationally representative sample of over 1,000 adults underscore the complexity of the health information disclosure decision and show that emotion plays a significant role, highlighting the need for re-examining the timing of consent. Theoretically, the study extends the dominant cognitive-consequentialist approach to privacy by incorporating the role of emotion. It further refines the privacy calculus to incorporate the moderating influence of contextual factors salient in the healthcare setting. The practical implications of this study include an improved understanding of consumer concerns and potential impacts regarding the electronic storage of health information that can be used to craft policy.
Despite the significant potential for performance gains from health IT (HIT), there has been limited study of the mechanisms underlying successful HIT implementations. We conducted an extensive longitudinal field study to gain an understanding of the interplay between technology and patterns of clinical work embodied in routines. We use the analytical device of narrative networks to identify where and how HIT influences patterns of work. We further draw upon adaptive structuration theory to conceptualize HIT as an intervention that alters the flow of events in a narrative network. Our findings suggest that the key to successful implementation is to manage the co-evolution process between routines and HIT and to actively orchestrate a virtuous cycle through agentic action. We propose a dynamic process model of adaptive routinization of HIT that delineates the major channels through which HIT and routines interact, identifies the different stages in the dynamic co-evolution process, and isolates the pivotal role of two forms of agency in enabling the virtuous cycle of co-evolution. This is one of the first studies to offer a processual, microlevel analysis of HIT implementation in a clinical setting.
In this paper we examine the use of electronic patient records (EPR) by clinical specialists in their development of multidisciplinary care for diagnosis and treatment of breast cancer. We develop a practice theory lens to investigate EPR use across multidisciplinary team practice. Our findings suggest that there are oppositional tendencies towards diversity in EPR use and unity which emerges across multidisciplinary work, and this influences the outcomes of EPR use. The value of this perspective is illustrated through the analysis of a yearlong, longitudinal case study of a multidisciplinary team of surgeons, oncologists, pathologists, radiologists, and nurse specialists adopting a new EPR. Each group adapted their use of the EPR to their diverse specialist practices, but they nonetheless orientated their use of the EPR to each others' practices sufficiently to support unity in multidisciplinary teamwork. Multidisciplinary practice elements were also reconfigured in an episode of explicit negotiations, resulting in significant changes in EPR use within team meetings. Our study contributes to the growing literature that questions the feasibility and necessity of achieving high levels of standardized, uniform health information technology use in healthcare.
Increasing global connectivity makes emerging infectious diseases (EID) more threatening than ever before. Various information systems (IS) projects have been undertaken to enhance public health capacity for detecting EID in a timely manner and disseminating important public health information to concerned parties. While those initiatives seemed to offer promising solutions, public health researchers and practitioners raised concerns about their overall effectiveness. In this paper, we argue that the concerns about current public health IS projects are partially rooted in the lack of a comprehensive framework that captures the complexity of EID management to inform and evaluate the development of public health IS. We leverage loose coupling to analyze news coverage and contact tracing data from 479 patients associated with the severe acute respiratory syndrome (SARS) outbreak in Taiwan. From this analysis, we develop a framework for outbreak management. Our proposed framework identifies two types of causal circlescoupling and decoupling circlesbetween the central public health administration and the local capacity for detecting unusual patient cases. These two circles are triggered by important information-centric activities in public health practices and can have significant influence on the effectiveness of EID management. We derive seven design guidelines from the framework and our analysis of the SARS outbreak in Taiwan to inform the development of public health IS. We leverage the guidelines to evaluate current public health initiatives. By doing so, we identify limitations of existing public health IS, highlight the direction future development should consider, and discuss implications for research and public health policy.
Information systems have great potential to reduce healthcare costs and improve outcomes. The purpose of this special issue is to offer a forum for theory-driven research that explores the role of IS in the delivery of healthcare in its diverse organizational and regulatory settings. We identify six theoretically distinctive elements of the healthcare context and discuss how these elements increase the motivation for, and the salience of, the research results reported in the nine papers comprising this special issue. We also provide recommendations for future IS research focusing on the implications of technology-driven advances in three areas: social media, evidence-based medicine, and personalized medicine.
The electronic gulf between shoppers and products makes evaluating a physical product on offer at an e-store a potentially problematic activity. We propose that the outcome of the product evaluation task is determined by the fit between the type of information provided and the type of information sought by the consumer and that this, in turn, influences a consumer's attitude toward an e-store. An experiment to compare the impact of one type of advanced evaluation support technology, the virtual model, with a more basic online catalog, is then described. Results indicate that virtual models are potentially valuable when a customer is concerned with self-image and considerably less valuable when concerned with functionality. In more general terms, variation in end-user attitudes toward the object of the task (evaluative attitude) influenced how informed consumers felt about a product when using different technologies. Feeling informed, in turn, had a strong effect on consumer attitudes toward the store. Our results highlight two important issues for online stores: (1) a consumer's information requirements depend on his or her attitude to a product rather than product attributes; and (2) meeting or not meeting these information requirements affects perceptions of the store. Business success in this context therefore appears to hinge on addressing the specific functional and image-related information needs of customers rather than simply providing more interactivity or technical functionality.
The information systems (IS) literature has focused considerable research on IS resistance, particularly in the health-care industry. Most of this attention has focused on the impact of IS resistance on systems' initial implementation, but little research has investigated whether and how post-adoption resistance affects performance. We focus on a particular type of post-adoption resistance, which we call IS avoidance, to identify situations in which individuals avoid working with adopted IS despite the need and opportunity to do so. We examine the effects of IS avoidance on patient care delivered by health-care groups across three levels of analysis: the individual level, the shared group level, and the configural group level. We find that IS avoidance is significantly and negatively related to patient care only at the configural group level, which suggests that patient care is not degraded by the number of doctors and/or nurses in a group avoiding a system, but rather by their locations in the group's workflow network configuration. We use qualitative data collected over 16 months at the research site to help explain these results. Implications for theory and practice are discussed.
This paper analyzes interactions between a firm that seeks to discriminate between normal users and hackers that try to penetrate and compromise the firm's information assets. We develop an analytical model in which a variety of factors are balanced to best manage the detection component within information security management. The approach not only considers conventional factors such as detection rate and false-positive rate, but also factors associated with hacker behavior that occur in response to improvements in the detection system made by the firm. Detection can be improved by increasing the system's discrimination ability (i.e., the ability to distinguish between attacks and normal usage) through the application of maintenance effort. The discrimination ability deteriorates over time due to changes in the environment. Also, there is the possibility of sudden shocks that can sharply degrade the discrimination ability. The firm's cost increases as hackers become more knowledgeable by disseminating security knowledge within the hacker population. The problem is solved to reveal the presence of a steady-state solution in which the level of system discrimination ability and maintenance effort are held constant. We find an interesting result where, under certain conditions, hackers do not benefit from disseminating security knowledge among one another. In other situations, we find that hackers benefit because the firm must lower its detection rate in the presence of knowledge dissemination. Other insights into managing detection systems are provided. For example, the presence of security shocks can increase or decrease the optimal discrimination level as compared to the optimal level without shocks.
To improve operational efficiencies while providing state of the art healthcare services, hospitals rely on information technology enabled physician referral systems (IT-PRS). This study examines learning curves in an IT-PRS setting to determine whether agents achieve performance improvements from cumulative experience at different rates and how information technologies transform the learning dynamics in this setting. We present a hierarchical Bayes model that accounts for different agent skills (domain and system) and estimate learning rates for three types of referral requests: emergency (EM), nonemergency (NE), and nonemergency out of network (NO). Furthermore, the model accounts for learning spillovers among the three referral request types and the impact of system upgrade on learning rates. We estimate this model using data from more than 80,000 referral requests to a large IT-PRS. We find that: (1) The IT-PRS exhibits a learning rate of 4.5% for EM referrals, 7.2% for NE referrals, and 12.3% for NO referrals. This is slower than the learning rate of manufacturing (on average 20%) and more comparable to other service settings (on average, 8%). (2) Domain and system experts are found to exhibit significantly different learning behaviors. (3) Significant and varying learning spillovers among the three referral request types are also observed. (4) The performance of domain experts is affected more adversely in comparison to system experts immediately after system upgrade. (5) Finally, the learning rate change subsequent to system upgrade is also higher for system experts in comparison to domain experts. Overall, system upgrades are found to have a long-term positive impact on the performance of all agents. This study contributes to the development of theoretically grounded understanding of learning behaviors of domain and system experts in an IT-enabled critical healthcare service setting.
We investigate the incentive issues that surround the adoption and sharing of electronic health records (EHR) and the potential role of a personal health record (PHR) platform in facilitating data sharing. Through our analysis, we find evidence that health-care providers may not have an incentive to share patients' records electronically even though EHR systems will increase consumer surplus, especially in the presence of provider heterogeneity and myopic consumers. In this context, we find that an independent PHR platform can create incentives for the providers to share their patients' records electronically with other providers by selectively subsidizing them. In a pluralistic health-care system like that in the United States, where health-care providers have varying incentives to implement electronic health records, an online PHR platform can provide a proxy for a national health information network,' wherein consumers can freely exchange their health records among competing providers.
We use panel data from multiple wards from two hospitals spanning a three-year period to investigate the impact of automation of the core error prevention functions in hospitals on medical error rates. Although there are studies based on anecdotal evidence and self-reported data on how automation impacts medical errors, no systematic studies exist that are based on actual error rates from hospitals. Further, there is no systematic evidence on how incremental automation over time and across multiple wards impacts the rate of medical errors. The primary objective of our study is to fill this gap in the literature by empirically examining how the automation of core error prevention functions affects two types of medical errors. We draw on the medical informatics literature and principal-agency theory and use a unique panel data set of actual documented medical errors from two major hospitals to analyze the interplay between automation and medical errors.We hypothesize that the automation of the sensing function (recording and observing agent actions) will have the greatest impact on reducing error rates. We show that there are significant complementarities between quality management training imparted to hospital staff and the automation of control systems in reducing interpretative medical errors. We also offer insights to practitioners and theoreticians alike on how the automation of error prevention functions can be combined with training in quality management to yield better outcomes. Our results suggest an optimal implementation path for the automation of error prevention functions in hospitals.
Web personalization allows online merchants to customize Web content to serve the needs of individual customers. Using data mining and clickstream analysis techniques, merchants can now adapt website content in real time to capture the current preferences of online customers. Though the ability to offer adaptive content in real time opens up new business opportunities for online merchants, it also raises questions of timing. One question is when to present personalized content to consumers. Consumers prefer early presentation that eases their selection process, whereas adaptive systems can make better personalized content if they are allowed to collect more consumers' clicks over time. A review of personalization research confirms that little work has been done on these timing issues in the context of personalized services. The current study aims to fill that gap. Drawing on consumer search theory, we develop hypotheses about consumer responses to differences in presentation timing and recommendation type and the interaction between the two. The findings establish that quality improves over the course of an online session but the probability of considering and accepting a given recommendation diminishes over the course of the session. These effects are also shown to interact with consumer expertise, providing insights on the interplay between the different design elements of a personalization strategy.
With the strong ongoing push toward investment in and deployment of electronic healthcare (e-healthcare) systems, understanding the factors that drive the use of such systems and the consequences of using such systems is of scientific and practical significance. Elaborate training in new e-healthcare systems is not a luxury that is typically available to healthcare professionalsi.e., doctors, paraprofessionals (e.g., nurses) and administrative personnelbecause of the 247 nature and criticality of operations of healthcare organizations, especially hospitals, thus making peer interactions and support a key driver of or barrier to such e-healthcare system use. Against this backdrop, using social networks as a theoretical lens, this paper presents a nomological network related to e-healthcare system use. A longitudinal study of an e-healthcare system implementation, with data gathered from doctors, paraprofessionals, administrative personnel, patients, and usage logs lent support to the hypotheses that: (1) ingroup and outgroup ties to doctors negatively affect use in all user groups; (2) ingroup and outgroup ties to paraprofessionals and administrative personnel positively affect use in both those groups, but have no effect on doctors' use; and (3) use contributes positively to patient satisfaction mediated by healthcare quality variablesi.e., technical quality, communication, interpersonal interactions, and time spent. This work contributes to the theory and practice related to the success of e-healthcare system use in particular, and information systems in general.
An important area of information systems (IS) research has been the identification of the individual-level beliefs that enable technology acceptance such as the usefulness, reliability, and flexibility of a system. This study posits the existence of additional beliefs that inhibit usage intentions and thus foster technology rejection rather than acceptance. We theorize that these inhibitors are more than just the antipoles of enablers (e.g., the opposite of usefulness or reliability) and so are distinct constructs worthy of their own investigation. Inhibitors are proposed to have effects on usage intentions beyond that of enablers as well as effects on enablers themselves. We report on a series of empirical studies designed to test the existence and effects of inhibitors. A candidate set of six inhibitors is shown to be distinct from enablers. These inhibitors are subsequently tested in a field study of 387 individuals nested within 32 different websites. Effects at both individual and website unit levels of analysis are tested using multilevel modeling. We find that inhibitors have negative effects on usage intentions, as well as on enablers, and these effects vary contingent upon individual or website unit levels of analysis. The overall results support the existence and importance of inhibitors in explaining individual intent to use-or not use-technology.
Firms need to balance efficiency gains obtained through exploiting existing knowledge assets with long-term competitive viability achieved through exploring new knowledge resources. Because the use of knowledge management systems (KMSs) continues to expand, understanding how these systems affect exploration and exploitation practices at the individual level is important to advance both knowledge management theory and practice. This study reports the results of a multi-industry survey investigating how psychological climate and KMS access influence solution reuse (exploitation) and solution innovation (exploration) in the context of technical support work. Our results show that KMS access does not directly determine solution innovation or solution reuse. Instead, KMS access strengthens the positive relationship between a climate for innovation and solution innovation and reverses the positive relationship between a climate for autonomy and solution innovation. The implications for knowledge management research and practice are discussed.
In this paper, we analyze a model of usage pricing for digital products with discontinuous supply functions. This model characterizes a number of information technology-based products and services for which variable increases in demand are fulfilled by the addition of blocks of computing or network infrastructure. Such goods are often modeled as information goods with zero variable costs; in fact, the actual cost structure resembles a mixture of zero marginal costs and positive periodic fixed costs. This paper discusses the properties of a general solution for the optimal nonlinear pricing of such digital goods. We show that the discontinuous cost structure can be accrued as a virtual constant variable cost. This paper applies the general solution to solve two related extensions by first investigating the optimal technology capacity planning when the cost function is both discontinuous and declining over time, and then characterizing the optimal costing for the discontinuous supply when it is shared by several business profit centers. Our findings suggest that the widely adopted full cost recovery policies are typically suboptimal.
Information specialists in enterprises regularly use distributed information retrieval (DIR) systems that query a large number of information retrieval (IR) systems, merge the retrieved results, and display them to users. There can be considerable heterogeneity in the quality of results returned by different IR servers. Further, because different servers handle collections of different sizes and have different processing and bandwidth capacities, there can be considerable heterogeneity in their response times. The broker in the DIR system has to decide which servers to query, how long to wait for responses, and which retrieved results to display based on the benefits and costs imposed on users. The benefit of querying more servers and waiting longer is the ability to retrieve more documents. The costs may be in the form of access fees charged by IR servers or user's cost associated with waiting for the servers to respond. We formulate the broker's decision problem as a stochastic mixed-integer program and present analytical solutions for the problem. Using data gathered from FedStats-a system that queries IR engines of several U.S. federal agencies-we demonstrate that the technique can significantly increase the utility from DIR systems. Finally, simulations suggest that the technique can be applied to solve the broker's decision problem under more complex decision environments.
Online discussion communities have become a widely used medium for interaction, enabling conversations across a broad range of topics and contexts. Their success, however, depends on participants' willingness to invest their time and attention in the absence of formal role and control structures. Why, then, would individuals choose to return repeatedly to a particular community and engage in the various behaviors that are necessary to keep conversation within the community going? Some studies of online communities argue that individuals are driven by self-interest, while others emphasize more altruistic motivations. To get beyond these inconsistent explanations, we offer a model that brings dissimilar rationales into a single conceptual framework and shows the validity of each rationale in explaining different online behaviors. Drawing on typologies of organizational commitment, we argue that members may have psychological bonds to a particular online community based on (a) need, (b) affect, and/or (c) obligation. We develop hypotheses that explain how each form of commitment to a community affects the likelihood that a member will engage in particular behaviors (reading threads, posting replies, moderating the discussion). Our results indicate that each form of community commitment has a unique impact on each behavior, with need-based commitment predicting thread reading, affect-based commitment predicting reply posting and moderating behaviors, and obligation-based commitment predicting only moderating behavior. Researchers seeking to understand how discussion-based communities function will benefit from this more precise theorizing of how each form of member commitment relates to different kinds of online behaviors. Community managers who seek to encourage particular behaviors may use our results to target the underlying form of commitment most likely to encourage the activities they wish to promote.
Online storage service providers grant a way for companies to avoid spending resources on maintaining their own in-house storage infrastructure and thereby allowing them to focus on their core business activities. These providers, however, follow a fixed, posted pricing strategy that charges the same price in each time period and thus bear all the risk arising out of demand uncertainties faced by their client companies. We examine the effects of providing a spot market with dynamic prices and forward contracts to hedge against future revenue uncertainty. We derive revenue-maximizing spot and forward prices for a single seller facing a known set of buyers. We perform a simulation study using publicly available traffic data regarding Amazon S3 clients from Alexa.com to validate our analytical results. Our field study supports our analysis and indicates that spot markets alone can enhance revenues to Amazon, but this comes at the cost of increased risks due to the increased market share in the spot markets. Furthermore, adding a forward contract feature to the spot markets can reduce risks while still providing the benefits of enhanced revenues. Although the buyers incur an increase in costs in the spot market, adding a forward contract does not cause any additional cost increase while transferring the risk to the buyers. Thus, storage grid providers can greatly benefit by applying a forward contract alongside the spot market.
Prior studies on product recommendation agents (RAs) have been based on the effort-accuracy perspective in which the amount of effort required to make a decision and the accuracy of such decisions are two dominant antecedents of user acceptance of RAs. The current study extends the effort-accuracy perspective by considering trade-off difficulty, a type of negative emotion that arises when attainment of one's goals is blocked by the attainment of other goals; consequently, one must make trade-offs among the conflicting goals. Many product purchase choices for which RAs are used require users to make trade-offs among conflicting product attributes. A key feature of RAs, the preference elicitation method (PEM), often compels users to make explicit trade-offs. We examine whether an RA's PEM generates trade-off difficulty, which, in turn, affects users' evaluations (i.e., perceived amount of effort and perceived accuracy of recommendations) and the resultant acceptance of the RA. Trade-off difficulty influences users' evaluations of an RA via perceived control over execution of the RA PEM. In addition, the decision context in which users employ a PEM moderates the degree to which that PEM generates trade-off difficulty. Specifically, a PEM generates a greater degree of trade-off difficulty in a choice context that leads to a loss than in a choice context that leads to a gain. Consequently, users exert more effort to cope with trade-off difficulty in a loss condition. Because users voluntarily spend more effort, the negative influence of perceived effort on users' acceptance of an RA-which is supported in prior studies-decreases in a loss condition. A laboratory experiment was conducted using two between-subject factors: two RAs, one that employed a trade-off-compelling PEM and the other a trade-off-hiding PEM, and two decision contexts, one of which was a loss condition and the other a gain condition. The results supported all of the hypotheses.
Record linkage techniques have been widely used in areas such as antiterrorism, crime analysis, epidemiologic research, and database marketing. On the other hand, such techniques are also being increasingly used for identity matching that leads to the disclosure of private information. These techniques can be used to effectively reidentify records even in deidentified data. Consequently, the use of such techniques can lead to individual privacy being severely eroded. Our study addresses this important issue and provides a solution to resolve the conflict between privacy protection and data utility. We propose a data-masking method for protecting private information against record linkage disclosure that preserves the statistical properties of the data for legitimate analysis. Our method recursively partitions a data set into smaller subsets such that data records within each subset are more homogeneous after each partition. The partition is made orthogonal to the maximum variance dimension represented by the first principal component in each partitioned set. The attribute values of a record in a subset are then masked using a double-bounded swapping method. The proposed method, which we call multivariate swapping trees, is nonparametric in nature and does not require any assumptions about statistical distributions of the original data. Experiments conducted on real-world data sets demonstrate that the proposed approach significantly outperforms existing methods in terms of both preventing identity disclosure and preserving data quality.
This paper introduces the idea of drawing upon the cognitive neuroscience literature to inform IS research (herein termed NeuroIS). Recent advances in cognitive neuroscience are uncovering the neural bases of cognitive, emotional, and social processes, and they offer new insights into the complex interplay between IT and information processing, decision making, and behavior among people, organizations, and markets. The paper reviews the emerging cognitive neuroscience literature to propose a set of seven opportunities that IS researchers can use to inform IS phenomena, namely (1) localizing the neural correlates of IS constructs, (2) capturing hidden mental processes, (3) complementing existing sources of IS data with brain data, (4) identifying antecedents of IS constructs, (5) testing consequences of IS constructs, (6) inferring the temporal ordering among IS constructs, and (7) challenging assumptions and enhancing IS theories. The paper proposes a framework for exploring the potential of cognitive neuroscience for IS research and offers examples of potentially fertile intersections of cognitive neuroscience and IS research in the domains of design science and human-computer interaction. This is followed by an example NeuroIS study in the context of e-commerce adoption using fMRI, which spawns interesting new insights. The challenges of using functional neuroimaging tools are also discussed. The paper concludes that there is considerable potential for using cognitive neuroscience theories and functional brain imaging tools in IS research to enhance IS theories.
This study develops a stochastic model to capture developer learning dynamics in open source software projects (OSS). A hidden Markov model (HMM) is proposed that allows us to investigate (1) the extent to which individuals learn from their own experience and from interactions with peers, (2) whether an individual's ability to learn from these activities varies as she evolves/learns over time, and (3) to what extent individual learning persists over time. We calibrate the model based on six years of detailed data collected from 251 developers working on 25 OSS projects hosted at Sourceforge. Using the HMM, three latent learning states (high, medium, and low) are identified, and the marginal impact of learning activities on moving the developer between these states is estimated. Our findings reveal different patterns of learning in different learning states. Learning from peers appears to be the most important source of learning for developers across the three states. Developers in the medium learning state benefit the most through discussions that they initiate. On the other hand, developers in the low and the high states benefit the most by participating in discussions started by others. While in the low state, developers depend entirely upon their peers to learn, whereas in the medium or high state, they can also draw upon their own experiences. Explanations for these varying impacts of learning activities on the transitions of developers between the three learning states are provided. The HMM is shown to outperform the classical learning curve model. The HMM modeling of this study contributes to the development of a theoretically grounded understanding of learning behavior of individuals. Such a theory and associated findings have important managerial and operational implications for devising interventions to promote learning in a variety of settings.
This study develops and tests the idea that the cross-business information technology integration (CBITI) capability of an acquirer creates significant value for shareholders of the acquirer in mergers and acquisitions (M&A). In M&A, integrating the IT systems and IT management processes of acquirer and target could generate benefits such as (a) the consolidation of IT resources and the reduction of overall IT costs of the combined firm, (b) the development of an IT-based coordination mechanism and the realization of cross-firm business synergies, (c) the minimization of potential disruptions to business operations, and (d) greater ability to comply with relevant laws and regulations and the reduction of regulatory compliance costs. We test these ideas in a sample of 141 acquisitions conducted by 86 Fortune 1000 firms. In the short run, acquirers that have high levels of CBITI capabilities receive positive and significant cumulative abnormal returns to their M&A announcements. Announcement period returns indicate that the capital markets value CBITI similarly in sameindustry and different-industry acquisitions. In the long run, acquirers with high levels of CBITI capabilities obtain significantly higher abnormal operating performance. They create significantly greater value in complementary acquisitions from different industries than in related acquisitions from the same industry. The findings have important implications for M&A research and practice.
This study extends existing information technology (IT) productivity research by evaluating the contributions of spending in IT outsourcing using a production function framework and an economy wide panel data set from 60 industries in the United States over the period from 1998 to 2006. Our results demonstrate that IT outsourcing has made a positive and economically meaningful contribution to industry output and labor productivity. It has not only helped industries produce more output, but it has also made their labor more productive. Moreover, our analysis of split data samples reveals systematic differences between high and low IT intensity industries in terms of the degree and impact of IT outsourcing. Our results indicate that high IT intensity industries use more IT outsourcing as a percentage of their output, but less as a percentage of their own IT capital, and they achieve higher returns from IT outsourcing. This finding suggests that to gain greater value from IT outsourcing, firms need to develop IT capabilities by intensively investing in IT themselves. By comparing the results from subperiods and analyzing a separate data set for the earlier period of 1987-1999, we conclude that the value of IT outsourcing has been stable from 1998 to 2006 and consistent over the past two decades. The high returns we find for IT outsourcing also suggest that firms may be underinvesting in IT outsourcing.
Collaborative filtering algorithms learn from the ratings of a group of users on a set of items to find personalized recommendations for each user. Traditionally they have been designed to work with one-dimensional ratings. With interest growing in recommendations based on multiple aspects of items, we present an algorithm for using multicomponent rating data. The presented mixture model-based algorithm uses the component rating dependency structure discovered by a structure learning algorithm. The structure is supported by the psychometric literature on the halo effect. This algorithm is compared with a set of model-based and instancebased algorithms for single-component ratings and their variations for multicomponent ratings. We evaluate the algorithms using data from Yahoo! Movies. Use of multiple components leads to significant improvements in recommendations. However, we find that the choice of algorithm depends on the sparsity of the training data. It also depends on whether the task of the algorithm is to accurately predict ratings or to retrieve relevant items. In our experiments a model-based multicomponent rating algorithm is able to better retrieve items when training data are sparse. However, if the training data are not sparse, or if we are trying to predict the rating values accurately, then the instance-based multicomponent rating collaborative filtering algorithms perform better. Beyond generating recommendations we show that the proposed model can fill in missing rating components. Theories in psychometric literature and the empirical evidence suggest that rating specific aspects of a subject is difficult. Hence, filling in the missing component values leads to the possibility of a rater support system to facilitate gathering of multicomponent ratings.
Companies that outsource information technology (IT) services usually focus on achieving multiple objectives. Correspondingly, outsourcing contracts typically specify a variety of metrics to measure and reward (or penalize) vendor performance. The specific types of performance metrics included in a contract strongly affect its incentive content and ultimately its outcome. One specific challenge is the measurement of performance when an outsourcing arrangement has a mix of objectives, some that are highly measurable and others that are not. Recent advances in contract theory suggest that the design of incentives for a given objective is affected by the characteristics of other objectives. However, there is little empirical work that demonstrates how relevant these multitask concerns are in real-world contracts. We apply contract theory to examine how objectives and incentives are related in IT outsourcing contracts that include multiple objectives with varying measurement costs. In our context, contracts generally share the objective of reducing IT costs but vary in the importance of increasing IT quality. We establish empirical results about performance measurement in IT outsourcing contracts that are consistent with recent theoretical propositions. We find that the use of strong direct incentives for a given measurable objective is negatively correlated with the presence of less measurable objectives in the contract. We show that outsourcing contracts that emphasize goals with high measurement costs employ more performance metrics than initiatives whose objectives have a lower measurement cost profile. Surprisingly, as the number of performance metrics increases, satisfactory outcomes decrease, which we explain within a multitask theory framework. Overall, our results provide empirical support for multitask principal-agent theory and important guidance in designing outsourcing contracts for complex IT services.
From a database perspective, business constraints provide an accurate picture of the real world being modeled and help enforce data integrity. Typically, rules are gathered during requirements analysis and embedded in code during the implementation phase. We propose that the rules be explicitly modeled during conceptual design, and develop a framework for understanding and classifying spatiotemporal set-based (cardinality) constraints and an associated syntax. The constraint semantics are formally specified using first-order logic. Modeling rules in conceptual design ensures they are visible to designers and users and not buried in application code. The rules can then be semiautomatically translated into logical design triggers yielding productivity gains. Following the principles of design science research, we evaluate the framework's expressiveness and utility with a case study.
Online word-of-mouth (WOM) such as consumer opinions, user experiences, and product reviews has become a major information source in consumer purchase decisions. Prior research on online WOM effect has focused mostly on low-involvement products such as books or CDs. For these products, retailer-hosted (internal) WOM is shown to influence sales overwhelmingly. Numerous surveys, however, suggest consumers often conduct pre-purchase searches for high-involvement products (e.g., digital cameras) and visit external WOM websites during the search process. In this study, we analyze the relative impact of external and internal WOMs on retailer sales for high-involvement products using a panel of sales and WOM data for 148 digital cameras from Amazon.com and three external WOM websites (Cnet, DpReview, and Epinions) over a four-month period. The results suggest that a retailer's internal WOM has a limited influence on its sales of high-involvement products, while external WOM sources have a significant impact on the retailer's sales. The findings imply that external WOM sources play an important role in the information search process.
Open-source software development is the next stage in the evolution of product development, particularly software products. Compared with the prevailing proprietary approaches, open-source software products are developed by co-opting external developers and prospective users. Although a core group of developers might still play a key role in the initial design and development, a notable aspect of the open-source software paradigm is the role of peripheral developers in the enhancement and popularization of the product. Peripheral developers are not formal members of the core development team. They voluntarily contribute their time and creative talent in improving the quality of the product or in popularizing the product through word-of-mouth advocacy. As volunteers, they are not subject to the traditional hierarchical controls, nor are they contractually obligated. Peripheral developers represent a novel and unique aspect of open-source software development, and there is a greater interest in tapping their potential. However, there has been limited evidence about how and when their participation has beneficial impacts. We examine how peripheral developers contribute to product quality and diffusion by utilizing longitudinal data on 147 open-source software products. Hierarchical linear modeling analysis indicates that peripheral developers make significant contributions to product quality and diffusion, especially on projects that are in the more mature stages of product development.
Peer-to-peer sharing networks have seen explosive growth recently. In these networks, sharing files is completely voluntary, and there is no financial reward for users to contribute. However, many users continue to share despite the massive free-riding by others. Using a large-scale data set of individual activities in a peer-topeer music-sharing network, we seek to understand users' continued-sharing behavior as a private contribution to a public good. We find that the more benefit users get from the network, in the form of downloads, browses, and searches, the more likely they are to continue sharing. Also, the more value users give to the network, in the form of downloads by other users and recognition by the network, the more likely they are to continue sharing. Moreover, our findings suggest that, overall, getting from is a stronger force for the continued-sharing decision than giving to.
Prior research concerning IT business value has established a link between firm-level IT investment and tangible returns such as output productivity. Research also suggests that IT is vital to intermediate processes such as those that produce intangible output. Among these, the use of IT in innovation and knowledge creation processes is perhaps the most critical to a firm's long-term success. However, little is known about the relationship between IT, knowledge creation, and innovation output. In this study, we contribute to the literature by comprehensively examining the contribution of IT to innovation production across multiple contexts using a quality-based measure of innovation output. Analyzing annual information from 1987 to 1997 for a panel of large U.S. manufacturing firms, we find that a 10% increase in IT input is associated with a 1.7% increase in innovation output for a given level of innovation-related spending. This relationship between IT, research and development (R&D), and innovation production is robust across multiple econometric methodologies and is found to be particularly strong in the mid to late 1990s, a period of rapid technological innovation. Our results also demonstrate the importance of IT in creating value at an intermediate stage of production, in this case, through improved innovation productivity. However, R&D and its related intangible factors (skill, knowledge, etc.) appear to play a more crucial role in the creation of breakthrough innovations.
The highly competitive and rapidly changing market for online services is becoming increasingly effective at locking users in through the coercive effects of switching costs. Although the information systems field increasingly recognizes that switching costs plays a big part in enforcing loyalty, little is known about what factors users regard as switching costs or why they perceive these costs. Consequently, it is hard for online services to know what lock-in strategies to use and when to apply them. We address this problem by first developing a theory-driven structure of online users' perceived switching costs that distinguishes between vendor-related and user-related factors. We then propose that important antecedent influences on switching costs from economic value, technical self-efficacy, and past investments are more complex and intertwined than previously thought. We empirically validated the proposed model using data collected from home users of Internet service providers. Our findings demonstrate that an online service's economic value more heavily influences users' perceptions of vendor-related switching costs than does technical self-efficacy. However, users' technical abilities outweigh economic value in influencing user-related switching costs. Furthermore, although we confirmed the commonly held notion that deeply invested users are generally more vulnerable to lockin, we also found that this relationship is contingent on users' technical abilities. Finally, we found that our multidimensional measure of switching costs is a valid predictor of user loyalty and is more powerful than previous global measures. Overall, this study uncovered a finer network of switching-cost production than had been previously established and suggests a new approach to modeling and exploiting online users' perceived switching costs.
The Internet has brought consumers increased access to information to make purchase decisions. One of the expected consequences is an increase in the price elasticity of demand, or the percent change in demand caused by a percent change in price, because consumers are better able to compare offerings from multiple suppliers. In this paper, we analyze the impact of the Internet on demand, by comparing the demand functions in the Internet and traditional air travel channels. We use a data set that contains information for millions of records of airline ticket sales in both online and offline channels. The results suggest that consumer demand in the Internet channel is more price elastic for both transparent and opaque online travel agencies (OTAs), in part, because of more leisure travelers self-selecting the online channel, relative to business travelers. Yet, after controlling for this channel self-selection effect, we still find differences in price elasticity across channels. We find that the opaque OTAs are more price elastic than the transparent OTAs, which suggests that product information can mitigate the price pressures that arise from Internet-enabled price comparisons. We discuss the broader implications for multichannel pricing strategy and for the transparency-based design of online selling mechanisms.
The introduction of product upgrades in a competitive environment is commonly observed in the software industry. When introducing a new product, a software vendor may employ behavior-based price discrimination (BBPD) by offering a discount over its market price to entice existing customers of the competitor. This type of pricing is referred to as competitive upgrade discount pricing and is possible because the vendor can use proof of purchase of a competitor's product as credible evidence to offer the discount. At the same time, the competitor may offer a discount to its own previous customers in order to induce them to buy its upgrade. We formulate a game-theoretic model involving an incumbent and entrant where both firms can offer discounts to existing customers of the incumbent. Although several equilibrium possibilities exist, we establish that an equilibrium with competitive upgrade discount pricing is observed only for a unique market structure and a corresponding unique set of prices. In this equilibrium, instead of leveraging its first mover advantage, the incumbent cedes market share to the entrant. Furthermore, the profits of both the incumbent and the entrant reduce with switching costs. This implies that the use of BBPD has product design implications because firms may influence the switching costs between their products by making appropriate compatibility decisions. In addition, lower switching costs result in reduced consumer surplus. Hence, a social planner may want to increase switching costs. The resulting policy implications are different from those prevalent in other industries such as mobile telecommunications where the regulators reduced switching costs by enforcing number portability.
The paper presents insights regarding the key learning-related factors a buyer should consider when deciding the extent to which information about bids is revealed in a procurement auction context. It offers the insights by analyzing the following two first-price sealed-bid policies in a private-value sequential auction with no winner dropouts: (i) iis, where only the winner's bid is revealed, and (ii) cis, where all bids are revealed. Our analysis identifies two important learning effectsthe extraction and the deception effectsas having significant welfare implications. Both these effects arise because of a bidder's desire to gain an informational advantage relative to his competitors, but their manifestations are different. The extraction effect occurs because of a bidder's incentive to learn about his competitors, and the deception effect is a consequence of the incentive to prevent an opponent from gaining the information. Both effects lead to higher bid prices, and either may be dominant from a procurer surplus standpoint. With the deception effect, social welfare can decrease even when the number of suppliers increases, a result that is counterintuitive. The paper also discusses how insights regarding the learning effects might apply to other policies.
The single site implementation study is an invaluable tool for studying the large-scale enterprise solution. Together with constructivist frameworks and ethnographic approaches it has allowed the development of rich local pictures of the immediate and adaptive response by user organizations to the take-up of what are, today, often generic packaged systems. However, to view the packaged enterprise solution principally at the place where the user encounters it also has limitations. It produces somewhat partial understandings of these complex artifacts. In particular, it downplays important influences from other sites and time frames. This paper argues that if we are to understand the full implications of enterprise solutions for organizations then we should study their biography. This idea points to how the career of workplace technology is often played out over multiple time frames and settings. To understand its shaping therefore requires scholars to go beyond the study of technology at a single locale or moment and, rather, attempt to follow it through space and time. The paper develops two ideas to aid this kind of study. We discuss better spatial metaphors that might help us explore the hybrid and extended spaces in which packaged software systems develop and evolve. We also review improved temporal understandings that may capture the multiple contemporary and historical time frames at play. The paper concludes by discussing some possible research directions that a focus on the biography of a technology might allow.
This paper empirically analyzes the joint effect of switching costs and network effects in determining the level of competition in the European mobile communications industry. Theoretical reasoning argues that switching costs and network effects may confer some market power that firms can strategically exploit to reduce competition and thus increase profits. Theoretical predictions are completely confirmed by the empirical evidence and important asymmetries between the market structures in the different European countries can be observed. These asymmetries are clearly related to the levels of switching costs and network effectsthe greater their importance, the lower the rivalry in the market. This suggests that the recent efforts of policymakers to reduce the negative consequences of switching costs and network effects have not been successful enough and these efforts must be strengthened, at least in several countries.
This paper is motivated by the success of YouTube, which is attractive to content creators as well as corporations for its potential to rapidly disseminate digital content. The networked structure of interactions on YouTube and the tremendous variation in the success of videos posted online lends itself to an inquiry of the role of social influence. Using a unique data set of video information and user information collected from YouTube, we find that social interactions are influential not only in determining which videos become successful but also on the magnitude of that impact. We also find evidence for a number of mechanisms by which social influence is transmitted, such as (i) a preference for conformity and homophily and (ii) the role of social networks in guiding opinion formation and directing product search and discovery. Econometrically, the problem in identifying social influence is that individuals' choices depend in great part upon the choices of other individuals, referred to as the reflection problem. Another problem in identification is to distinguish between social contagion and user heterogeneity in the diffusion process. Our results are in sharp contrast to earlier models of diffusion, such as the Bass model, that do not distinguish between different social processes that are responsible for the process of diffusion. Our results are robust to potential self-selection according to user tastes, temporal heterogeneity and the reflection problem. Implications for researchers and managers are discussed.
This study develops a comprehensive model to predict and explain the use of collaborative technologies (CT) and the task performance of individual users as a result of using CT. The integrated model attempts to capture how the individual user's extent of use of CT is a function of both the technical features and the structures embedded within or created by the interactions among the technology, group, and organization. The model developed is tested using data collected from a national bank with 279 members working in 40 different workgroups. A hierarchical linear model (HLM) is used to test the hypotheses generated from the model. Results show that our integrated model provides a more complete explanation of the use of CT and task performance beyond those of the individual-level factors. The study is an early effort to develop an integrated theory to provide comprehensive insight into individual use of CT in a group or organizational context.
Wireless telecommunications have become over time a ubiquitous tool that not only sustains our increasing need for flexibility and efficiency, but also provides new ways to access and experience both utilitarian and hedonic information goods and services. This paper explores the parallel market evolution of the two main categories of wireless services-voice and data-in leading technology markets, inspecting the differences and complex interactions between the associated adoption processes. We propose a model that addresses specific individual characteristics of these two services and the stand-alone/add-on relationship between them. In particular, we acknowledge the distinction between the nonoverlapping classes of basic consumers, who only subscribe to voice plans, and sophisticated consumers, who adopt both services. We also account for the fact that, unlike voice services, data services rapidly evolved over time due to factors such as interface improvement, gradual technological advances in data transmission speed and security, and the increase in volume and diversity of the content and services ported to mobile Internet. Moreover, we consider the time gap between the market introduction of these services and allow for different corresponding consumer learning curves. We test our model on the Japanese wireless market. The empirical analysis reveals several interesting results. In addition to an expected one-way effect of voice on data adoption at the market potential level, we do find two-way codiffusion effects at the speed of adoption level. We also observe that basic consumers impact the adoption of wireless voice services in a stronger way compared to sophisticated consumers. This, in turn, leads to a decreasing average marginal network effect of voice subscribers on the adoption of wireless voice services. Furthermore, we find that the willingness of voice consumers to consider adopting data services is positively related to both time and penetration of 3G-capable handsets among voice subscribers.
Business organizations are generating growing volumes of data about their employees, customers, and suppliers. Much of these data cannot be exploited for business value due to privacy and confidentiality concerns. National statistical agencies share sensitive data collected from individuals and businesses by modifying the data so individuals and firms cannot be identified but statistical utility is preserved. We build on this literature to develop a hybrid approach to data masking for business organizations. We demonstrate the validity of the hybrid approach, which we call multiple imputation with multimodal perturbation (MIMP), using Monte Carlo simulation and illustrate its application in a specific business context. Results of our analysis open new areas of research for information systems scholarship and new potential revenue sources for business organizations.
Consumer-generated media, particularly blogs, can help companies increase the visibility of their products without spending millions of dollars in advertising. Although a number of companies realize the potential of blogs and encourage their employees to blog, a good chunk of them are skeptical about losing control over this new media. Companies fear that employees may write negative things about them and that this may bring significant reputation loss. Overall, companies show mixed response toward negative posts on employee blogs- some companies show complete aversion; others allow some negative posts. Such mixed reactions toward negative posts motivated us to probe for any positive aspects of negative posts. In particular, we investigate the relationship between negative posts and readership of an employee blog. In contrast to the popular perception, our results reveal a potential positive aspect of negative posts. Our analysis suggests that negative posts act as catalyst and can exponentially increase the readership of employee blogs, suggesting that companies should permit employees to make negative posts. Because employees typically write few negative posts and largely write positive posts, the increase in readership of employee blogs generally should be enough to offset the negative effect of few negative posts. Therefore, not restraining negative posts to increase readership should be a good strategy. This raises a logical question: what should a firm's policy be regarding employee blogging? For exposition, we suggest an analytical framework using our empirical model
Cooperative caching is a popular mechanism to allow an array of distributed caches to cooperate and serve each others' Web requests. Controlling duplication of documents across cooperating caches is a challenging problem faced by cache managers. In this paper, we study the economics of document duplication in strategic and nonstrategic settings. We have three primary findings. First, we find that the optimum level of duplication at a cache is nondecreasing in intercache latency, cache size, and extent of request locality. Second, in situations in which cache peering spans organizations, we find that the interaction between caches is a game of strategic substitutes wherein a cache employs lesser resources towards eliminating duplicate documents when the other caches employs more resources towards eliminating duplicate documents at that cache. Thus, a significant challenge will be to simultaneously induce multiple caches to contribute more resources towards reducing duplicate documents in the system. Finally, centralized decision making, which as expected provides improvements in average latency over a decentralized setup, can entail highly asymmetric duplication levels at the caches. This in turn can benefit one set of users at the expense of the other, and thus will be challenging to implement.
Distributed software development has become a common reality with the advent of off-shore development and the need to be close to markets. Also, the dynamic nature of the environment in which businesses operate suggests the use of agile development methods. Whereas distributed software development requires the use of formal processes advocated by plan-driven approaches, rapidly changing environments are appropriate candidates for the use of agile development methods. This tension in agile distributed development poses conflicting demands between alignment and adaptability in the software development process. We conducted a multisite case study of three projects that use agile distributed development to examine how these organizations developed contextual ambidexterity-the ability to pursue conflicting demands simultaneously. Our findings, presented as a conceptual framework, indicate that conflicting demands between alignment and adaptability posed by agile distributed development can be addressed by a set of balanced practices that shape performance management and social context-two important antecedents of contextual ambidexterity.
Extant research is equivocal about the organizational performance effects of customer relationship management (CRM) technology use, with some studies reporting positive effects and other studies reporting no effects at all. The present research effort posits that these mixed findings may potentially be explained by two factors: (1) CRM technology use may have different effects on different customers, and (2) different CRM tools may have different performance consequences. This study investigates this possibility by building on relationship marketing and management theory to propose and test a model of the customer- and firm-level consequences of the organizational use of CRM interaction support and customer prioritization tools. The results of data analysis of 295 customer firms nested within 10 provider firms reveal that firm use of CRM interaction support tools is positively related to customers' relationship perceptions, regardless of customer account size. In contrast, the data indicate that use of CRM prioritization tools appears to have positive effects on a firm's larger customers and negative effects on smaller customers. The results also suggest that when considered at an aggregate level, customer perceptions of the exchange relationship are predictive of organizational performance and that the association between these two variables is significant for larger customer accounts but insignificant for smaller accounts. Overall, the study's results help explain some of the inconsistent findings reported in the literature regarding the performance implications of CRM technology use and suggest that use of the technology may serve to enhance organizational performance, at least over the short term.
Many software firms offer a fully functional version of their products free of charge, for a limited trial period, to ease consumers' uncertainty about the functionalities of their products and to help the diffusion of their new software. This paper examines the trade-off between the effects of reduced uncertainty and demand cannibalization, uncovers the condition under which software firms should introduce the time-locked free trial software, and finds the optimal free trial time. As software firms have the option of providing free trial software with full functionalities but a limited trial time or limited functionalities for an unlimited trial time, we develop a unified framework to provide useful guidelines for deciding which free trial strategy is preferred in the presence of network externalities and consumer uncertainty.
Open-source software poses a serious challenge to proprietary software vendors. Lock in customers seems a tempting strategy for proprietary software vendors, who attempt to lock in customers by creating switching costs. This paper examines whether such a lock-in strategy will indeed benefit proprietary software vendors facing competition from open-source software, who can credibly commit future prices. Developing a two-period duopoly model in which software products are differentiated and customers are heterogeneous, we find that the lock-in strategy is actually counterproductive in competing against open-source software. In fact, giving customers the freedom of choice may end up benefiting the proprietary software vendor. In terms of the broader effect, we find that lock-in reduces overall social welfare, but certain customers may actually be better off with it. Finally, we show that the lock-in strategy works differently for different types of customers in the software market (i.e., foresighted versus myopic customers). This suggests that customer behavior could significantly alter the equilibrium strategy of software vendors.
Organizations are increasingly choosing to implement service-oriented architectures to integrate distributed, loosely coupled applications. These architectures are implemented as services, which typically use XMLbased messaging to communicate between service consumers and service providers across enterprise networks. We propose a scheme for caching fragments of service response messages to improve performance and service quality in service-oriented architectures. In our fragment caching scheme, we decompose responses into smaller fragments such that reusable components can be identified and cached in the XML routers of an XML overlay network within an enterprise network. Such caching mitigates processing requirements on providers and moves content closer to users, thus reducing bandwidth requirements on the network as well as improving service times. We describe the system architecture and caching algorithm details for our caching scheme, develop an analysis of the expected benefits of our scheme, and present the results of both simulation and case studybased experiments to show the validity and performance improvements provided by our caching scheme. Our simulation experimental results show an up to 60% reduction in bandwidth consumption and up to 50% response time improvement. Further, our case study experiments demonstrate that when there is no resource bottleneck, the cache-enabled case reduces average response times by 40%-50% and increases throughput by 150% compared to the no-cache and full message caching cases. In experiments contrasting fragment caching and full message caching, we found that full message caching provides benefits when the number of possible unique responses is low while the benefits of fragment caching increase as the number of possible unique responses increases. These experimental results clearly demonstrate the benefits of our approach.
Prior IS research on technological change has focused primarily on organizational information systems and technology innovation; however, there is a growing need to understand the dynamics of supply-side forces in the introduction of new technologies. In this paper we investigate how the interdependencies among information technology components, products, and infrastructure affect the release of new technologies. Going beyond the ad hoc heuristic approaches applied in previous studies, we empirically validate the existence of several patterns of supply-side technology relationships in the context of wireless networking. We use vector autoregression (VAR) to model the comovements of new component, product, and infrastructure introductions and provide evidence of strong Granger-causal interdependencies. We also demonstrate that substantial improvements in forecasting can be gained by incorporating these cross-level effects into models of technological change. This paper provides some of the first research that empirically demonstrates these cross-level effects and also provides an exposition of VAR methodology for both analysis and forecasting in IS research.
Research in face-to-face teams shows conflicting results about the impact of behavioral controls on trust; some research shows that controls increase the salience of good behavior, which increases trust while other research shows that controls increase the salience of poor behavior that decreases trust. The only study in virtual teams, which examined poorly functioning teams, found that controls increased the salience of poor behavior, which decreased trust. We argue that in virtual teams behavioral controls amplify the salience of all behaviors (positive and negative) and that an individual's selective perception bias influences how these behaviors are interpreted. Thus the link from behavioral controls to trust is more complex than first thought. We conducted a 22 experiment, varying the use of behavioral controls (controls, no controls) and individual team member behaviors (reneging behaviors designed to reduce trust beliefs and fulfilling behaviors designed to increase trust beliefs). We found that behavioral controls did amplify the salience of all behaviors; however, contrary to what we expected, this actually weakened the impact of reneging and fulfilling behaviors on trust. We believe that completing a formal evaluation increased empathy and the awareness of context in which the behaviors occurred and thus mitigated extreme perceptions. We also found that behavioral controls increased the selective perception bias which induced participants to see the behaviors their disposition to trust expected rather than the behaviors that actually occurred.
The existence and persistence of price dispersion for identical products in online markets have been welldocumented in the literature. Possible explanations of this price dispersion, derived mainly using hedonic price models, have seen only modest success. In this paper, we propose a competitive model based on online retailers' differentiation mainly in service provided and recognition enjoyed to explain price dispersion. Our exploratory empirical analyses, using cross-sectional data, demonstrate that the competitive model provides a better explanation of the association between prices and online retailers' service and recognition levels. In addition, our competitive model is able to explain observations that are seemingly inconsistent with the hedonic model such as the negative association between service and price. This paper contributes to the literature on price dispersion by offering a differentiation model that provides a good fit with data and by proposing a theory that explains previous counterintuitive observations of prices. Our model also helps an e-tailer to choose a desirable position in the competitive market.
The quality of data contained in accounting information systems has a significant impact on both internal business decision making and external regulatory compliance. Although a considerable body of literature exists on the issue of data quality, there has been little research done at the task level of a business process to develop effective control strategies to mitigate data quality risks. In this paper, we present a methodology for managing the risks associated with the quality of data in accounting information systems. This methodology first models the error evolution process in transactional data flow as a dynamical process; it then finds optimal control policies at the task level to mitigate the data quality-related risks using a Markov decision process model with risk constraints. The proposed Markov decision methodology facilitates the modeling of multiple dimensions of error dependence, captures the correlated impact among control procedures, and identifies an optimal control policy. A revenue realization process of an international production company is used to illustrate this methodology.
This paper employs a modified investment game to study how online reputation ratings are assigned, and thus how electronic reputations are formed in transactions where buyers and sellers interact anonymously. Of particular interest are the important questions of how online reputations evolve and how specific reputation information is interpreted by market participants. We vary the level of uncertainty in the transaction environment, and measure the effects of this manipulation on buyers' trust and their subsequent rating behaviors. We distinguish between a reputation mechanism and specific reputation information, finding the former has an association with the overall decision of whether to transact in the marketplace, while the latter shows significance in purchase decisions regarding specific sellers. We also find that aggregate reputation information is weighted differently than singular reputation information. Finally, we show that when reputations are increasingly noisy, buyers are less likely to react negatively to poor ratings and are more likely to give sellers the benefit of the doubt when seemingly uncooperative outcomes occur.
Through a grounded analysis of the National Aeronautics and Space Administration (NASA's) enterprise information system (IS) implementation in the months immediately following the go-live, we show how NASA can be characterized as an institutionally plural organization, rife with diverse institutional logics, some consistent and some contradictory to each other. The enterprise system is introduced in accordance with the logic of managerial rationalism, but some of the institutional logics that organizational actors draw upon and reproduce contradict the logic of managerial rationalism in certain situations. In these situations, organizational actors loosely couple elements of their practices from the practices implied by the enterprise system, thus satisfying the demands associated with both institutional fields. We identify four generalizable forms of loose coupling that result from these institutional contradictions: temporal, material, procedural, and interpretive, and discuss their effects on both the system implementation and local practices. Further, we show how, through the use of institutional logics, researchers can identify fundamental institutional contradictions that explain regularities in the situated responses to enterprise system implementations-regularities that are consistently identified in the literature across a variety of organizational contexts.
Today, few firms could survive for very long without their computer systems. IT has permeated every corner of firms. Firms have reached the current state in their use of IT because IT has provided myriad opportunities for firms to improve performance and, firms have availed themselves of these opportunities. Some have argued, however, that the opportunities for firms to improve their performance through new uses of IT have been declining. Are the opportunities to use IT to improve firm performance diminishing? We sought to answer this question. In this study, we develop a theory and explain the logic behind our empirical analysis; an analysis that employs a different type of event study. Using the volatility of firms' stock prices to news signaling a change in economic conditions, we compare the stock price behavior of firms in the IT industry to firms in the utility and transportation and freight industries. Our analysis of the IT industry as a whole indicates that the opportunities for firms to use IT to improve their performance are not diminishing. However, there are sectors within the IT industry that no longer provide value-enhancing opportunities for firms. We also find that IT products that provided opportunities for firms to create value at one point in time, later become necessities for staying in business. Our results support the key assumption in our work.
We examine how one industry's productivity is affected by the IT capital of its customers and how this effect depends on industries' relative concentration. These customer-driven IT spillovers result from customers' IT investments in various information systems that reduce transaction costs through information sharing and coordination and lead to more efficient production and logistics upstream. The magnitude of IT spillovers depends on relative industry concentration because customers in more concentrated industries relative to those of their suppliers are better able to retain the benefits from their IT investments. We model customer-driven effects based on production theory and empirically test the model using two industry-level data sets covering different and overlapping time periods (1987-1999 and 1998-2005), different scopes of the economy (manufacturing only versus all industries), and different levels of industry aggregation. We find that, given an increase in a downstream industry's IT capital, there is a significant increase in downstream industry output as well as significant increases in upstream industry output. Moreover, the magnitude of IT spillovers is related to relative industry concentration: A 1% decrease in a customer's relative industry concentration increases spillovers by roughly 1%. Thus, further increases in IT capital can be justified along the supply chain, and an industry's relative concentration-which can reflect market power-in part determines the distribution of productivity benefits.
We propose a model to study expectation confirmation in information systems. The proposed model is based on the assimilation-contrast model and prospect theory, and suggests that both are needed to account for the magnitude and direction of the deviations between experiences and expectations. Using the technology acceptance model's (TAM) primary construct-namely, perceived usefulness-expectations and experiences were conceptualized and operationalized to test our model. Data were collected in a field study from 1,113 participants at two points in time. Using polynomial modeling and response surface analysis, we demonstrated that our model offers a good explanation of the relationship among information systems expectations, experiences, and use. We discuss theoretical and practical implications.
Aprevalent assumption in the literature is that trust and risk are always relevant in online marketplaces, and that there is always a need to build trust and reduce risk irrespective of context. Challenging this assumption, this study seeks to identify the boundaries of the effects of trust and risk on transaction activity in the context of institutional structures in online marketplaces. The perceived effectiveness of institutional structures (PEIS), defined as the extent buyers believe that appropriate conditions are in place to facilitate transactions with sellers, sets the boundaries of trust and risk by moderating their effects on transaction activity in a quadratic (inverted-U) fashion. Specifically, at the lower boundary condition of PEIS (among buyers who believe institutional structures are ineffective), the high situational uncertainty they perceive should make these buyers unwilling to become vulnerable to sellers, thus rendering trust and risk immaterial to their decision making. Trust and risk should also be immaterial at the higher boundary condition of PEIS (among buyers who believe institutional structures are very effective), because the insufficient situational uncertainty makes trust and risk irrelevant to these buyers' decision making because of a lack of vulnerability. Only between these two boundary conditions (among buyers who perceive moderate levels of PEIS), and thus a moderate degree of situational uncertainty and vulnerability in the marketplace, should trust and risk have a significant effect on transaction activity. Data from 398 buyers on eBay's and Amazon's online marketplaces support the quadratic moderating role of PEIS on the effect of risk on transaction activity, but not on the effect of trust. Theoretical and practical implications on specifying the boundaries of the effects of trust and risk and understanding the direct and moderating role of institutional structures are discussed.
Dealing with spam is very costly, and many organizations have tried to reduce spam-related costs by installing spam filters. Relying on modern econometric methods to reduce the selection bias of installing a spam filter, we use a unique data setting implemented at a German university to measure the costs associated with spam and the costs savings of spam filters. Our methodological framework accounts for effect heterogeneity and can be easily used to estimate the effect of other IS technologies implemented in organizations. The majority of costs stem from the time that employees spend identifying and deleting spam, amounting to an average of approximately five minutes per employee per day. Our analysis, which accounts for selection bias, finds that the installation of a spam filter reduces these costs by roughly one third. Failing to account for the selection bias would lead to a result that suggests that installing a spam filter does not reduce working time losses. However, cost savings only occur when the spam burden is high, indicating that spam filters do not necessarily reduce costs and are therefore no universal remedy. The analysis further shows that spam filters alone are a countermeasure against spam that exhibits only limited effectiveness because they only reduce costs by one third.
External financing is critical to ventures that do not have a revenue source but need to recruit employees, develop products, pay suppliers, and market their products/services. There is an increasing belief among entrepreneurs that electronic word-of-mouth (eWOM), specifically blog coverage, can aid in achieving venture capital financing. Conflicting findings reported by past studies examining eWOM make it unclear what to make of such beliefs of entrepreneurs. Even if there were generally agreed-upon results, a stream of literature indicates that because of the differences in traits between the prior investigated contexts and venture capital financing, the findings from the prior studies cannot be generalized to venture capital financing. Extant studies also fall short in examining the role of time and the status of entities generating eWOM in determining the influence of eWOM on decision making. To address this dearth of literature in a context that attracts billions of dollars every year, we investigate the effect of eWOM on venture capital financing. This study entails the challenging task of gathering data from hundreds of ventures along with other sources including VentureXpert, surveys, Google Blogsearch, Lexis-Nexis, and Archive.org. The key findings of our econometric analysis are that the impact of negative eWOM is greater than is the impact of positive eWOM and that the effect of eWOM on financing decreases with the progress through the financing stages. We also find that the eWOM of popular bloggers helps ventures in getting higher funding amounts and valuations. The empirical model used in this work accounts for inherent selection biases of entrepreneurs and venture capitalists, and we conduct numerous robustness checks for potential issues of endogeneity, selection bias, nonlinearities, and popularity cutoff for blogs. The findings have important implications for entrepreneurs and suggest ways by which entrepreneurs can take advantage of eWOM.
Firms are investing millions to deploy Web-based self-services at their call centers. The rationale for such investment is that the firm's cost of interacting with its customers through the Web-based channel is an order of magnitude cheaper than the assisted channels such as telephony. We conduct a field study at the call center of a prominent U.S. health insurance firm to examine this cost-saving rationale of the Web-based selfservice channel. On the one hand, the Web channel may substitute for the telephony channel in some cases. On the other hand, the Web also exposes customers to a vast amount of information about their health policy, claims, and coverage; this information can create uncertainty leading to customers seeking more information and hence making more telephone calls. We designed a quasi-natural experiment in our field setting and used difference-in-difference specifications to show that the Web-based self-service usage leads to a 14% increase in telephone calls. We conduct several robustness checks to show that our specifications are robust to any potential selection of customers in the Web-based self-service usage. We further find that the impact of Web portal usage is moderated by the Web portal characteristics. We find that if the information is unambiguous and easily retrievable on the Web, calls for such information decline by 29%. However, for ambiguous information, the calls increase substantially. Our research provides insights into the challenges and opportunities of self-service technologies design.
In this study, we examine how consumers respond to firms' use of two types of information for personalization: product preferences and name. We collect a unique data set of over 10 million e-mail advertisements sent by a website to over 600,000 customers who could buy the advertised products from the online merchant. We estimate a two-stage hierarchical model using Bayesian analysis to account for observable and unobservable consumer heterogeneity. Our analysis suggests several interesting results regarding consumers' responses to firms' use of information. When firms use product-based personalization (where the use of information is not explicitly mentioned), consumers respond positively. On the other hand, consumers respond negatively when firms are explicit in their use of personally identifiable information (i.e., a personalized greeting). We also find that negative responses to personalized greetings are moderated by consumers' familiarity with firms. The main contribution of this study is that it not only indicates the economic benefits of personalization in e-mails but also highlights consumers' concerns over the use of information in personalization.
Increasingly, online discussion communities are used to support activities ranging from software development to political campaigns. An important feature of an online discussion community is its content boundaries, which are individual perceptions of what materials and discussions are part of the community and what are not, and how that community is related to others within a larger system. Yet in spite of its importance, many community infrastructures allow individual participants to reshape content boundaries by simultaneously associating their contributions with multiple online discussion communities. This reshaping behavior is a controversial aspect of the creation and management of many types of online discussion communities. On one hand, many communities explicitly discourage boundary reshaping behaviors in their frequently asked questions or termsof- use document. On the other hand, community infrastructures continue to allow such reshaping behaviors. To explain this controversy, we theorize how the extent of boundary reshaping in an online discussion community has simultaneously positive and negative effects on its member dynamics and responsiveness. We test predictions about the conflicting effects of reshaping behaviors with 60 months of longitudinal data from 140 USENET newsgroups, focusing on cross-posting activities as a form of reshaping behavior. Empirical results are consistent with the proposed hypotheses that reshaping behaviors within a discussion community affect member dynamics and community responsiveness in both positive and negative ways. Taken together, the findings highlight the boundary-related design challenges faced by managers seeking to support ongoing activity within online discussion communities.
Managers routinely seek to understand firm performance relative to the competitors. Recently, competitive intelligence (CI) has emerged as an important area within business intelligence (BI) where the emphasis is on understanding and measuring a firm's external competitive environment. A requirement of such systems is the availability of the rich data about a firm's competitors, which is typically hard to acquire. This paper proposes a method to incorporate competitive intelligence in BI systems by using less granular and aggregate data, which is usually easier to acquire. We motivate, develop, and validate an approach to infer key competitive measures about customer activities without requiring detailed cross-firm data. Instead, our method derives these competitive measures for online firms from simple site-centric data that are commonly available, augmented with aggregate data summaries that may be obtained from syndicated data providers. Based on data provided by comScore Networks, we show empirically that our method performs well in inferring several key diagnostic competitive measures-the penetration, market share, and the share of wallet-for various online retailers.
Online advertising has transformed the advertising industry with its measurability and accountability. Online software and services supported by online advertising is becoming a reality as evidenced by the success of Google and its initiatives. Therefore, the choice of a pricing model for advertising becomes a critical issue for these firms. We present a formal model of pricing models in online advertising using the principal-agent framework to study the two most popular pricing models: input-based cost per thousand impressions (CPM) and performance-based cost per click-through (CPC). We identify four important factors that affect the preference of CPM to the CPC model, and vice versa. In particular, we highlight the interplay between uncertainty in the decision environment, value of advertising, cost of mistargeting advertisements, and alignment of incentives. These factors shed light on the preferred online-advertising pricing model for publishers and advertisers under different market conditions.
Online auction markets play increasingly important roles for resource allocations in distributed systems. This paper builds upon a market-based framework presented by Guo et al. (Guo, Z., G. J. Koehler, A. B. Whinston. 2007. A market-based optimization algorithm for distributed systems. Management Sci. 53(8) 1345-1458), where a distributed system optimization problem is solved by self-interested agents iteratively trading bundled resources in a double auction market run by a dealer. We extend this approach to a dynamic, asynchronous Internet market environment and investigate how various market design factors including dealer inventory policies, market communication patterns, and agent learning strategies affect the computational market efficiency, market liquidity, and implementation. We prove finite convergence to an optimal solution under these various schemes, where individual rational and budget-balanced trading leads to an efficient auction outcome. Empirical investigations further show that the algorithmic implementation is robust to a number of dealer and agent manipulations and scalable to larger sizes and more complicated bundle trading markets. Interestingly, we find that, though both asynchronous communication and asymmetric market information negatively affect the speed of market convergence and lead to more agent welfare loss, agents' ability to predict market prices has a positive effect on both. Contrary to conventional wisdom that a dealer's intertemporal liquidity provisions improve market performance, we find that the dealer's active market intervention may not be desirable in a simple market trading environment where an inherent market liquidity effect dominates, especially when the dealer owns a significant amount of resources. Different from the traditional market insight, our trading data suggest that high trading volume does not correlate to low price volatility and quicker price discovery.
Online social media such as blogs are transforming how consumers make consumption decisions, and the music industry is at the forefront of this revolution. Based on data from a leading music blog aggregator, we analyze the relationship between music blogging and full-track sampling, drawing on theories of online social interaction. Our results suggest that intensity of music sampling is positively associated with the popularity of a blog among previous consumers and that this association is stronger in the tail than in the body of music sales distribution. At the same time, the incremental effect of music popularity on sampling is also stronger in the tail relative to the body. In the last part of the paper, we discuss the implications of our results for music sales and potential long-tailing of music sampling and sales. Put together, our analysis sheds new light on how social media are reshaping music sharing and consumption.
Performance-based advertising is becoming increasingly popular in the online advertising industry, where advertisers pay the publisher only when the advertisement generates an action (e.g., a click-through or a purchase). This paper illustrates that adopting this emerging advertising scheme has profound impacts on one fundamental function of advertising-signaling product quality. We identify several important dimensions that affect the signaling function of performance-based advertising relative to its traditional counterpart (impression-based advertising). These include: (1) information-total advertising expenditure is determined after the demand is realized, so it is unobservable to consumers when making purchase decisions; (2) ad performance- the measured performance (e.g., recorded click-throughs) includes actions generated by first-time buyers (i.e., advertising performance) and actions generated by repeat buyers (i.e., product performance), which increases the cost of signaling through advertising; (3) demand uncertainty-the merchant pays only when a response to the advertisement is generated, which reduces the merchant's advertising uncertainty. We build a model of performance-based advertising by explicitly incorporating these factors, and we derive the conditions under which switching to performance-based advertising will (a) disable or strengthen the signaling function of advertising, (b) help or hurt the merchant, and (c) lead to a higher or lower advertising expenditure.
Prior work on software release policy implicitly assumes that testing stops at the time of software release. In this research, we propose an alternative release policy for custom-built enterprise-level software projects that allows testing to continue for an additional period after the software product is released. Our analytical results show that the software release policy with postrelease testing has several important advantages over the policy without postrelease testing. First, the total expected cost is lower. Second, even though the optimal time to release the software is shortened, the reliability of the software is improved throughout its lifecycle. Third, although the expected number of undetected bugs is higher at the time of release, the expected number of software failures in the field is reduced. We also analyze the impact of market uncertainty on the release policy and find that all our prior findings remain valid. Finally, we examine a comprehensive scenario where in addition to uncertain market opportunity cost, testing resources allocated to the focal project can change before the end of testing. Interestingly, the software should be released earlier when testing resources are to be reduced after release.
Simultaneously achieving efficiency and flexibility in enterprise software production has been a considerable challenge for firms. Newer software development paradigms such as component-based and model-driven development attempt to overcome this challenge by emphasizing modular design of complex systems. However, there is a paucity of rigorous empirical research on the use of such software methodologies and the associated extent to which trade-offs between efficiency and flexibility can be influenced. Addressing this gap, we investigate the performance outcomes of a model-driven, component-based software development methodology using data collected from an enterprise software development firm that deployed such a methodology for its product development processes. Examining the design, development, and implementation of 92 business software components of the firm's enterprise resource planning product, we discuss how the design of software components, specifically component granularity, affects development efficiency (development effort and defects) and flexibility (customization effort). Our results suggest that (a) components that are coarse grained are associated with higher flexibility (lower customization effort) but are also associated with lower development efficiency (more development effort and defects), and (b) defect proneness of a component plays a mediating role on the relationship between component granularity and flexibility. These findings present strong evidence for the existence of trade-offs between efficiency and flexibility in mass-customized software product life cycles. They establish component granularity as a key design dimension that needs to be managed judiciously to enable potential trade-off shifting mechanisms through the use of software methodologies that emphasize modular design approaches.
The bullwhip effect is a major source of supply chain inefficiency. Whereas prior literature has identified a number of potential contributing factors and recommended such remedies as information sharing enabled by information technology (IT) or electronic linkage (EL), few studies have provided empirical support. We use industry-level data to examine whether EL use with buyer and supplier industries helps reduce the bullwhip effect as measured by inventory-demand variance ratio. Our major findings are that (1) EL use with supplier industries reduces the bullwhip effect, whereas (2), surprisingly, EL use with buyer industries increases it, but (3) this adverse effect tends to be mitigated by IT use. These findings point to the possible asymmetric effects of EL use in supply chains and provide a different perspective to the existing conclusions in the literature that EL use improves performance. Combining the above results, we have learned that the use of EL tends to behave differently depending on whether it is used upstream or downstream in the supply chain. This also sheds light on the conditions under which such investment may be more (or less) beneficial.
The emergence of information-intensive business process outsourcing (BPO) relationships calls for the study of exchange performance beyond traditional considerations of the contractual structure that facilitates cooperative intent to include the information structure that facilitates the mutual exchange of information to enact cooperative intent and coordinate actions between the user firm and the service provider. Yet, there has been little analysis of the drivers and performance effects of the information structure of BPO relationships, including its linkages to the underlying contractual structure. This study integrates perspectives in neo-institutional economics and information processing to develop and test the theoretical argument that the extent of use and performance effects of the information structure of the BPO relationship are greater in time and materials BPO contracts than in fixed-price BPO contracts. Survey data on 134 BPO relationships provide empirical support for our hypotheses. The synergistic impact of incentives and information on BPO performance emphasizes that their joint assessment is necessary to enhance the explanatory power of extant theories of organization. This result also has implications for achieving maximum benefits from complex BPO arrangements that are more likely to be characterized by time and material contracts.
The implementation of enterprise systems has yielded mixed and unpredictable outcomes in organizations. Although the focus of prior research has been on training and individual self-efficacy as important enablers, we examine the roles that the social network structures of employees, and the organizational units where they work, play in influencing the postimplementation success. Data were gathered across several units within a large organization: immediately after the implementation, six months after the implementation, and one year after the implementation. Social network analysis was used to understand the effects of network structures, and hierarchical linear modeling was used to capture the multilevel effects at unit and individual levels. At the unit level of analysis, we found that centralized structures inhibit implementation success. At the individual level of analysis, employees with high in-degree and betweenness centrality reported high task impact and information quality. We also found a cross-level effect such that central employees in centralized units reported implementation success. This suggests that individual-level success can occur even within a unit structure that is detrimental to unit-level success. Our research has significant implications for the implementation of enterprise systems in large organizations.
This paper examines the effects of IT-related spillovers on firm-level productivity improvements over a longterm horizon. In contrast, prior research has largely focused on the direct and contemporaneous impacts of IT investments. As a result, we do not fully understand how IT investments are associated with ongoing productivity improvements in future periods and how spillovers influence these gains. In this paper, we examine whether firms receive incremental benefits from IT-related spillovers and whether these spillovers lead to more persistent returns. We focus on the spillovers that accrue to firms from their interindustry transactions, especially the IT services industry. We model and estimate the impact of spillovers on long-run productivity using firmlevel data from the manufacturing, transportation, trade, and services sectors. We find that spillover impacts are highly significant, but that the magnitude and persistence of the impacts vary. Firms with high IT intensity receive greater spillover benefits from the IT services industry. Moreover, these benefits are sustained over a long-term horizon. However, the impact of IT-related spillovers does not persist in low IT intensity firms regardless of the source. Overall, our results shed light on the existence and sources of IT-related spillovers and on their important role in shaping the long-run returns to IT investment. Our results also help explain the findings of excess returns to IT investment in the IT productivity literature.
This paper investigates the challenges faced in designing an integrated information platform for emergency response management and uses the Beijing Olympic Games as a case study. The research methods are grounded in action research, participatory design, and situation-awareness oriented design. The completion of a more than two-year industrial secondment and six-month field studies ensured that a full understanding of user requirements had been obtained. A service-centered architecture was proposed to satisfy these user requirements. The proposed architecture consists mainly of information gathering, database management, and decision support services. The decision support services include situational overview, instant risk assessment, emergency response preplan, and disaster development prediction. Abstracting from the experience obtained while building this system, we outline a set of design principles in the general domain of information systems (IS) development for emergency management. These design principles form a contribution to the information systems literature because they provide guidance to developers who are aiming to support emergency response and the development of such systems that have not yet been adequately met by any existing types of IS. We are proud that the information platform developed was deployed in the real world and used in the 2008 Beijing Olympic Games.
This paper uses newly collected panel data that allow for significant improvements in the measurement and modeling of information technology (IT) productivity to address some longstanding empirical limitations in the IT business value literature. First, we show that using generalized method of moments-based estimators to account for the endogeneity of IT spending produces coefficient estimates that are only about 10% lower than unadjusted estimates, suggesting that the effects of endogeneity on IT productivity estimates may be relatively small. Second, analysis of the expanded panel suggests that (a) IT returns are substantially lower in midsize firms than in Fortune 500 firms; (b) they materialize more slowly in large firms-in midsize firms, unlike in larger firms, the short-run contribution of IT to output is similar to the long-run output contribution; and (c) the measured marginal product of IT spending is higher from 2000 to 2006 than in any previous period, suggesting that firms, and especially large firms, have been continuing to develop new, valuable IT-enabled business process innovations. Furthermore, we show that the productivity of IT investments is higher in manufacturing sectors and that our productivity results are robust to controls for IT labor quality and outsourcing levels.
This research investigates information security management as an administrative innovation. Although a number of institutional theories deal with information systems (IS) innovation in organizations, most of these institutional-centered frameworks overlook external economic efficiency and internal organizational capability in the presence of pressures of institutional conformity. Using Korea as the institutional setting, our research model posits that economic-based consideration will moderate the institutional conformity pressure on information security adoption while organization capability will influence the institutional confirmation of information security assimilation. The model is empirically tested using two-stage survey data from a field study of 140 organizations in Korea. The results indicate that in addition to institutional influences, our six proposed economic-based and organizational capability moderating variables all have significant influences on the degree of the adoption and assimilation of information security management. We conclude with implications for research in the area of organizational theory and the information security management literature, and for practices regarding how managers can factor into their information security planning the key implementation variables discovered in this study. The robust setting of the study in Korean firms allows us to generalize the theory to a new context and across cultures.
Virtual worlds are relatively nascent IT platforms with the potential to radically transform business processes and generate significant payoffs. However, in striving to achieve specific outcomes, firms may incur significant risks. Although many companies claim to have attained substantial benefits from their virtual world initiatives, many others have recently scaled down or even abandoned their experimental virtual world projects. This paper assesses the value proposition of virtual world initiatives from the real options perspective. Specifically, we argue that virtual worlds act as a firm's growth option, and we adopt the lens of real options to evaluate the value of this emerging and uncertain technological platform. We employ the event study method to assess the stock market's perception of the future revenue streams of 261 virtual world initiatives announced between 2006 and 2008. Our results indicate that, overall, the market reacts positively to virtual world initiatives. Our findings also show that investors' reactions to virtual world initiatives are contingent on four key characteristics of virtual world initiatives: interpretive flexibility (i.e., technologies that allow managers to experiment), divisibility (i.e., ability to incrementally implement the technology), strategic importance (i.e., an initiative that affects a process of strategic importance to the firm), and exploitable absorptive capacity (i.e., ability to exploit the knowledge acquired through the initiative). We discuss the key implications for real-world practitioners and suggest directions for future research.
We analyze learning and knowledge transfer in a computing call center. The information technology (IT) technical services provided by call centers are characterized by constant changes in relevant knowledge and a wide variety of support requests. Under this IT problem-solving context, we analyze the learning curve relationship between problem-solving experience and performance enhancement. Based on data collected from a university computing call center consisting of different types of consultants, our empirical findings indicate that (a) the learning effect-as measured by the reduction of average resolution time-occurs with experience, (b) knowledge transfer within a group occurs among lower-level consultants utilizing application-level knowledge (as opposed to technical-level knowledge), and (c) knowledge transfers across IT problem types. These estimates of learning and knowledge transfer contribute to the development of an empirically grounded understanding of IT knowledge workers' learning behavior. The results also have implications for operational decisions about the staffing and problem-solving strategy of call centers.
We consider a publisher that earns advertising revenue while providing content to serve a heterogeneous population of consumers. The consumers derive benefit from consuming content but suffer from delivery delays. A publisher's content provision strategy comprises two decisions: (a) the content quality (affecting consumption benefit) and (b) the content distribution delay (affecting consumption cost). The focus here is on how a publisher should choose the content provision strategy in the presence of a content pirate such as a peer-to-peer (P2P) network. Our study sheds light on how a publisher could leverage a pirate's presence to increase profits, even though the pirate essentially encroaches on the demand for the publisher's content. We find that a publisher should sometimes decrease the delivery speed but increase quality in the presence of a pirate (a quality focused strategy). At other times, a distribution focused strategy is better; namely, increase delivery speed, but lower quality. In most cases, however, we show that the publisher should improve at least one dimension of content provision (quality or delay) in the presence of a pirate.
We consider advertising problems under an information technology (IT) capacity constraint encountered by electronic retailers in a duopolistic setting. There is a considerable amount of literature on advertising games between firms, yet introducing an IT capacity constraint fundamentally changes this problem. In the presence of information processing constraints, although advertising may still cause a customer to switch, it may not result in a sale, i.e., the customer may be lost by both firms. This situation could occur when customers have a limited tolerance for processing delays and leave the website of a firm because of slow response. In such situations, attracting more traffic to a firm's site (by increasing advertising expenditure) may not generate enough additional revenue to warrant this expenditure. We use a differential game formulation to obtain closedform solutions for the advertising effort over time in the presence of IT capacity constraints. Based on these solutions, we present several useful managerial insights.
We econometrically evaluate information worker productivity at a midsize executive recruiting firm and assess whether the knowledge that workers accessed through their electronic communication networks enabled them to multitask more productively. We estimate dynamic panel data models of multitasking, knowledge networks, and productivity using several types of micro-level data: (a) direct observation of more than 125,000 email messages over a period of 10 months; (b) detailed accounting data on individuals' project output and team membership for more than 1,300 projects spanning five years; and (c) survey and interview data about the same workers' IT skills, IT use, and information sharing. We find that (1) more multitasking is associated with more project output, but diminishing marginal returns, and (2) recruiters whose network contacts have heterogeneous knowledge-an even distribution of expertise over many project types-are less productive on average but more productive when juggling diverse multitasking portfolios. These results show how multitasking affects productivity and how knowledge networks, enabled by IT, can improve worker performance. The methods developed can be replicated in other settings, opening new frontiers for research on social networks and IT value.
With the lack of timely and relevant patient information at the point of care increasingly being linked to adverse medical outcomes, effective management and exchange of patient data has emerged as a strategic imperative for the healthcare industry. Healthcare informaticians have suggested that electronic health record systems (EHRS) can facilitate information sharing within and between healthcare stakeholders such as physician practices, hospitals, insurance companies, and laboratories. We examine the assimilation of EHRS in physician practices through a novel and understudied theoretical lens of physicians' identities. Physician practices and the physicians that lead them occupy a central position in the healthcare value chain and possess a number of unique characteristics that differentiate them from other institutional contexts, including a strong sense of affiliation with other physicians, potent professional identities, and a desire for autonomy. We investigate two salient physician identities, those of careprovider and physician community, grounded in the roles physicians play and the groups with which they affiliate. We argue that these identities and their evolution, triggered by EHRS, manifest as both identity reinforcement and deterioration, and are important drivers of EHRS assimilation. We use survey data from 206 physician practices, spread across the United States, to test our theoretical model. Results suggest that physician community identity reinforcement and physician community identity deterioration directly influence the assimilation of EHRS. We further find that the effects of careprovider identity reinforcement and careprovider identity deterioration on EHRS assimilation are moderated by governmental influence. Theoretical and pragmatic implications of the findings are discussed.
Despite many success stories, B2B e-commerce penetration remains low. Many firms introduce electronic channels in addition to their traditional sales channels but find that buyer usage of the e-channel over time does not keep up with initial expectations. Firms must understand the underlying factors that drive channel usage and how these factors change over time and across buyers. Using panel data pertaining to the purchase histories of 683 buyers over a 43-month period, we estimate a dynamic discrete choice model in a B2B setting that (i) recognizes how price, channel inertia, and inventory change over time; (ii) allows buyers to dynamically trade off these factors when making e-channel adoption decisions; and (iii) takes into account buyer heterogeneity. We find that channel usage is both heterogeneous and dynamic across buyers. Our findings reveal the dynamic tradeoff between channel inertia and the adverse price effect, which interact in opposing directions as the e-channel grows more popular over time: price increases resulting from more bids deter buyers, whereas channel inertia built from sampling experience helps retain repeat buyers for the new channel. Second, we find that the buyers' size and diversity influence purchase decisions, and the e-channel appears more attractive to small and/or diversified buyers. Based on our analysis, we postulate that the seller's allocation decisions of products across channels, if not aligned with buyer behavior, can alienate some buyers. Based on the parameter estimates from the buyer response model, we propose an improved channel allocation that enables firms to selectively attract more buyers to the e-channel and improve revenues. Channel acceptance increases as a result of smart allocation when firms understand and account for individual buyers' channel usage behavior.
E-governments have become an increasingly integral part of the virtual economic landscape. However, e-government systems have been plagued by an unsatisfactory, or even a decreasing, level of trust among citizen users. The political exclusivity and longstanding bureaucracy of governmental institutions have amplified the level of difficulty in gaining citizens' acceptance of e-government systems. Through the synthesis of trust-building processes with trust relational forms, we construct a multidimensional, integrated analytical framework to guide our investigation of how e-government systems can be structured to restore trust in citizen-government relationships. Specifically, the analytical framework identifies trust-building strategies (calculative-based, prediction-based, intentionality-based, capability-based, and transference-based trust) to be enacted for restoring public trust via e-government systems. Applying the analytical framework to the case of Singapore's Electronic Tax-Filing (E-Filing) system, we advance an e-government developmental model that yields both developmental prescriptions and technological specifications for the realization of these trust-building strategies. Further, we highlight the impact of sociopolitical climates on the speed of e-government maturity.
Existing research has long considered service quality as a primary determinant of user satisfaction with information technology (IT) service delivery. In response to the knowledge-intensive and collaborative nature of IT service delivery in the contemporary business context, we advance the theoretical understanding of user satisfaction by re-conceptualizing IT service delivery as a bilateral, relational process between the IT staff and users. Based on this reconceptualization, we draw on social capital theory to examine the antecedents of user satisfaction with IT service delivery. Specifically, we posit that two major dimensions of social capital, i.e., cognitive capital and relational capital, not only positively affect user satisfaction but also strengthen the established relationship between service quality and user satisfaction. Furthermore, we propose that the effect of the other dimension of social capitalstructural capitalon user satisfaction is fully mediated through cognitive capital and relational capital. A field study of 159 users in four financial companies provides general empirical support for our hypotheses. Theoretical and practical implications of these findings are discussed.
In this paper we consider the impact of trust on a new visitor's intention to revisit a website, but instead of using the typical expectancy-value theories as our conceptual basis, we look at the issue from the perspective of cognitive complexity and humans as cognitive misers. Starting with the suggestion that it is cognitively taxing to distrust, we propose that in order to conserve on cognitive resources, once a new visitor has convinced him or herself that a website is trustworthy enough, that user will drop trustworthiness from their concerns and only consider other characteristics of the website (e.g., task-technology fit, aesthetic appeal, etc.) in determining their revisit intention. This leads to what we call a trust tipping point and two different worlds of trust. Above the tipping point revisit intention is constructed in one way, and below the trust tipping point it is constructed in a quite different way. This perspective results in very different recommendations for website designers as to the likely payoff from improving task-technology fit, aesthetic appeal, or trustworthiness, depending upon where their existing website stands relative to the trust tipping point. To test our hypotheses we used data from 314 student website users, and expanded a technique called piecewise regression (Neter et al. Applied Linear Statistical Models, 4th ed.) to allow us to analyze data as two different linear surfaces, joined at the tipping point. We found good support for our assertions that users operate differently above and below a trust tipping point.
In this paper we explore the underlying consumer heterogeneity in competitive markets for subscription-based information technology services that exhibit network effects. Insights into consumer heterogeneity with respect to a given service are paramount in forecasting future subscriptions, understanding the impact of price and information dissemination on market penetration growth, and predicting the adoption path for complementary products that target the same customers as the original service. Employing a continuous-time utility model, we capture the behavior of a continuum of consumers who are differentiated by their intrinsic valuations from using the service. We study service subscription patterns under both perfect and imperfect information dissemination. In each case, we first specify the conditions under which consumer rational behavior supported by the utility model can explain a general observed adoption path, and if so, we explicitly derive the analytical closed-form expression for the consumer valuation distribution. We further explore the impact of awareness and distribution skewness on adoption. In particular, we highlight the practical forecasting importance of understanding the information dissemination process in the market as observed past adoption may be explained by several distinct awareness and heterogeneity scenarios that may lead to divergent adoption paths in the future. Moreover, we show that in the later part of the service lifecycle the subscription decision for new customers can be driven predominantly by information dissemination instead of further price markdowns. We also extend our results to time-varying consumer valuation scenarios. Furthermore, based on our framework, we advance a set of heuristic methods to be applied to discrete-time real industry data for estimation and forecasting purposes. In an empirical exercise, we apply our methodology to the Japanese mobile voice services market and provide relevant managerial insights from the analysis.
In this study, we model firms that sell a product and a complementary online service, where only the latter displays positive network effects. That is, the value each consumer derives from the service increases with the total number of consumers that subscribe to the service. In addition, the service is valuable only to consumers who buy the product. We consider two pricing strategies: (1) bundle pricing, in which the firm charges a single price for the product and the service, and (2) separate pricing, in which the firm sets the prices of the product and the service separately, and consumers self-select whether to buy both or only the product. We show that in contrast to the common result in the bundling literature, often the monopolist chooses not to offer the bundle (he either sells the service separately or not at all) although bundling would increase both consumer surplus and social welfare. Thus, underprovision of the service can be the market outcome. We also demonstrate that network effects may cause the underprovision of the service.
Information systems researchers, like those in many other disciplines in the social sciences, have debated the value and appropriateness of using students as research subjects. This debate appears in several articles that have been published on the subject as well as in the review process. In this latter arena, however, the debate has become increasingly like a script the actors (authors and reviewers) simply read their parts of the script; some avoid the underlying issues whereas others cursorily address generalizability without real consideration of those issues. As a result, despite the extent of debate, we seem no closer to a resolution. Authors who use student subjects rely on their scripted arguments to justify the use of student subjects and do not always consider whether those arguments are valid. But reviewers who oppose the use of student subjects are equally culpable. They, too, rely on scripted arguments to criticize work using student subjects, and do not always consider whether those arguments are salient to the particular study. By presenting and reviewing one version of this script in the context of theoretical discussions of generalizability, we hope to demonstrate its limitations so that we can move beyond these scripted arguments into a more meaningful discussion. To do this, we review empirical studies from the period 1990-2010 to examine the extent to which student subjects are being used in the field and to critically assess the discussions within the field about the use of student samples. We conclude by presenting recommendations for authors and reviewers, for determining whether the use of students is appropriate in a particular context, and for presenting and discussing work that uses student subjects.
Many enterprises that participate in dynamic markets need to make product pricing and inventory resource utilization decisions in real time. We describe a family of statistical models that addresses these needs by combining characterization of the economic environment with the ability to predict future economic conditions to make tactical (short-term) decisions, such as product pricing, and strategic (long-term) decisions, such as level of finished goods inventories. Our models characterize economic conditions, called economic regimes, in the form of recurrent statistical patterns that have clear qualitative interpretations. We show how these models can be used to predict prices, price trends, and the probability of receiving a customer order at a given price. These regime models are developed using statistical analysis of historical data and are used in real time to characterize observed market conditions and predict the evolution of market conditions over multiple time scales. We evaluate our models using a testbed derived from the Trading Agent Competition for Supply Chain Management, a supply chain environment characterized by competitive procurement, sales markets, and dynamic pricing. We show how regime models can be used to inform both short-term pricing decisions and long-term resource allocation decisions. Results show that our method outperforms more traditional short- and long-term predictive modeling approaches.
Notwithstanding potential benefits, such as quality of interorganizational relationships and operational and strategic gains, adoption of information technology (IT)-enabled interorganizational business process standards (IBPS) is still limited. Given that these standards are designed for interorganizational business processes, we suggest that adoption of these standards depends not only on the factors pertinent to a focal firm but also on factors that represent synergies between a focal firm and its trading partners. In this paper, building on the technological, organizational, and environmental (TOE) framework and interorganizational theories, we propose a model that postulates that a set of TOE factors will have synergistic effects (i.e., interactions between a focal firm's and its partner's factors) on IBPS adoption. We tested our model in a study of 248 firms (124 dyads) in the high-tech industry implementing RosettaNet-based IBPS and found that three TOE factors (i.e., process compatibility, standards uncertainty, and technology readiness) had synergistic effects and two factors (i.e., expected benefits and relational trust) had direct effects on IBPS adoption. We also found that IBPS adoption led to greater relationship quality (i.e., partnering satisfaction) and operational efficiency (i.e., cycle time). Further, we found that IBPS adoption mediated the effect of TOE factors on partnering satisfaction and cycle time.
The sale of digital items, such as avatars and decorative objects, is becoming an important source of revenue for virtual community (VC) websites. However, some websites are unable to leverage this source of revenue, and there is a corresponding lack of understanding about what motivates people to purchase digital items in VCs. To explain the phenomenon, we develop a model based on the theory of self-presentation. The model proposes that the desire for online self-presentation is a key driver for such purchases. We also hypothesize that the social influence factors of online self-presentation norms and VC involvement as well as personal control in the form of online presentation self-efficacy are antecedents of the desire for online self-presentation. The model was validated by using survey data collected from Cyworld (N = 217) and Habbo (N = 197), two online social network communities that have been pioneers in the sale of digital items. This work contributes to our understanding of the purchase of digital items by extending the theory of self-presentation and adds to the broader line of research on online identity. It also lends insights into how VC providers can tap this source of revenue.
This paper analyzes how the presence of organic listing as a competing information source affects advertisers' sponsored bidding and the equilibrium outcomes in search advertising. We consider a game-theoretic model in which two firms bid for sponsored advertising slots provided by a monopolistic search engine and then compete for consumers in price in the product market. Firms are asymmetrically differentiated in market preference and are given different exposure in organic listing aligned with their market appeal. We identify two aspects of a firm's sponsored bidding incentive, namely, the promotive and the preventive incentives. The presence of organic listing alters firms' sponsored bidding incentives such that the stronger firm has primarily preventive incentive, whereas the weaker has mainly promotive incentive. We show that the preventive incentive decreases and the promotive incentive increases as the difference in firms' market appeal decreases, and as a result, even the weaker firm may outbid the stronger competitor under such a co-listing setting. We further examine how the presence of organic listing affects the equilibrium outcomes by comparing it with a benchmark case in which there is only a sponsored list. We show that the differentiated exposure in the organic list gives the weaker advertiser chances to win a better sponsored position, which improves the overall information structure the search engine provides. As a result, the equilibrium social welfare, sales diversity, and consumer surplus increase. Although the presence of the free exposure from the organic list may reduce advertisers' sponsored bidding incentive per se, the overall effect benefits the search engine's growth in the long run.
This study seeks to clarify the nature of control in the context of information privacy to generate insights into the effects of different privacy assurance approaches on context-specific concerns for information privacy. We theorize that such effects are exhibited through mediation by perceived control over personal information and develop arguments in support of the interaction effects involving different privacy assurance approaches (individual self-protection, industry self-regulation, and government legislation). We test the research model in the context of location-based services using data obtained from 178 individuals in Singapore. In general, the results support our core assertion that perceived control over personal information is a key factor affecting context-specific concerns for information privacy. In addition to enhancing our theoretical understanding of the link between control and privacy concerns, these findings have important implications for service providers and consumers as well as for regulatory bodies and technology developers.
We examine the effects of human resource management (HRM) practices (e.g., career development, social support, compensation, and security) on information technology (IT) professionals' job search behavior. Job search is a relatively novel dependent variable in studies of voluntary withdrawal behavior in general and for IT professionals in particular. From a universalistic perspective, HRM practices individually and in combination exhibit independently additive effects on job search behavior. Our study contrasts this perspective with configurational theory, hypothesizing that proposed ideal-type configurations of HRM practices have synergistic effects on job search behavior. We contribute to the IT and broader HRM literature by theoretically explicating and empirically demonstrating with IT professionals the power of configurational theory to explain the relationship between HRM practices and job search behavior. Our empirical results show that two configurations of HRM practicesHuman Capital Focused (HCF) and Task Focused (TF), which are high and low on all HRM practices, respectivelyexhibit a synergistic relationship with the job search behavior of IT professionals. HCF has lower job search behavior than would be expected based on the independently additive effects of the HRM practices, whereas TF has correspondingly higher job search behavior. Our results also show that less than perfect horizontal fit detracts from the synergy of these extreme configurations. Just as importantly, several other nonextreme configurations of HRM practices exhibit independently additive effects for the HRM practices but not synergy, suggesting that synergy is limited to extreme configurations. We also discuss a number of implications for research and practice.
We study departures from network neutrality through implementing a quality of service tiering regime in which an Internet service provider charges for prioritization on a nondiscriminatory basis. We find that quality of service tiering may be more efficient in the short run because it better allocates the existing network capacity and in the long run because it provides higher investment incentives due to the increased demand for priority services by the entry of new congestion sensitive content providers. Which network regime is the most efficient depends on the distribution of congestion sensitivity among content providers, but a guideline is that the regime that provides higher incentives for infrastructure investments is more efficient in the long run.
When companies purchase information technology (IT) products for their employees, departments, or divisions, whether to standardize on one product or to allow the users to make their own choices is an important decision for IT managers to make. By consolidating demand and committing to buy from a single seller, standardization ensures product compatibility within the corporation and has a potential to induce intense price competition among sellers, but this potential is subject to whether competing products are compatible and the relative competitive advantages of the sellers. This paper studies when it is optimal for an employer to commit to exclusive purchase from a single seller to enforce standardization and sellers' incentives to invest in mutual compatibility. Our results suggest that the employer is more likely to make such a commitment when the competing products are compatible, less vertically differentiated, and/or more horizontally differentiated. We also find that the sellers agree to cooperate and invest in mutual compatibility only when the gap between their competitive advantages is moderate, but the availability of third party converters that enable partial compatibility can induce more collaboration among the sellers.
By studying the change in employees' network positions before and after the introduction of a social networking tool, I find that information-rich networks (low in cohesion and rich in structural holes), enabled by social media, have a positive effect on various work outcomes. Contrary to the notion that network positions are difficult to alter, I show that social media can induce a change in network structure, one from which individuals can derive economic benefits. In addition, I consider two intermediate mechanisms by which an information-rich network is theorized to improve work performanceinformation diversity and social communicationand quantify their effects on productivity and job security. Analysis shows that productivity, as measured by billable revenue, is more associated with information diversity than with social communication. However, the opposite is true for job security. Social communication is more correlated with reduced layoff risks than with information diversity. This, in turn, suggests that information-rich networks enabled through the use of social media can drive both work performance and job security, but that there is a trade-off between engaging in social communication and gathering diverse information.
Companies have increasingly advocated social media technologies to transform businesses and improve organizational performance. This study scrutinizes the predictive relationships between social media and firm equity value, the relative effects of social media metrics compared with conventional online behavioral metrics, and the dynamics of these relationships. The results derived from vector autoregressive models suggest that social media-based metrics (Web blogs and consumer ratings) are significant leading indicators of firm equity value. Interestingly, conventional online behavioral metrics (Google searches and Web traffic) are found to have a significant yet substantially weaker predictive relationship with firm equity value than social media metrics. We also find that social media has a faster predictive value, i.e., shorter wear-in time, than conventional online media. These findings are robust to a consistent set of volume-based measures (total blog posts, rating volume, total page views, and search intensity). Collectively, this study proffers new insights for senior executives with respect to firm equity valuations and the transformative power of social media.
Despite the popular use of social media by consumers and marketers, empirical research investigating their economic values still lags. In this study, we integrate qualitative user-marketer interaction content data from a fan page brand community on Facebook and consumer transactions data to assemble a unique data set at the individual consumer level. We then quantify the impact of community contents from consumers (user-generated content, i.e., UGC) and marketers (marketer-generated content, i.e., MGC) on consumers' apparel purchase expenditures. A content analysis method was used to construct measures to capture the informative and persuasive nature of UGC and MGC while distinguishing between directed and undirected communication modes in the brand community. In our empirical analysis, we exploit differences across consumers' fan page joining decision and across timing differences in fan page joining dates for our model estimation and identification strategies. Importantly, we also control for potential self-selection biases and relevant factors such as pricing, promotion, social network attributes, consumer demographics, and unobserved heterogeneity. Our findings show that engagement in social media brand communities leads to a positive increase in purchase expenditures. Additional examinations of UGC and MGC impacts show evidence of social media contents affecting consumer purchase behavior through embedded information and persuasion. We also uncover the different roles played by UGC and MGC, which vary by the type of directed or undirected communication modes by consumers and the marketer. Specifically, the elasticities of demand with respect to UGC information richness are 0.006 (directed communication) and 3.140 (undirected communication), whereas those for MGC information richness are insignificant. Moreover, the UGC valence elasticity of demand is 0.180 (undirected communication), whereas that for MGC valence is 0.004 (directed communication). Overall, UGC exhibits a stronger impact than MGC on consumer purchase behavior. Our findings provide various implications for academic research and practice.
Enabling consumers to self-design unique products that match their idiosyncratic preferences is the key value driver of modern mass customization systems. These systems are increasingly becoming social, allowing for consumer-to-consumer interactions such as commenting on each other's self-designed products. The present research examines how receiving others' feedback on initial product configurations affects consumers' ultimate product designs and their satisfaction with these self-designed products. Evidence from a field study in a European car manufacturer's brand community and from two follow-up experiments reveals that receiving feedback from other community members on initial self-designs leads to less unique final self-designs, lower satisfaction with self-designed products, lower product usage frequency, and lower monetary product valuations. We provide evidence that the negative influence of feedback on consumers' satisfaction with self-designed products is mediated by an increase in decision uncertainty and perceived process complexity. The implications of socially enriched mass customization systems for both consumer welfare and seller profitability are discussed.
Firms nowadays are increasingly proactive in trying to strategically capitalize on consumer networks and social interactions. In this paper, we complement an emerging body of research on the engineering of word-of-mouth effects by exploring a different angle through which firms can strategically exploit the value-generation potential of the user network. Namely, we consider how software firms should optimize the strength of network effects at utility level by adjusting the level of embedded social media features in tandem with the right market seeding and pricing strategies in the presence of seeding disutility. We explore two opposing seeding cost models where seeding-induced disutility can be either positively or negatively correlated with customer type. We consider both complete and incomplete information scenarios for the firm. Under complete information, we uncover a complementarity relationship between seeding and building social media features that holds for both disutility models. When the cost of any of these actions increases, rather than compensating by a stronger action on the other dimension to restore the overall level of network effects, the firm will actually scale back on the other initiative as well. Under incomplete information, this complementarity holds when seeding disutility is negatively correlated with customer type but may not always hold in the other disutility model, potentially leading to fundamentally different optimal strategies. We also discuss how our insights apply to asymmetric networks.
Given the demand for authentic personal interactions over social media, it is unclear how much firms should actively manage their social media presence. We study this question empirically in a health care setting. We show that active social media management drives more user-generated content. However, we find that this is due to an incremental increase in user postings from an organization's employees rather than from its clients. This result holds when we explore exogenous variation in social media policies, employees, and clients that are explained by medical marketing laws, medical malpractice laws, and distortions in Medicare incentives. Further examination suggests that content being generated mainly by employees can be avoided if a firm's postings are entirely client focused. However, most firm postings seem not to be specifically targeted to clients' interests, instead highlighting more general observations or achievements of the firm itself. We show that untargeted postings like these provoke activity by employees rather than clients. This may not be a bad thing because employee-generated content may help with employee motivation, recruitment, or retention, but it does suggest that social media should not be funded or managed exclusively as a marketing function of the firm.
In a social network, adoption probability refers to the probability that a social entity will adopt a product, service, or opinion in the foreseeable future. Such probabilities are central to fundamental issues in social network analysis, including the influence maximization problem. In practice, adoption probabilities have significant implications for applications ranging from social network-based target marketing to political campaigns, yet predicting adoption probabilities has not received sufficient research attention. Building on relevant social network theories, we identify and operationalize key factors that affect adoption decisions: social influence, structural equivalence, entity similarity, and confounding factors. We then develop the locally weighted expectation-maximization method for Nave Bayesian learning to predict adoption probabilities on the basis of these factors. The principal challenge addressed in this study is how to predict adoption probabilities in the presence of confounding factors that are generally unobserved. Using data from two large-scale social networks, we demonstrate the effectiveness of the proposed method. The empirical results also suggest that cascade methods primarily using social influence to predict adoption probabilities offer limited predictive power and that confounding factors are critical to adoption probability predictions.
In this study we examine the effect of customers' participation in a firm's social media efforts on the intensity of the relationship between the firm and its customers as captured by customers' visit frequency. We further hypothesize and test for the moderating roles of social media activity and customer characteristics on the link between social media participation and the intensity of customer-firm relationship. Importantly, we also quantify the impact of social media participation on customer profitability. We assemble a novel data set that combines customers' social media participation data with individual customer level transaction data. To account for endogeneity that could arise because of customer self-selection, we utilize the propensity score matching technique in combination with difference in differences analysis. Our results suggest that customer participation in a firm's social media efforts leads to an increase in the frequency of customer visits. We find that this participation effect is greater when there are high levels of activity in the social media site and for customers who exhibit a strong patronage with the firm, buy premium products, and exhibit lower levels of buying focus and deal sensitivity. We find that the above set of results holds for customer profitability as well. We discuss theoretical implications of our results and offer prescriptions for managers on how to engage customers via social media. Our study emphasizes the need for managers to integrate knowledge from customers' transactional relationship with their social media participation to better serve customers and create sustainable business value.
Social media are fundamentally changing the way we communicate, collaborate, consume, and create. They represent one of the most transformative impacts of information technology on business, both within and outside firm boundaries. This special issue was designed to stimulate innovative investigations of the relationship between social media and business transformation. In this paper we outline a broad research agenda for understanding the relationships among social media, business, and society. We place the papers comprising the special issue within this research framework and identify areas where further research is needed. We hope that the flexible framework we outline will help guide future research and develop a cumulative research tradition in this area.
The content created by the users of social networking sites has reached such high levels of quality and variety that it is comparable to that produced by professional agencies. Therefore, understanding what types of content users generate and the underlying motivational factors is vital to the success of the sites. The extant research on content generation has primarily focused on the amount of content and on how to encourage participation in content creation, and less attention has been paid to the content itself and how social relations affect the types of content that users upload. This study aims to empirically document the relationship between social ties and the similarities between the types of content that people create online. We collected a large data set from the photo-hosting website Flickr detailing the users' social relations over time in conjunction with their photo-uploading behavior. We found that around the time of the formation of a social tie, members of dyads began to upload more similar photos than they did before that time. After a social tie was formed, this similarity evolved in different ways in different subgroups of dyads. Whereas the similarity between photos uploaded by dyads experiencing notably different popularity levels on the site continued to grow, the dyads of users with similar levels of popularity gradually began to upload less similar photos. In cultural production, individuals appear to present themselves as unique; this feature is more salient when the social contacts are similar in popularity status. Photo-shooting behaviors have been found to exhibit the same patterns. Furthermore, we show that the most divergent uploading behavior is observed when a high-popularity user initiates a tie with a user with lower popularity. We use social psychological motivations to explain these results.
We study the market for apps on Facebook, the dominant social networking platform, and make use of a rule change by Facebook by which highly engaging apps were rewarded with further opportunities to engage users. The rule change led to new applications with significantly higher user ratings being developed. Moreover, user ratings became more important drivers of app success. Other drivers of app success are also affected by the rule change; sheer network size became a less important driver for app success, update frequency benefitted apps more in staying successful, and active users of Facebook apps declined less rapidly with age. Our results show that social media channels do not necessarily have to be managed through hard exclusion of participants but can also be steered through softer changes in reward and incentive systems.
Applying behavioral economic theories, we hypothesize that consumers have sticky reference prices for individual information goods, but their perceived value for customizable bundle offers can be significantly influenced by the framing of a multipart pricing scheme. The potential impacts of these framing effects are measurable changes in consumer behavior and sales outcomes. We conducted a series of behavioral experiments and a large-scale natural field experiment involving actual purchases of customized information good bundles to confirm and analyze the hypothesized effects. The results demonstrate a consumer's willingness to purchase a customized bundle of information goods, the size of the resulting bundling, and the consumer's perceptions of the transaction are each significantly influenced by the design of the multipart pricing scheme. These results hold even when the final price and size of a customized bundle are the same across differing schemes. We discuss the potential tradeoffs in economic outcomes that result from price framing (e.g., likelihood of sale versus size of purchased bundles) and the implications for information good retailers.
As a result of newer communication technologies and an increase in virtual communication, employees often find themselves multicommunicating, or participating in multiple conversations at the same time. This research seeks to explore multicommunicating from the perspective of the person juggling multiple conversations at the same timethe focal individual. To better understand this phenomenon, we extend previous theorizing by including the concepts of the episode initiator (whether the second conversation was focal or partner initiated), the fit of the set of media used in the episode, one process gain (conversation leveraging), and process losses. Employing a series of pilot studies and a main study, the resulting model was analyzed using structural equation modeling, finding overall support for the model. Findings suggest that experienced intensity is an important factor influencing process losses experienced during multicommunicating, whereas episode initiator influences process losses and the process gain. Further, media fit moderates the relationship between intensity and process losses. The importance of multicommunicating in the workplace is discussed, the theoretical and practical contributions of this research are described, and limitations and suggestions for future research are outlined.
As the ability to measure technology resource usage gets easier with increased connectivity, the question whether a technology resource should be priced by the amount of the resource used or by the particular use of the resource has become increasingly important. We examine this issue in the context of pricing of wireless services: should the price be based on the service, e.g., voice, multimedia messages, short messages, or should it be based on the traffic generated? Many consumer advocates oppose discriminatory pricing across services believing that it enriches carriers at the expense of consumers. The opposition to discrimination has grown significantly, and it has even prompted the U.S. Congress to question executives of some of the biggest carriers. With this ongoing debate on discrimination in mind, we compare two pricing regimes here. One regime, namely, service pricing, involves pricing different services differently. The other one, namely, traffic pricing, involves pricing the traffic (i.e., bytes) transmitted. We show why the common wisdom, that discriminatory pricing across services increases profits and harms consumers, may not always hold. We also show that such discrimination can increase social welfare.
Combinatorial auctions have been suggested as a means to raise efficiency in multi-item negotiations with complementarities among goods because they can be applied in procurement, energy markets, transportation, and the sale of spectrum auctions. The combinatorial clock (CC) auction has become very popular in these markets for its simplicity and for its highly usable price discovery, derived by the use of linear prices. Unfortunately, no equilibrium bidding strategies are known. Given the importance of the CC auction in the field, it is highly desirable to understand whether there are efficient versions of the CC auction providing a strong game theoretical solution concept. So far, equilibrium strategies have only been found for combinatorial auctions with nonlinear and personalized prices for very restricted sets of bidder valuations. We introduce an extension of the CC auction, the CC+ auction, and show that it actually leads to efficient outcomes in an ex post equilibrium for general valuations with only linear ask prices. We also provide a theoretical analysis on the worst case efficiency of the CC auction, which highlights situations in which the CC leads to highly inefficient outcomes. As in other theoretical models of combinatorial auctions, bidders in the field might not be able to follow the equilibrium strategies suggested by the game-theoretical predictions. Therefore, we complement the theoretical findings with results from computational and laboratory experiments using realistic value models. The experimental results illustrate that the CC+ auction can have a significant impact on efficiency compared to the CC auction.
Digital divide initiatives in developing countries are an important avenue for the socioeconomic advancement of those countries. Yet little research has focused on understanding the success of such initiatives. We develop a model of technology use and economic outcomes of digital divide initiatives in developing countries. We use social networks as the guiding theoretical lens because it is well suited to this context, given the low literacy, high poverty, high collectivism, and an oral tradition of information dissemination in developing countries. We test our model with longitudinal data gathered from 210 families in a rural village in India in the context of a digital divide initiative. As theorized, we found that the social network constructs contributed significantly to the explanation of technology use (R2 D 0039). Also as we predicted, technology use partially mediated the effect of social network constructs on economic outcomes (R2 D0047). We discuss implications for theory and practice.
Diversity is a defining characteristic of global collectives facilitated by the Internet. Though substantial evidence suggests that diversity has profound implications for a variety of outcomes including performance, member engagement, and withdrawal behavior, the effects of diversity have been predominantly investigated in the context of organizational workgroups or virtual teams. We use a diversity lens to study the success of nontraditional virtual work groups exemplified by open source software (OSS) projects. Building on the diversity literature, we propose that three types of diversity (separation, variety, and disparity) influence two critical outcomes for OSS projects: community engagement and market success. We draw on the OSS literature to further suggest that the effects of diversity on market success are moderated by the application development stage. We instantiate the operational definitions of three forms of diversity to the unique context of open source projects. Using archival data from 357 projects hosted on SourceForge, we find that disparity diversity, reflecting variation in participants' contribution-based reputation, is positively associated with success. The impact of separation diversity, conceptualized as culture and measured as diversity in the spoken language and country of participants, has a negative impact on community engagement but an unexpected positive effect on market success. Variety diversity, reflected in dispersion in project participant roles, positively influences community engagement and market success. The impact of diversity on market success is conditional on the development stage of the project. We discuss how the study's findings advance the literature on antecedents of OSS success, expand our theoretical understanding of diversity, and present the practical implications of the results for managers of distributed collectives.
Firms often disclose information security risk factors in public filings such as 10-K reports. The internal information associated with disclosures may be positive or negative. In this paper, we evaluate how the nature of the disclosed security risk factors, believed to represent the firm's internal information regarding information security, is associated with future breach announcements reported in the media. For this purpose, we build a decision tree model, which classifies the occurrence of future security breaches based on the textual contents of the disclosed security risk factors. The model is able to accurately associate disclosure characteristics with breach announcements about 77% of the time. We further explore the contents of the security risk factors using text-mining techniques to provide a richer interpretation of the results. The results show that the disclosed security risk factors with risk-mitigation themes are less likely to be related to future breach announcements. We also investigate how the market interprets the nature of information security risk factors in annual reports. We find that the market reaction following the security breach announcement is different depending on the nature of the preceding disclosure. Thus, our paper contributes to the literature in information security and sheds light on how market participants can better interpret security risk factors disclosed in financial reports at the time when financial reports are released.
In information security outsourcing, it is the norm that the outsourcing firms and the outsourcers (commonly called managed security service providers, MSSPs) need to coordinate their efforts for better security. Nevertheless, efforts are often private and thus both firms and MSSPs can suffer from double moral hazard. Furthermore, the double moral hazard problem in security outsourcing is complicated by the existence of strong externality and the multiclient nature of MSSP services. In this prescriptive research, we first show that the prevailing contract structure in security outsourcing, bilateral refund contract, cannot solve double moral hazard. Adding breach-contingent sunk cost or external payment cannot solve double moral hazard either. Furthermore, positive externality can worsen double moral hazard. We then propose a new contract structure termed multilateral contract and show that it can solve double moral hazard and induce first-best efforts from all contractual parties when an MSSP serves two or more client firms, regardless of the externality. Firm-side externality significantly affects how payments flow under a multilateral contract when a security breach happens. When the number of client firms for an MSSP increases, we show that the contingent payments under multilateral contracts for any security breach scenario can be easily calculated using an additive method, and thus are computationally simple to implement.
In recent years, with the emergence and growth of illegal file sharing on the Internet, individual piracy of digital goods, i.e., consumers making illegal copies on their own rather than relying on purchasing copies from commercial pirates, has stirred substantial controversy. Threatened by this growth, the information goods industry took legal action by suing the file sharing peer-to-peer networks and the consumers who illegally share copyrighted material on these networks. In this paper we demonstrate that each one of these two actions aimed to fight individual piracy can backfire by providing strategic disadvantage to legal publishers of information goods. In particular, we show that in the presence of commercial piracy (i) a higher population of consumers who are capable of individual piracy can increase a legal publisher's profits; and (ii) a higher detection and prosecution rate for individual piracy can reduce a legal publisher's profits. Both effects can be observed in markets where commercial piracy is suppressed because the legal publisher can be coerced to take a price cut to minimize the loss of market share. The latter effect can also be observed in markets with active commercial piracy presence because the legal publisher can be forced to raise prices and concede market share to piracy. Our results suggest that information goods producers may be better off by considering their copyright protection policies concerning individual piracy from a more strategic point of view.
In this paper we examine why firms seek to control and own interorganizational information systems, or IOS. Practitioners have largely cited the issues related to control and ownership of IOS, referred to as IOS governance in this paper, as the key reason behind the failure of many IOS initiatives. We distinguish between two IOS governance modes, transactional and financial, and develop a mediated-moderation model to explain the factors influencing the governance choices. In our model, the key motivators of IOS governance are the criticality and the replaceability of the resources that firms procure, which affect IOS governance through their influence on the degree of operational integration existing between partners. We hypothesize that while resource criticality will increase the needs for financial and transactional governance because of its positive impact on operational integration, resource replaceability will negatively influence governance needs because of its negative impact on operational integration. Furthermore, technological uncertainty associated with the resources is argued to negatively impact the extent of IOS governance exercised by firms by weakening the positive impact of resource criticality and strengthening the negative impact of resource replaceability on operational integration respectively. We empirically test our model using data gathered from a survey of 159 United States manufacturing firms. Results show that resource criticality positively affects the extent of financial and transactional IOS governance by increasing the need for operational integration, whereas resource replaceability negatively affects them by reducing the need for operational integration. Furthermore, technological uncertainty creates disincentives for IOS governance primarily by weakening the positive influence of resource criticality on operational integration, while no statistically significant effect of technological uncertainty on the relationship between resource replaceability and operational integration was discerned.
Technological advances enable sellers to price discriminate based on a customer's revealed purchasing intentions. E-tailers can track items in online shopping carts and radio frequency identification tags enable retailers to do the same in brick-and-mortar stores. To leverage this information, it is important to understand how this new visibility impacts pricing and market outcomes. We propose a model in which a seller sets prices for goods A and B, allowing for the possibility of sequentially revising the price for good B if the buyer reveals a preference for good A by making an initial purchase decision. We derive comparative statics results for the prices of products that have superadditive or subadditive values, and also for the associated profits. We also run simulations for a range of distributions of buyer values, to compare sequential pricing with mixed bundling. The results indicate that information technology-enabled sequential pricing can increase profits relative to mixed bundling or pure components pricing for substitute goods due to a reduction of intraseller competition. We also consider the case of goods with positively or negatively correlated values and find that when sellers can condition the second good's price on the buyer's decision to purchase the first good, sequential pricing increases profits when customer's values for the goods are highly positively correlated.
The growth in the application of information technology to student and employee learning underscores the need to understand the impact of technology-mediated learning (TML) methods. Using previous developed TML models, based on social cognitive theory and adaptive structuration meta-theory, the effectiveness of three training methods were examined in this study: technology mediated (using both vicarious and enactive learning), and collaborative and combined (collaborative plus technology mediated). The study also focused on the learning process. The experimental study results showed a significant positive influence of enactive learning enabled TML and collaborative training on specific training outcomes, and the combined training method shows positive results on all training outcomes. The study results also showed that faithful appropriation of the training methods during the learning process has a moderator effect on training outcomes. The study provides important research implications for theory and practice.
The relationship between information technology (IT) and a key organizational design variable, firm size, is an important area of study, particularly given the ongoing transition to an information-based economy. To better understand the more nuanced aspects of the relationship, we formulated a bidirectional and time-lagged model that incorporates different perspectives from organizational theories and transaction cost economics. Our two modelsthe bidirectional and one-year lagged model and the bidirectional and two-year lagged model-were tested using nine-year panel data on IT spending, IT stock, coordination costs, firm size, and relevant control variables for 277 manufacturing firms. We found a sequential interaction between IT and firm size in both of the two models: as a firm grows in size, its coordination activities increase; the firm then uses more IT to handle the increased activities of coordination; this increased use of IT, in turn, decreases coordination costs, and eventually, the size of the firm decreases. It was also found that the presence of coordination costs is necessary for the sequential interaction between IT and firm size, indicating coordination between and within firms is a major reason for firms to invest in IT and for IT effect to take place on firm size. This study has taken an initial step by attempting to empirically examine dual causality and longitudinal effects between IT and firm size, and to reconcile different theoretical perspectives on the relationship between them. We hope this work can act as a catalyst for developing a better understanding of the complex relationship between IT and organizations, with the ultimate goal of offering robust prescriptions for successful structural change.
This research proposes that the forming of a business-to-consumer (B2C) customer relationship is part of a multiphased technology adoption process where attraction is the first step in this sequence. A conceptual model, called the electronic commerce (e-commerce) attraction model (eCAM), offers a theoretical foundation for guiding two empirical studies (N D345 and N D240, respectively) investigating how initial customer perceptions of a website influence attraction toward this website. The results support the eCAM as a new theoretical lens for understanding electronic commerce-based attraction. Comparisons are made between the proposed eCAM and previously established adoption models (i.e., the Technology Acceptance Model and WebQual) as well as the discriminant validity of the constructs in these models. Results demonstrate that the eCAM provides additional insights for understanding how website design influences e-commerce attraction and adoption. The implications of these results for future research and website design are discussed.
With reward (carrot) and punishment (stick) widely applied by organizations to regulate mandatory IT usage, it is imperative to understand how these incentives influence employee compliance behavior. Drawing upon control theory and regulatory focus theory, this study investigates the relationships among regulatory focus, reward, punishment, and compliance behavior in mandatory IT settings. Survey data were collected from 186 employees in companies where enterprise resource planning (ERP) compliance was mandated. Analyses reveal that punishment expectancy is a strong determinant of compliance behavior, whereas the main effect of reward expectancy is not significant. Moreover, the relationship between reward expectancy and compliance behavior is moderated by promotion focus and the relationship between punishment expectancy and compliance behavior is moderated by prevention focus. This study provides an in-depth understanding of reward and punishment in mandatory IT settings and suggests that regulatory focus plays an important role in affecting employees' compliance with organizational controls.
Combinatorial auctions are used in a variety of application domains, such as transportation or industrial procurement, using a variety of bidding languages and different allocation constraints. This flexibility in the bidding languages and the allocation constraints is essential in these domains but has not been considered in the theoretical literature so far. In this paper, we analyze different pricing rules for ascending combinatorial auctions that allow for such flexibility: winning levels and deadness levels. We determine the computational complexity of these pricing rules and show that deadness levels actually satisfy an ex post equilibrium, whereas winning levels do not allow for a strong game theoretical solution concept. We investigate the relationship of deadness levels and the simple price update rules used in efficient ascending combinatorial auction formats. We show that ascending combinatorial auctions with deadness level pricing rules maintain a strong game theoretical solution concept and reduce the number of bids and rounds required at the expense of higher computational effort. The calculation of exact deadness levels is a IIP 2 -complete problem. Nevertheless, numerical experiments show that for mid-sized auctions this is a feasible approach. The paper provides a foundation for allocation constraints in combinatorial auctions and a theoretical framework for recent Information Systems contributions in this field.
Crowd-funded markets have recently emerged as a novel source of capital for entrepreneurs. As the economic potential of these markets is now being realized, they are beginning to go mainstream, a trend reflected by the explicit attention crowdfunding has received in the American Jobs Act as a potential avenue for economic growth, as well as the recent focus that regulators such as the U.S. Securities and Exchange Commission have placed upon it. Although the formulation of regulation and policy surrounding crowd-funded markets is becoming increasingly important, the behavior of crowdfunders, an important aspect that must be considered in this formulation effort, is not yet well understood. A key factor that can influence the behavior of crowd funders is information on prior contribution behavior, including the amount and timing of others' contributions, which is published for general consumption. With that in mind, in this study, we empirically examine social influence in a crowd-funded marketplace for online journalism projects, employing a unique data set that incorporates contribution events and Web traffic statistics for approximately 100 story pitches. This data set allows us to examine both the antecedents and consequences of the contribution process. First, noting that digital journalism is a form of public good, we evaluate the applicability of two competing classes of economic models that explain private contribution toward public goods in the presence of social information: substitution models and reinforcement models. We also propose a new measure that captures both the amount and the timing of others' contribution behavior: contribution frequency (dollars per unit time). We find evidence in support of a substitution model, which suggests a partial crowding-out effect, where contributors may experience a decrease in their marginal utility from making a contribution as it becomes less important to the recipient. Further, we find that the duration of funding and, more importantly, the degree of exposure that a pitch receives over the course of the funding process, are positively associated with readership upon the story's publication. This appears to validate the widely held belief that a key benefit of the crowdfunding model is the potential it offers for awareness and attention-building around causes and ventures. This last aspect is a major contribution of the study, as it demonstrates a clear linkage between marketing effort and the success of crowd-funded projects.
Existing research provides little insight into how social influence affects the adoption and diffusion of competing innovative artifacts and how the experiences of organizational members who have worked with particular innovations in their previous employers affect their current organizations' adoption decision. We adapt and extend the heterogeneous diffusion model from sociology and examine the conditions under which prior adopters of competing open source software (OSS) licenses socially influence how a new OSS project chooses among such licenses and how the experiences of the project manager of a new OSS project with particular licenses affects its susceptibility to this social influence. We test our predictions using a sample of 5,307 open source projects hosted at SourceForge. Our results suggest the most important factor determining a new project's license choice is the type of license chosen by existing projects that are socially closer to it in its interproject social network. Moreover, we find that prior adopters of a particular license are more infectious in their influence on the license choice of a new project as their size and performance rankings increase. We also find that managers of new projects who have been members of more successful prior OSS projects and who have greater depth and diversity of experience in the OSS community are less susceptible to social influence. Finally, we find a project manager is more likely to adopt a particular license type when his or her project occupies a similar social role as other projects that have adopted the same license. These results have implications for research on innovation adoption and diffusion, open source software licensing, and the governance of economic exchange.
Information systems must be used effectively to obtain maximum benefits from them. However, despite a great deal of research on when and why systems are used, very little research has examined what effective system use involves and what drives it. To move from use to effective use requires understanding an information system's nature and purpose, which in turn requires a theory of information systems. We draw on representation theory, which states that an information system is made up of several structures that serve to represent some part of the world that a user and other stakeholders must understand. From this theory, we derive a high-level framework of how effective use and performance evolve, as well as specific models of the nature and drivers of effective use. The models are designed to explain the effective use of any information system and offer unique insights that would not be offered by traditional views, which tend to consider information systems to be just another tool. We explain how our theory extends existing research, provides a rich platform for research on effective use, and how it contributes back to the theory of information systems from which it was derived.
Information technology (IT) infrastructure outsourcing arrangements involve multiple services and processes that are interdependent. The interdependencies pose significant challenges in designing appropriate incentives to influence a provider's effort-allocation decisions. By integrating process modeling fundamentals with multitask agency theory, we enumerate the base set of possible interrelationships among different IT service processes and derive corresponding optimal incentives. Our results demonstrate the impacts of risk profile, random noise, value-cost ratio, and process structure on optimal incentive rates. We find that the current practice of treating IT services as essentially independent is optimal only in limited settings where both the service provider and customer are risk neutral. Interestingly, incongruent performance measures require optimal incentive rates to respond in complex ways to the strength of coupling between services and the complementarity and substitutability of services. We also analyze more complex process scenarios using different combinations of the base set. The results demonstrate that, while the findings from the base set largely hold, the value-cost ratio of the services and the performance measure congruity can pose unique challenges in determining incentive rates.
Interactive online decision aids often employ user-decision aid dialogues as forms of user-system interaction to help construct and elicit users' attribute preferences about a product type. This study extends prior research on online decision aids by investigating the effects of a decision aid's user-system interaction mode (USIM), which can be either user-guided or system-controlled, on users' effort-related (number of iterations of using the aid and perceived cognitive effort expended in using it) and quality-related (perceived quality of the aid and acceptance of the product advice it provides) assessments. A contingency approach with two moderating factors is employed. One factor is the decision strategy (additive-compensatory or elimination) employed by the aid, and the other is the users' product knowledge (high or low). A laboratory experiment was conducted to compare online decision aids with different USIMs. Although the results largely confirm that users assess the user-guided USIM more positively than the system-controlled USIM, the effects of USIM are stronger in two settings: for the elimination-based aid than for the additivecompensatory- based aid and for users with low product knowledge than for those with high product knowledge, especially in terms of effort assessments. This research advances the theoretical understanding of the effects of interaction between two critical components of online decision aids (USIMs and decision strategies) and the moderating role of user characteristics (product knowledge) in affecting users' evaluations. It also provides practitioners with design advice for developing these aids.
Open source software (OSS) communities live and die with the continuous contributions of programmers who often participate without direct remuneration. An intriguing question is whether such sustained participation in OSS projects yields economic benefits to the participants. Moreover, as participants engage in OSS projects, they take on different roles and activities in the community. This raises additional questions of whether different forms of participation in OSS communities are associated with different economic rewards and, if so, in which contexts. In this paper, we draw upon theories of signaling and job matching to hypothesize that participants who possess proof of their skills in OSS projects are financially rewarded for their activities in the labor market. More specifically, we distinguish between participation in OSS communities that is associated with a signaling value for unobserved productivity characteristics and an additional value that accrues to participants whose OSS roles and activities match those in their paid employment. Following a cohort of OSS programmers over a six-year period, we empirically examine the wages and OSS performance of participants in three of the foremost OSS projects operating within the Apache Software Foundation. Controlling for individual characteristics and other wage-related factors, our findings reveal that credentials earned through a merit-based ranking system are associated with as much as an 18% increase in wages. Moreover, we find that participants who have OSS project management responsibilities receive additional financial rewards if their professional job is in IT management. These findings suggest that rank within an OSS meritocracy is a credible and precise signal of participants' productive capacity and that participants' roles and activities in an OSS community have additional financial value when aligned with their paid employment.
Personalization technologies today enable retailers to tailor online purchase interactions to the individual preferences and needs of customers. With personalization being increasingly perceived as a source of competitive advantage, there is a growing trend toward pursuing technology-enabled personalization strategies in online retailing. However, the choice of a retailer whether or not to select into technology-enabled personalization and its implications for customer loyalty are at best ambiguous. This paper is an attempt to resolve this apparent ambiguity. Specifically, the paper conceptualizes retailer selection into technology-enabled personalization strategies relevant to two steps of an online purchase, namely, transaction personalization strategy and decision personalization strategy, based on the operating characteristics of a retailer. The implications of the retailers' self-selection into technology-enabled personalization strategies for customer loyalty are then empirically investigated with data collected from 422 retailers. Further, based on a counterfactual analysis, the paper reveals the implications of making a normatively incorrect decision with respect to personalization strategy. Contrary to popular belief, the results of this study indicate that personalization may not be uniformly beneficial in terms of customer loyalty to all retailers. Although a majority of retailers pursue transaction personalization and realize benefits by way of improved customer loyalty, we find that the choice of a retailer to pursue decision personalization is self-selected and dependent on idiosyncratic characteristics related to its operating context. Retailers that have relatively large-scale operations, provide greater variety and realize higher customer satisfaction with product selection, and that do not necessarily compete on price (i.e., realize lower customer satisfaction with prices relative to competing retailers) are more likely to pursue the decision personalization strategy. Although some retailers pursue decision personalization because they clearly stand to benefit from doing so, other retailers are better off not following suit. Theoretical contributions of the study, managerial implications of the study findings, limitations, and directions for future research are identified.
Privacy is of prime importance to many individuals when they attempt to develop online social relationships. Nonetheless, it has been observed that individuals' behavior is at times inconsistent with their privacy concerns, e.g., they disclose substantial private information in synchronous online social interactions, even though they are aware of the risks involved. Drawing on the hyperpersonal framework and the privacy calculus perspective, this paper elucidates the interesting roles of privacy concerns and social rewards in synchronous online social interactions by examining the causes and the behavioral strategies that individuals utilize to protect their privacy. An empirical study involving 251 respondents was conducted in online chat rooms. Our results indicate that individuals utilize both self-disclosure and misrepresentation to protect their privacy and that social rewards help explain why individuals may not behave in accordance with their privacy concerns. In addition, we find that perceived anonymity of others and perceived intrusiveness affect both privacy concerns and social rewards. Our findings also suggest that higher perceived anonymity of self decreases individuals' privacy concerns, and higher perceived media richness increases social rewards. Generally, this study contributes to the information systems literature by integrating the hyperpersonal framework and the privacy calculus perspective to identify antecedents of privacy trade-off and predict individuals' behavior in synchronous online social interactions.
The value of promotional marketing and word-of-mouth (WOM) is well recognized, but few studies have compared the effects of these two types of information in online settings. This research examines the effect of marketing efforts and online WOM on product sales by measuring the effects of online coupons, sponsored keyword search, and online reviews. It aims to understand the relationship between firms' promotional marketing and WOM in the context of a third party review platform. Using a three-year panel data set from one of the biggest restaurant review websites in China, the study finds that both online promotional marketing and reviews have a significant impact on product sales, which suggests promotional marketing on third party review platforms is still an effective marketing tool. This research further explores the interaction effects between WOM and promotional marketing when these two types of information coexist. The results demonstrate a substitute relationship between the WOM volume and coupon offerings, but a complementary relationship between WOM volume and keyword advertising.
This article analytically and experimentally investigates how firms can best capture the business value of information technology (IT) investments through IT contract design. Using a small sample of outsourcing contracts for enterprise information technology (EIT) projects in several industries-coupled with reviews of contracts used by a major enterprise software maker-the authors determine the common provisions and structural characteristics of EIT contracts. The authors use these characteristics to develop an analytical model of optimal contract design with principal-agent techniques. The model captures a set of key characteristics of EIT contracts, including a staged, multiperiod project structure; learning; probabilistic binary outcomes; variable fee structures; possibly risk-averse agents; and implementation risks. The model characterizes conditions under which multistage contracts enable clients to create and capture greater project value than single-stage projects, and how project staging enables firms to reduce project risks, capture learning benefits, and increase development effort. Finally, the authors use controlled laboratory experiments to complement their analytical approaches and demonstrate robustness of their key findings.
This article investigates the economic consequences of data errors in the information flows associated with business processes. We develop a process modeling-based methodology for managing the risks associated with such data errors. Our method focuses on the topological structure of a process and takes into account its effect on error propagation and risk mitigation using both expected loss and conditional value-at-risk risk measures. Using this method, optimal strategies can be designed for control resource allocation to manage risk in a business process. Our work contributes to the literature on both ex ante risk management-based business process design and ex post risk assessments of existing business processes and control models. This research applies not only to the literature on and practice of process design and risk management but also to business decision support systems in general. An order-fulfillment process of an online pharmacy is used to illustrate the methodology.
This paper considers on-demand services and its impact on market structure, firm profitability, and consumer welfare. The unique properties of on-demand services are the conversion of fixed costs to variable costs, removal of capacity constraint, and fast setup time (which enables quick entry by any firm at any time when there is opportunity), whereas privacy and security concerns and switching costs have been noted as the biggest barriers from adopting on-demand services. With a stylized model capturing these benefits and barriers to using on-demand services, we establish several results. First, we show that conversion of fixed cost to variable cost enables new and small firms to enter existing markets and leads to the creation of new markets. Second, we show that competition and the threat of new entrants can be an important driver of a firm's decision to switch to on-demand services. In addition, a firm's barriers to using on-demand services can influence another firm's entry decision. Third, we show that two identical firms may employ different technologies in equilibrium. Fourth, we show that fast setup time and removal of capacity constraint associated with on-demand services make it impossible for firms to make supranormal return and would lead to a perfect competitive market, even when there is only one firm, under very general conditions. Such a result still holds even when there exists an economy of scale (e.g., quantity discount) from using on-demand services. On the other hand, when there are barriers preventing firms from offering similar products and products are substantially differentiated, on-demand services can amplify this advantage of entry barriers by enabling firms to further increase prices and enhance their profitability. Therefore, contrary to the common belief that offering on-demand services is best for firms offering commodity products, we show on-demand services to be more profitable for firms with differentiated products.
This study frames antecedents of effective information use, outlining a nomological network that firms follow to achieve integrated information delivery and effective information use. Our focus is on senior business executives' assessment of information delivered by their organizations' information systems. We first clarify the definition of information as it relates to information delivery and effective use. Then, drawing from institutional theory and the resource-based view of the firm, we propose a research model consisting of external institutional pressure, internal information systems (IS) resources, integrated information delivery, and effective information use and empirically test it through a field survey of senior business executives and post hoc qualitative analysis. Our findings position information delivery as an important research construct leading to effective information use and value. Our study also highlights the important role of the IS function as a facilitator of effective information use and a nurturer of a strong information culture in organizations. Finally, we offer practical advice on how senior executives assess and improve integrated information delivery and effective use.
Topical locality on the Web is the notion that pages tend to link to other topically similar pages and that such similarity decays rapidly with link distance. This supports meaningful Web browsing and searching by information consumers. It also allows topical Web crawlers, programs that fetch pages by following hyperlinks, to harvest topical subsets of the Web for applications such as those in vertical search and business intelligence. We show that the Web exhibits another property that we call status locality. It is based on the notion that pages tend to link to other pages of similar status (importance) and that this status similarity also decays rapidly with link distance. Analogous to topical locality, status locality may also be exploited by Web crawlers. Collections built by such crawlers include pages that are both topically relevant and also important. This capability is crucial because of the large numbers of Web pages addressing even niche topics. The challenge in exploiting status locality while crawling is that page importance (or status) is typically recognized through global measures computed by processing link data from billion of pages. In contrast, topical Web crawlers depend on local information based on previously downloaded pages. We solve this problem by using methods developed previously that utilize local characteristics of pages to estimate their global status. This leads to the design of new crawlers, specifically of utility-biased crawlers guided by a Cobb-Douglas utility function. Our crawler experiments show that status and topicality of Web collections present a trade-off. An adaptive version of our utility-biased crawler dynamically modifies output elasticities of topicality and status to create Web collections that maintain high average topicality. This can be done while simultaneously achieving significantly higher average status as compared to several benchmarks including a state-of-the-art topical crawler.
Universal access (UA) to the Internet and the associated information infrastructure has become an important economic and societal goal. However, UA initiatives tend to focus on issues such as physical access and geographical ubiquity, and they measure adoption through penetration rates. In this paper, we apply an interpretive case study approach to analyze the Philadelphia wireless initiative to provide insights into the nature of UA and extend this concept to also consider universal use (UU). UU is important because simply providing access does not guarantee use. UU is presented as a conceptual goal that starts with the challenge of physical access, but which necessarily also leads to considerations of use. The results show that the human and technological elements underlying individual access and use are deeply embedded within various institutional elements and collectives that enable but also constrain meaningful use. We integrate our findings into a multilevel framework that shows how access and use are influenced by both micro and macro factors. This framework provides new insights into the study of the information infrastructure, digital divide, and public policy.
We analyze the impact of information technology (IT) on the technical efficiency of firms in the context of their observed competitive settings. Because competition can be a driver of efficiency and industries display varying degrees of competitiveness, firm-level efficiency is likely to display considerable heterogeneity. To shed light on these questions, we analyze the economic impact of IT on technical efficiency, a key component of efficiency, in heterogeneous competitive settings. Our study employs a number of econometric techniques, including a stochastic frontier and a generalized method of moments approach, on data from firms in a wide cross-section of industries. We find, after controlling for firm-level heterogeneity and potential endogeneity, that IT is positively associated with gains in technical efficiency but its impact is moderated by the degree of competition. Firms display large variation in their levels of technical efficiency partly because of the heterogeneous market competitiveness conditions they face. In more competitive industries, firms tend to deploy IT more intensively and use it more efficiently. Our study makes a distinct contribution relative to prior studies that have focused on the productivity impacts of IT while assuming perfect competition and not allowing for potential heterogeneity in firm-level efficiency. Overall, our results demonstrate that IT and competition are significant determinants of gains in technical efficiency and provide insight into how competition affects the returns to IT investment.
We explore how Internet browsing behavior varies between mobile phones and personal computers. Smaller screen sizes on mobile phones increase the cost to the user of browsing for information. In addition, a wider range of offline locations for mobile Internet usage suggests that local activities are particularly important. Using data on user behavior at a (Twitter-like) microblogging service, we exploit exogenous variation in the ranking mechanism of posts to identify the ranking effects. We show that (1) ranking effects are higher on mobile phones suggesting higher search costs: links that appear at the top of the screen are especially likely to be clicked on mobile phones and (2) the benefit of browsing for geographically close matches is higher on mobile phones: stores located in close proximity to a user's home are much more likely to be clicked on mobile phones. Thus, the mobile Internet is somewhat less Internet-like: search costs are higher and distance matters more. We speculate on how these changes may affect the future direction of Internet commerce.
We identify two post-acceptance information system (IS) usage behaviors related to how employees leverage implemented systems. Routine use (RTN) refers to employees' using IS in a routine and standardized manner to support their work, and innovative use (INV) describes employees' discovering new ways to use IS to support their work. We use motivation theory as the overarching perspective to explain RTN and INV and appropriate the rich intrinsic motivation (RIM) concept from social psychology to propose a conceptualization of RIM toward IS use, which includes intrinsic motivation toward accomplishment (IMap), intrinsic motivation to know (IMkw), and intrinsic motivation to experience stimulation (IMst). We also consider the influence of perceived usefulness (PU)-a representative surrogate construct of extrinsic motivation toward IS use-on RTN and INV. We theorize the relative impacts of the RIM constructs and PU on RTN and INV and the role of personal innovativeness with IT (PIIT) in moderating the RIM constructs' influences on INV. Based on data from 193 employees using a business intelligence system at one of the largest telecom service companies in China, we found (1) PU had a stronger impact on RTN than the RIM constructs, (2) IMkw and IMst each had a stronger impact on INV than either PU or IMap, and (3) PIIT positively moderated the impact of each RIM construct on INV. Our findings provide insights on managing RTN and INV in the post-acceptance stage.
Digital technologies have made networks ubiquitous. A growing body of research is examining these networks to gain a better understanding of how firms interact with their consumers, how people interact with each other, and how current and future digital artifacts will continue to alter business and society. The increasing availability of massive networked data has led to several streams of inquiry across fields as diverse as computer science, economics, information systems, marketing, physics, and sociology. Each of these research streams asks questions that at their core involve information in networks-its distribution, its diffusion, its inferential value, and its influence on social and economic outcomes. We suggest a broad direction for research into social and economic networks. Our analysis describes four kinds of investigation that seem most promising. The first studies how information technologies create and reveal networks whose connections represent social and economic relationships. The second examines the content that flows through networks and its economic, social, and organizational implications. A third develops theories and methods to understand and utilize the rich predictive information contained in networked data. A final area of inquiry focuses on network dynamics and how information technology affects network evolution. We conclude by discussing several important crosscutting issues with implications for all four research streams, which must be addressed if the ensuing research is to be both rigorous and relevant. We also describe how these directions of inquiry are interconnected: results and ideas will pollinate across them, leading to a new cumulative research tradition.
In this study, we focus on the factors that influence online innovation community members' continued participation in the context of open source software development (OSSD) communities. Prior research on continued participation in online communities has primarily focused on social interactions among members and benefits obtained from these interactions. However, members of these communities often play different roles, which have been examined extensively, albeit in a separate stream of research. This study attempts to bridge these two streams of research by investigating the joint influence of community response and members' roles on continued participation. We categorize OSSD community members into users and modifiers and empirically examine the differential effects of community response across these roles. By analyzing a longitudinal data set of activities in the discussion forums of more than 300 OSSD projects, we not only confirm the positive influence of community response on members' continued participation but also find that community response is more influential in driving the continuance behavior of users than that of modifiers. In addition, this research highlights the importance of modifiers, a key subgroup of OSSD participants that has been largely overlooked by prior research.
Internet retailers have been making significant investments in Web technologies, such as zoom, alternative photos, and color swatch, that are capable of providing detailed product-oriented information and, thereby, mitigating the lack of touch and feel, which, in turn, is expected to lower product returns. However, a clear understanding of the relationship between these technologies and product returns is still lacking. Our study attempts to fill this gap by using several econometric models to explore the said relationship. Our unique and rich data set from a women's clothing company allows us to measure technology usage at the product level for each consumer. The results show that, in this context, zoom usage has a negative coefficient, suggesting that a higher use of the zoom technology is associated with fewer returns. Interestingly, we find that a higher use of alternative photos is associated with more returns and, perhaps more importantly, with lower net sales. Color swatch, on the other hand, does not seem to have any effect on returns. Thus, our findings show that different technologies have different effects on product returns. We provide explanations for these findings based on the extant literature. We also conduct a number of tests to ensure the robustness of the results.
Open source software is becoming increasingly prominent, and the economic structure of open-source development is changing. In recent years, firms motivated by revenues from software services markets have become the primary contributors to open-source development. In this paper we study the role of services in open source software development and explore the choice between open source and proprietary software. Specifically, our economic model jointly analyzes the investment and pricing decisions of the originators of software and of subsequent open-source contributors. We find that if a contributor is efficient in software development, the originator should adopt an open-source strategy, allowing the contributor to offer higher total quality and capture the higher end of the market while the originator focuses on providing software services to lower end consumers. Conversely, if the contributor is not efficient in development, the originator should adopt a proprietary software development strategy, gaining revenue from software sales and squeezing the contributor out of the services market. In certain cases an increase in originator development efficiency can result in increased contributor profits. Finally, we find that, somewhat counterintuitively, an increase in contributor development efficiency can reduce overall social welfare.
Prior studies on the business value of information technology (IT) mainly focus on the impact of IT investments on productivity and firm profitability. Few have considered its implication on expected and actual product or service quality. This paper fills this gap by investigating the impact of past healthcare IT (HIT) expenditure on the malpractice insurance premium (MIP) and the moderating effect of past HIT expenditure on the relationship between past MIP and current quality of patient care in a longitudinal model. Based on archival panel data on costs, operations, and patient care outcomes of 66 hospitals in the U.S. state of Washington from 1998 to 2007, we find that past HIT expenditure is negatively associated with MIP, supporting our argument that HIT provides value that is anticipated by insurers and is captured by a change in MIP. We find that past HIT is positively associated with quality of patient care. We also find that past MIP is positively associated with quality of patient care, supporting the premise that hospitals respond to MIP by making risk mitigation efforts. However, we find that past HIT moderates this relationship negatively, suggesting a reliance on HIT at the expense of risk mitigation.
Recommender systems are becoming a salient part of many e-commerce websites. Much research has focused on advancing recommendation technologies to improve accuracy of predictions, although behavioral aspects of using recommender systems are often overlooked. In our studies, we explore how consumer preferences at the time of consumption are impacted by predictions generated by recommender systems. We conducted three controlled laboratory experiments to explore the effects of system recommendations on preferences. Studies 1 and 2 investigated user preferences for television programs across a variety of conditions, which were surveyed immediately following program viewing. Study 3 investigated the granularity of the observed effects within individual participants. Results provide strong evidence that the rating presented by a recommender system serves as an anchor for the consumer's constructed preference. Viewers' preference ratings are malleable and can be significantly influenced by the recommendation received. The effect is sensitive to the perceived reliability of a recommender system and, thus, not a purely numerical or priming-based effect. Finally, the effect of anchoring is continuous and linear, operating over a range of perturbations of the system. These general findings have a number of important implications (e.g., on recommender systems performance metrics and design, preference bias, potential strategic behavior, and trust), which are discussed.
The business case for investing in information technology (IT) has received increasing scrutiny in recent years. We propose that IT investments create additional business value through interactions with other business processes. In this paper, we formalize the interaction effect of IT by focusing on one core function, namely, research and development (R&D). We hypothesize that investments in IT can interact with and complement a firm's R&D investments, enhancing the firm's shareholder value creation potential. We test this by hypothesis by estimating the interaction impact of IT and R&D investments on Tobin's q, a forward-looking measure of firm performance using a recent multiyear, firm-level, archival data set. Our results suggest that the interaction effect of R&D and IT on Tobin's q is positive and significant after controlling for other firm- and industryspecific effects. Our findings provide rigorous empirical support for recent anecdotal evidence in the managerial literature with respect to the manner in which IT is enabling R&D-intensive innovation processes. Our analysis underscores the need for coordinated investments in IT and R&D, and permeating IT capabilities throughout other business processes such as R&D.
The development of information systems and software applications increasingly needs to deliver culturally rich and affective experiences for user groups. In this paper, we explore how the collaborative practices across different expert groups can enable this experiential dimension of use to be integrated into the development of a software product. In an empirical study of computer games developmentan arena in which the novelty and richness of the user experience is central to competitive successwe identify the challenges of conceptualizing and realizing a desired user experience when it cannot be readily specified in an initial design template, nor represented within the expertise of existing groups. Our study develops a theoretical framework to address these challenges. Through this framework, we are able to show how achieving a desired user experience requires developer groups to not only work across the boundaries that arise from specialized expertise, but also across wider fields centred on cultural production and software development, respectively. We find that their ability to do this is supported by distinctive envisioning practices that sustain an emerging shared vision for each game. The key research contributions that we then make are (a) grounding envisioning practices as a means of theorizing the collaborative practices centred on conceptualizing the user experience; (b) identifying how these practices are interwoven with the producing practices of software development, thus enabling collaboration to span expert groups and disparate fields; and (c) theorizing the role of vision as an emerging conceptual boundary object in these practices.
The hypercompetitive aspects of modern business environments have drawn organizational attention toward agility as a strategic capability. Information technologies are expected to be an important competency in the development of organizational agility. This research proposes two distinct roles to understand how information technology competencies shape organizational agility and firm performance. In their enabling role, IT competencies are expected to directly enhance entrepreneurial and adaptive organizational agility. In their facilitating role, IT competencies should enhance firm performance by helping the implementation of requisite entrepreneurial and adaptive actions. Furthermore, we argue that the effects of the dual roles of IT competencies are moderated by multiple contingencies arising from environmental dynamism and other sources. We test our model and hypotheses through a latent class regression analysis on data from a sample of 109 business-to-business electronic marketplaces. The results provide support for the enabling and facilitating roles of IT competencies. Moreover, we find that these dual effects vary according to environmental dynamism. The results suggest that managers should account for (multiple) contingencies (observed and unobserved) while assessing the effects of IT competencies on organizational agility and firm performance.
The Internet has provided IS researchers with the opportunity to conduct studies with extremely large samples, frequently well over 10,000 observations. There are many advantages to large samples, but researchers using statistical inference must be aware of the p-value problem associated with them. In very large samples, p-values go quickly to zero, and solely relying on p-values can lead the researcher to claim support for results of no practical significance. In a survey of large sample IS research, we found that a significant number of papers rely on a low p-value and the sign of a regression coefficient alone to support their hypotheses. This research commentary recommends a series of actions the researcher can take to mitigate the p-value problem in large samples and illustrates them with an example of over 300,000 camera sales on eBay. We believe that addressing the p-value problem will increase the credibility of large sample IS research as well as provide more insights for readers.
Third-party quality assurance seals have emerged as a prominent mechanism to reduce uncertainty and increase purchase conversion in online markets. However, systematic studies of the effectiveness of these seals are scarce. In this study, we exploit a unique data set of 9,098 shopping sessions at an online retailer's website to empirically measure the value and effectiveness of assurance seals on the likelihood of purchase by shoppers. The data set is collected from a randomized field experiment conducted by a large seal provider, which enables us to infer the causal impacts of the presence of an assurance seal. We find strong evidence that the presence of the assurance seal increases the likelihood of purchase conversion. We discuss the implications of our findings for online retailers, third-party certifiers, policymakers, and researchers.
This paper develops an innovative real options (RO) model for valuing multistage information technology (IT) projects that can be viewed as comprising meta stages. In RO literature, multistage investment programs have been treated as either interproject or intraproject programs, with intraproject programs being evaluated using n- fold Geske compound options and interproject programs valued using the so-called subsidy-to-exercise price logic. Our innovative RO model integrates the Geske compound option model with the subsidy-to-exercise price approach to value sequential investment programs that are neither purely interproject nor purely intraproject in nature but are composed of meta-stages. A meta-stage as a whole can be considered an interproject stage resulting in cash flows, but internally it consists of several intraproject stages that do not result in cash flows. We show that a key problem in IT, which is migrating to a Service-Oriented Architecture (SOA) for integrating a firm's many disparate applications, systems, data, and business processes, is best viewed as an investment program comprising meta-stages. Examining SOA migration from an RO lens is particularly apt at this time not only because of the importance of SOA but also because doubts have surfaced about the value of SOA. We illustrate our RO model by applying it to the simulated case of a firm migrating to SOA. We also develop a software tool based on the Mathematica computational platform so that practitioners can easily apply our innovative options pricing model to determine the true value of SOA in their business contexts.
Virtual communities continue to play a greater role in social, political, and economic interactions. However, how users value information from these communities and how that affects their behavior and future expectations is not fully understood. Stock message boards provide an excellent setting to analyze these issues given the large user base and market uncertainty. Using data from 502 investor responses from a field experiment on one of the largest message board operators in South Korea, our analyses revealed that investors exhibit confirmation bias, whereby they preferentially treat messages that support their prior beliefs. This behavior is more pronounced for investors with higher perceived knowledge about the market and higher strength of belief (i.e., sentiment) toward a particular stock. We also find a negative interaction effect between the perceived knowledge and the strength of prior belief on confirmation bias. Those exhibiting confirmation bias are also more overconfident; as a result, they trade more actively and expect higher market returns than is warranted. Collectively, these results suggest that participation in virtual communities may not necessarily lead to superior financial returns.
We examine how network centrality and closure, two key aspects of network structure, affect technology adoption. In doing so, we consider the content of potential information flows within the network and argue that the impact of network structure on technology adoption can be better understood by separately examining its impact from two groups of alterscurrent and potential adopters. We contend that increased network centrality and closure among current adopters contribute positively to adoption, whereas the same among potential adopters has exactly the opposite impact. Accordingly, we propose a dynamic view where the fraction of current adopters in the network positively moderates the impact of network centrality and closure. We empirically test the theory by analyzing the adoption of software version control technology by open source software projects. Our results strongly support the theory.
We investigate how intellectual property rights (IPR) enforcement against developers and users of open source software (OSS) affects the success of related OSS projects.We hypothesize that when an IPR enforcement action is filed, user interest and developer activity will be negatively affected in two types of related OSS projectsthose that display technology overlap with the OSS application in dispute and business projects that are specific to the disputed OSS platform. We examine two widely publicized lawsuitsSCO v. IBM and FireStar/DataTern v. Red Hatusing data from SourceForge.net. Our difference-in-difference estimates show that in the months following the filing of SCO v. IBM, OSS projects that exhibit high technology overlap with the disputed OSS experienced a 15% greater decline in user interest and 45% less developer activity than projects in the control group; OSS projects that are intended for business and specific to the disputed OSS platform had a 34% greater decline in user interest and 86% less developer activity than the control group. We find similar results following the filing of FireStar/DataTern v. Red Hat. Our results are also robust to a variety of robustness checks, including a falsification exercise and subsample analyses.
We investigate the impact of outsourcing on the long-term market performance of the firm. Outsourcing initiatives vary in terms of uncertainty in business requirements, complexity of coordination between the outsourcing firm and provider, and the consequent choice of the governing contract (fixed or variable price). Using theories from institutional economics, strategy, and information systems, we argue that firms pursuing large-scale, fixed price outsourcing, which are characterized by lower business uncertainty and simpler coordination requirements, will realize higher market returns relative to similar firms in the same industry who did not outsource. In contrast, variable price contracts that proxy for higher business uncertainty and coordination complexity may have a higher risk of failure and loss of shareholder value; however, prior outsourcing experience and prior association with the vendor may reduce uncertainty in the outsourcing relationship to help the outsourcing firm better manage challenges associated with complex, variable price engagements. We posit that financial markets are either not privy to or unlikely to accurately interpret such intangible information on the antecedents of outsourcing success during the announcement period. The delay in incorporation of this information in market prices results in positive long-term abnormal returns to fixed price contracts. Variable price contracts characterized by prior association between participant firms and greater outsourcing experience also realize positive long-term abnormal returns. Data on the hundred largest outsourcing initiatives implemented between 1996 and 2005 strongly support our hypotheses. The results imply that firms who retain simple functions and tasks in-house as well as those who outsource complex functions without pertinent experience or association with the vendor experience significant loss of shareholder value.
Contextual ambidexterity of an interorganizational relationship (IOR) is the ability of its management system to align partners' activities and resources for short-term goals and adapt partners' cognitions and actions for long-term viability. It is an alternative to structural ambidexterity in which separate units of the IOR pursue short- and long-term goals. We theorize that when utilized to coordinate the IOR, information technology (IT)-enabled operations and sensemaking, along with interdependent decision making, promote the IOR's contextual ambidexterity. We test our hypotheses on both sides of a customer-vendor relationship using data collected from (1) the account executives of one of the world's largest supply chain vendors (n = 76) and (2) its customers (n = 238). We find commonalities and differences in the influence coordination mechanisms have on contextual ambidexterity from the vendor's and the customer's perspectives. For both customers and vendors, contextual ambidexterity improves the quality and performance of the relationship, and decision interdependence promotes contextual ambidexterity. For customers, using operations support systems (OSSs) and interpretation support systems (ISSs) enhances contextual ambidexterity. For vendors, the impact of both OSS use and ISS use on contextual ambidexterity depends on the duration of the relationship. Our study shows that IT-enabled operations and sensemaking are key enablers of IOR ambidexterity and that vendors should combine these IT capabilities with relationship-specific knowledge that accumulates with relationship duration.
Electronic networks of practice have become a prevalent means for acquiring new knowledge. Knowledge seekers commonly turn to online repositories constructed by these networks to find solutions to domain-specific problems and questions. Yet little is understood about the process by which such knowledge is evaluated and adopted by knowledge seekers. This study examines how individuals filter knowledge encountered in online forums, a common platform for knowledge exchange in an electronic network of practice. Drawing on dual process theory, we develop research hypotheses regarding both central and peripheral evaluation of knowledge. These hypotheses are examined in a field experiment in which participants evaluate online solutions for computer programming problems. Results show that peripheral cues (source expertise and validation) have a greater influence on knowledge filtering decisions than does the content quality of the solution. Moreover, elaboration increases the effect of content quality but does not seem to attenuate the effect of peripheral cues. Implications for research and practice are discussed.
In this paper, we explore the economics of free under perpetual licensing. In particular, we focus on two emerging software business models that involve a free component: feature-limited freemium (FLF) and uniform seeding (S). Under FLF, the firm offers the basic software version for free, while charging for premium features. Under S, the firm gives away for free the full product to a percentage of the addressable market uniformly across consumer types. We benchmark their performance against a conventional business model under which software is sold as a bundle (labeled as charge for everything or CE) without free offers. In the context of consumer bounded rationality and information asymmetry, we develop a unified two-period consumer valuation learning framework that accounts for both word-of-mouth (WOM) effects and experience-based learning, and use it to compare and contrast the three business models. Under both constant and dynamic pricing, for moderate strength of WOM signals, we derive the equilibria for each model and identify optimality regions. In particular, S is optimal when consumers significantly underestimate the value of functionality and cross-module synergies are weak. When either cross-module synergies are stronger or initial priors are higher, the firm decides between CE and FLF. Furthermore, we identify nontrivial switching dynamics from one optimality region to another depending on the initial consumer beliefs about the value of the embedded functionality. For example, there are regions where, ceteris paribus, FLF is optimal when the prior on premium functionality is either relatively low or high, but not in between. We also demonstrate the robustness of our findings with respect to various parameterizations of cross-module synergies, strength of WOM effects, and number of periods. We find that stronger WOM effects or more periods lead to an expansion of the seeding optimality region in parallel with a decrease in the seeding ratio. Moreover, under CE and dynamic pricing, second period price may be decreasing in the initial consumer valuation beliefs when WOM effects are strong and the prior is relatively low. However, this is not the case under weak WOM effects. We also discuss regions where price skimming and penetration pricing are optimal. Our results provide key managerial insights that are useful to firms in their business model search and implementation.
Managers of emerging platforms must decide what level of platform performance to invest in at each product development cycle in markets that exhibit two-sided network externalities. High performance is a selling point for consumers, but in many cases it requires developers to make large investments to participate. Abstracting from an example drawn from the video game industry, we build a strategic model to investigate the trade-off between investing in high platform performance versus reducing investment in order to facilitate third party content development. We carry out a full analysis of three distinct settings: monopoly, price-setting duopoly, and price-taking duopoly. We provide insights on the optimum investment in platform performance and demonstrate how conventional wisdom about product development may be misleading in the presence of strong cross-network externalities. In particular, we show that, contrary to the conventional wisdom about winner-take-all markets, heavily investing in the core performance of a platform does not always yield a competitive edge. We characterize the conditions under which offering a platform with lower performance but greater availability of content can be a winning strategy.
The measurement of the impact of IT spillovers on productivity is an important emerging area of research. Studies of IT spillovers often adopt a production function approach commonly used for measuring R&D spillovers, in which an external pool of IT investment is modeled using weighted measures of the IT investments of other firms, industries, or countries. We show that when using this approach, measurement error in a firm's own IT inputs can exert a significant upward bias on estimates of social returns to IT investment. This problem is particularly severe for IT spillovers because of the high levels of measurement error in most available IT data. The presence of the bias term can be demonstrated by using instrumental variable techniques to remove the effects of measurement error in a firm's own IT inputs. Using panel data on IT investment, we show that measurement error corrected estimates of IT spillovers are 40% to 90% lower than uncorrected estimates. This bias term is increasing in the correlation between the IT pool and firms' own IT investment. Therefore, estimates from models of spillover pools are less sensitive to the issues identified in this paper when the spillover paths minimize the correlation between a firm's own IT investment and the constructed external IT pool. Implications for researchers, policy makers, and managers are discussed.
There is growing recognition that firms' information technology (IT)-enabled business models (i.e., how interfirm transactions with suppliers, customers, and partners are structured and executed) are a distinctive source of value creation and appropriation. However, the concept of business models' (BMs) IT enablement remains coarse in the information systems and strategic management literatures. Our objectives are to introduce a framework to elaborate the concept of IT-enabled BMs and to identify areas for future research that will enhance our understanding of the subject. We introduce the idea that two business-to-business (B2B) IT capabilitiesdyadic IT customization and network IT standardization-are the mediating execution mechanisms between the strategic intent of interfirm collaboration and the (re)configuration of BMs to both create and appropriate value. We develop the logic that B2B IT capabilities for BM (re)configuration operate at two levelsIT customization at the dyadic relationship level and IT standardization at the interfirm network levelthat together provide the complementary IT capabilities for firms to exchange content, govern relationships, and structure interconnections between products and processes with a diverse set of customers, suppliers, and partners. We discuss how these two complementary B2B IT capabilities are pivotal for firms to pursue different sources of value creation and appropriation. We identify how a firm's governance choices to engage in interfirm collaboration and its interfirm networks coevolve with its B2B IT capabilities as fruitful areas for future research.
This paper discusses the value of context in theory development in information systems (IS) research. We examine how prior research has incorporated context in theorizing and develop a framework to classify existing approaches to contextualization. In addition, we expound on a decomposition approach to contextualization and put forth a set of guidelines for developing context-specific models. We illustrate the application of the guidelines by constructing and comparing various context-specific variations of the technology acceptance model (TAM)-i.e., the decomposed TAM that incorporates interaction effects between context-specific factors, the extended TAM with context-specific antecedents, and the integrated TAM that incorporates mediated moderation and moderated mediation effects of context-specific factors. We tested the models on 972 individuals in two technology usage contexts: a digital library and an agile Web portal. The results show that the decomposed TAM provides a better understanding of the contexts by revealing the direct and interaction effects of context-specific factors on behavioral intention that are not mediated by the TAM constructs of perceived usefulness and perceived ease of use. This work contributes to the ongoing discussion about the importance of context in theory development and provides guidance for context-specific theorizing in IS research.
This paper studies the effect of online product reviews on different players in a channel structure. We consider a retailer selling two substitutable products produced by different manufacturers, and the products differ in both their qualities and fits to consumers' needs. Online product reviews provide additional information for consumers to mitigate the uncertainty about the quality of a product and about its fit to consumers' needs. We show that the effect of reviews on the upstream competition between the manufacturers is critical in understanding which firms gain and which firms lose. The upstream competition is affected in fundamentally different ways by quality information and fit information, and each information type has different implications for the retailer and manufacturers. Quality information homogenizes consumers' perceived utility differences between the two products and increases the upstream competition, which benefits the retailer but hurts the manufacturers. Fit information heterogenizes consumers' estimated fits to the products and softens the upstream competition, which hurts the retailer but benefits the manufacturers. Furthermore, reviews may also alter the nature of upstream competition from one in which consumers' own assessment on the quality dimension plays a dominant role in consumers' comparative evaluation of products to one in which fit dimension plays a dominant role. If manufacturers do not respond strategically to reviews and keep the same wholesale prices regardless of reviews (i.e., the upstream competition is assumed to be unaffected by reviews), then, we show that reviews never hurt the retailer and the manufacturer with favorable reviews, and never benefit the manufacturer with unfavorable reviews, a finding that demonstrates why reviews' effect on upstream competition is critical for firms in online marketplaces.
We investigate the dynamics of blog reading behavior of employees in an enterprise blogosphere. A dynamic model is developed and calibrated using longitudinal data from a Fortune 1,000 IT services firm. Our modeling framework allows us to segregate the impact of textual characteristics (sentiment and quality) of a post on attracting readers from retaining them. We find that the textual characteristics that appeal to the sentiment of the reader affect both reader attraction and retention. However, textual characteristics that reflect only the quality of the posts affect only reader retention. We identify a variety-seeking behavior of blog readers where they dynamically switch from reading on one set of topics to another. The modeling framework and findings of this study highlight opportunities for the firm to influence blog-reading behavior of its employees to align it with its goals. Overall, this study contributes to improved understanding of reading behavior of individuals in communities formed around user generated content.
We study the role of different contract types in coordinating the joint product improvement effort of a client and a customer support center. The customer support center's costly efforts at joint product improvement include transcribing and analyzing customer feedback, analyzing market trends, and investing in product design. Yet this cooperative role must be adequately incentivized by the client, since it could lead to fewer service requests and hence lower revenues for the customer support center. We model this problem as a sequential game with double-sided moral hazard in a principal-agent framework (in which the client is the principal). We follow the contracting literature in modeling the effort of the customer support center, which is the first mover, as either unobservable or observable; in either case, the efforts are unverifiable and so cannot be contracted on directly. We show that it is optimal for the client to offer the customer support center a linear gain-share contract when efforts are unobservable, even though it can yield only the second-best solution for the client. We also show that the cost-plus contracts widely used in practice do not obtain the optimal solution. However, we demonstrate that if efforts are observable then a gain-share and cost-plus options- based contract is optimal and will also yield the first-best solution. Our research provides a systematic theoretical framework that accounts for the prevalence of gain-share contracts in the IT industry's joint improvement efforts, and it provides guiding principles for understanding the increased role for customer support centers in product improvement.
Consumer informedness plays a critical role in determining consumer choice in the presence of information technology deployed by competing firms in the marketplace. This paper develops a new theory of consumer informedness. Using data collected through a series of stated choice experiments in two different research contexts, we examine how consumer characteristics and observed behaviors moderate the influence of price and product informedness on consumer choice. The results indicate that different types of consumer informedness amplify different consumer behaviors in specific consumer segments. In particular, we found that price informedness is more influential among consumers in the commodity segment. They exhibit greater trading down behavior, which represents stronger preferences for choosing the products that provide the best price. In contrast, product informedness is more influential among consumers in the differentiated segment. This group exhibits greater trading out behavior, involving stronger preferences for choosing products that best suit their specific needs. These results suggest that firm information strategy should take into account consumers' characteristics, their past observed behaviors, and the impact of consumer informedness. We also discuss the theoretical contributions of this research and its broader implications for firm-level information strategy.
Decision strategies in dynamic environments do not always succeed in producing desired outcomes, particularly in complex, ill-structured domains. Information systems often capture large amounts of data about such environments. We propose a domain-independent, iterative approach that (a) applies data mining classification techniques to the collected data in order to discover the conditions under which dynamic decision-making strategies produce undesired or suboptimal outcomes and (b) uses this information to improve the decision strategy under these conditions. In this paper, we formally develop this approach and illustrate it by providing detailed examples of its application to a chronic disease care problem in a healthcare management organization, specifically the treatment of patients with type 2 diabetes mellitus. In particular, the proposed iterative approach is used to improve treatment strategies by predicting and eliminating treatment failures, i.e., insufficient or excessive treatment actions, based on information that is available in electronic medical record systems. We also apply the proposed approach to a manufacturing task, resulting in substantial decision strategy improvements, which further demonstrates the generality and flexibility of the proposed approach.
In this research commentary we show that the discipline of information systems (IS) has much that can be learned from the history of the discipline of medicine. We argue that as interest in historical studies of information systems grows, there are important historical lessons to be drawn from disciplines other than IS, with the medical discipline providing fertile ground. Of particular interest are the circumstances that surrounded the practice of the medical craft in the 1800's-circumstances that drove a process of unification and specialization resulting in the modern conceptualization of medical education, research, and practice. In analyzing the history of the field of medicine, with its long-established methods for general practice, specialization, and sub-specialization we find that it serves as an example of a discipline that has dealt effectively with its initial establishment as a scientific discipline, exponential growth of knowledge and ensuing diversity of practice over centuries, and has much to say in regards to a number of discipline-wide debates of IS. Our objective is to isolate the key factors that can be observed from the writings of leading medical historians, and examine those factors from the perspective of the information systems discipline today. Through our analysis we identify the primary factors and structural changes which preceded a modern medical discipline characterized by unification and specialization. We identify these same historic factors within the present-day information systems milieu and discuss the implications of following a unification and specialization strategy for the future of the disciplines of information.
Online product reviews are increasingly important for consumer decisions, yet we still know little about how reviews are generated in the first place. In an effort to gather more reviews, many websites encourage user interactions such as allowing one user to subscribe to another. Do these interactions actually facilitate the generation of product reviews? More importantly, what kind of reviews do such interactions induce? We study these questions using data from one of the largest product review websites where users can subscribe to one another. By applying both panel data and a flexible matching method, we find that as users become more popular, they produce more reviews and more objective reviews; however, their numeric ratings also systematically change and become more negative and more varied. Such trade-off has not been previously documented and has important implications for both product review and other user-generated content websites.
Online retailers are increasingly providing service technologies, such as technology-based and human-based services, to assist customers with their shopping. Despite the prevalence of these service technologies and the scholarly recognition of their importance, surprisingly little empirical research has examined the fundamental differences among them. Consequently, little is known about the factors that may favor the use of one type of service technology over another. In this paper, we propose the Model of Online Service Technologies (MOST) to theorize that the capacity of a service provider to accommodate the variability of customer inputs into the service process is the key difference among various types of service technologies. We posit two types of input variability: Service Provider-Elicited Variability (SPEV), where variability is determined in advance by the service provider; and User-Initiated Variability (UIV), where customers determine variability in the service process. We also theorize about the role of task complexity in changing the effectiveness of service technologies. We then empirically investigate the impact of service technologies that possess different capacities to accommodate input variability on efficiency and personalization, the two competing goals of service adoption. Our empirical approach attempts to capture both the perspective of the vendor who may deploy such technologies, as well as the perspective of customers who might choose among service technology alternatives. Our findings reveal that SPEV technologies (i.e., technologies that can accommodate SPEV) are more efficient, but less personalized, than SPEUIV technologies (i.e., technologies that can accommodate both SPEV and UIV). However, when task complexity is high (vs. low), the superior efficiency of SPEV technologies is less prominent, while both SPEV and SPEUIV technologies have higher personalization. We also find that when given a choice, a majority of customers tend to choose to use both types of technologies. The results of this study further our understanding of the differences in efficiency and personalization experienced by customers when using various types of online service technologies. The results also inform practitioners when and how to implement these technologies in the online shopping environment to improve efficiency and personalization for customers.
Phishing is a major threat to individuals and organizations. Along with billions of dollars lost annually, phishing attacks have led to significant data breaches, loss of corporate secrets, and espionage. Despite the significant threat, potential phishing targets have little theoretical or practical guidance on which phishing tactics are most dangerous and require heightened caution. The current study extends persuasion and motivation theory to postulate why certain influence techniques are especially dangerous when used in phishing attacks. We evaluated our hypotheses using a large field experiment that involved sending phishing messages to more than 2,600 participants. Results indicated a disparity in levels of danger presented by different influence techniques used in phishing attacks. Specifically, participants were less vulnerable to phishing influence techniques that relied on fictitious prior shared experience and were more vulnerable to techniques offering a high level of self-determination. By extending persuasion and motivation theory to explain the relative efficacy of phishers' influence techniques, this work clarifies significant vulnerabilities and lays the foundation for individuals and organizations to combat phishing through awareness and training efforts.
Product fit uncertainty (defined as the degree to which a consumer cannot assess whether a product's attributes match her preference) is proposed to be a major impediment to online markets with costly product returns and lack of consumer satisfaction. We conceptualize the nature of product fit uncertainty as an information problem and theorize its distinct effect on product returns and consumer satisfaction (versus product quality uncertainty), particularly for experience (versus search) goods without product familiarity. To reduce product fit uncertainty, we propose two Internet-enabled systems-website media (visualization systems) and online product forums (collaborative shopping systems)-that are hypothesized to attenuate the effect of product type (experience versus search goods) on product fit uncertainty. Hypotheses that link experience goods to product returns through the mediating role of product fit uncertainty are tested with analyses of a unique data set composed of secondary data matched with primary direct data from numerous consumers who had recently participated in buy-it-now auctions. The results show the distinction between product fit uncertainty and quality uncertainty as two distinct dimensions of product uncertainty and interestingly show that, relative to product quality uncertainty, product fit uncertainty has a significantly stronger effect on product returns. Notably, whereas product quality uncertainty is mainly driven by the experience attributes of a product, product fit uncertainty is mainly driven by both experience attributes and lack of product familiarity. The results also suggest that Internet-enabled systems are differentially used to reduce product (fit and quality) uncertainty. Notably, the use of online product forums is shown to moderate the effect of experience goods on product fit uncertainty, and website media are shown to attenuate the effect of experience goods on product quality uncertainty. The results are robust to econometric specifications and estimation methods. The paper concludes by stressing the importance of reducing the increasingly prevalent information problem of product fit uncertainty in online markets with the aid of Internet-enabled systems.
The widespread use of personal communication technologies (PCTs) for commercial message dissemination necessitates understanding that PCTs might lead to better commercial performance in different situations. Building primarily on apparatgeist and social construction theories, this research proposes that consumer responses to PCT-disseminated commercial messages are jointly influenced by the PCT (i.e., technology) that carries general symbolic meanings about its nature and purpose (its spirit), and the context culture (i.e., the cultural milieu) in which it is used. We began with focus groups' assessments of two commonly utilized PCTs-email and short message service-which revealed their comparative symbolic meanings in terms of intimacy or formality of communication-to be in line with extant literature. Then, in a commercial setting where retailers leverage PCTs to disseminate product discount coupons, we examined the difference between two distinct environments that differed in their context-cultural dimensions (their cultural milieus of social interaction and communication)-i.e., China (an environment of high context-cultural dimension) and Switzerland (an environment of low context-cultural dimension). To do so, we first validated the context-cultural differences through a survey (study 1) and conducted two matching field experiments in the two countries involving more than one thousand consumers (study 2). Results support our propositions, demonstrating favorable commercial performance for SMS use in the high context-cultural environment and for email use in the low context-cultural environment. Follow-up surveys (study 3) corroborated the results and provided deeper insights into how both PCTs' general meanings and pertinent values in the cultural milieus we studied led to consumer responses. Besides presenting empirical evidence to inform the selection of appropriate PCTs for commercial communications, this research contributes to the theoretical development of apparatgeist and social construction theories via its joint examination of technologies and consumers' environments.
This study examines the role of project managers' (PM) practical intelligence (PI) in the performance of software offshore outsourcing projects. Based on the extant literature, we conceptualize PI for PMs as their capability to resolve project related work problems, given their long-range and short-range goals; PI is targeted at resolving unexpected and difficult situations, which often cannot be resolved using established processes and frameworks. We then draw on the information processing literature to argue that software offshore outsourcing projects are prone to severe information constraints that lead to unforeseen critical incidents that must be resolved adequately for the projects to succeed. We posit that PMs can use PI to effectively address and resolve such incidents, and therefore the level of PMs' PI positively affects project performance. We further theorize that project complexity and familiarity contribute to its information constraints and the likelihood of critical incidents in a project, thereby moderating the relationship between PMs' PI and project performance. To evaluate our hypotheses, we analyze longitudinal data collected in an in-depth field study of a leading software vendor organization in India. Our data include project and personnel level archival data on 530 projects completed by 209 PMs. We employ the critical incidents methodology to assess the PI of the PMs who led these projects. Our findings indicate that PMs' PI has a significant and positive impact on project performance. Further, projects with higher complexity or lower familiarity benefit even more from PMs' PI. Our study extends the literatures on project management and outsourcing by conceptualizing and measuring PMs' PI, by theorizing its relationship with project performance, and by positing how that relationship is moderated by project complexity and familiarity. Our study provides unique empirical evidence of the importance of PMs' PI in software offshore outsourcing projects. Given that PMs with high PI are scarce resources, our findings also have practical implications for the optimal resource allocation and training of PMs in software offshore services companies.
Underlying the design of any information system is an explicit or implicit conceptual model of the domain that the system supports. Because of the importance of such models, researchers and practitioners have long focused on how best to construct them. Past research on constructing conceptual models has generally focused on their semantics (their meaning), to discover how to convey meaning more clearly and completely, or their pragmatics (the importance of context in model creation and use), to discover how best to create or use a model in a given situation. We join these literatures by showing how semantics and pragmatics interact. Specifically, we carried out an experiment to examine how the importance of clear semantics in conceptual models-operationalized in terms of ontological clarity-varies depending on the pragmatics of readers' knowledge of the domain shown in the model. Our results show that the benefit of ontological clarity on understanding is concave downward (follows an inverted-U) as a function of readers' prior domain knowledge. The benefit is greatest when readers have moderate knowledge of the domain shown in the model. When readers have high or low domain knowledge, ontological clarity has no apparent benefit. Our study extends the theory of ontological clarity and emphasizes the need to construct conceptual models with readers' knowledge in mind.
We define an economic network as a linked set of entities, where links are created by actual realizations of shared economic outcomes between entities. We analyze the predictive information contained in a specific type of economic network, namely, a product network, where the links between products reflect aggregated information on the preferences of large numbers of individuals to co-purchase pairs of products. The product network therefore reflects a simple smoothed model of demand for related products. Using a data set containing more than 70 million observations of a nonstatic co-purchase network over a period of two years, we predict network entities' future demand by augmenting data on their historical demand with data on the demand for their immediate neighbors, in addition to network properties, specifically, local clustering and PageRank. To our knowledge, this is the first study of a large-scale dynamic network that shows that a product network contains useful distributed information for demand prediction. The economic implications of algorithmically predicting demand for large numbers of products are significant.
We study the problem of optimally choosing the composition of the offer set for firms engaging in web-based personalization. A firm can offer items or links that are targeted for immediate sales based on what is already known about a customer's profile. Alternatively, the firm can offer items directed at learning a customer's preferences. This, in turn, can help the firm make improved recommendations for the remainder of the engagement period with the customer. An important decision problem faced by a profit maximizing firm is what proportion of the offer set should be targeted toward immediate sales and what proportion toward learning the customer's profile. We study the problem as an optimal control model, and characterize the solution. Our findings can help firms decide how to vary the size and composition of the offer set during the course of a customer's engagement period with the firm. The benefits of the proposed approach are illustrated for different patterns of engagement, including the length of the engagement period, uncertainty in the length of the period, and the frequency of the customer's visits to the firm. We also study the scenario where the firm optimizes the size of the offer set during the planning horizon. One of the most important insights of this study is that frequent visits to the firm's website are extremely important for an e-tailing firm even though the customer may not always buy products during these visits.
Abroad motivation for our research is to build manipulation resistant news recommender systems. There are several algorithms that can be used to generate news recommendations, and the strategies for manipulation resistance are likely specific to the algorithm (or class of algorithm) used. In this paper, we will focus on a common method used on the front page by many media sites of recommending the N most popular articles (e.g., New York Times, BBC, CNN, Wall Street Journal all prominently use this). We show that whereas recommendation of the N most read articles is easily susceptible to manipulation, a probabilistic variant is more robust to common manipulation strategies. Furthermore, for the N most popular recommender, probabilistic selection has other desirable properties. Specifically, the (N + 1)th article, which may have just missed making the cut-off, is unduly penalized under common user models. Small differences are easily amplified initially, an observation that can be used by manipulators. Probabilistic selection, on the other hand, creates no such artificial penalty. We use classical results from urn models to derive theoretical results for special cases and study specific properties of the probabilistic recommender.
By software vendors offering, via the cloud, software-as-a-service (SaaS) versions of traditionally on-premises application software, security risks associated with usage become more diversified. This can greatly increase the value associated with the software. In an environment where negative security externalities are present and users make complex consumption and patching decisions, we construct a model that clarifies whether and how SaaS versions should be offered by vendors. We find that the existence of version-specific security externalities is sufficient to warrant a versioned outcome, which has been shown to be suboptimal in the absence of security risks. In high security-loss environments, we find that SaaS should be geared to the middle tier of the consumer market if patching costs and the quality of the SaaS offering are high, and geared to the lower tier otherwise. In the former case, when security risk associated with each version is endogenously determined by consumption choices, strategic interactions between the vendor and consumers may cause a higher tier consumer segment to prefer a lower inherent quality product. Relative to on-premises benchmarks, we find that software diversification leads to lower average security losses for users when patching costs are high. However, when patching costs are low, surprisingly, average security losses can increase as a result of SaaS offerings and lead to lower consumer surplus. We also investigate the vendor's security investment decision and establish that, as the market becomes riskier, the vendor tends to increase investments in an on-premises version and decrease investments in a SaaS version. On the other hand, in low security-loss environments, we find that SaaS is optimally targeted to a lower tier of the consumer market, average security losses decrease, and consumer surplus increases as a result. Security investments increase for both software versions as risk increases in these environments.
Design theories provide explicit prescriptions, such as principles of form and function, for constructing an artifact that is designed to meet a set of defined requirements and solve a problem. Design theory generation is increasing in importance because of the increasing number and diversity of problems that require the participation and proactive involvement of academic researchers to build and test artifact-based solutions. However, we have little understanding of how design theories are generated. Drawing on key contributions by Herbert A. Simon, including the ideas of satisfice and bounded rationality and reviewing a large body of information systems and problem-solving literature, we develop a normative framework for proactive design theorizing based on the notion of heuristic theorizing. Heuristics are rules of thumb that provide a plausible aid in structuring the problem at hand or in searching for a satisficing artifact design. An example of a problem-structuring heuristic is problem decomposition and an example of an artifact design heuristic is analogical design. We define heuristic theorizing as the process of proactively generating design theory for prescriptive purposes from problem-solving experiences and prior theory by constantly iterating between the search for a satisficing problem solution, i.e., heuristic search, and the synthesis of new information that is generated during heuristic search, i.e., heuristic synthesis. Heuristic search involves alternating between structuring the problem at hand and generating new artifact design components, whereas heuristic synthesis involves different ways of thinking, including reflection and learning and forms of reasoning, that complement the use of heuristics for theorizing purposes. We illustrate the effectiveness of our heuristic theorizing framework through a detailed example of a multiyear design science research program in which we proactively generated a design theory for solving problems in the area of intelligent information management and so-called big data in the finance domain. We propose that heuristic theorizing is a useful alternative to established theorizing approaches, i.e., reasoning-based approaches. Heuristic theorizing is particularly relevant for proactive design theorizing, which emphasizes problem solving as being highly intertwined with theorizing, involves a greater variety of ways of thinking than other theorizing approaches, and assumes an engaged relationship between academics and practitioners.
Digital distribution channels raise many new challenges for managers in the media industry. This is particularly true for movie studios where high-value content can be stolen and released through illegitimate digital channels, even prior to the release of the movie in legal channels. In response to this potential threat, movie studios have spent millions of dollars to protect their content from unauthorized distribution throughout the lifecycle of films. They have focused their efforts on the pre-release period under the assumption that pre-release piracy could be particularly harmful for a movie's success. However, surprisingly, there has been little rigorous research to analyze whether, and how much, pre-release movie piracy diminishes legitimate sales. In this paper, we analyze this question using data collected from a unique Internet file-sharing site. We find that, on average, pre-release piracy causes a 19.1% decrease in revenue compared to piracy that occurs post-release. Our study contributes to the growing literature on piracy and digital media consumption by presenting evidence of the impact of Internet-based movie piracy on sales and by analyzing pre-release piracy, a setting that is distinct from much of the existing literature.
Human capital is becoming more critical as the global economy becomes more information intensive and service intensive. Although information systems (IS) researchers have studied some dimensions of human capital, the role of industry-specific human capital has remained understudied. The information technology enabled business process outsourcing (BPO) industry provides an ideal setting to study returns to human capital, because jobs in this industry are standardized and many professionals in this new industry have come from other industries. We build on IS and economics literature to theorize returns to human capital in the BPO industry, and we test the theory using data for over 2,500 BPO professionals engaged in call center work and other nonvoice services (e.g., accounting, finance, human resources, etc.) in India during the 20062008 time period. We find higher returns to industry-specific human capital than to firm-specific and general human capital. We also find that junior-level professionals, whose jobs are relatively more standardized, have higher returns to industry-specific human capital than senior-level professionals. We discuss implications for further research and practice in the global economy where inter-industry transfers and migration of skills are becoming increasingly common.
In this paper, we propose an analytical lens for studying social status production processes across a wide variety of user-generated content (UGC) platforms. Various streams of research, including those focused on social network analysis in social media, online communities, reputation systems, blogs, and multiplayer games, have discussed social status production online in ways that are diverse and incompatible. Drawing on Bourdieu's theory of fields of cultural production, we introduce the notion of an online field and associated sociological concepts to help explain how diverse types of producers and consumers of content jointly generate unique power relations online. We elaborate on what role external resources and status markers may play in shaping social dynamics in online fields. Using this unifying theory we are able to integrate previous research findings and propose an explanation of social processes behind both the similarity across UGC platforms, which all offer multiple ways of pursuing distinction through content production, as well as the differences across such platforms in terms of which distinctions matter. We elaborate what role platform design choices play in shaping which forms of distinction count and how they are pursued as well as implications these have for status gaining strategies. We conclude the paper by suggesting how our theory can be used in future qualitative and quantitative research studies.
Millions of people participate in online social media to exchange and share information. Presumably, such information exchange could improve decision making and provide instrumental benefits to the participants. However, to benefit from the information access provided by online social media, the participant will have to overcome the allure of homophilywhich refers to the propensity to seek interactions with others of similar status (e.g., religion, education, income, occupation) or values (e.g., attitudes, beliefs, and aspirations). This research assesses the extent to which social media participants exhibit homophily (versus heterophily) in a unique contextvirtual investment communities (VICs). We study the propensity of investors in seeking interactions with others with similar sentiments in VICs and identify theoretically important and meaningful conditions under which homophily is attenuated. To address this question, we used a discrete choice model to analyze 682,781 messages on Yahoo! Finance message boards for 29 Dow Jones stocks and assess how investors select a particular thread to respond. Our results revealed that, despite the benefits from heterophily, investors are not immune to the allure of homophily in interactions in VICs. The tendency to exhibit homophily is attenuated by an investor's experience in VICs, the amount of information in the thread, but amplified by stock volatility. The paper discusses important implications for practice.
Online communities are new social structures dependent on modern information technology, and they face equally modern challenges. Although satisfied members regularly consume content, it is considerably harder to coax them to contribute new content and help recruit others because they face unprecedented social comparison and criticism. We propose that engagementa concept only abstractly alluded to in information systems researchis the key to active participation in these unique sociotechnical environments. We constructed and tested a framework that demonstrates what engagement is, where it comes from, and how it powerfully explains both knowledge contribution and word of mouth. Our results show that members primarily contribute to and revisit an online community from a sense of engagement. Nonetheless, word of mouth is partly influenced by prior satisfaction. Therefore, engagement and satisfaction appear to be parallel mediating forces at work in online communities. Both mediators arise from a sense of communal identity and knowledge self-efficacy, but engagement also emerges from validation of self-identity. Nevertheless, we also found signs that the contributions of the most knowledgeable users are not purely from engagement, but also from a competing sense of self-efficacy. Our findings significantly contribute to the area of information systems by highlighting that engagement is a concrete phenomenon on its own, and it can be directly modeled and must be carefully managed.
The objective of this work is to examine various psychological forces underlying the behavior of people's online gambling, an increasingly popular form of entertainment in the gaming industry. Drawing on extant theories, we first developed a model of how cumulative outcomes, recent outcomes, and prior use affect online gambling behavior differently. We empirically tested the model using longitudinal panel data collected over eight months from 22,304 actual users of a gambling website. The results of a multilevel panel data analysis strongly supported our hypotheses. First, consistent with gambling theory, individuals' online gambling was found to increase with any increase in a cumulative net gain or cumulative net loss. Second, as the availability heuristic prescribes, a recent loss reduced online gambling, whereas a recent gain increased it. Third, consistent with the literature on repeated behavior, regular use and extended use moderated the relationship between current and subsequent gambling. Taken together, the present study clarifies how people react differently to immediate and cumulative outcomes and also how regular use and extended use facilitate routine behavior in the context of online gambling. In general, our findings suggest that the three perspectives, i.e., gambling theory, the availability heuristic, and repeated behavior, should be taken into account to understand online gambling, which is in essence a series of risk-taking attempts with the potential of eventually becoming routine behavior. This study is expected to offer valuable insights into other types of online games that could engage people in risking real or cyber money and, at the same time, could be easily enmeshed with everyday life (e.g., fantasy sports, online virtual worlds).
This paper presents and extends Latent Growth Modeling (LGM) as a complementary method for analyzing longitudinal data, modeling the process of change over time, testing time-centric hypotheses, and building longitudinal theories. We first describe the basic tenets of LGM and offer guidelines for applying LGM to Information Systems (IS) research, specifically how to pose research questions that focus on change over time and how to implement LGM models to test time-centric hypotheses. Second and more important, we theoretically extend LGM by proposing a model validation criterion, namely d-separation, to evaluate why and when LGM works and test its fundamental properties and assumptions. Our d-separation criterion does not rely on any distributional assumptions of the data; it is grounded in the fundamental assumption of the theory of conditional independence. Third, we conduct extensive simulations to examine a multitude of factors that affect LGM performance. Finally, as a practical application, we apply LGM to model the relationship between word-of-mouth communication (online product reviews) and book sales over time with longitudinal 26-week data from Amazon. The paper concludes by discussing the implications of LGM for helping IS researchers develop and test longitudinal theories.
We study the problem where a decision maker uses a linear classifier over attribute values (e.g., age, income, etc.) to classify agents into classes (e.g., creditworthy or not). Sometimes the attribute values are altered and/or hidden by agents to obtain a favorable but undeserved classification. Our main goal is to develop methods to thwart agents from hiding or distorting attribute values to obtain a favorable but incorrect classification. Intentionally altered attributes to obtain strategic goals have been studied. In this paper we develop methods that handle strategic hiding (i.e., nondisclosure) and then merge them with methods to thwart strategic distortion in the context of classification.
We use the under-recognized income accounting identity to provide an important theoretical basis for using the Cobb-Douglas production function in IT productivity analyses. Within the income accounting identity we partition capital into non-IT and IT capital and analytically derive an accounting identity (AI)-based Cobb-Douglas form that both nests the three-input Cobb-Douglas and provides additional terms based on wage rates and rates of return to non-IT and IT capital. To empirically confirm the theoretical derivation, we use a specially constructed data set from a subset of the U.S. manufacturing industry that involve elaborate calculations of rates of returna data set that is infeasible to obtain for most productivity studiesto estimate the standard Cobb-Douglas and our AI-based form. We find that estimates from our AI-based form correspond with those of the Cobb-Douglas, and our AI-based form has significantly greater explanatory power. In addition, empirical estimation of both forms is relatively robust to the assumption of intertemporally stable input shares required to derive the AI-based form, although there may be limits. Thus, in the context of future research the Cobb-Douglas form and its application in IT productivity work have a theoretically and empirically supported basis in the accounting identity. A poor fit to data or unexpected coefficient estimates suggests problems with data quality or intertemporally unstable input shares. Our work also shows how some returns to IT that do not show up in output elasticities can be found in total factor productivity (TFP)the novel ways inputs are combined to produce output. The critical insight for future research is that many unobservables that have been considered part of TFP can be manifested in rates of return to IT capital, non-IT capital, and laborrates of return that are separated from TFP in our AI-based form. Finally, finding that the additional rates of return terms partially explain TFP confirms the need for future IT productivity researchers to incorporate time-varying TFP in their models.
