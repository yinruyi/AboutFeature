To obtain the greatest benefit from its information system, an organization must determine which applications will provide the most benefit to organizational performance. This study reviews data collected from 310 manufacturing firms in Israel and 197 such firms in the U.S. For each firm, data were obtained about the benefits derived from using information systems, as perceived by a senior manager, and the organization's operating characteristics. Data were pooled across both countries. No meaningful relationship was found between the benefit a firm derives from its overall information systems application portfolio and its organizational operating characteristics. However, for two individual applications, the benefit derived is linked significantly to the organization's operating characteristics. Thus the model relating benefits from information systems to the organization's operating environment, first demonstrated by data collected in Israel, is confirmed by the data collected in the U.S. The model applies across both countries, even though there may be differences between the two countries, for example, in culture, size of businesses, and relationship with customers and suppliers.
There have been several attempts in the past to assess the impact of information technology on firm performance that have yielded conflicting results. Researchers have been unable to conclude that IT spending by an organization results in increases in key performance indicators. Two major recent studies have attempted to address the issue by putting greater emphasis on the theoretical underpinnings of the solution to the problem, although they chose different theoretical frameworks. The present study extends that work to yield a framework that shows the relationship between firm performance and both IT and corporate investments. The data used to validate the framework exceeds that used in previous analyses in both quality and quantity, thereby permitting appropriate statistical analyses. A large database consisting of over 2,000 observations of 624 firms was constructed, using data provided by the International Data Corporation, Standard & Poor's Compustat, and Moody's. This allowed us to pose the following research questions: (a) Can the relationship between sets of investment measures and firm performance be demonstrated (as opposed to individual measures)? (b) How are IT investments related to a firm's market value, market share, sales, and assets? and (c) Is there a difference in the effect of computer capital and noncomputer capital? ;Seven measures of firm performance were initially incorporated as outputs in the framework, related to sales, assets, and market value. Similarly, seven input measures of IT and corporate investments were initially included. Two output measures and one input were eventually eliminated to formulate a refined framework with strong explanatory power. After careful editing, canonical analyses were performed, resulting in several important findings. Both IT and corporate investments have a strong positive relationship with sales, assets, and equity, but not with net income. Spending on IS staff and staff training is positively correlated with firm performance, even more so than computer capital.
With the enormous investments in Information Technology (IT), the question of payoffs from IT has become increasingly important. Organizations continue to question the benefits from IT investments especially in conjunction with corporate initiatives such as business process reengineering (BPR). Furthermore, the impact of technology on nonfinancial outcomes such as customer satisfaction and quality is gaining interest. ;However, studies examining the IT-performance relationship have been far from conclusive. The difficulty in identifying impacts from technology has been the isolation of benefits of IT from other factors that may also contribute to organizational performance. Furthermore, benefits from technology investments may be realized over an extended period of time. Finally, IT benefits may accrue when they are done in concert with other organizational initiatives such as business process reengineering. This calls for studies that take into account control variables as well as data that span time periods. ;In this study, we examine monthly data collected from eight hospitals over a recent three-year time period. We specify propositions that relate investments in IT to performance, and the combined effect of technology and BPR on performance. We draw upon the literature in health-care management to incorporate appropriate control variables in the analyses. Our results provide support for the IT-performance relationship that is observed after certain time lags. Such a relationship may not be evident in cross-sectional or snapshot data analyses. Also, results indicate support for the impact of technology contingent on BPR practiced by hospitals.
A key issue facing IT researchers and practitioners has been the difficulty in realizing strategic payoff from IT investment. This study, drawn on sociological theories of embeddedness, addresses this key issue, with particular attention to the perspective of EDI network initiator. Cross-case analysis is conducted comparing three initiators of sophisticated EDI networks, who realized different levels of strategic payoffs. Results reveal that the achievement of strategic payoffs is a function of EDI embeddedness, which is defined as how central or peripheral a specific EDI network is to managing interfirm interdependence. In a model of EDI initiator strategic payoff, we argue that EDI embeddedness, which is influenced by existing interfirm relationship, moderates the impact of adopter EDI use on initiator strategic payoff derived from the EDI investment. Specifically, while high embeddedness motivates adopter strategic use, low embeddedness deters such use. The model is validated against three reported cases in the literature.
Information technology (IT) investment decisions have traditionally focused on financial or technological issues. Responding to what appears to be a lack of payoff in IT investments, researchers as well as practitioners recently have suggested that traditional valuation analyses are incomplete and have called for additional work to identify hidden or seldom-considered costs and benefits. The present paper attempts to improve understanding of a chief source of these hidden costs and benefits: those changes in the social subsystem brought about by a new IT. ;Fifty IT decision-makers in a broad variety of industries were interviewed to gain insight into what, when, and how often social subsystem considerations are included in IT investment-decision processes. Data from the interviews show that in practice some of those issues are often minimized, excluded, or put off until the IT is implemented-thus affecting optimality of investment choices and IT payoff. The paper extends existing theory by describing systematic patterns of inclusion and exclusion of these costs and benefits. In addition, a decision aid is provided to help IT executives begin thinking about which social subsystem costs and benefits they should incorporate in various decisions. Suggestions are also made on how data regarding social subsystem costs and benefits might be gathered. By incorporating social subsystem costs and benefits in the IT investment process, decision-makers gain a greater appreciation for hidden costs and benefits, and thus clarify anticipated IT payoff.
The influence of IT investments on organizational performance is revisited. Bounded rationality, organizational controls, and political forces may constrain optimal selection of inputs and appropriate substitution between inputs. For example, firms may not be able to attain an optimal level of IT by substituting IT for labor (for reasons such as pressure from the labor union). Besides estimating a link between IT investments and firm output, this paper presents a study of the link between IT investment levels and the efficiency of processes. ;Nonparametric and parametric techniques were applied to financial data on hospitals collected over a period of eighteen years. We found that cost and technical and allocative efficiencies are statistically significant in the production framework. We also found that hospitals that were characterized by high technical efficiency also used a greater amount of IT capital than firms that exhibited low technical efficiency. A group of hospitals exhibiting high technical efficiency also exhibited low allocative efficiency, indicating that, while processes may have been efficient, resource allocation and budgeting between various categories of capital and labor have not been efficient. Our results also differ from previously published results because we find that IT labor had a negative contribution to productivity and that non-IT capital had a greater contribution to productivity than IT capital.
Despite significant progress in evaluating the productivity payoffs from information technology (IT), the inability of traditional firm-level economic analysis to account fully for the intangible impacts of IT has led to calls for a more inclusive and comprehensive approach to measuring IT business value. In response to this call, we develop a process-oriented model to assess the impacts of IT on critical business activities within the value chain. Our model incorporates corporate goals for IT and management practices as key determinants of realized IT payoffs. Using survey data from 304 business executives worldwide, we found that corporate goals for IT can be classified into one of four types: unfocused, operations focus, market focus, and dual focus. Our analysis confirms that these goals are useful indicators of payoffs from IT in that executives in firms with more focused goals for IT perceive greater payoffs from IT across the value chain. In addition, we found that management practices such as strategic alignment and IT investment evaluation contribute to higher perceived levels of IT business value.
A comprehensive review was conducted of IT value articles in the Communications of the ACM, Information Systems Research, Journal of Management Information Systems, and MIS Quarterly from 1993 to 1998. IT-value measures published during this period were documented, classified, analyzed, and reported. The review of these journal articles revealed a schism between the use of organization-level measures and other measures, Communications of the ACM and Information Systems Research also provided strong evidence of a schism between the use of quantitative and qualitative measures in IT-value research. The Journal of Management Information Systems and MIS Quarterly data provided more limited evidence of this schism as well. These schisms have become more pronounced over time. This may be due partly to an increasing reliance on secondary data set analyses that use only quantitative measures and organization-level analyses. The current research confirmed what many researchers suspect-schisms exist, and may be deepening, in IT-value research.
Information technology (IT) value has been measured at various levels of analysis, yet few authors would contend that the search for value has reached a point where practitioners and theoreticians are satisfied with its outcomes. We present a new perspective that emphasizes the importance of understanding where potential value lies and how best to relate it contextually to the measurement of the firm's realized value across multiple levels of analysis. We develop the idea that complementary assets (especially business process design and human capital) influence the firm's realization of value, using concepts such as locus of value and value conversion contingencies. Expanding beyond earlier process models of IT value, which begin with IT expenditure, our analysis of IT value emphasizes the consideration of potential value for an IT investment both in ex ante project selection, and ex post investment evaluation. We illustrate and validate the application of our framework using IT investments in a variety of business domains.
Insider trading and asymmetric information have been the subject of a significant body of research since the 1960s. Yet little work has been directed at analyzing the impact of different market regulations. Along with difficulties in correctly identifying trades made on inside information, empirical field study methods have not been capable of analyzing the impact of different market regulations. We develop a controllable networked market trading environment that incorporates accurate identification of information possessed by each trader studied and that provides the flexibility necessary to analyze market impacts of different regulatory schemes to limit trading on inside information. We illustrate our methods through a series of controlled induced-value laboratory experiments using human subjects. Subject rewards are performance-based, with cash incentives tied to the outcomes of each market transaction. Experimental results indicate that markets with inside, privately informed traders led to greater trading volumes than markets with traders having access to private information only. In addition to reporting the results of initial sets of the experiments, we use these outcomes to frame future research issues involving the use of IT systems in surveillance and links between trading patterns and insider activity.
Internet traffic pricing is necessary for the vitality of electronic commerce because uncontrolled congestion creates a detrimental effect on quality of the Internet services. Pricing approaches based on negative externality have potential to address the issue of congestion. However, most externality-based pricing approaches require the knowledge of consumers' private demand characteristics, and this requirement is often pointed out as the single most important shortcoming of these mechanisms. The fact that the Internet is a public good presents challenging information extraction problems for network managers in implementing any pricing mechanism. Ideally, we seek an incentive-compatible mechanism-a means of extracting the required information that provides no incentives for users to alter their behavior in an attempt to manipulate the information extraction and price setting processes. We present a solution based on a new nonparametric statistical technique that was developed for this purpose. While the results in this paper are presented in the context of our prior research on pricing, the approach presented here applies to information extraction and implementation in other resource pricing approaches.
The online database industry has annual sales of US$6.5 billion for a product that can be easily appropriated, duplicated, reused, and redistributed. This paper examines how the industry developed dynamic pricing and delivery strategies as a response to technological and market changes, and shows how each strategy specifically compensated for the public good properties of information. Readers will see that specific pricing strategies reduce the incentive to improperly reuse downloaded information. Thus, these strategies can lead to the sustainability and growth of the online database industry. These findings are then extended to the broader context of information delivery via the Internet.
Our existing knowledge of business process reengineering (BPR) is mainly derived from the experiences of private sector organizations, which have fundamentally different characteristics from public organizations. This paper represents a first step in understanding how BPR may be different in public organizations. Drawing on the public administration literature, it examines the differences between public and private organizations and their implications for BPR. Following that, it examines the BPR experience of a large public organization through an intensive case study. The case analysis shows that while there are similarities in the BPR experiences of public and private organizations, there are also notable differences. In this specific case, there were social and political pressures to reengineer, press publicity to promote BPR, a reengineering team comprised mainly of neutral staff, performance benchmarks adapted from the private sector, high-level approval for redesigned processes, and a pilot site implementation to secure further funding. It concludes with lessons learned for implementing BPR in public organizations.
An increasing number of computer applications today use multimedia content such as images, sound, and video over distributed networks of computers. Often, a dispersed set of users, with varying demands, requires ongoing access to this content. Effective placement of the multimedia content at different locations/processors thus becomes essential to ensure acceptable quality of service at a reasonable cost. Achieving this requires the consideration of a set of issues quite different from that required for traditional data distribution. These include (a) scale, both in terms of individual objects and in aggregate, (b) importance of form or appearance, making resolution levels an important, controllable variable, and (c) the temporal dimension, placing stringent demands on response time. These concerns make distribution of multimedia content more than a straightforward extension of traditional distribution approaches. We develop a model and a supporting approach to facilitate effective distribution of multimedia content, focusing on multimedia applications in corporate intranets. The model consists of multiple criteria to reflect different aspects of quality of service and cost which we formulate by leveraging variance in resolution levels to capture trade-offs among these criteria. Since the multiple-criteria allocation model is NP-complete, we propose a decision support approach that generates locally efficient solutions using designer-specified targets and evaluates them using fuzzy-set-based heuristics. The complete model and the approach have been implemented in a prototype to ensure feasibility. We demonstrate use of the prototype for a medical imaging application that illustrates applicability and usefulness of our proposals.
Provided the increasing prevalence of electronic exchange environments-including propriety electronic data interchange (EDI) and some Internet-based EDI (e-commerce) systems-we argue that management's decision-making focus vis-g-vis electronic data interchange (EDI) assumes a tactical disposition rather than a strategic one. We offer that the formulation and execution of tactics may be organized around the general question of how to effectively integrate EDI with internal systems, since this appears to be crucial for obtaining the expected performance advantages. We distinguish between two integration concepts, including the integration between the EDI systems and internal systems (interface integration), and the integration among the internal systems (internal integration). Based on theory and literature, we propose that interface integration is favorably related to performance outcomes, and that interface integration and internal integration are positively related. Using data from the Group Insurance industry, we obtain supportive findings. We conclude that tactical EDI planning should centrally focus on interface integration regardless of how intensively management plans to use EDI. We further conclude that management may want to consider internal integration as a risk factor during EDI implementation, because the ability to establish high interface integration may be inhibited or advanced by low or high internal integration, respectively.
Software preannouncements are often called vaporware (systems or features announced long before a ship date). The challenge confronting software vendors and consumers is understanding the balance between the need to inform the market and the negative consequences of unfulfilled promises. Based on signaling theory from marketing science and research, this study looks at the perceived importance of software preannouncement factors on customers, of unfulfilled promises and unreliable software on a company's reputation, and whether vendor dependence changes these perceptions. Database administrators were surveyed on the perceptions of their database software vendor. Fulfilling commitments to software functionality was more strongly correlated with vendor reputation than on-time delivery of the software. Customer dependence on the vendor was not correlated with perceptions of vendor reputation and credibility. Thus, unlike other industries, it seems that vendors can use software delivery time preannouncements for competitive purposes with minimal concern for the impact on customers, provided the software ultimately delivers the features and functionality promised and is largely free of errors.
Researchers and practitioners alike have taken note of the potential value of an organization's IT infrastructure. TT infrastructure expenditures account for over 58 percent of an organization's IT budget and the percentage is growing at If percent a year. Some even have called IT infrastructure the new competitive weapon and see it as being crucial in developing a sustained competitive advantage. Unique characteristics of an IT infrastructure determine the value of that infrastructure to an organization. One characteristic, IT infrastructure flexibility, has captured the attention of researchers and practitioners. In fact, in most recent surveys featuring issues of most importance to IT executives, the development of a flexible and responsive IT infrastructure and related topics are always at or near the top of the responses. Although the importance of IT infrastructure flexibility has been established, the development of a valid, reliable instrument to measure this construct has not been reported in the literature. The purpose of this paper is to better define the IT infrastructure flexibility construct and to develop a valid, reliable measurement instrument for this construct. In addition to the definition and operationalization of the IT infrastructure flexibility construct, this study explores the instrument's predictive validity with possible antecedent and consequent variables.
Introducing multiple editions of the same software is a relatively recent innovation in the software market. The editions serve to differentiate among different user segments. Introduction of similar low- and high-end products in other markets has been analyzed using segmentation theory. However, the software market is fundamentally different from other product markets in two respects: (1) Software is characterized by negligible marginal production cost, and (2) the option of offering upgrades also exists. We analyze the problem of software introduction using segmentation theory. Our analysis shows that if cannibalization is low, the vendor should introduce the full software as one edition. This result differs from that obtained in prior research, which showed that the seller should introduce two distinct products in such cases. When cannibalization is high, introducing multiple editions simultaneously is optimal under a variety of conditions. The strategy of introducing a highend edition in the first period followed by the low-end edition in the second period is optimal only when the consumers are extremely impatient and the software is large. A significant result of our analysis is that offering upgrades is clearly superior to other strategies only in a very restricted range of parameters. Our analysis also suggests that the vendor's profit is higher when it announces the future strategy. Our theoretical results are supported by evidence from the software market.
This exploratory research investigates the nature of explanation use and factors that influence it during users' interaction with a knowledge-based system (KBS) for decision-making. It draws upon several cognitive perspectives to help understand when, why, and how explanations are used. A verbal protocol analysis was conducted based on a laboratory experiment involving a KBS for financial analysis. Major categories of explanation use were identified, and accounted for with relevant cognitive perspectives. Results show that explanations were requested to deal with comprehension difficulties caused by various types of perceived anomalies in KBS output. There were qualitative and quantitative differences in the nature and extent of explanation use between novices and experienced professionals. These results offer new insights to why explanations are useful and important, what factors influence explanation use, and what information should be included in explanations.
The Internet commerce technologies have significantly reduced sellers' costs of collecting buyer preference information and managing multiple prices. Advanced manufacturing technologies have also improved sellers' manufacturing flexibility. These changes allow an online seller to offer custom products at discriminatory prices. We show that these technologies offer significant advantages to an early adopter who gains market share and profits at the expense of the conventional seller. Not only does the customizing seller charge more for customized products, it also provides standard products but charges more for them than in a conventional market. The benefits of customization disappear when both the competing sellers adopt customization. They now compete not just on prices but also on degree of customization. Consequently, we see that the sellers over-customize to the detriment of their profits. Both the sellers know this when choosing their customization strategies and yet they both end up choosing to customize. A seller that does not customize sees a sharp decrease in profits if its competitor customizes. This is an instance of the Prisoner's Dilemma type of situation in technology adoption. This confirms some key findings in IT productivity and strategic IT investments literature.
This research examines an important issue that has not been specifically addressed in the GSS literature, namely, the effects of GSS and task type on group processes and social interactions. Most previous GSS research has focused on studying group outcomes, whereas the current research studies group interaction processes using two different coding methods. A conceptual research model is derived from the Theory of Time, Interaction and Performance (TIP theory) and a review of the literature on social psychology and GSS. Task and social interactions are studied from an influence perspective. The task type is varied as preference task and intellective task. The support level is varied in a GSS support and non-GSS support (i.e., face-to-face) discussion. Research results report that GSS could significantly influence group interaction processes, and tend to enhance group task interactions but dampen group social interactions.
This paper extends the limits-to-value model of Davern and Kauffman to explore market and process-level factors that impact value flows to firms for their information technology (IT) investments. We characterize IT value in terms of potential value and realized value, and show how each is subject to different effects limits to value-that diminish the benefits of the investment. Our typology identifies barriers specific to the valuation process (industry and organizational barriers), and to the conversion process (resource, knowledge and usage barriers). Following the development of our analytical framework from existing economic and organizational theories of IT valuation and technology adoption and diffusion, we analyze a series of case studies of Internet-based travel reservation systems in electronic commerce (EC). These cases provide evidence in support of the usefulness of the framework, and illustrate the extent of the difficulties faced by organizations in making their investments in EC systems pay off.
Increasingly, organizations collaborate to complement their core competencies. New product development, for example, is often a collaborative process, with customers and suppliers contributing complementary knowledge and skills. This study uses grounded theory to determine how and why information technology facilitates interorganizational learning. Semi-structured interviews in the disk drive industry were coded to develop a conceptual model. An important finding is that organizations collaborate closely through virtual integration. They need interorganizational learning to help them cope with the complexity of new products and the capital intensity in the disk drive industry. However, effective interorganizational collaboration needs trust. The main contribution of the model is in explaining the role of information technology in lower and higher levels of interorganizational learning, cognitive and affective trust, and virtual and humanistic interorganizational collaboration.
Despite the ever increasing importance of information technology (IT) in firms, the extent to which IT management practices are applied creatively to critical tasks varies widely across firms. For over a decade, firms have employed IT steering committees to manage their IT resources. However, the impacts of such committees on the IT management function have not been examined in depth. This paper hypothesized relationships between the level of sophistication of IT steering committees and level of IT sophistication of management within firms, and tested those relationships empirically via a field survey of 213 IT managers in the financial services industry. Results of the study suggest that presence and roles of IT steering committees are significantly related to the level and nature of IT management sophistication within firms. Firms interested in achieving the most benefit from their steering committees should carefully select their preferred roles depending on the type and the level of IT management sophistication desired. The article concludes with discussion and implications for IT researchers and firms' executives.
Organizations are continually influenced by notions of management promoted through broadly held visions of managerial practice. These notions often incorporate models that generally prescribe information technologies as enabling agents for directed organizational change. Such concepts reflect highly cohesive, self-referential systems of beliefs, gears, and rules that structure perspectives about computerization and work in organizations. To achieve breakthrough changes in efficiency, performance, or competitive advantage, organizations must translate these high concepts into a specific model of change appropriate for their organizational context. ;This study shows how abstract and institutional-level conceptions about change are translated into actionable and individual-level realities, and how within this translation the organization's ability to reform can be locked into a constraining process. As consultants bridge institutionalized conceptions of management to discrete organizational activities, participants of change adopt not only the vision of change but also new ways to talk, act, and plan. This adoption may inhibit change by blocking effective discussion and forcing compliance to ill-fitting prescriptions.
Advances in genetic testing and data mining technologies have increased the availability of genetic information to insurance companies and insureds (applicants and policy holders) in the individual health insurance market (IHIM). Regulators, concerned that insurance companies will use this information to discriminate against applicants who have a genetic risk factor but who are still healthy, have implemented genetic privacy legislation in at least Is states. However, in previous work we have demonstrated that such legislation will have unintended consequences it will reduce consumer participation in the market without making those remaining better off. This paper identifies a mechanism, a pure bundling strategy, that insurance companies may implement in this regulatory environment to restore (or maximize) consumer participation in the market and to discourage such discrimination among insureds. This problem is examined through System Dynamics, a simulation-based modeling technique. The results will have significant implications for policy designs implemented by insurance companies, and for legislation implemented by industry regulators, and therefore, for the insurability of the individuals that rely on this market for health insurance coverage.
Previous research shows that groupware improves the exchange of information within groups. However, the additional information does not often lead to better group decisions, probably because individuals fail to process the new information they receive. This study explored the use of groupware processes that required individuals in groups to categorize information, in order to induce group members to better attend to the new information received from others and to integrate it into their own individual decision-making processes. Different groupware processes had different effects on attention to and integration of information, and ultimately on decision quality. Groupware processes that provided categories to organize information and groupware processes that required the receiver of information to categorize information increased attention to information and integration of information, which led to improved individual decision quality.
Although tacit knowledge constitutes the major part of what we know, it is difficult for organizations to fully benefit from this valuable asset. This is because tacit knowledge is inherently elusive, and in order to capture, store, and disseminate it, it is argued that it first has to be made explicit. However, such a process is difficult, and often fails due to three reasons: (1) we are not necessarily aware of our tacit knowledge, (2) on a personal level, we do not need to make it explicit in order to use it, and (3) we may not want to give up a valuable competitive advantage. During an empirical study of recommender system usage, it was noticed how such technology could be used to circumvent these problems, and make tacit knowledge, in the form of our professional interests, available to the organization as a whole. Using Polanyi's theories, it will be shown how intranet documents can be used to make tacit knowledge tangible without becoming explicit, suggesting that tacitly expressed entities are not necessarily beyond the reach of information technology.
Today, a second generation of computer-based reengineering tools employs knowledge systems technology to automate and support key intellectual activities required for effective process redesign. But a central question remains as to the effectiveness of redesign through such knowledge systems. The research described in this paper is focused on testing the effectiveness of knowledge-based, process-redesign systems. We employ one such system, called KOPeR-lite, as a platform for experimentation to assess the relative efficacy of redesigns generated by computer versus those developed by people. In this sense, we conduct a modified Turing Test to compare redesign performance of reengineering analysts with that of the knowledge system. KOPeR-lite performs comparatively well in certain respects, but human subjects outperform the machine in others. The results provide evidence to support claims of redesign efficacy through knowledge systems, and they offer insight into the relative strengths and weaknesses of people and software applications in the reengineering domain. This study further opens up new lines of research and highlights implications for process redesign and practice, including issues associated with leading adoption of knowledge system technology and extension of redesign automation systems such as KOPeR-lite.
This paper proposes a four-tiered framework for classifying and understanding the myriad of information systems development methodologies that have been proposed in the literature. The framework is divided into four levels: paradigms, approaches, methodologies, and techniques. This paper primarily focuses on the two intermediate levels: approaches and methodologies. The principal contribution of the framework is in providing a new kind of deep structure for better understanding the intellectual core of methodologies and approaches and their interrelationships. It achieves this goal by articulating a parsimonious set of foundational features that are shared by subsets of methodologies and approaches. To illustrate how the framework's deep structure provides a better understanding of methodologies' intellectual core, it is applied to eleven examples. The paper also introduces and illustrates a procedure for accommodating and assimilating new information systems development methodologies in addition to the eleven already discussed. This procedure provides the framework with the necessary flexibility for handling the continuing proliferation of new methodologies.
Very large-scale conversation (VLSC) involves the exchange of thousands of electronic mail (e-mail) messages among hundreds or thousands of people. Usenet newsgroups are good examples (but not the only examples) of online sites where VLSCs take place. To facilitate understanding of the social and semantic structure of VLSCs, two tools from the social sciences-social networks and semantic networks-have been extended for the purposes of interface design. As interface devices, social and semantic networks need to be flexible, layered representations that are useful as a means for summarizing, exploring, and cross-indexing the large volumes of messages that constitute the archives of VLSCs. This paper discusses the design criteria necessary for transforming these social scientific representations into interface devices. The discussion is illustrated with the description of the Conversation Map system, an implemented system for browsing and navigating VLSCs.
Organizations require ways to efficiently distribute information such as news releases, seminar announcements, and memos. While the machinery for information storage, manipulation, and retrieval exists, research dealing directly with its distribution in an organizational context is scarce. In this paper, we address this need by first examining the pros and cons of the conventional mailing lists approach and then proposing new workflow mechanisms that improve the efficiency and effectiveness of information distribution through e-mail. The proposed approach is relevant to other information distribution approaches beyond e-mail. The main contributions of this study include: (1) offering a workflow perspective on organizational information distribution; (2) analysis of workflows in two new information distribution methods based on dynamic mailing lists and profile matching, respectively; and (3) proposing a new way of matching supply and demand of information that extends existing information filtering algorithms.
This paper presents a descriptive evaluation of 54 case and field studies from 79 published papers spanning two decades of group support systems (GSS) research. It organizes the methodology and results of these Studies into a four-factor framework consisting of contextual factors, intervening factors, adaptation factors, and outcome factors. The tables will provide the GSS researcher with a summary of what has been studied. The appendices provide a detailed description of the methodology and the results.
The management of Information Technology (IT) and Information Systems (IS) is considered a complex exercise by academics and practitioners alike. The reason for this is that there are ubiquitous portfolios of tangible and intangible benefits that are offered to an organization following the adoption of IT/IS that, in turn, all need managing to ensure realization. Organizations also have to take into account the direct and often larger indirect costs that are typically associated with IT/IS deployments. To provide managers with a critical insight into the management of new technology, this paper uses a case study research strategy to examine the technology management experiences of a leading U.K. manufacturing organization during its adoption of a vendor-supplied Manufacturing Resource Planning (MRPII) information system. Following the lack of attention given to human and organizational technology management factors while implementing MRPII, the vendor-based information system was later abandoned and deemed a failure. In addressing those technology management factors that were later identified as important, it was found that key employees were able to overcome a number of organizational barriers and develop and implement a bespoke MRPII system that significantly improved the organization's competitive position. Technology management taxonomies that contributed to the failure and later successful implementation of MRPII are identified and discussed. The organization's experiences in solving the problems associated with the implementation of their IS offers a learning opportunity for those companies that are seeking a competitive advantage through technology management.
Advocates of software risk management claim that by identifying and analyzing threats to success (i.e., risks) action can be taken to reduce the chance of failure of a project. The first step in the risk management process is to identify the risk itself, so that appropriate countermeasures can be taken. One problem in this task, however, is that no validated lists are available to help the project manager understand the nature and types of risks typically faced in a software project. This paper represents a first step toward alleviating this problem by developing an authoritative list of common risk factors. We deploy a rigorous data collection method called a ranking-type Delphi survey to produce a rank-order list of risk factors. This data collection method is designed to elicit and organize opinions of a panel of experts through iterative, controlled feedback. Three simultaneous surveys were conducted in three different settings: Hong Kong, Finland, and the United States. This was done to broaden our view of the types of risks, rather than relying on the view of a single culture-an aspect that has been ignored in past risk management research. In forming the three panels, we recruited experienced project managers in each country. The paper presents the obtained risk factor list, compares it with other published risk factor lists for completeness and variation, and analyzes common features and differences in risk factor rankings in the three countries. We conclude by discussing implications of our findings for both research and improving risk management practice.
Recently, despite huge incentives and subsequent increases in investment in customer relationship management technology, many firms have not been able to increase their customer satisfaction index ratings. The purpose of this paper is to gauge whether IT management practices differ among firms where IT has a major role in transforming marketing, operations, or both, which give the firms advantage by affecting their customer service. Several research hypotheses are tested using data obtained from a survey of 213 IT-leaders in the financial services industry. The results clearly indicate that the IT-leader firms have a higher level of IT management sophistication and a higher role for their IT-leaders compared to IT-enabled customer focus, IT-enabled operations focus, and IT-laggard firms. This paper concludes with the implications for both researchers and practitioners.
Neural networks have been shown to be a promising tool for forecasting financial time series, Several design factors significantly impact the accuracy of neural network forecasts. These factors include selection of input variables, architecture of the network, and quantity of training data. The questions of input variable selection and system architecture design have been widely researched, but the corresponding question of how much information to use in producing high-quality neural network models has not been adequately addressed. In this paper, the effects of different sizes of training sample sets on forecasting currency exchange rates are examined. It is shown that those neural networks-given an appropriate amount of historical knowledge-can forecast future currency exchange rates with 60 percent accuracy, while those neural networks trained on a larger training set have a worse forecasting performance. In addition to higher-quality forecasts, the reduced training set sizes reduce development cost and time.
This research explores the concept of the information technology (IT) competence of business managers, defined as the set of IT-related explicit and tacit knowledge that a business manager possesses that enables him or her to exhibit IT leadership in his or her area of business. A manager's knowledge of technologies, applications, systems development, and management of IT form his or her explicit IT knowledge. This domain further extends to include knowing who knows what, which enables the manager to leverage the knowledge of others. Tacit IT knowledge is conceptualized as a combination of experience and cognition. Experience relates to personal computing, IT projects, and overall management of IT. Cognition refers to two mental models: the manager's process view and his or her vision for the role of IT. The outcomes expected from IT-competent business managers are chiefly two behaviors: an increased willingness to form partnerships with IT people and an increased propensity to lead and participate in IT projects.
Eliciting requirements from users and other stakeholders is of central importance to information systems development. Despite this importance, surprisingly little research has measured the effectiveness of various requirements elicitation techniques, The present research first discusses theory relevant to information requirements determination in general and elicitation in particular. We then develop a model of the requirements elicitation process. This model and its underlying theory were then used to construct a new requirements elicitation prompting technique. To provide a context for testing the relative effectiveness of the new technique, two other questioning methodologies were also operationalized as prompting techniques: (1) the interrogatories technique, which involves asking who, what, when where, how, and why questions; and (2) a semantic questioning scheme, which involves asking questions based on a theoretical model of knowledge structures, To measure the usefulness of the prompting techniques in eliciting requirements, a set of generic requirements categories was adapted from previous research to capture requirements evoked by users. The effectiveness of the three methods in eliciting requirements for a software application was then tested in an experiment with users. Results showed that the new prompting technique elicited a greater quantity of requirements from users than did the other two techniques. Implications of the findings for research and systems analysis practice are discussed.
Despite the important role of vendors in the IT procurement process, very few studies have considered vendor characteristics and their effects on the decision outcome of IT managers. In this paper we present a discrete choice model to examine the effects of vendor characteristics on the purchase decisions of IT managers. Our intent is to empirically assess the effects of product variety, brand name, average price, and network externalities in the selection of computer vendors. To ensure that the effects are not technology-dependent, we deliberately use long time series data to calibrate the model. Annual data at the vendor level from 1965 to 1993 is used to infer the choice criteria of IT managers in three computer categories: mainframe, minis, and small systems. Our empirical findings indicate that a broader product line and a strong brand can effectively enhance the choice probability of a vendor. Implications of these findings and possible extensions are also discussed.
Information technology (IT) changes rapidly, seriously challenging IT management. In response, many organizations create a formal group of IT professionals to evaluate emerging IT so they can better cope with its change. A survey based on structured interviews was mailed to a nationwide sample of 1,000 IT organizations. Two hundred forty-six respondents provided data to identify categories of coping mechanisms to handle changing IT. Five categories emerged: Education and Training, Internal Procedures, Vendor Support, Consultant Support, and Endurance. Organizations apply Education and Training more extensively than the others. Thus the research contributes to understanding the means by which organizations cope with rapid IT change. The research also found that organizations with a group dedicated to investigating emerging IT cope more extensively, but not more successfully, than do those without one. Thus the research contributes not only by providing an understanding of how organizations cope with rapid IT change, but also by suggesting the need to achieve more from the group charged with emerging IT.
The pervasive role of telecommunications in contemporary commerce is well documented, and has dramatically increased the demand for services. Across the world, countries are seeking to improve telecommunications infrastructure and benefit from anticipated increases in economic activity, and a causal relation between the two is often tacitly assumed. This paper analyzes aggregate data at the national level to see if there is any empirical evidence that supports this assumption. We apply the well established Granger test For causality using time series data for levels of telecommunications infrastructure and economic activity from thirty countries. We find that the evidence for causality from levels of telecommunications infrastructure to economic activity is stronger than that for causality in the opposite direction. Moreover, this pattern appears to hold for both industrialized and developing economies, even though the former has strong service sectors that are heavily dependent on telecommunications. These findings provide additional insights into the complex relationship between telecommunications and economic activity. Some potential policy implications are also discussed. Granger causality tests have not seen much application in the IS literature, and we mention some IS research issues that may benefit from such analysis.
Drawing both from the IS literature on software project risk management and the contingency research in Organization Theory literature, the present study develops an integrative contingency model of software project risk management. Adopting a profile deviation perspective of fit, the outcome of a software development project (Performance) is hypothesized to be influenced by the fit between the project's risk (Risk Exposure) and how project risk is managed (Risk Management Profile). The research model was tested with longitudinal data obtained from project leaders and key users of 75 software projects. The results support the contingency model proposed and suggest that in order to increase project performance a project's risk management profile needs to vary according to the project's risk exposure. Specifically, high-risk projects were found to call for high information processing capacity approaches in their management. However, the most appropriate management approach was found td depend on the performance criterion used. When meeting project budgets was the performance criterion, successful high-risk projects had high levels of internal integration, as well as high levels of formal planning. When system quality was the performance criterion, successful high-risk projects had high levels of user participation.
Beliefs of organizational ownership relate to whether information and knowledge created by an individual knowledge worker are believed to be owned by the organization. Beliefs about property rights affect information and knowledge sharing. This study explored factors that help determine an individual's beliefs about the organizational ownership of information and expertise that he or she has created. Four different situations of organizational ownership (information vs. expertise/internal vs, external sharing) were considered. The study found that a belief in self-ownership was positively associated with organizational ownership-suggesting a collaborative type of ownership situation for both information and expertise and for both internal (intraorganizational) and external (interorganizational) sharing situations. Organizational culture and the type of employee also influenced the beliefs of organizational ownership in all four scenarios. We conclude the paper with implications for practice and future research.
A hallmark of the new economy is the ability of organizations to realize economic value from their collection of knowledge assets as well as their assets of information, production distribution, and affiliation. Despite the competitive necessity of becoming a knowledge-based organization, senior managers have found it difficult to transform their firms through programs of knowledge management. This is particularly true if their organizations have long histories of process and a tradition of business success. This research examines the issue of effective knowledge management from the perspective of organizational capabilities. This perspective suggests that a knowledge infrastructure consisting of technology, structure, and culture along with a knowledge process architecture of acquisition, conversion, application, and protection are essential organizational capabilities or preconditions for effective knowledge management. Through analysis of surveys collected from over 300 senior executives, this research empirically models and uncovers key aspects of these dimensions. The results provide a basis for understanding the competitive predisposition of a firm as it enters a program of knowledge management.
The core capabilities of an organization include critical skills of employees, management systems, and norms and values. Core capabilities may be transferred formally and explicitly. However, much knowledge, particularly knowledge with rich tacit dimensions, is transferred informally through processes of socialization and internalization. We focus on two transfer mechanisms - mentoring and storytelling - that can leverage the knowledge of an organization, particularly its tacit knowledge, to build core capabilities. We draw on relevant research in learning and cognitive psychology to clarify the conditions under which mentoring and storytelling can be most effective as carriers of knowledge. Finally, we present recommendations for specific managerial practices that follow from our analysis.
Knowledge is now recognized as an important basis for competitive advantage and many firms are beginning to establish initiatives to leverage and manage organizational knowledge. These include efforts to codify knowledge in repositories as well as efforts to link individuals using information technologies to overcome geographic and temporal barriers to accessing knowledge and expertise. We suggest that Knowledge Management (KM) efforts, to be successful, need to be sensitive to features of the context of generation, location, and application of knowledge. To this end, we highlight the situated organizational learning perspective that views knowledge as embedded in individuals, in connections between individuals, and in artifacts as a useful lens to examine phenomena related to the establishment of KM initiatives. In an ethnographic case study of an effort to change knowledge-work processes in a market research firm, we apply the situated knowledge perspective to highlight the factors responsible for the limited success of the initiative in the firm. This study suggests that a consideration of the situated knowledge web and the alignment of the initiatives with the features of the knowledge web are central to success in knowledge management efforts in firms.
This paper draws on primary and secondary data to propose a taxonomy of strategies, or schools, for knowledge management. The primary purpose of this framework is to guide executives on choices to initiate knowledge management projects according to goals, organizational character, and technological, behavioral, or economic biases. It may also be useful to teachers in demonstrating the scope of knowledge management and to researchers in generating propositions for further study.
Prior research examines several knowledge management processes, considering each as universally appropriate. Instead, we propose that the context influences the suitability of a knowledge management process. We develop a contingency framework, including two attributes of the organizational subunit's tasks: process or content orientation, and focused or broad domain, and links knowledge management processes to them: internalization for focused, process-oriented tasks; externalization for focused, content-oriented tasks; combination for broad, content-oriented tasks; and socialization for broad, process-oriented tasks. The empirical research was done at the Kennedy Space Center (KSC), based on several interviews and survey data from 159 individuals across 8 subunits. The results supported the contingency framework. All the knowledge management processes except externalization had a positive impact in the expected cell. At the overall level, combination and externalization, but not internalization and socialization, affect knowledge satisfaction. Some implications for practice and research are identified.
This paper represents a step toward a theory of knowledge reusability, with emphasis on knowledge management systems and repositories, often called organizational memory systems. Synthesis of evidence from a wide variety of sources suggests four distinct types of knowledge reuse situations according to the knowledge reuser and the purpose of knowledge reuse. The types involve shared work producers, who produce knowledge they later reuse; shared work practitioners, who reuse each other's knowledge contributions; expertise-seeking novices; and secondary knowledge miners. Each type of knowledge reuser has different requirements for knowledge repositories. Owing to how repositories are created, reusers' requirements often remain unmet. Repositories often require considerable rework to be useful for new reusers, but knowledge producers rarely have the resources and incentives to do a good job of repurposing knowledge. Solutions include careful use of incentives and human and technical intermediaries.
We trace in pragmatic terms some of what we know about knowledge, information technology, knowledge management practice and research, and provide two complementary frameworks that highlight potential opportunities for building a research agenda in this area. The papers in this special issue are then discussed.
The problem of runaway information systems (IS) projects can be exacerbated by the reluctance of organizational members to transmit negative information concerning a project and its status. Drawing upon relevant bodies of literature, this paper presents a model of the reluctance to report negative project news and develops hypotheses to be tested. An experiment, which was designed to test these hypotheses for both internal and external reporting alternatives, is then described. Two factors are manipulated: (1) the level of impact associated with project failure should an individual fail to report negative information, and (2) the level of observed behavioral wrongdoing associated with the project. The results explain a significant portion of the variance in the reluctance to report negative information and suggest that there are some differences in internal and external reporting behavior. Implications for research and practice are discussed.
Several information goods, such as movie distribution rights or newspapers, are sold either at spot prices, or through forward subscription buying. Our paper considers a firm that offers an information good through spot buying, forward buying at a reduced price, or a combination of the two. The time lag between forward buying and spot buying brings about an uncertainty in a consumer's reservation price for the good at the time of advance purchase. We propose a consumer decision-making model that captures this fundamental feature and provides interesting insights into the key elements of consumer behavior. We establish that a consumer offered the choice between forward buying and waiting to (possibly) buy the good on spot faces the tradeoff between a lower unit price and the value of updated preferences. We also establish that consumers preferring forward buying have a relatively high expectation and low uncertainty in their reservation prices for the good at the time of advance purchase, while those preferring spot buying have a relatively low expectation and high uncertainty in their reservation prices for the good. ;We apply the model to formulate and analyze the firm's problem when it is either a price taker or a price setter. When the firm is a price taker, the choice is whether to offer the good for only forward buying, only spot buying, or a combination of the two. With an example, we show that when both the spot price and the discount on forward buying are moderate in values, the seller chooses the mixed strategy of offering both forward and spot buying simultaneously. When the firm is a price setter, the goal is to choose the offering(s) and the price level(s). With the example, we show how firms selling information goods can increase their revenues by using a mixed offering strategy with both spot and forward offerings. This strategy lends itself to second-degree price discrimination by the seller when there are groups of customers potentially heterogeneous in terms of the distribution of their reservation prices. Our work takes significant importance in the context of information goods, which are becoming increasingly prominent and are being delivered on the Web through the mechanisms of forward and spot buying.
When producers of goods (or services) are confronted by a situation in which their offerings no longer perfectly match consumer preferences, they must determine the extent to which the advertised features of the product reflect the product's actual attributes. We find that the two important determinants of sellers' advertising strategy are the Repeg Cost Ratio, and the Repeat Sales Coefficient. The interplay of these two factors gives rise to four possible strategic scenarios. We show that sellers' strategy is clearly explainable in three out of these four scenarios. In the ambiguous fourth scenario, we show that sellers' strategy for information production goods will differ considerably from information consumption goods based on product complexity and cost of product return (borne by the buyer). Finally, we demonstrate that markets are often characterized by self-reinforcing limits on the extent of opportunistic advertising by sellers.
Dynamic pricing mechanisms occur on the Internet when buyers and sellers negotiate the final transaction price for the exchange of goods or services. These mechanisms are used in online auctions (e.g., eBay.com, uBid.com) and name-your-own-price (Priceline.com) formats, for example. The current research studies the dynamics of one instance of dynamic pricing-group-buying discounts-used by MobShop.com., whose products' selling prices drop as more buyers place their orders. We collect and analyze changes in the number of orders for MobShop-listed products over various periods of time, using an econometric model that reflects our understanding of bidder behavior in the presence of dynamic pricing and different levels of bidder participation. We find that the number of existing orders has a significant positive effect on new orders placed during each three-hour period, indicating the presence of a positive participation externality effect. We also find evidence for expectations of falling prices, a price drop effect. This occurs when the number of orders approaches the next price drop level and the price level for transacting will fall in the near future. The results also reveal a significant ending effect, as more orders were placed during the last three-hour period of the auction cycles. We also assess the efficacy of group-buying business models to shed light on the recent failures of many group-buying Web sites.
This study examines the adoption of electronic bill presentment and payment (EBPP) technology. EBPP continues to grow and will become a multibillion dollar e-commerce industry. The technology adoption configuration in this context is quite interesting because it involves four stakeholders: billers, bill consolidators, banks, and consumers. Banks and bill consolidators compete to act as an intermediary between billers and consumers. Network externalities play a significant role: the more billers that adopt the technology, the more consumers are willing to use the services. Our analysis is based on the welfare economics concept of finding the socially optimum adoption configuration and the resulting adoption pattern in a market with sponsored technologies. The results show that due to network externalities, billers are more likely to adopt the existing technology early, though the next technology might be superior to the current one. When the higher costs of early adoption are taken into account, the model shows that billers are more willing to wait, ceteris paribus. Our results also show that anticipation of a new and better, but compatible, technology might cause billers to wait, depending on what benefits they expect by adopting early, and how much cost they anticipate to incur upgrading their technology later.
Design of a retail banking distribution strategy is an important issue in that industry. This paper shows the effect of new electronic distribution technologies such as PC banking on the choice of a bank's distribution strategy. We present a competitive model of distribution strategy choice, including heterogeneous consumers and banks, that allows a rich variety of customer preference and technology cost parameters. Sensitivity analysis shows how several parameters affect the competitive outcome. This analysis suggests that changing consumer behavior and attitudes, instead of banks' cost structure with new technologies significantly affects the bank's distribution strategy choice. If the segment of consumers that prefers PC banking remains small relative to the segment that prefers branches, then there will still be a market for specialized branch banks. Branch banking without PC banking services will be a viable strategy until the segment that prefers PC banking grows larger (amounting to about 40 percent of all transactions). Banks offering both branch and PC banking services can prevent successful and profitable entry by virtual banks (Internet banks offering only PC banking services) as long as the segment of customers that prefer PC banking remains relatively small (less than two-thirds of all transactions). Beyond this fraction, virtual banks will be profitable. This analysis suggests that it may be a long time (if ever) before virtual banks turn a profit.
For over a decade, empirical studies in the information technology (IT) value literature have examined the impact of technology investments on various measures of performance. However, the results of these studies, especially those examining the contribution of IT to productivity, have been mixed. One reason for these mixed empirical findings may be that these studies have not effectively accounted for the impact of technology investments that increase production efficiency and improve product quality on firm productivity. In particular, it is commonly assumed that such investments should lead to gains in both profits and productivity. However, using a closed-form analytical model we challenge this underlying assumption and demonstrate that investments in certain efficiency-enhancing technologies may be expected to decrease the productivity of profit-maximizing firms. More specifically, we demonstrate that investments in technologies that reduce the firm's fixed overhead costs do not affect the firm's product quality and pricing decisions but do increase profits and improve productivity. In addition, we demonstrate that investments in technologies that reduce the variable costs of designing, developing, and manufacturing a product encourage the firm to improve product quality and to charge a higher price. Although this adjustment helps the firm to capture higher profits, we show that it will also increase total production costs and will, under a range of conditions, decrease firm productivity. Finally, we show that the direction of firm productivity following such investments depends upon the relationship between the fixed costs of the firm and the size of the market.
Second-degree price discrimination, that is, vertical differentiation, is widely practiced by firms selling physical goods to consumers with heterogeneous valuations. This strategy leads to market segmentation and has been shown to be optimal by many researchers. On the other hand, researchers have also demonstrated, under certain restrictive conditions, that vertical differentiation may not be optimal for information goods. We analyze vertical differentiation for a monopolist, continuing the practice of modeling consumer valuation as a linear function of product quality and consumer type but generalizing assumptions about marginal costs and consumer distributions. We show that the firm's optimal product line depends on the benefit-to-cost ratio of qualities in the choice vector. We find that a vertical differentiation strategy is not optimal when the highest quality product has the best benefit-to-cost ratio. Many information goods satisfy this property.
An essential feature of active Decision Support Systems (DSS) is the ability to take the initiative in performing decision-related tasks. One possibility for providing active high cognitive level decision support is through facilitating alternative generation in DSS. The method proposed in this work enables the generation of several diverse alternatives in a single run. The method relies on the principles of effective problem-solving/decision-making and facilitates divergent processes, the separation of alternative generation from evaluation, as well as the diminishing of human cognitive biases. A hybrid DSS based on genetic algorithms (GA) and fuzzy sets is used to operationalize the approach. The paper outlines the design requirements for alternative generation in DSS and discusses the inadequacies of the what-if simulation and traditional optimization methods in light of these requirements. The paper further elaborates on the appropriateness of GA as a tool for alternative generation in DSS for solving complex ill-structured problems. The method is illustrated using marketing mix problem in a simulated business environment. The results suggest that the GA-based alternative generation leads to promising diverse alternatives. An active DSS incorporating the proposed method reduces the time-consuming manual search for promising alternatives and provides a higher degree of man-machine collaboration.
The main purpose of this study is to determine the mix of organizational and technical skills demanded of Webmasters, and the degree to which those skills influence job performance. The study is composed of two parts. First, a job-content analysis of 800 Webmaster positions is conducted in order to determine the mix of skills demanded of Webmasters by employers. Second, a survey of 232 Webmasters is conducted to test the relationships between those skills and job performance. The job-content analysis suggested that employers seek technical skills over organizational skills, and, in contrast, the survey results showed that Webmasters regard organizational skills as more important in performing their jobs. Structured equation modeling on the survey data showed that deficiency in both technical and organizational skills leads to lower job performance. Moreover, the effect of organizational skill deficiencies on job performance was found to be larger than that of technical skill deficiencies. For researchers, the establishment of an empirical link between job skills and job performance opens the field to further research in the skills of information systems personnel. For employers, the results suggest more attention should be paid to attracting organizational skills when recruiting information systems personnel, such as Webmasters.
Despite the advances in group decision support system (GDSS) research, few GDSS studies concentrate on problem-modeling tools to support decisions that cross boundaries of functional areas within the business. These decisions have a substantial effect on the profitability of the firm and account for much time and effort of senior management. This research investigates the effect of problem structuring and modeling with a GDSS on coordinated decision-making of managers in a group faced with a mixed-motive production-planning task. In a laboratory experiment, a GDSS with features supporting problem modeling is contrasted with a GDSS without such features. The results indicate that the groups using a GDSS with a problem-modeling tool outperformed the groups using a GDSS without a problem-modeling tool, but they were less efficient with respect to the time and number of messages it takes the group to converge to a final solution. User confidence in the solution did not differ between the two groups. The results of this study indicate that the problem modeling feature of a GDSS significantly influences group decision process and outcomes.
Before software project managers can enhance productivity and satisfaction of the software project team member, the effect of task characteristics, goal orientations, and coordination strategies on design and coding-task outcomes must be understood. A research model, which suggests that task interdependence, goal conflict, and coordination strategies significantly affect productivity and satisfaction associated with software design and coding activities, is presented. Issues such as contingency/design misfit, conflicting contingencies, and the extent of deviation to theoretically prescribed coordination mechanisms applied to contingencies are used to make predictions on productivity and process satisfaction. A 2x2x2 factorial experiment was utilized. Overall, projects characterized by low task interdependence exhibited greater productivity than projects with high task interdependence. Also, in general, organic coordination was more productive than mechanistic coordination. There was also a significant interaction between task interdependence and coordination strategy. Low goal conflict and organic coordination each lead to greater process satisfaction. Productivity results for the goal conflict manipulation was opposite to the hypothesized direction. Unconflicted contingencies addressed with consistent coordination and partially conflicted contingencies, regardless of the coordination used, exhibited significant gains in productivity. In comparison, unconflicted contingencies with inconsistent coordination and conflicted contingencies, regardless of the coordination applied, resulted in lower productivity. This suggests that there are instances where multiple contingencies, which warrant the use of different coordination strategies, can be adequately addressed with a specific coordination strategy.
This paper presents a meta-analysis that investigates five moderators (task, tool, the type of group, the size of the group, and facilitation) and their influences on the overall effects of group support systems (GSS). Results show that process satisfaction is higher for idea-generation tasks than for decision-making tasks. The GSS tool (that is, the use of level 1 or level 2 GSS) influences decision quality. Level 1 tools support the exchange of information, whereas, level 2 tools are designed to aid in decision-making. Decision quality is higher when using level 2 tools, however, there is no difference in the number of ideas generated when using level 1 or level 2 tools. Decision quality is lower for virtual teams, but there is no difference in the number of ideas generated between virtual teams and face-to-face teams using GSS. Group size is an important moderator when measuring decision time and satisfaction with process. The former is shorter for larger groups, and the latter is higher for larger groups. Process facilitation leads to higher decision quality and higher satisfaction with the process. These results illustrate the importance of examining the moderators of GSS use and the viability of conducting a meta-analysis to investigate a large body of research with seemingly conflicting or equivocal results.
Object-oriented (OO) technology was expected to rapidly replace traditional functional software technology due to its productivity and quality improvement potential in software development. Still, OO technology is not yet fully understood and utilized by information systems (IS) organizations. Despite the growing interest and attention of the IS researchers and practitioners, empirical research on the assimilation process of OO technology has been limited. The present study assesses the current status of OO technology assimilation in IS organizations and identifies the factors influencing such assimilation from a software process innovation perspective. ;Innovation attributes and organizational characteristics were tested as determinants of the organizational OO technology assimilation based on a survey of 220 organizations. Logistic regression analysis was used to assess the relationships of innovation and organizational variables with the level of OO technology assimilation. The findings indicate that, among the innovation characteristics, perceived complexity and perceived maturity of technology have been found to have positive relationships with organizational assimilation of OO technology. Among the organizational characteristics, intensity of new technology education was positively related to organizational assimilation of OO technology, and satisfaction with existing technology was negatively related to organizational assimilation of OO technology.
The explosion in Internet usage and huge government funding initiatives in digital libraries have drawn attention to research on digital libraries. Whereas the traditional focus of digital library research has been on the technological development. there is now a call for user-focused research. Although millions of dollars have been spent on building usable systems, research on digital libraries has shown that potential users may not use the systems in spite of their availability. There is a need for research to identify the factors that determine users' adoption of digital libraries. Using the technology acceptance model (TAM) as a theoretical framework, this study investigates the effect of a set of individual differences (computer self-efficacy and knowledge of search domain) and system characteristics (relevance, terminology, and screen design) on intention to use digital libraries. Based on a sample of 585 users of a university's award-winning digital library, the results strongly support the utilization of TAM in predicting users' intention to adopt digital libraries, and demonstrate the effects of critical external variables on behavior intention through perceived ease of use and perceived usefulness. All of the individual differences and system characteristics have significant effects on perceived ease of use of digital libraries. In addition, relevance has the strongest effect on perceived usefulness of digital libraries.
With the move to distributed systems and an increasing emphasis on the use of object-orientation for new system design, effective distribution of object-oriented applications is becoming an important concern for designers. Early research in this area has focused on object-clustering schemes for shared memory configurations that have limited value to business applications, which must be distributed over loosely coupled networks. These applications also exhibit the properties of simpler structural relationships and a large number of instances, demanding approaches closer to fragmentation and allocation instead of clustering. This paper develops an approach to distribution of object-oriented applications over geographically dispersed sites in loosely coupled networks-taking account of concerns such as encapsulation, inheritance, messaging, and implicit joins. The approach consists of two phases. First, we develop a scheme for generating class fragments, which ensures that encapsulation is not violated and inheritance is not stretched across sites. Second, considering the message-intensive operation of object-oriented systems, we devise models for allocation of class fragments to sites that minimize inter-site traffic. A nonarbitrary procedure to compile traffic volume estimates exploiting the notion of implicit joins in object-oriented applications provides the natural linkage between the two phases. A research prototype was implemented to establish feasibility of the proposals. We demonstrate usefulness of the approach by its application for distribution of a real-world information system.
Today's workflow management systems offer work items to workers using rather primitive mechanisms. Although most workflow systems support a role-based distribution of work, they have problems dealing with unavailability of workers as a result of vacation or illness, overloading, context-dependent suitability, deadlines, and delegation. As a result, the work is offered to too few, too many, or even the wrong set of workers. Current practice is to offer a work item to one person, thus causing problems when the person is not present or too busy, or to offer it to a set of people sharing a given role, thus not incorporating the qualifications and preferences of people. Literature on work distribution is typically driven by considerations related to authorizations and permissions. However, workflow processes are operational processes where there is a highly dynamic trade-off between quality and performance. For example, an approaching deadline and an overloaded specialist may be the trigger to offer work items to less qualified workers. This paper addresses this problem by proposing a systematic approach to dynamically create a balance between quality and performance issues in workflow systems. We illustrate and evaluate the proposed approach with a realistic example and also compare how a workflow system would implement this scenario to highlight the shortcomings of current, state of the art workflow systems. Finally, a detailed simulation model is used to validate our approach.
The trend toward physically dispersed work groups has necessitated a fresh inquiry into the role and nature of team leadership in virtual settings. To accomplish this, we assembled thirteen culturally diverse global teams from locations in Europe, Mexico, and the United States, assigning each team a project leader and task to complete. The findings suggest that effective team leaders demonstrate the capability to deal with paradox and contradiction by performing multiple leadership roles simultaneously (behavioral complexity). Specifically, we discovered that highly effective virtual team leaders act in a mentoring role and exhibit a high degree of understanding (empathy) toward other team members. At the same time, effective leaders are also able to assert their authority without being perceived as overbearing or inflexible. Finally, effective leaders are found to be extremely effective at providing regular, detailed, and prompt communication with their peers and in articulating role relationships (responsibilities) among the virtual team members. This study provides useful insights for managers interested in developing global virtual teams, as well as for academics interested in pursuing virtual team research.
For the design of an intelligent assistant system aimed at supporting operators' decision in subway control, we modeled operators' activity and know-how. As a result, we introduce the notion of a contextual graph, which appears as a simple solution to describe and manage operational decision-making.
Crises demand swift and effective decision-making; yet there are many problems in training personnel on the skills necessary to achieve the goals of crisis management. This paper has three objectives concerning training for crisis management. First we integrate diverse literatures and present a framework for an understanding of the unique challenges in crisis management training, and the role of training systems with capabilities for simulation, immersion, and critiquing. Second, we describe an example of a trainer for ship damage control, called DC-Train, which addresses these challenges. This system consists of a first-principles simulator that generates large numbers of realistic scenarios. an immersive multimedia interface that helps elicit psychological processes involved in actual crisis management, and a critiquing expert system that provides real-time and post-session feedback on human decision-making performance. Finally, we present an empirical method for evaluating the effectiveness of such a system for crisis management training. Results of evaluation experiments with participants in a ship damage control training program indicate that the described computer-based trainer has psychological realism and improves decision-making performance.
The recent proliferation of information technology designed to support or enhance an individual professional's task performance has made the investigation of technology acceptance increasingly challenging and significant. This study investigates technology acceptance by individual professionals by examining physicians' decisions to accept telemedicine technology. Synthesized from relevant prior research, a generic research framework was built to provide a necessary foundation upon which a research model for telemedicine technology acceptance by physicians could be developed. The research model was then empirically examined, using data collected from more than 400 physicians practicing in public tertiary hospitals in Hong Kong. Results of the study suggest several areas where individual professionals might subtly differ in their technology acceptance decision-making, as compared with end users and business managers in ordinary business settings. Specifically, physicians appeared to be fairly pragmatic, largely anchoring their acceptance decisions in the usefulness of the technology rather than in its ease of use. When making decisions to accept a technology, physicians expressed considerable concerns about the compatibility of the technology with their practices, placed less importance on controlling technology operations, and attached limited weight to peers' opinions about using the technology. Based on results obtained from this study, the initially proposed framework for technology acceptance by individual professionals was revised to a hierarchical, three-layer structure with the individual context at the inner core, the implementation context on the outermost layer, and the technological context residing in the middle. Implications for information systems research and telemedicine management practice that have emerged from the study's findings are also discussed.
Asynchronous Learning Networks (ALN) are a form of e-learning that emphasizes the use of the Internet to support class discussions and activities. This paper presents a qualitative study of role changes that occur when faculty become online or virtual professors. In 20 semi-structured interviews of faculty, coded with pattern analysis software, the authors captured role changes enacted by instructors in ALN settings-cognitive roles, affective roles, and managerial roles. The cognitive role, which relates to mental processes of learning, information storage, and thinking, shifts to one of deeper cognitive complexity. The affective role, which relates to influencing the relationships between students, the instructor, and the classroom atmosphere, required faculty to find new tools to express emotion, yet they found the relationship with students more intimate. The managerial role, which deals with class and course management, requires greater attention to detail, more structure, and additional student monitoring. Overall, faculty reported a change in their teaching persona, toward more precision in their presentation of materials and instructions, combined with a shift to a more Socratic pedagogy, emphasizing multilogues with students. The main sources of frustration and of fulfillment of the virtual professor are explored.
This paper puts forth a vision and an architecture for a community knowledge evolution system. We propose augmenting a multimedia document repository (digital library) with innovative knowledge evolution support, including computer-mediated communications, community process support, decision support, advanced hypermedia features, and conceptual knowledge structures. These tools, and the techniques developed around them, would enable members of a virtual community to learn from, contribute to, and collectively build upon the community's knowledge and improve many member tasks. The resulting Collaborative Knowledge Evolution Support System (CKESS) would provide an enhanced digital library infrastructure serving as an ever-evolving repository of the community's knowledge, which members would actively use in everyday tasks and regularly update.
Multimedia content is a central component of on-demand training and education delivered over the World Wide Web. Supporting asynchronous collaboration around educational multimedia is potentially a significant tool for delivering online educational content effectively. A multimedia annotation system tightly integrated with e-mail provides a powerful platform on which to base such functionality. In this paper we describe a series of studies of such a system. First, we built a prototype annotation system and refined it based on results of laboratory tests. We then extended the system to support asynchronous collaboration for on-demand training and studied its effectiveness in two corporate training courses, assessing student experience, instructor experience, and user interface appropriateness. Having identified possibilities for enhancing engagement and collaboration with the tool, we conducted another set of laboratory studies. Through this iterative process we are creating a platform and identifying processes for its use, which enable students and instructors to exploit the advantages of asynchronous education while compensating for the reduction in face-to-face interaction.
We develop and illustrate a performance-centered design (PCD) methodology for structuring knowledge-intensive, ill-defined processes. PCD provides a holistic view of a performance environment by considering the complex interdependencies between the organizational context, business processes, and individual performers. The context for our theoretical exposition is the fuzzy front-end of the new product development (NPD) process. Despite the fact that front-end concept definition and selection is central to a firm's innovation capability, these activities are ill-structured and typically the most poorly managed in the entire innovation process. Through a case study, we illustrate the proposed PCD methodology as applied to the fuzzy front-end and additionally illustrate how electronic performance support technology can be utilized to support the fuzzy front-end process. Although specifically applied within the context of one firm, we contend that the PCD methodology is applicable to other knowledge-intensive and relatively unstructured processes.
Of the techniques available for idea generation with group support systems (GSS), little research attention has been given to techniques that challenge problem assumptions or that use unrelated stimuli to promote creativity. When implementing such techniques with GSS, choices must be made regarding how to configure the GSS to deploy the initial creative stimuli and to present the pool of emerging ideas that act as additional stimuli. ;This paper reports the results of an experiment that compares Electronic Brainstorming (few unnamed rotating dialogues) with Assumption Reversals (many related stimuli. many named dialogues, free movement among dialogues) and Analogies (many unrelated stimuli, many named dialogues, free movement among dialogues). ;Analogies produced creative, but fewer, ideas, due to the use of unrelated stimuli. Assumption Reversals produced the most, but less creative. ideas, possibly due to fragmentation of the group memory and cognitive inertia caused by lack of forced movement among dialogues.
It is common that text documents are characterized and classified by key words, index terms, or headings. We have developed a new methodology based on prototype matching. The prototype is an interesting document or a part of an extracted, interesting text. This prototype is matched with the existing document database or with the monitored document flow. The claim is that the new methodology is capable of extracting the contents of the document. To verify this hypothesis, a test with the Bible was designed. Different translations in English, Latin, Greek, and Finnish were selected to test materials. Verification tests that included the search of the ten nearest books to every book of the Bible were performed with a designed prototype version of the software application. The test results are reported in this paper.
The importance of trust as a key facilitator of electronic commerce is increasingly being recognized in academic and practitioner communities. However, empirical research in this area has been beset by conflicting conceptualizations of the trust construct, inadequate attention to its underlying dimensions, causes, and effects, and lack of a validated trust scale. This paper addresses these limitations in part by theoretically conceptualizing and empirically validating a scale to measure individual trust in online firms. The proposed scale taps into three key dimensions of trust: trustee's ability, benevolence, and integrity, An iterative testing and refinement procedure using two field surveys of online retailing and online banking users, leads to a final seven-item trust scale that exhibits adequate levels of reliability, convergent validity, discriminant validity, and nomological validity. It is expected that the scale presented in this paper will assist future empirical research on trust in online entities.
Enterprise Resource Planning (ERP) software systems integrate key business and management processes within and beyond a firm's boundary. Although the business value of ERP implementations has been extensively debated in trade periodicals in the form of qualitative discussion or detailed case studies, there is little large-sample statistical evidence on whether the benefits of ERP implementation exceed the costs and risks. With multiyear multi-firm ERP implementation and financial data, we find that firms that invest in ERP tend to show higher performance across a wide variety of financial metrics. Even though there is a slowdown in business performance and productivity shortly after the implementation, financial markets consistently reward the adopters with higher market valuation (as measured by Tobin's q). Due to the lack of mid- and long-term post-implementation data, future research on the long-run impact of ERP is proposed.
Digital products are now widely traded over the Internet. Many researchers have started to investigate the optimal competitive strategies and market environments for such products. This paper studies the competitive decisions made about software, a major class of digital products that can be easily sold through computer networks. Instead of focusing on traditional competitive dimensions, such as price or quantity, we study the number of functions that should be incorporated into the software. Using, game theoretic analysis, we show that there is no fixed strategy that is optimal for software developers in a duopoly market with one-stage simultaneous moves. This happens because, given one developer's decision, there is always an incentive for the other developer to deviate and achieve higher payoffs. Nevertheless, a unique reactive equilibrium does emerge if we consider the two-stage variation of the model, where the two developers both enjoy substantial profits by serving different segments of the market. Essentially, the first mover commits himself to a certain functionality level that induces a rational follower to target his software to the (previously) unattended segment. We discuss our results in light of scale economies in the software development process and market segmentation.
This study makes an initial attempt to validate an integrated, theoretically driven performance model of information systems (IS) projects. IS project performance is defined in terms of task, psychological, and organizational outcomes. We draw upon different theoretical perspectives including IS, organizational teams, and project management to link six categories of variables to IS project performance: technology characteristics, project characteristics, task characteristics, people characteristics, organizational characteristics, and work processes. Data collected via a field survey of IS project leaders in 84 manufacturing organizations were used to test the proposed model. Support is found for three conclusions: (1) IS project performance is a multidimensional construct, (2) certain preconditions falling into the above categories have to exist to achieve a high performing IS project, and (3) there is a possible cross-relationship among the variables studied by IS research, organizational teams research, and project management research. We discuss the implications of this study for future research and managerial practice.
This paper reports on a comparative case study of 13 industrial firms that implemented an enterprise resource planning (ERP) system. It compares firms based on their dialectic learning process. All firms had to overcome knowledge barriers of two types: those associated with the configuration of the ERP package, and those associated with the assimilation of new work processes. We found that both strong core teams and carefully managed consulting relationships addressed configuration knowledge barriers. User training that included both technical and business processes, along with a phased implementation approach, helped firms to overcome assimilation knowledge barriers. However, all firms in this study experienced ongoing concerns with assimilation knowledge barriers, and we observed two different approaches to address them. In a piecemeal approach, firms concentrated on the technology first and deferred consideration of process changes. In a concerted approach, both the technology and process changes were undertaken together. Although most respondents clearly stated a preference for either piecemeal or concerted chance, all firms engaged in practices that reflected a combination of these approaches.
Customer Relationship Management (CRM) systems require extensive configuration during which users come into extensive contact with the technical implementation team. Previous research examining other Enterprise Resource Planning (ERP) modules has shown that user perception of the responsiveness of such teams, as an indicator of a possible social exchange, is significantly associated with an increased favorable assessment of the new system and ultimately its adoption, the reason being that perceived responsiveness creates a constructive social exchange. However, previous research, using survey data alone, did not examine causation. The objective of this study is to examine, using a quasi-experimental design, whether different degrees of actual responsiveness in different sites during CRM implementation result in significant differences in the users' favorable assessment of the correctness and ultimately their approval of a new CRM. The data support these hypotheses, but show that the downstream effects of actual responsiveness are mediated by perceived responsiveness. Implications concerning the social exchange relationship during CRM adoption are discussed.
Employees' nonwork-related Web surfing behavior results in millions of dollars of expenditure for organizations. This paper proposes the use of a behavior-based artificial intelligence system to profile employee Web usage behavior. Two artificial neural networks (ANN) incorporating genetic algorithm techniques were developed for this purpose. The system was validated with two different data sets. The classification performance of the neural network models was compared to that of a statistical method. The results indicate that one of the ANN models, namely the simple recurrent network, was a superior classifier for this behavior-based problem. In addition, the uncertainty inherent in such classification decisions was examined with a loss matrix, and the holdout samples were reclassified using, a loss matrix. The output of this intelligent system can be highly beneficial to managers in designing effective Web management policies.
Text is the predominant form of organizational information. Comprehending text-based information requires intensive cognitive processing effort on the part of readers. Drawing on multimedia literature, this study identified a characteristic of multimedia presentations, namely complementary cues, which have the potential to improve the comprehensibility of organizational information. A set of hypotheses about the benefits of multimedia over text-based presentations was generated based on the theoretical perspective that we developed. These predictions were tested through a laboratory experiment using a simulated multimedia intranet. Results show that multimedia facilitates the retention and subsequent recall of explanative information but not of descriptive information. Explanative information is organized facts connected by their underlying functional relationships. Descriptive information consists of isolated facts without an explanation of the relationships between these facts. The ability to retain and recall explanative information, in turn, leads to a greater ability to make correct inferences about new organizational situations.
The study examines a method for supporting multiparty negotiations by means of a Negotiation Support System (NSS). More specifically, this study investigated the effect of visualization support on the development of shared mental models among negotiators who resolved a spatial planning dispute. The objective of this study is to determine how to support the development of shared mental models in order to stimulate more productive negotiations. A further goal is to provide guidelines for the design of NSS. Compared with a control condition, visualization improved three aspects of negotiations: visualization support aided negotiators' convergence of perceptions of reality and had positive socio-emotional consequences in terms of increasing cohesiveness and entitativity. As a result, groups with visualization support reached consensus more easily and were more satisfied with the process. In sum, the current study provides support for the idea that presenting negotiators with unambiguous information helps negotiators develop shared mental models.
The inclusion of social subsystem costs and benefits in information technology (IT) investment choices has been a difficult problem for IT decision-makers. Past research has shown that although some organizations adequately and consistently consider social subsystem issues when making IT investment decisions, many do not. This demonstrates a discrepancy between prescriptive theory and descriptive evidence. Our study addresses this theory-practice disconnection by investigating which firms, and under what conditions IT investments are likely to follow or violate prescriptions. Data collected from a national sample of 200 firms shed light on the firm and situational factors that affect the consideration of social subsystem issues during the IT investment decision process. The amount of social subsystem disruption associated with the IT in question, the strategic relevance of the IT to the organization, and the firm's continuous-learning culture each have direct or interactive influences on the decision process. Specifically, they impact the consideration of social subsystem costs and benefits for IT investments. Organizational size and industry are unrelated to this facet of decision-making. Overall, the empirical results help us better understand (1) what kinds of IT decisions cause stronger evaluation of social subsystem costs and benefits, (2) what types of firms give the greatest consideration to these issues, and (3) which intangible social subsystem costs or benefits are seen as the most important.
Despite the rising tide of investments in information technologies (IT) infrastructures, empirical evidence about the effects of such investment moves is scarce. Stock market investors provide one appropriate perspective on the value-creation and growth potential of IT infrastructure investments through their reactions to specific IT infrastructure investment moves by business firms. This research utilizes the event-study analysis approach to examine if IT infrastructure investments are associated with significantly positive abnormal stock market returns and rises in trading volume when firms announce such investments. Drawing upon a sample of IT infrastructure announcements in the early 1990s, this research finds significant evidence that positive abnormal returns and increased trading volume are associated with IT infrastructure investment announcements. Further, when such investments are contrasted with investments in IT applications, evidence exists that infrastructure investments generate greater excess returns and a larger increase in trading volume than applications investments do. The evidence provides empirical support for the potential of IT infrastructure investments to be perceived as a platform for growth and revenue generation opportunities in contemporary business firms.
Turnover of information system (IS) personnel is a critical problem for organizations. To gain a better understanding of turnover, researchers have explored career orientations that characterize an employee's internal motivations and desires. The inability of an organization to match career desires is often related to measures indicative of turnover in IS employees, including intent to leave and career dissatisfaction, though empirical evidence is indirect and inconclusive. Using career orientations, this study explicitly models the impact of the discrepancy between the wants of employees and employee perceptions of how their organization satisfies those wants. The model is based on discrepancy theory and predicts the gap is closely related to the turnover indicators. Model predictions hold true for a sample of 153 IS personnel. These results indicate the importance of developing career plans that employees perceive as matching their wants.
This study investigates the job satisfaction of information technology (IT) professionals in an environment where computer aided software engineering (CASE) tools are used. Although the recent downturn in the economy might have temporarily eased the IT labor shortage, issues of recruitment and retention of qualified personnel are key to the success of IS development projects. This study presents a model of the combination of CASE tool usage and job satisfaction as related to internal career orientation. Two hypotheses based on this model were tested using empirical evidence collected through a survey method. The first examines whether the career orientation of IS personnel influences their job satisfaction. The second incorporates the impact of CASE tool usage on this relationship. The results indicate that in a CASE tool environment, personnel with a predominant technical career orientation have more job satisfaction than those with a predominant managerial orientation. However, there is a significant and positive synergy between the sophistication of the CASE tool used and managerial competence orientation leading to higher job satisfaction These findings indicate that combating the IT personnel shortage through task automation may also increase worker satisfaction, thereby decreasing turnover. Careful selection of the CASE tool for use may result in this win-win situation.
The concept of organizational learning (OL) is receiving an increasing amount of attention in the research and practice of management information systems (MIS) due to its potential for affecting organizational outcomes, including control and intelligence, competitive advantage, and the exploitation of knowledge and technology. As such, further development of the salient issues related to OL is warranted, especially measurement of the construct. Based on a domain definition grounded in the literature, this research represents the initial work in developing an empirically reliable and valid measure of organizational learning. The rigorous method utilized in the derivation of this measure, which integrates two methodological frameworks for instrument development, is the main strength of this work. The result is an eight-factor, 28-item instrument for assessing OL, derived from a sample of 119 knowledge-based firms. The empirically derived factors are awareness, communication, performance assessment, intellectual cultivation, environmental adaptability, social learning, intellectual capital management, and organizational grafting. MIS function managers can use these factors to gauge organizational or subunit success in the creation and diffusion of new applications of information technology.
Past information systems research on real options has focused mainly on evaluating information technology (IT) investments that embed a single, a priori known option (such as, deferral option, prototype option). In other words, only once a specific isolated option is identified as being embedded in a target IT investment, does this research call upon using real options analysis to evaluate the option. In effect, however, because real options are not inherent in any IT investment, they usually must be planned and intentionally embedded in a target IT investment in order to control various investment-specific risks, just like financial risk management uses carefully chosen options to actively manage investment risks. Moreover, when an IT investment involves multiple risks, there could be numerous ways to reconfigure the investment using different series of cascading (compound) options. In this light, we present an approach for managing IT investment risk that helps to rationally choose which options to deliberately embed in an investment so as to optimally control the balance between risk and reward. We also illustrate how the approach is applied to an IT investment entailing the establishment of an Internet sales channel.
A number of theoretical models have been presented in group support systems (GSS) literature, which suggest that various GSS structures such as anonymity and simultaneity, influence group interaction, which in turn influences group productivity and meeting outcomes. Examples of such theories include the adaptive structuration theory and the balance of forces model and they could generally be described as dynamic or procedural in nature. Much of the empirical research that tests such theories, however, is deterministic in that it often compares final outcomes between various levels of technological support without measuring and testing (1) the influence that the technological structures have on group interaction and group dynamics, and (2) the corresponding influence that group interaction has on meeting outcomes. This paper reports a study that examines the validity of such dynamic theories by examining the relationships between GSS structures, group dynamics, and meeting outcomes over time. Four process constructs (production blocking, free riding, sucker effect, and evaluation apprehension) and three meeting outcome constructs (group cohesion, affective reward, and self-reported learning) were initially selected for the study. Structural equation modeling was used to analyze longitudinal survey data gathered from an experiment conducted with naturally occurring groups. The model tested was found to be valid and GSS was found to be effective in reducing process losses. However, the findings also revealed that process losses vary in the degree to which they influence meeting outcomes and certain meeting outcomes, such as affective reward, were found to be heavily influenced by other meeting outcomes, such as group cohesion and self-reported learning. Theoretical implications of the study and methodology are discussed.
Throughout its history, the information systems (IS) discipline has engaged in extensive self-examination, particularly with regard to its apparent diversity. Our overall objective in this study is to better understand the diversity in IS research, and the extent to which diversity is universal across journals that publish IS research. We developed a classification system that comprises five key characteristics of diversity (reference discipline, level of analysis, topic, research approach, and research method) based on a review of prior literature. We then examined articles over a five-year period, from 1995 to 1999, in five journals acknowledged as the top journals of the field, at least in North America. Analyses reveal considerable diversity in each of the key characteristics. Perhaps not surprisingly, the research approach used is more focused with most studies being conducted using hypothetico-deductive approaches, whereas reference discipline is perhaps the most diverse of the characteristics examined. An interesting finding is that IS itself emerged as a key reference discipline in the late 1990s. The Journal of Management Information Systems and Information Systems Research publish articles displaying the greatest diversity, and MIS Quarterly and Decision Sciences publish articles that focus on subsets of the field. Our research provides a foundation for addressing the direction that diversity in the IS discipline takes over time. In the shorter term, researchers can use our classification system as a guide to writing abstracts and selecting key words, and the findings of our journal analyses to determine the best outlet for their type of research.
Software reuse-the application of existing software artifacts in the development of a new system-has been claimed to dramatically improve systems development productivity and quality. These claims have been particularly pronounced with respect to the reuse of object-oriented (OO) software artifacts. However, the empirical evaluations of these claims are relatively sparse and often inconsistent. This paper begins to address the gap in the literature. A verbal-protocol study was conducted in which analysts created a model for a problem (the target) and were given an example problem and solution (the source) to reuse. The results show little support for reuse in OO analysis. First, reuse had no effect on the quality of the OO analysis models. Subjects given a highly reusable example produced solutions that were no better than those of subjects in the control group. Second, the degree of similarity between the source and target problems did have an effect on the reuse process, although it did not impact the reuse outcome. Subjects given the example with the most similarity to the target problem quickly recognized the reuse potential, attempted a fair amount of reuse, but made several errors stemming from lazy copying. Subjects given an example with a lesser (but still significant) degree of similarity were often unable to recognize the reuse potential, and thus engaged in less reuse activity. Thus, the characteristics of the source-target comparison that facilitate noticing the reuse potential of the source do not necessarily help in applying the source solution to the target problem. These results suggest that the claims associated with reuse should be treated with a healthy dose of skepticism.
A key decision by the manager of an advertisement-supported Web site is the balance between content and advertising. Content is costly but attracts viewers, whereas advertisement generates revenues but repels viewers. The period-by-period balancing decision is further complicated by the growth and diffusion. nature of Web site viewership This decision problem is modeled as a control problem that captures the essence of the business model of such Web sites. Using,this model we show that it may be optimal for the Web site to initially have negative cash flows fewer advertisement's and more content. This is more than compensated for by future profits from the Web site. We use the solution to the control, problem to also, develop p a forward-looking measure of Web traffic called the, discounted total traffic. We empirically examine this new measure and find that it better predicts capitalization than backward-looking measures like page views.
We develop an analytical model of a separating equilibrium for a two-tier fee-based and sponsorship-based information Web site. We examine the monopolist's choice of content quality and price for a fee-based site targeted at high-type consumers and the content quality level for a sponsored site offered free to all consumers. We show how a reduction in the potential for advertising revenues results in lower content quality on the free site, but permits the seller to raise the fee charged to high-type consumers. We also show how differences in consumer tolerances to ads affects content quality, banner ad volume, and usage fees. In particular, the seller can increase profits by making ads more attractive to either high- or low-type consumers, but rarely both at the same time. We show the conditions that determine which consumer segment the seller should seek to improve ad relevancy.
Price-comparison engines allow customers to compare product offerings of online sellers and reveal almost complete information on the alternatives, and hence create erosion in store loyalty. Consequently, the competitive dynamics of online sales are affected in markets where price-comparison shopping is diffusing rapidly. We develop a dynamic competitive pricing model that-deals with an asymmetric, duopolistic market where the segment sizes are determined through a diffusion process. Our diffusion-of-innovations approach allows us to dynamically capture, the proportion of informed and uninformed customers in-a homogenous' goods market. We use this model to analyze how strategic profit maximization behavior evolves,over time. this analysis shows that the increasing numbers of price-comparison shoppers pull prices down, and the rate at which prices decrease is shaped by the diffusion. curve and brand preference. Our analysis shows that stores with loyal customers, or with a preference for their brands, can attain higher profits I further into the-diffusion process. The direct implication is that firms should use their information technology;. operations, and marketing capabilities to create, enhance, and cultivate stronger preferences ford and loyalty to, their brand names to survive the inevitable information-rich markets of tomorrow.
Much research in conceptual data modeling has focused on developing techniques for view integration, or combining local conceptual schemas into a global schema. Local schemas are argued to be important in verifying conceptual data. requirements before proceeding to database design. View integration is claimed to fulfill two purposes. First, a global conceptual schema is a prerequisite to logical design and implementation.. Second, global schemas are thought,to be useful in improving organizational, communication among diverse user, groups, with different perspectives and information needs. However, performing view integration is difficult. Moreover, there i no empirical evidence that global schemas either impede local verification or support communication. Drawing. on classification research, this paper develops and tests claims about the impact of L schema structure (local versus. global) on verification and communication. Local schemas are hypothesized to better support verify cation than global-schemas. When different local views contain conflicting structure, local schemas are expected to be superior in supporting communication. However, when local views contain complementary structure, global schemas. are expected to be superior in supporting communication. A laboratory experiment was conducted to test these predictions. The results-support the hypotheses. Implications for the practice of database design and for further research are considered.
This paper presents and tests a conceptual model, linking perceptions of the internal work environment and external markets to information technology (IT) worker turnover. The model focuses on organizational commitment (OC) as the primary predictor of turnover intention. We suggest that OC mediates perceptions of the workplace and external environment on turnover intention Specifically, we hypothesize that OC mediates the influence of (1) job satisfaction, (2) perceived job characteristics , (3) perceived competitiveness of pay, aind (4) perceived job alternatives on turnover intention. Also, perceived job alternatives are modeled as having a direct effect on turnover intention. Analysis provides moderate empirical support for the research model. OC and perceived job alternatives demonstrated distinct effects on turnover intention. In addition, OC mediated the influence of job satisfaction, perceived job characteristics, and perceived competitiveness of pay on turnover intention. Findings suggest that through cultivating positive beliefs about the job and attitudes toward the employer, managers may counter the influence of external markets on IT workers' turnover intention.
This paper investigates the effects of group support systems (GSS) and content facilitation on individual knowledge acquisition in general, and on changes in an individual's knowledge structures in particular as indicated through concept mapping development. We present a model explaining the enabling effects of GSS and content facilitation on group processes (group participation, quality of feedback, domination, and communication barrier), cooperation in learning, and individual knowledge structures (knowledge complexity, integration, and commonality). An experiment that employed a 2X2 factorial design was used to explore the main and interaction effects of GSS and content facilitation on knowledge acquisition. Experimental subjects were randomly assigned to one of the four treatment groups; that is, nonfacilitation and non-GSS, nonfacilitation and GSS, facilitation and non-GSS, facilitation and GSS. Results of the experiment indicated that both GSS and content facilitation positively affect certain aspects of individual knowledge acquisition. Content facilitation particularly enhanced learners' knowledge commonality, whereas GSS enhanced the quality of feedback and cooperation in learning, and reduced domination and communication barrier. However, the results also indicated that GSS and content facilitation have crossover interaction effects on group participation and knowledge commonality. The effects of combining GSS and content facilitation were not additive in this study. Explanations are presented.
Business-to-business (B2B) electronic commerce has become an important issue in the debate about electronic commerce. How should the intermediary charge suppliers and buyers to maximize profits from such a marketplace? We analyze a monopolistic B2B marketplace owned by an independent intermediary. The marketplace exhibits two-sided network effects where the value of the marketplace, to buyers is dependent on the number of suppliers, and the value to suppliers is dependent on the number of buyers and suppliers. When these two-sided network effects exist, we find that the optimal price for buyers and the fraction of buyers in the electronic market are dependent on the, switching cost and the strength of the network effect of both types: buyers and suppliers,. The same is,true for the optimal price for suppliers and the fraction of suppliers in the electronic market. In other words, the parameters that define the buyers also affect the optimal,price for suppliers and the fraction of suppliers in the electronic market, and vice-versa. Our results. also point to some counterintuitive optimal pricing strategies that depend on the nature of the industry served by the marketplace.
The need for timely information in the e-business world provides the impetus to develop a flexible database system with the capability to adapt and maintain performance levels under changing queries and changing business environments. Recognizing the importance of providing fast access to a variety of read-only applications in today's e-business world, we introduce the systems architecture for developing and implementing a flexible database system to achieve considerable gains in processing times of read queries. The key component of a flexible database system is query mining, the concept of determining relationships among query properties, alternative database structures, and query processing times. We validate the flexible database system concept through extensive laboratory experiments, where we embed learning tools to demonstrate the implementation of query mining.
We describe the emerging competition between music companies and their star acts and the role of online distribution in this industry. We then contrast this with the lack of competition newspapers will face from their reporters, writers, and photographers, but identify other possible competitors for newspaper publishers. We examine what resources have previously enabled record companies to lock in their star acts and ways in which technology has altered artists' abilities to reach the market independently and thus their dependency upon record companies. We examine which resources have seen their value eroded in the newspaper industry and the remaining value that the newspaper company still creates, other than bundling stories, adding advertising, and printing and selling the papers. We consider what part of the newspaper business is vulnerable, if any, and where threats may arise. We combine the resource-based view of competitive advantage to examine which industry may have become newly easy to enter, and the theory of newly vulnerable markets to assess which industry may actually have become vulnerable as a result. Our analyses are then used to create a computer simulation model to make the implications more explicit under a range of assumptions.
Work system analysis and design is complex and nondeterministic. In this paper we describe Brahms, a multiagent modeling and simulation environment for designing complex interactions in human-machine systems. Brahms was originally conceived as a business process design tool that simulates work practices, including social systems of work. We describe our modeling and simulation method for mission operations work systems design, based on a research case study in which we used Brahms to design mission operations for a proposed discovery mission to the Moon. We then describe the results of an actual method application project-the Brahms Mars Exploration Rover. Space mission operations are similar to operations of traditional organizations; we show that the application of Brahms for space mission operations design is relevant and transferable to other types of business processes in organizations.
Although team-based work systems are pervasive in the workplace, the use of collaborative systems designed to facilitate and support ongoing teamwork is a relatively recent development. An understanding of how teams embrace and use such collaborative systems-and the relationship of that usage to teamwork quality and team performance-is critical for organizational success. We present a theoretical model in which usage of a collaborative system intervenes between teamwork quality and team performance for tasks that are supported by the system. We empirically validate the model in a setting where established teams voluntarily used a collaborative system over a four-month period to perform tasks with measurable outcomes. Our principal finding is that collaborative system use intervenes between teamwork quality and performance for tasks supported by the system but not for unsupported tasks.
Field research and laboratory experiments suggest that, under certain circumstances, people using group support systems (GSS) can be significantly more productive than people who do not use them. Yet, despite their demonstrated potential, GSS have been slow to diffuse across organizations. Drawing on the Technology Transition Model, the paper argues that the high conceptual load of GSS (i.e., understanding of the intended effect of GSS functionality) encourages organizations to employ expert facilitators to wield the technology on behalf of others. Economic and political factors mitigate against facilitators remaining long term in GSS facilities that focus on supporting nonroutine, ad hoc projects. This especially hampers scaling GSS technology to support distributed collaboration. An alternative and sustainable way for organizations to derive value from GSS lies in an approach called collaboration engineering: the development of repeatable collaborative processes that are conducted by practitioners themselves. To enable the development of such processes, this paper proposes the thinkLet concept, a codified packet of facilitation skill that can be applied by practitioners to achieve predictable, repeatable patterns of collaboration, such as divergence or convergence. A thinkLet specifies the facilitator's choices and actions in terms of the GSS tool used, the configuration of this tool, and scripted prompts to accomplish a pattern of collaboration in a group. Using thinkLets as building blocks, facilitators can develop and transfer repeatable collaborative processes to practitioners. Given the limited availability of expert facilitators, collaboration engineering with thinkLets may become a sine qua non for organizations to effectively support virtual work teams.
Collaborative technologies such as group support systems (GSS) are often developed to improve the effectiveness and efficiency of teams; however, the satisfaction users have with the processes and outcomes of the teamwork itself often determines the ultimate adoption and sustained use of collaborative technologies. Much of the research on teamwork has focused on meetings in particular and, consequently, satisfaction with the process and outcomes of meetings, referred to collectively as meeting satisfaction. Research on meeting satisfaction in GSS-supported groups has been equivocal, indicating the need for advancement in our theoretical understanding of the construct. To that end, this paper presents a causal model of meeting satisfaction derived from goal setting theory. The model is tested with an empirical study consisting of 15 GSS groups and 11 face-to-face (FTF) groups engaged in the lost at sea task. The results of analysis using structural equation modeling indicate support for the model's integrity across both GSS and FTF groups. Implications for researchers and practitioners are discussed, including how the model can be used to improve future research on the use of collaborative technology to support teamwork.
The volume of qualitative data (QD) available via the Internet is growing at an increasing pace and firms are anxious to extract and understand users' thought processes, wants and needs, attitudes, and purchase intentions contained therein. An information systems (IS) methodology to meaningfully analyze this vast resource of QD could provide useful information, knowledge, or wisdom firms could use for a number of purposes including new product development and quality improvement, target marketing, accurate user-focused profiling, and future sales prediction. In this paper, we present an IS methodology for analysis of Internet-based QD consisting of three steps: elicitation; reduction through IS-facilitated selection, coding, and clustering; and visualization to provide at-a-glance understanding. Outcomes include information (relationships), knowledge (patterns), and wisdom (principles) explained through visualizations and drill-down capabilities. ;First we present the generic methodology and then discuss an example employing it to analyze free-form comments from potential consumers who viewed soon-to-be-released film trailers provided that illustrates how the methodology and tools can provide rich and meaningful affective, cognitive, contextual, and evaluative information, knowledge, and wisdom. The example revealed that qualitative data analysis (QDA) accurately reflected film popularity. A finding is that QDA also provided a predictive measure of relative magnitude of film popularity between the most popular film and the least popular one, based on actual first week box office sales. The methodology and tools used in this preliminary study illustrate that value can be derived from analysis of Internet-based QD and suggest that further research in this area is warranted.
In this study, we explore the nature of team interaction and the role of temporal coordination in asynchronously communicating global virtual project teams (GVPT). Drawing on Time, Interaction, and Performance (TIP) theory, we consider how and why virtual team behavior is temporally patterned in complex ways. We report on the results of an experiment consisting of 35 virtual project teams comprised of 175 members residing in the United States and Japan. Through content and cluster analysis, we identify distinct patterns of interaction and examine how these patterns are associated with differential levels of GVPT performance. We also explore the role of temporal coordination mechanisms as a means to synchronize temporal patterns in GVPTs. Our results suggest that successful enactment of temporal coordination mechanisms is associated with higher performance. However, we found that temporal coordination per se is not the driver of performance; rather, it is the influence of coordination on interaction behaviors that affects performance.
We present a novel system and methodology for generating and then browsing multiple taxonomies over a document collection. Taxonomies are generated using a broad set of capabilities, including meta data, key word queries, and automated clustering techniques that serve as a seed taxonomy. The taxonomy editor, eClassifier, provides powerful tools to visualize and edit each taxonomy to make it reflective of the desired theme. Cluster validation tools allow the editor to verify that documents received in the future can be automatically classified into each taxonomy with sufficiently high accuracy. ;In general, those seeking knowledge from a document collection may have only a vague notion of exactly what they are attempting to understand, and would like to explore related topics and concepts rather than simply being given a set of documents. For this purpose, we have developed MindMap, an interface utilizing multiple taxonomies and the ability to interact with a document collection.
Numerous methodological issues arise when studying teams that span multiple boundaries. The main purpose of this paper is to raise awareness about the challenges of conducting field research on teams in global firms. Based on field research across multiple firms (software development, product development, financial services, and high technology), we outline five types of boundaries that we encountered in our field research (geographical, functional, temporal, identity, and organizational) and discuss methodological issues in distinguishing the effects of one boundary where multiple boundaries exist. We suggest that it is important to: (1) appropriately measure the boundary of interest to the study, (2) assess and control for other influential boundaries within and across teams, and (3) distinguish the effects of each boundary on each team outcome of interest. Only through careful attention to methodology can we properly assess the effects of team boundaries and appreciate their research and practical implications for designing and using information systems to support collaborative work.
Ten years ago, we presented the DeLone and McLean Information Systems (IS) Success Model as a framework and model for measuring the complex-dependent variable in IS research. In this paper, we discuss many of the important IS success research contributions of the last decade, focusing especially on research efforts that apply, validate, challenge, and propose enhancements to our original model. Based on our evaluation of those contributions, we propose minor refinements to the model and propose an updated DeLone and McLean IS Success Model. We discuss the utility of the updated model for measuring e-commerce system success. Finally, we make a series of recommendations regarding current and future measurement of IS success.
Theft of software and other intellectual property has become one of the most visible problems in computing today. This paper details the development and empirical validation of a model of software piracy by individuals in the workplace. The model was developed from the results of prior research into software piracy, and the reference disciplines of the theory of planned behavior, expected utility theory, and deterrence theory. A survey of 201 respondents was used to test the model. The results indicate that individual attitudes, subjective norms, and perceived behavioral control are significant precursors to the intention to illegally copy software. In addition, punishment severity, punishment certainty, and software cost have direct effects on the individual's attitude toward software piracy, whereas punishment certainty has a significant effect on perceived behavioral control. Consequently, strategies to reduce software piracy should focus on these factors. The results add to a growing stream of information systems research into illegal software copying behavior and have significant implications for organizations and industry groups aiming to reduce software piracy.
Organizational approaches to managing information systems (IS) professionals have been making headlines as technology-intensive businesses search for ways to cope with an ever-changing economic landscape. Consequently, understanding and predicting employee quitting or separation behavior is crucial. We incorporate human capital theory from economics to form an alternative theoretical perspective for understanding IS professionals' separation and retention. This shift allows us to focus on precursors to observed separation, rather than attitudinal precursors of intentions. We introduce three new constructs: pressure to separate, retention frontiers, and separation thresholds. These constructs provide a basis for identifying when an employee is close to leaving the firm and for a new approach to analyze the potential effectiveness of action taken by a firm to change separation behavior: pre-implementation retention intervention assessment (PRI-assessment). We illustrate the application of the new approach using data on the observed separation behavior of 661 IS professionals at a large multidivision firm.
We extend critical success factors (CSF) methodology to facilitate participation by many people within and around the organization for information systems (IS) planning. The resulting new methodology, called critical success chains (CSC), extends CSF to explicitly model the relationships between IS attributes, CSF, and organizational goals. Its use is expected to help managers to (1) consider a wider range of development ideas, (2) better balance important strategic, tactical, and operational systems in the development portfolio, (3) consider the full range of options to accomplish desired objectives, and (4) better optimize the allocation of resources for maintenance and small systems. ;We trace the development of CSF and make the case for extending it. In two case studies, one at Rutgers University and another at Digia, Inc., we demonstrate the use of CSC in planning. At Rutgers, we use CSC to observe employees' preferences for new systems features, to model the reasons why they think that the features are important to the firm, and to generate strategic IS project proposal ideas. At Digia, we use CSC to generate ideas for new financial services applications based on mobile communications technology for which Digia would be a part of the value chain. From our experience in the case studies, we define a practical procedure for data gathering and analysis to uncover and model CSC in the firm and to generate ideas for important IS projects.
Seeking to improve software development, many organizations attempt to deploy formalized methodologies. This typically entails substantial behavioral change by software developers away from previous informal practices toward conformance with the methodology. Developers' resistance to such change often results in failure to fully deploy and realize the benefits of the methodology. The present research draws upon theories of intention formation and innovation diffusion to advance knowledge about why developers accept or resist following methodologies. Results from a field study within a large organization indicate that developers' intentions are directly influenced by their perceptions of usefulness, social pressure, compatibility, and organizational mandate. This pattern of intention determinants is quite different from that typically observed in studies of information technology tool adoption, revealing several key differences between the domains of tool versus methodology adoption. Specifically, although organizational mandate had a significant effect on intentions, the strength of its direct influence was the lowest among the four significant constructs, and usefulness, compatibility, and social pressure all influenced intentions directly, above and beyond the effects of organizational mandate. The findings suggest, contrary to popular belief, that an organizational mandate is not sufficient to guarantee use of the methodology in a sustained manner.
Employing media richness theory, a model is developed to open the black box surrounding the impact of computer-mediated communication systems on decision quality. The effects on decision quality of two important communication system factors, cue multiplicity and feedback immediacy, are examined in light of three important mediating constructs: social perceptions, message clarity, and ability to evaluate others. A laboratory experiment examining two tasks and employing face-to-face, electronic meeting, electronic conferencing, and electronic mail communication systems is used to assess the model's validity. Results provide consistent support for the research model as well as media richness theory. ;Richer media facilitate social perceptions (total socio-emotional communication and positive socio-emotional climate) and perceived ability to evaluate others' deception and expertise. Leaner media (electronic mail and electronic conferencing) facilitate communication clarity when participants have less task-relevant knowledge. The impacts of these mediating constructs on decision quality were found to depend on the levels of participant expertise and deception. In general, it was found that richer media can have significantly positive impacts on decision quality when participants' task-relevant knowledge is high. Moreover, effects of participant deception can be mitigated by employing richer media.
Making sense of new information technology (IT) and the many buzzwords associated with it is by no means an easy task for executives. Yet doing so is crucial to making good innovation decisions. This paper examines how information systems (IS) executives respond to what has been termed organizing visions for IT, grand ideas for applying IT, the presence of which is typically announced by much buzz and hyperbole. Developed and promulgated in the wider interorganizational community, organizing visions play a central role in driving the innovation adoption and diffusion process. Familiar and recent examples include electronic commerce, data warehousing, and enterprise systems. ;A key aspect of an organizing vision is that it has a career That is, even as it helps shape how IS managers think about the future of application and practice in their field, the organizing vision undertakes its own struggle to achieve ascendancy in the community. The present research explores this struggle, specifically probing how IS executives respond to visions that are in different career stages. Employing field interviews and a survey, the study identifies four dimensions of executive response focusing on a vision's interpretability, plausibility, importance, and discontinuity. Taking a comparative approach, the study offers several grounded conjectures concerning the career dynamics of organizing visions. For the IS executive, the findings help point the way to a more proactive, systematic, and critical stance toward innovations that can place the executive in a better position to make informed adoption decisions.
Knowledge is recognized as an important weapon for sustaining competitive advantage and many companies are beginning to manage organizational knowledge. Researchers have investigated knowledge management factors such as enablers, processes, and performance. However, most current empirical research has explored the relationships between these factors in isolation. To fill this gap, this paper develops a research model that interconnects knowledge management factors. The model includes seven enablers: collaboration, trust, learning, centralization, formalization, T-shaped skills, and information technology support. The emphasis is on knowledge creation processes such as socialization, externalization, combination, and internalization. To establish credibility between knowledge creation and performance, organizational creativity is incorporated into the model. Surveys collected from 58 firms were analyzed to test the model. The results confirmed the impact of trust on knowledge creation. The information technology support had a positive impact on knowledge combination only. Organizational creativity was found to be critical for improving performance; neglecting ideas can undermine a business. The results may be used as a stepping stone for further empirical research and can help formulate robust strategies that involve trade-offs between knowledge management enablers.
In manufacturing, the interaction between the design of a product and the process to manufacture this product is studied in detail. Consider, for example, material requirements planning (MRP) as part of current enterprise resource planning (ERP) systems, which is mainly driven by the bill of material (BOM). For information-intensive products such as insurances, and many other services, the workflow process typically evolves or is redesigned without careful consideration of the structure and characteristics of the product. In this paper, we present a method named product-based workflow design (PBWD). PBWD takes the product specification and three design criteria as a starting point, after which formal models and techniques are used to derive a favorable new design of the workflow process. The ExSpect tool is used to support PBWD. Finally, using a real case study, we demonstrate that a full evaluation of the search space for a workflow design may be feasible depending on the chosen design criteria and the specific nature of the product specifications.
This study examines whether technically anonymous comments entered by participants during group support system (GSS) brainstorming sessions are, in fact, unidentifiable. Hypotheses are developed and tested about the influences of comment length, comment evaluative tone, duration of group membership, and prior communication among group members on the accuracy of attributions they made about the identity of the authors of these technically anonymous comments. Data on prior communication and group history about each of the 32 small groups was collected before participants began using a GSS for brainstorming. Immediately after the session, each member was asked to attribute authorship to a sample of the session's anonymous comments (comment authorship was known to the researchers). The study's participants made attributions that were significantly more accurate than chance guessing. Factors that had a positive influence on attribution accuracy include evaluative tone of comments (especially humorous comments) and amount of prior communication received from other group members. Vividness of comment tone and comment length was not significantly correlated with attribution accuracy. Although the attributions of anonymous comments were more accurate than expected by chance, most of the attributions were incorrect. Implications and consequences of both accurate and inaccurate attribution are discussed along with suggestions for future research.
This study examines the potential applications of the rational expectations hypothesis (REH) and adaptive learning theory in IT investment and adoption decision-making. Despite the fact that rationality is commonly assumed in economic analyses, the REH's assumptions make it a unique theory and allow us to offer new perspectives on IS/IT adoption and investment decision-making. Our application of these theoretical perspectives to the IT adoption context-the first time in the IS literature to our knowledge that REH has been used to examine the mechanism for business value expectations formation-will allow us to treat the investment and adoption issues using a perspective that is based on a longer time horizon. Such settings require managers, as economic agents, to form a set of expectations about the values of various variables related to the business value of IT. Rational expectations and adaptive learning assume that decision-makers are able to utilize all available decision-relevant information efficiently and can learn the true value of a prospective investment over time. We present a number of propositions that characterize this perspective, and discuss some illustrative examples that demonstrate the efficacy of the theoretical perspective that we present to characterize the business value expectations formation process in IT adoption.
Even if bandwidth on the Internet is limited, compression technologies have made online music piracy a foremost problem in intellectual copyright protection. However, due to significantly larger sizes of video files, movies are still largely pirated by duplicating DVDs, VCDs, and other physical media. In the case of DVDs, movie studios have historically maintained different technology codes or formats across various regions of the world, primarily to control the timing of theatrical releases in these parts of the world. This paper formulates an analytical model to study the implications of maintaining different or incompatible technology standards in DVD and other optical disc players on global pricing and piracy of movie discs. Our formulation develops two distinct piracy types, namely, regional and global piracy, signifying if consumers will pirate movies released for their own region or those meant for other regions. Our results find that maintaining separate technology standards is very critical when there is piracy, as losses from global piracy can be higher than when only regional piracy exists. Further, we observe that piracy is not a victimless crime, in that not only do producers suffer losses but consumers in regions with high willingness to pay for quality also stand to lose. In addition, we find that increasing homogeneity in consumer preferences for quality across regions may not be beneficial to digital product vendors unless there is also uniformity in copyright protection laws. We conclude with recommendations for research and practice for movie studios as well as producers for other goods that are dependent on copyright protection such as books and pharmaceuticals.
Software such as operating systems, word processing, spreadsheets, graphics, and others often serves as a base for a number of third-party add-in products or plug-ins. These add-ins enhance the functionality of the base product. Unless protected by patents, these add-ins can potentially be bundled into the base software. The impact of this bundling on the profits of the base software producer and the consumer depends on the proportion of consumers that value the add-in and the penalty that some consumers incur from finding only a bundled product available when they do not desire the add-in. Using a model of the market, we show that the price of the bundle will be less than the sum of the prices of the base and add-in software when they are sold separately. We also show that the total consumer surplus and the social welfare increase if the base software producer's profit increases with bundling.
Executives need to master different mechanisms for analyzing their firms' investment opportunities in uncertain, difficult times. Rapidly changing business conditions require firms to move quickly, with total commitment and the rapid deployment of capital, resources, and management attention, often in several directions at the same time. However, high levels of strategic uncertainty and environmental risk, combined with limits on available funding, require firms to limit their commitment. In brief, we require high levels of strategic commitment to numerous projects, while simultaneously preserving our flexibility and withholding commitment. Whereas achieving both is clearly impossible, techniques exist that enable executives (1) to identify and to delimit their range of investment alternatives that must be considered, and to do so rapidly and reliably, (2) to divide investments into discrete stages that can be implemented sequentially, (3) to determine which chunks can safely and profitably be developed as strategic options, with value that can be captured when subsequent stage investments are made later; and (4) to quantify and to estimate the value of these strategic options with a significant degree of accuracy, so that selections can be made from a portfolio of investment alternatives. This paper also avoids restrictions of common option valuation models by providing a technique that is general enough to be used when the data required by common models are not available or the assumptions are not satisfied.
Research in the information systems (IS) field has often been characterized as fragmented. This paper builds on a belief that for the field to move forward and have an impact on practitioners and other academic fields, the existing work must be examined and systematized. It is particularly important to systematize research on the factors that underlie success of organizational IS. The goal here is to conceptualize the IS success antecedents (ISSA) area of research through surveying, synthesizing, and explicating the work in the domain. Using a combination of qualitative and quantitative research methods, a taxonomy of 12 general categories is created, and existing research within each category is examined. Important lacunae in the direction of work have been determined. It is found that little work has been conducted on the macro-level independent variables, the most difficult variables to assess, although these variables may be the most important to understanding the ultimate value of IS to organizations. Similarly, ISSA research on success variables of consequence to organizations was found severely lacking. Variable analysis research on organizational-level success variables was found to be literally nonexistent in the IS field, whereas research in the organizational studies field was found to provide useful directions for IS researchers. The specifics of the 12 taxonomy areas are analyzed and directions for research in each of them provided. Thus, researchers and practitioners are directed toward available research and receive suggestions for future work to bring ISSA research toward an organized and cohesive future.
This paper demonstrates that quality-contingent pricing is a useful mechanism for mitigating the negative effects of quality uncertainty in e-commerce and information technology services. Under contingency pricing of an information good or service, the firm preannounces a rebate for poor performance. Consumers determine performance probabilities using publicly available historical performance data, and the firm may have additional private information with respect to its future probability distribution. Examining the monopoly case, we explicate the critical role of private information and differences in belief between the firm and market in the choice of pricing scheme. Contingent pricing is useful when the market underestimates the firm's performance; then it is optimal for the firm to offer a full-price rebate for mis-performance, with a correspondingly higher price for meeting the performance standard. We study the competitive value of contingency pricing in a duopoly setting where the firms differ in their probabilities of meeting the performance standard, but are identical in other respects. Contingency pricing is a dominant strategy for a firm when the market underestimates the firm's performance. Whereas both firms would earn equal profits if they were constrained to standard pricing, the superior firm earns greater profits under contingency pricing by setting lower expected prices. We show that contingency pricing is efficient as well, and consumer surplus increases because more consumers buy from the superior firm.
The past few years have seen an explosion in the number of e-marketplaces, including a variety of electronic exchanges in the B2B arena, but many of these have also collapsed (e.g., ChemdexNentro). The question addressed in this paper is what are the underlying factors that affect which transactions are likely to be supportable by B2B exchanges. In particular, we identify and study three factors: supplier management, idiosyncratic investments in information systems, and codifiability (i.e., digitalizability) of product and order-fulfillment specifications underlying transactions. We show that transaction codifiability plays a fundamental role in influencing the nature of sustainable contracting and IT investments in e-markets. Hypotheses are derived from an analytical model of codifiability in e-marketplaces; these hypotheses are supported by several case studies by the authors and others on the key success factors underlying B2B exchanges.
This paper seeks to add to the nascent research literature on virtual teams and virtual team leadership by investigating the issues facing virtual team leaders as they implement and lead virtual teams. In particular, the way in which leaders develop relationships with their virtual team members is explored. A research framework involving action learning was instituted, with data collection and analysis based on grounded theory approaches. In all, seven virtual team leaders from a variety of New Zealand organizations took part in the study. The data showed very clearly that the leaders considered it essential to build some level of personal relationship with their virtual team members before commencing a virtual working relationship. A unifying framework of three interrelated theoretical steps, which illustrates how a virtual leader builds relationships with virtual team members, is introduced. These three steps are assessing conditions, targeting level of relationship, and creating strategies. This study is the first to identify the steps a virtual team leader undertakes when building relationships with virtual team members. The implications for virtual team practice and research are discussed.
It has been argued that the buyer's trust of the vendor is a critical precursor to a transactional relationship in an e-commerce environment. This study uses an experimental survey to test a model that includes a number of factors such as trust mechanisms, system trust, and vendor reputation. The results suggest that one trust mechanism, vendor guarantees, has a direct influence on system trust. Further, within e-commerce situations, system trust plays an important role in the nomological network by directly affecting trust in vendors and indirectly affecting attitudes and intentions to purchase. These results held in the case of both firms with and without an established reputation. The results demonstrate the importance of interventions such as self-reported vendor guarantees that affect system trust in enabling successful e-commerce outcomes.
The performance of firms in the software industry depends considerably on the quality of their software development processes. Managing software development is a challenging task, as management controls need to impose discipline and coordinate action to ensure goals are met while simultaneously incorporating autonomy to motivate software developers to be innovative and produce quality work. How should such firms manage software development projects so that their development processes are flexible and predictable-resulting in products that meet quality goals and that are delivered within budget and on time? The management literature suggests two approaches to control: the process approach and the structure approach. The process approach recommends control of activities through specifying methods (behavior control) and through specifying performance criteria (outcome control). In contrast, the structure approach recommends control through centrally devised standards for activities (standardization) and by the delegation of authority for decision-making (decentralization). This study synthesizes these two approaches to suggest that formal managerial control is exerted through a matrix of control comprising four modes: standardization of methods, standardization of performance criteria, decentralization of methods, and decentralization of performance criteria. ;We test the association of the modes of control with performance in a sample of 56 firms in the software industry in the United States. The results suggest that performance is enhanced by establishing uniform performance criteria across projects (standardization of performance criteria) while giving each project team the authority to make decisions with respect to methods (decentralization of methods). However, standardization of methods across all projects and decentralization of performance criteria by delegating the authority to make decisions about performance criteria to project teams were both not significantly related to performance. The matrix of control and its relationship to performance has theoretical and practical implications for managing software development. This model of control is also likely to be useful in other knowledge-work-intensive settings.
Researchers have emphasized that existing training strategies must be modified in order to adequately prepare users to employ collaborative applications. We utilize findings from the vast amount of training research conducted thus far and point to some problems that might occur when existing strategies are applied to train users of collaborative applications. We test our ideas by conducting a longitudinal field study of a collaborative work flow application. As proposed in a recent knowledge-level framework, our findings indicate that training programs must not solely focus on developing users' system proficiency skills but must also educate users about the business processes that the collaborative application will support. This additional knowledge will enable users to deal with technology-induced changes in the business processes due to the deployment of the collaborative application. Furthermore, we find that training programs should sensitize users to the interdependencies that exist among their tasks and make them aware of the collective consequences of their individual actions. We also found that users have to engage in collective problem solving efforts and continuously learn new knowledge during the process of appropriation of the collaborative application. We propose a training framework that integrates these ideas to prepare users to make effective use of collaborative applications. The proposed framework calls for trainers to be continuously engaged with users and help refine their knowledge during the process of appropriation. We suggest that theoretical foundations rooted in collective learning be adopted to guide training research in collaborative applications.
Modem organizations offer services through multiple channels, such as branches, ATMs, telephones, and Internet sites, and are supported by multifunctional software architectures. Different functional modules share data, which are typically stored in multiple local databases. Functional modules are usually not integrated across channels, as channels are implemented at different times within independent software projects and are subject to varying requirements of availability and performance. This lack of channel and functional integration raises data quality problems that can impact the quality of the products and services of an organization. In particular, in complex systems in which data are managed in multiple databases, timeliness is critical. This paper focuses on time-related factors of data quality and provides a model that can help companies to evaluate data currency, accuracy, and completeness in software architectures with different degrees of integration across channels and functionalities. The model is validated through simulation based on empirical data on financial information systems. Results indicate how architectural choices on the degree of data integration have a varying impact on currency, accuracy, and completeness depending on the type of financial institution and on customer profiles.
Motivated by the growing importance of data quality in data-intensive, global business environments and by burgeoning data quality activities, this study builds a conceptual model of data quality problem solving. The study analyzes data quality activities at five organizations via a five-year longitudinal study. ;The study finds that experienced practitioners solve data quality problems by reflecting on and explicating knowledge about contexts embedded in, or missing from, data. Specifically, these individuals investigate how data problems are framed, analyzed, and resolved throughout the entire information discourse. Their discourse on contexts of data, therefore, connects otherwise separately managed data processes, that is, collection, storage, and use. Practitioners' context-reflective mode of problem solving plays a pivotal role in crafting data quality rules. These practitioners break old rules and revise actionable dominant logic embedded in work routines as a strategy for crafting rules in data quality problem solving.
Over the past two decades, numerous empirical studies have been conducted on the contribution of information technology (IT) to productivity and other measures of firm performance. However, few theoretical studies have attempted to explain the contingencies under which IT investments may or may not be valuable to a firm in a competitive market. This research proposes a duopoly competition model to study the impacts of IT investments on firm performance and productivity. We show that the extent to which a profit-maximizing firm benefits from IT investments is a function of, among other things, market sensitivities to the price and quality of the products and services offered by the firm and its competitor. We demonstrate that, under duopolistic competition, the effects of IT investments are not as deterministic as under monopolistic competition. We further show that the effect of IT investments on productivity, in a duopoly market, are contingent on market sensitivities to changes in the price and quality of products and services offered by the firm and its competitor, as well as on fixed and overhead costs being sufficiently large in relation to market size-an important condition in a monopoly market. Especially, the price sensitivity has a positive effect on the impact of IT investments on productivity and quality sensitivity has a negative effect. We submit that firms are better off making efficiency-enhancing IT investments if the market in which they operate is more price sensitive than quality sensitive.
Advances in corporate householding are needed to address certain categories of data quality problems caused by data misinterpretation. In this paper, we first summarize some of these data quality problems and our more recent results from studying corporate householding applications and knowledge exploration. Then we outline a technical approach to a corporate householding knowledge processor (CHKP) to solve a particularly important type of corporate householding problem-entity aggregation. We illustrate the operation of the CHKP by using a motivational example in account consolidation. Our CHKP design and implementation uses and expands on the COntext INterchange (COIN) technology to manage and process corporate householding knowledge.
Knowledge about work processes is a prerequisite for performing work. We investigate whether a certain mode of knowledge, knowing-why, affects work performance and whether the knowledge held by different work roles matters for work performance. We operationalize these questions in the specific domain of data production processes and data quality. We analyze responses from three roles within data production processes, data collectors, data custodians, and data consumers, to investigate the effects of different knowledge modes held by different work roles on data quality. We find that work roles and the mode of knowledge do matter. Specifically, data collectors with why-knowledge about the data production process contribute to producing better quality data. Overall, knowledge of data collectors is more critical than that of data custodians.
The increased chance of deception in computer-mediated communication and the potential risk of taking action based on deceptive information calls for automatic detection of deception. To achieve the ultimate goal of automatic prediction of deception, we selected four common classification methods and empirically compared their performance in predicting deception. The deception and truth data were collected during two experimental studies. The results suggest that all of the four methods were promising for predicting deception with cues to deception. Among them, neural networks exhibited consistent performance and were robust across test settings. The comparisons also highlighted the importance of selecting important input variables and removing noise in an attempt to enhance the performance of classification methods. The selected cues offer both methodological and theoretical contributions to the body of deception and information systems research.
Effective requirements elicitation is essential to the success of software development projects. Many papers have been written that promulgate specific elicitation methods. A few model elicitation in general. However, none have yet modeled elicitation in a way that makes clear the critical role played by situational knowledge. This paper presents a unified model of the requirements elicitation process that emphasizes the iterative nature of elicitation as it transforms the current state of the requirements and the situation to an improved understanding of the requirements and, potentially, a modified situation. One meta-process of requirements elicitation, selection of an appropriate elicitation technique, is also captured in the model. The values of this model are: (1) an improved understanding of elicitation helps analysts improve their elicitation efforts and (2) as we improve our ability to perform elicitation, we improve the likelihood that systems we create will meet their intended customers' needs.
Trust has been a focus of research on virtual collaboration in distributed teams, e-commerce, e-learning, and telemedicine. Central to several models of trust and virtual collaboration is user's disposition to trust. This construct, however, has generally been conceptualized in as a stand-alone trait without a substantive theoretical background in personality theory. This paper advances the interpersonal circumplex model (ICM) as a theoretical framework for understanding the role of personal traits in collaboration in virtual contexts. The ICM posits that tendencies in interpersonal interaction stem from personal dispositions that can be understood in terms of dimensions of power and affiliation, fundamental constituents of user's personality. We develop a model that proposes that interpersonal traits, specifically, personality type as defined by the circumplex, affect the individual's disposition to trust, perceived trustworthiness, communication, and thereby affects willingness to collaborate and the sustainability and productivity of the collaboration. The model enables us to unpack the black box concepts of disposition to trust, faith in others, and trusting stance that are currently incorporated in theories of trust in information systems. The theory also enables explanation of trust dynamics at the dyadic and group levels. We develop propositions positing that individual's traits and dyadic complementarity are mediating factors in interpersonal trust and willingness to use new technologies and significantly affect the initiation, duration, and productivity of computer-mediated collaboration.
Heterogeneous sharing in synchronous collaboration is important with the proliferation of diverse computing environments, such as wearable computers and handheld devices. We present here a data-centric design for synchronous collaboration of users with heterogeneous computing platforms. Our approach allows clients with different capabilities to share different subsets of data in order to conserve communication bandwidth. We have built a robust middleware consisting of a distributed repository of shared data objects and a client-server-based infrastructure. Using the middleware, we have developed a framework for building collaborative applications for clients with different display and processing capabilities. We discuss the design and implementation of our middleware and framework and evaluate them by building four complex sample applications that demonstrate scalability, good performance, and high degree of code reusability.
The dramatic increase in distance learning (DL) enrollments in higher education is likely to continue. However, research on DL, which includes psychomotor, cognitive, and affective skills, is virtually nonexistent. Indeed, DL for psychomotor skills has been viewed as impossible. Laboratory coursework, which we define as including the acquisition of psychomotor, cognitive, and affective skills, has become a limiting factor in the growth of DL. What is needed is a synergistic integration of technologies and human-computer interface (HCI) principles from computer-supported collaborative learning (CSCL), collaborative learning systems, and immersive presence technologies to enable achievement of psychomotor learning objectives. This paper defines the cornputer-supported collaborative learning requiring immersive presence (CSCLIP) research area, provides a theoretical foundation for CSCLIP, and develops an agenda for research in CSCLIP to establish a foundation for the study of this emerging area. It also briefly describes a CSCLIP-based telecommunications lab Currently under development. CSCLIP is presented as a major research opportunity for information systems researchers interested in empirical research as well as technical development.
Collaboration is essential in many mission-critical activities. Consequently, numerous methods and tools are available supporting collaborative processes such as strategic planning, risk management, requirements definition, and so on. These methods typically emphasize the collaborative, value-creating activities, but there is often less emphasis on quality aspects. Quality assurance (QA) techniques have been well-known in engineering for a long time, and their effectiveness and efficiency has been empirically evaluated in many domains. In this paper, we propose to integrate repeatable QA techniques and collaborative processes. We evaluate our idea in the context of a collaborative process for requirements negotiation. We propose pre-process techniques to be used before the actual negotiation, in-process techniques for checking quality during a negotiation, as well as post-process inspection techniques. These techniques help a project team reduce unnecessary complexity and to mitigate risks stemming from defects in requirements negotiation results. We present the results of a feasibility study we conducted to test our approach.
Business process reengineering (BPR) projects have been carried out for many years, with varying degrees of success. Two key reasons for failure are distinguished from the literature: insufficient stakeholder involvement and poor analyses of the business processes. The purpose of this paper is to present a decade of field experiences with collaborative BPR. Nine BPR projects were executed and analyzed in detail, leading to the identification of 87 themes regarding the efficiency and effectiveness of the project. These themes were organized into 12 categories of lessons learned that provide insight into the best practices and together informed the evolution of the collaborative business engineering (CBE) approach to BPR. The CBE approach combines a BPR process with collaboration and simulation modeling support to address the above-mentioned reasons for failure in BPR. The field experiences show that the CBE approach can be successfully applied for BPR projects in real life.
Organizations must be creative continuously to Survive and thrive in today's highly competitive, rapidly changing environment. A century of creativity research has produced several descriptive models creativity, and hundreds of prescriptions for interventions that demonstrably improve creativity. This paper presents the cognitive network model (CNM) as a causal model of the cognitive mechanisms that give rise to creative solutions in the human mind. The model may explain why creativity prescriptions work as they do. The model may also provide a basis for deriving new techniques to further enhance creativity. The paper tests the model in an experiment where 61 four-person groups used either free-brainstorming or one of three variations on directed-brainstorming to generate solutions for one of two unstructured tasks. In both tasks, people using directed-brainstorming produced more solutions with high creativity ratings, produced solutions with higher average creativity ratings, and produced higher concentrations of creative solutions than did people using free-brainstorming. Significant differences in creativity were also found among the three variations on directed-brainstorming. The findings were consistent with the CNM.
Although user satisfaction is widely used by researchers and practitioners to evaluate information system success, important issues related to its meaning and measurement across population subgroups have not been adequately resolved. To be most useful in decision-making, instruments like end-user computing satisfaction (EUCS), which are designed to evaluate system success, should be robust. That is, they should enable comparisons by providing equivalent measurement across diverse samples that represent the variety of conditions or population subgroups present in organizations. ;Using a sample of 1,166 responses, the EUCS instrument is tested for measurement invariance across four dimensions-respondent positions, types of application, hardware platforms, and modes of development. While the results suggest that the meaning of user satisfaction is context sensitive and differs across population subgroups, the 12 measurement items are invariant across all four dimensions. The 12-item summed scale enables researchers or practitioners to compare EUCS scores across the instrument's originally intended universe of applicability.
As business-to-consumer online shopping grows, e-commerce channel providers will need to explore ways to anticipate consumers' needs to deliver an efficient shopping experience. Yet the consumers' decision-making process and its relationship to the selection of the online channel are not well understood. Utilizing Simon's decision-making model, we examined support for decision-making phases using 134 online consumers. We also extended the model to include consumers' cost savings and time savings, as well as their satisfaction with the e-commerce channel. Structural equation modeling results indicate that the online shopping channel supported the overall decision-making process. In particular, we found strong support for the design and choice phases of online consumers' decision-making process. Our results also indicate that support for the decision-making process was mediated by the cost savings and time savings gained by the online consumers and led to their greater channel satisfaction.
We present an economic model that enables the study of incentives for business-to-business (B2B) e-procurement systems investments that permit inventory coordination and improved operational control. We focus on the information technology adoption behavior of firms in the presence of transaction costs, agency costs and information uncertainty. We conclude that it is appropriate to rethink the prior theory and develop an extended transaction-cost theory perspective that incorporates the possibility of shocks. We distinguish among three kinds of B2B e-procurement systems platforms. Proprietary platform procurement systems involve traditional electronic data interchange (EDI) technologies. Open platform procurement systems are associated with e-market Web technologies. Hybrid platforms involve elements of both. We specify an analytical model that captures the key elements of our perspective, including the conditions under which strong conclusions can be made about the likely observed equilibrium e-procurement solutions of the firms. Our results explain the coexistence of both proprietary and open platforms, showing that larger firms tend to adopt costlier procurement technology solutions, such as proprietary EDI, which provides greater supply certainty. Smaller firms adopt less costly procurement technologies that entail greater supply uncertainties, such as open platform procurement systems. Two guidelines emerge for practitioners: (1) adoption of standard e-procurement platforms needs to be understood in terms of the controllable risk tradeoffs that are offered to small and large firms, and (2) gauging the business value impacts of exogenous shocks is critical to decision-making.
Understanding the cognitive activities of analysts during information requirements determination (IRD) has been recognized as a key indicator of IRD success. The research presented here examines one such cognitive activity: analysts' determination of the sufficiency of information gathered during the elicitation of requirements. Research in behavioral decision-making has identified various heuristics, or stopping rules, that are used to gauge the sufficiency of the information obtained and to terminate information acquisition. Despite the fact that analysts undoubtedly employ such stopping rules in requirements elicitation, no research has studied this phenomenon. In the present research, we present a classification of stopping rules appropriate for information gathering problems. Stopping-rule use was identified for 54 practicing systems analysts participating in a requirements determination problem in a laboratory setting. Results indicated that analyst experience influences the application of specific cognitive stopping rules, and that the use of these stopping rules has an impact on requirements determination outcomes. In addition, the use of certain stopping rules resulted in greater quantity and completeness of requirements elicited from users. Theoretical implications for the elicitation of information and practical implications for the training of systems analysts are discussed.
This study reexamines the value relevance of e-commerce announcements using an event study methodology. Event studies have become an increasingly popular technique for information systems research by giving researchers a tool to measure the notoriously elusive value of information technology. We find evidence that the traditional event study methodology may not provide an accurate measure of abnormal returns during periods of high market volatility, and propose an alternative methodology. The alternative methodology does not use an estimation period, and takes into account extreme or unusual market movements in the period in which the e-commerce announcement was made. Using the alternative methodology, we find evidence of positive abnormal returns for e-commerce announcements made in the fourth quarter of 1998, but no abnormal returns to e-commerce announcements made in the fourth quarter of 2000. We also find significant differences in value depending on the type of e-commerce initiative. In 2000, e-commerce initiatives with a digital product were valued significantly more than e-commerce initiatives with a tangible product, while in 1998 no such difference existed. In 1998, business-to-business e-commerce initiatives, e-commerce initiatives with a tangible product, and e-commerce initiatives by pure-play Internet firms were valued more than similar initiatives in 2000. The study makes a significant contribution for understanding the value of e-commerce initiatives in highly volatile markets and demonstrates how market values of e-commerce changed from 1998 to 2000. Furthermore, this study shows the importance of carefully considering both the time frame examined and the methodology used when assessing the value relevance of e-commerce initiatives as to avoid inflating the magnitude of any observed effects.
Grounded in the technology-organization-environment (TOE) framework, we develop a research model for assessing the value of e-business at the firm level. Based on this framework, we formulate six hypotheses and identify six factors (technology readiness, firm size, global scope, financial resources, competition intensity, and regulatory environment) that may affect value creation of e-business. Survey data from 612 firms across 10 countries in the financial services industry were collected and used to test the theoretical model. To examine how e-business value is influenced by economic environments, we compare two subsamples from developed and developing countries. Based on structural equation modeling, our empirical analysis demonstrates several key findings: (1) Within the TOE framework, technology readiness emerges as the strongest factor for e-business value, while financial resources, global scope, and regulatory environment also significantly contribute to e-business value. (2) Firm size is negatively related to e-business value, suggesting that structural inertia associated with large firms tends to retard e-business value. (3) Competitive pressure often drives firms to adopt e-business, but e-business value is associated more with internal organizational resources (e.g., technological readiness) than with external pressure to adopt. (4) While financial resources are an important factor in developing countries, technological capabilities become far more important in developed countries. This suggests that as firms move into deeper stages of e-business transformation, the key determinant of e-business value shifts from monetary spending to higher dimensions of organizational capabilities. (5) Government regulation plays a much more important role in developing countries than in developed countries. These findings indicate the usefulness of the proposed research model and theoretical framework for studying e-business value. They also provide insights for both business managers and policy-makers.
This study seeks to assess the business value of e-commerce capability and information technology (IT) infrastructure in the context of electronic business at the firm level. Grounded in the IT business-value literature and enhanced by the resource-based theory of the firm, we developed a research framework in which both the main effects and the interaction effects of e-commerce and IT on firm performance were tested. Within this theoretical framework, we formulated several hypotheses. We then developed a multidimensional e-commerce capability construct, and after establishing its validity and reliability, tested the hypotheses with empirical data from 114 companies in the retail industry. Controlling for variations of firm size and subindustry effects, our empirical analysis found a strong positive interaction effect between IT infrastructure and e-commerce capability. This suggests that their complementarity positively contributes to firm performance in terms of sales per employee, inventory turnover, and cost reduction. The results are consistent with the resource-based theory, and provide empirical evidence to the complementary synergy between front-end e-commerce capability and back-end IT infrastructure. Combined together, they become more effective in producing business value. Yet the value of this synergy has not been recognized in the IT payoff literature. The productivity paradox observed in various studies has been attributed to variation in methods and measures, yet we offer an additional explanation: ignoring complementarities in business value measurement implies that the impact of IT was seriously underestimated. Our results emphasized the integration of resources as a feasible path to e-commerce value-companies need to enhance the integration between front-end e-commerce capability and back-end IT infrastructure in order to reap the benefits of e-commerce investments.
For decades, information technology has been posited to have a major impact on firm performance. Investigations into this line of inquiry have almost always used constructs related to individual firm performance as their dependent measures, an approach that made sense under historical economic conditions. In recent years, however, value chains are giving way to digital supply networks with electronic interactions between tiers in the flow of goods and services. Such an environment makes it imperative to develop sophisticated measures of the performance of entire networks of firms, as opposed to individual firm performance. ;Using game-theoretic concepts, this paper explores several dimensions of networked organizational performance as a construct, as a set of measures, and as a construct within a nomology. It describes a program of research in which some empirical validation has already been completed and other work is now underway. We first validate measures for a dyadic view of network performance, followed by an n-firm perspective.
To a large extent, trust determines expected utility derived from business transactions where the trusting party is dependent upon others, but lacks control over them. In many instances, this typifies the relationship between clients and an enterprise resource planning (ERP) customization vendor. This exploratory study examines how trust is built during an ERP implementation, and the relative weight of this trust compared with the perceived qualities of the implemented ERP itself in determining clients' assessment that the business relationship with the vendor is worthwhile. The data, collected from companies that were involved in the process of implementing a new ERP with the on-site assistance of a certain customization vendor, show that all three trust antecedents suggested by Zucker's seminal study of trust-process-based, characteristic-based, and institution-based mechanisms-contribute significantly to client trust. ;The data also show that client trust in this customization vendor and the perceived usefulness of the ERP both contribute to client assessment that their business relationship with the vendor is worthwhile, showing that both getting the job done and creating a trust-based relationship contribute to this assessment. The implications of the importance of creating trust in ERP implementation and the means of doing how to do so are discussed.
Although the use of real options for valuation of information technology (IT) investments has been documented, little research has been conducted to examine its relevance for valuing and prioritizing a portfolio of projects. Complexities of IT projects along with the effect of project interdependencies raise several challenges in applying real options for prioritization of IT investments. We examine a large U.S.based energy utility firm in a deregulated environment that is considering investment in a portfolio of 31 projects to provide a range of Internet-enabled energy services to customers. Using real data on expected project benefits and costs for different competitive scenarios, we develop a nested options model that extends prior research by incorporating the impact of project interdependencies to calculate the option value of all projects. Our nested options model provides a better understanding of project interdependencies on valuation and prioritization decisions, and provides insights into the business value of IT infrastructure projects that provide the managerial flexibility to launch future projects. We present a real options portfolio optimization algorithm for dynamic multiperiod portfolio optimization by incorporating the project values based on real options analysis in a portfolio management model with budget constraints.
Although the Web has grown to several billion pages over the past few years, just a few of the Web sites get most of the visits. Such sites, called portals, attract visitors and advertisers and provide a lot of valuable content at no charge to the visitors. The portals attract a disproportionate amount of the Internet advertising dollars and have the ability to influence the success of new electronic commerce ventures. Using monthly audience data, we examine relative market shares of Web sites in search engines, travel, financial, news, and other categories. We find clear evidence of increasing disparity in page views, with the top Web sites getting an increasing share of the market. Using economic modeling, we show that this disparity is a result of a development externality that exists in this industry: the sites that have more viewers get more revenues; this in turn allows them to develop more content and attract an even greater number of viewers.
We address the concept of poaching, the risk that in any transactional relationship, information that is transferred between parties for purposes specified in the contract will deliberately be used by the receiving party for purposes outside the contract, to its own economic benefit, and to the detriment of the party that provided the information. We argue that this form of transactional risk, a component of transaction costs, is increasingly important in our service-centered, information-driven, postindustrial economy. Using case examples and a discussion of the related literature, we demonstrate and discuss the conditions under which shared information creates the potential for poaching, examine the impact and efficacy of traditional remedies for contractual problems in managing poaching, and identify additional mechanisms for managing poaching risk. Our analysis suggests that these risks and their remedies are fundamentally different in nature from those considered in previous theories of supplier relations and contractual governance.
Information technology (IT) infrastructure investments are an extremely important part of e-business and constitute a major portion of IT investments in many organizations. IT infrastructure investments include investments in connectivity, systems integration, and data storage that may be used by multiple applications. Prior research has recognized the importance of a flexible IT infrastructure as a source of competitive advantage. Evidence regarding the value of IT infrastructures is anecdotal, and there is a realization that large investments in IT infrastructures are often difficult to justify. This paper expands on the idea that the value of an IT infrastructure depends on its use in an organizational context, and presents a relatively simple approach to understanding and assessing the value of IT infrastructure investments. This approach is based on the asset valuation literature in finance. An example is provided to illustrate the proposed approach, and managerial implications are discussed.
We develop a model based on the theory of incomplete contracts for how ownership structure of interorganizational systems (IOS) can affect information exploitation and information technology adoption. Our model yields several propositions that suggest the appropriate strategic actions that a firm may take when there is potential for IOS adopters to question whether adopting the IOS will be value-maximizing. We analyze and illustrate the related strategic thinking in a real-world context involving a financial risk management IOS. We present a case study of the ownership and spin-off of RiskMetrics, developed by New York City-based investment bank, J.P. Morgan, in the late 1980s. The firm first gave RiskMetrics to its correspondent banking, treasury, and investment clients for free, in the context of its clearing account relationship services. Later, the bank spun off the product to an independent company that offered fee-based services. We model the bank's clients in terms of their heterogeneous portfolio risks, and their effects on the value a client can gain from adopting the technology. We also examine the value they may lose if their private portfolio risk information is exploited. A key roadblock to the adoption of the free service may have been the potential for strategic information exploitation by the service provider. When Morgan spun off RiskMetrics with multiparty ownership, wider adoption occurred. Our theory interprets this strategic move as an appropriate means to maximize long-term profits when information exploitation may occur.
We develop a multichannel model of separating equilibrium where a seller markets a durable good to high- and low-type consumers in two different channels-an online Internet storefront and an offline bricks-and-mortar store. We show how the digital divide, where high-type consumers dominate the online channel and low-type consumers dominate the offline channel, artificially segments the marketplace, thereby mitigating the classic cannibalization problem. This allows the seller to more efficiently market its goods to each consumer segment. We show conditions under which low-type consumers are initially served in the offline channel, but subsequently bridging the divide results in their not being served in either channel. We also examine the implications of bridging the digital divide when the seller uses delay by engaging in intertemporal price discrimination.
Enabled by advances in grid and network computing architectures for the delivery of on-demand computing services, the vision of an e-services economy in which computing will be as ubiquitous as a utility is becoming a possibility in business computing. Major firms in the computing industry such as IBM, Hewlett-Packard, and Sun Microsystems are focusing on agility and flexibility of computing resources and gearing up for their own versions of on-demand computing and information technology (IT) outsourcing solutions. The successful introduction of these new computing models requires the development of appropriate pricing mechanisms that are consistent with the enabling technologies. Our paper introduces the notion of contingent auctions to address this lacuna. In contingent auctions, users bid for computing resources in an auction, but are relieved from the contract (paying a penalty) if demand is not realized. We study different mechanisms-ranging from an advance commitment (capacity reservation) to no commitment (pay-as-you-go)-under demand uncertainty. We consider markets in which the demand for computing is uncertain and, moreover, users' value of computing and demand realization may be related. We show how the different levels of commitment affect prices, revenues, and resource utilization under different market conditions. Our results reiterate the need to address the availability-commitment dichotomy in the design of business models for on-demand computing and IT outsourcing.
This paper develops a series of two-stage duopoly models of quality-price competition and a series of monopoly models of quality-price choice in order to examine the impact of information technology (IT) investments on firm profit, firm productivity, and consumer welfare. We solve the duopoly and monopoly models for four cost functions, where each function makes a different assumption about the form of the marginal cost of production. These models are used to conduct a two-by-four comparison [(monopoly, duopoly) x (four cost functions)] of the impact of IT investments on economic performance. The analysis reveals that together market structure and cost structure play a critical role in determining the form of the relationship between IT investment and economic measures. Specifically, moving from monopoly to duopoly and moving from zero marginal cost to marginal cost as a function of quality increase the number of economic measures for which the directional effects of IT investment are ambiguous, or depend on model parameter values.
The Internet search engine market has seen a proliferation of entrants over the past few years. Whereas Yahoo was the early market leader, there has been entry by both lower-quality engines and higher-quality ones (such as Google). Prior work on quality differentiation requires that low-quality products have low prices in order to survive in a market with high-quality products. However, the price charged to users of search engines is typically zero. Therefore, consumers do not face a tradeoff between quality and price. Why do lower-quality products survive in such a market? We develop a vertical differentiation model that explains this phenomenon. The quality of the results provided by a search engine is inherently stochastic, and there is no charge for using an engine. Therefore, users who try out one engine may consult a lower-quality engine in the same session. This residual demand allows lower-quality products to survive in equilibrium. We then extend our model to incorporate horizontal differentiation as well and show that residual demand leads to higher quality and less differentiation in this market. Engines want to attract competitors' customers and therefore have a strong incentive to be similar to each other.
In this study, we conduct an empirical analysis of the performance of five popular data mining methods-neural networks, logistic rearession, linear discriminant analysis, decision trees, and nearest neighbor-on two binary classification problems from the credit evaluation domain. Whereas most studies comparing data mining methods have employed accuracy as a performance measure, we argue that, for problems such as credit evaluation, the focus should be on minimizing misclassification cost. We first generate receiver operating characteristic (ROC) curves for the classifiers and use the area under the curve (AUC) measure to compare aggregate performance of the five methods over the spectrum of decision thresholds. Next. using the ROC results, we propose a method for tuning the classifiers by identifying optimal decision thresholds. We compare the methods based on expected costs across a range of cost-probability ratios. In addition to expected cost and AUC, we evaluate the models on the basis of their generalizability to unseen data, their scalability to other problems in the domain, and their robustness against changes in class distributions. We found that the performance of logistic regression and neural network models was superior under most conditions. In contrast. decision tree and nearest neighbor models yielded higher costs, and were much less generalizable and robust than the other models. An important finding, of this research is that the models can be effectively tuned post hoc to make them cost sensitive, even though they were built without incorporating misclassification costs.
Creating electronic communities is a critical venture in the digital economy. However, fraud and misrepresentation have led to widespread skepticism and distrust of electronic communities. We develop an evolutionary model to explore the issue of trust within an electronic community from a dynamic process perspective. This model emphasizes large populations, continuous change in community memberships, and imperfect information and memory. As the term trust is often used in the context of individual interaction, at a group level we propose using the term health to measure the sustained competitive advantages of honest members over cheaters throughout the evolution of a community. We find conditions under which an electronic community is healthy and attracts outside population. We find that many factors. such as information dissemination speed, honest players' payoffs and possible losses. new community members' initial trust status, and the replacement rate of community members, all affect the health of an electronic community, and that some of them also affect a community's size. We then discuss the implications of our research for e-community practices.
A feature central to the success of e-commerce Web sites is the design of an effective interface to present product information. However, the suitability of the prevalent information formats in supporting various online shopping tasks is not known. Using the cognitive fit theory as the theoretical framework, we developed a research model to investigate the fit between information format and shopping task, and examine its influence on consumers' online shopping performance and perceptions of shopping ping experience. The competition for attention theory from the marketing literature and the scanpath theory from vision research were employed to support the analyses. An experiment was conducted to examine the effects of two types of information formats (list versus matrix) in the context of two types of shopping tasks (searching versus browsing). The results show that when there is a match between the information format and the shopping task, consumers can search the information space more efficiently and have better recall of product information. Specifically, the list format better supports browsing tasks, and the matrix format facilitates searching tasks. However, a match between the information format and the shopping task has no effect on cognitive effort or attitude toward using the Web site. Overall, this research supports the application of the cognitive fit theory to the study of Web interface design. It also demonstrates the value in integrating findings from cognitive science and vision research to understand the processes involved. As the information format has been shown to affect consumers' online shopping behavior, even when the information content is held constant, the practical implications for Web site designers include providing both types of information format on their Web sites and matching the appropriate information format to the individual consumer's task.
The widespread use of information technology (IT) to create electronic linkages among supply chain partners with the objective of reducing transaction costs may have unintended adverse effects on supply chain flexibility. Increasing business dynamics, changing customer preferences, and disruptive technological shifts pose the need for two kinds of flexibility that interenterprise information systems must address-the ability of interenterprise linkages to support changes in offering characteristics (offering flexibility) and the ability to alter linkages to partner with different supply chain players (partnering flexibility). This study explores how enterprises in supply chains may forge supply chain linkages that enable both types of flexibility jointly, and allow them to deal with ubiquitous change. ;Drawing on March and Simon's coordination theory, we propose two design principles: (1) advance structuring of interorganizational processes and information exchange that allows partnering organizations to be loosely coupled, and (2) IT-supported dynamic adjustment that allows enterprises to quickly sense change and adapt their supply chain linkages. This study reports on a survey of 41 supply chain relationships in the IT industry. For design principle, our empirical investigation of factors shows (1) that modular design of interconnected processes and structured data connectivity are associated with higher supply chain flexibility, and (2) that deep coordination-related knowledge is critical for supply chain flexibility. Also, sharing a broad range of information with partners is detrimental to supply chain flexibility, and organizations should instead focus on improving the quality of information shared. ;For industry managers, the study provides clear insights for information infrastructure design. To manage their interdependencies, enterprises need to encapsulate their interconnected processes in modular chunks, and support these with IT platforms for information exchange in structured formats. Enterprises also need to nurture their execution capabilities by putting in place the information systems to process information exchanged with partners, augmenting their understanding of factors such as how partner actions need to trigger adaptive responses. For researchers, the study initiates a new stream of theorizing that focuses on the role of the information infrastructure in managing the tension between competing goals of offering flexibility and partnering flexibility.
This study investigates the effect of varying project complexity on the group interaction processes of small information technology (IT) project teams. The projects included two complex tasks (i.e., LAN and WAN development tasks) and a less complex development task (i.e., a small business Web site development task). The study found that project complexity can affect the group interaction process. Participants reported significantly higher expectations, group integration, communication, and participation while working on less complex projects. Efforts to organize project personnel and define roles were more effective with less complex projects. Power struggles and noninvolvement remained a problem for teams regardless of project complexity. This study identifies and confirms key problem areas that can lead to project failure as IT projects become more complex. The results should interest both researchers and information systems managers, because the study is among the first to extend the common body of knowledge concerning group interaction and task complexity to IT projects.
The majority of the reported research and development efforts on automated techniques and tools for conceptual database design have focused on design from first principles. Very few have used case-based reasoning, where cases of conceptual design are stored, indexed, and used for future designs. Furthermore, there is a general lack of reported research on validating and verifying such systems. In this paper, we describe our approach in using case-based reasoning for conceptual database design. To test and demonstrate the feasibility of our approach and its theoretical foundation, two prototype systems were constructed. In the absence of existing matching conceptual design constructs, the first system uses first principles of conceptual design to assist a human designer in arriving at a design for a new problem. In contrast, the second system uses constructs from previously stored design cases. The two are tightly integrated. A novel approach in structuring the case base was developed. Unique aspects of the case-base architecture and its learning mechanism are described. In order to measure user preference, an experiment was designed and conducted. Findings indicate that reuse of schemata not only is preferred by the users over the design from the first principles, but also results in fewer errors.
This paper conducts a two-period dynamic analysis of sourcing mode choices for e-commerce projects implemented by large firms during 1999-2002. We differentiate e-commerce assets that are the focus of a sourcing decision in terms of whether they are in the growth or maturity stages. We also consider hybrid governance mechanisms, such as minority equity arrangements, as a potential sourcing mode in addition to the conventional distinction between insourcing (i.e., hierarchical governance) and outsourcing (i.e., market governance). The rapid evolution in e-commerce technologies and their markets during this period allows us to test whether asset maturity plays any role in sourcing decisions. Results indicate that when the strategic intent of an e-commerce project is more business focused during the growth phase, hybrid governance is preferred over hierarchical governance for sourcing of e-commerce assets. Strategic intent is found not to influence sourcing mode choices during the technology/market maturity phase. Hierarchical governance is the preferred sourcing mode during the growth phase, when task complexity is high. For managing task complexity, as technologies and their markets mature, both hierarchical and hybrid governance modes become preferable to the market governance mode.
Virtual teams cut across national, organizational, and functional boundaries, often resulting in diversity in team composition. This paper presents the results of a laboratory study involving groupware-supported, culturally homogeneous, and heterogeneous virtual teams where collaborative conflict management style, a team's cultural orientation as measured by the degree of individualism-collectivism, and group diversity affect several group performance variables. Collaborative conflict management style was positively related to performance, group diversity was found to have a moderating influence between collaborative style and group performance, and collaborative style was influenced by the individualistic-collectivistic orientations. Consistent with prior research, we found that collectivistic orientations help enhance the level of collaborative conflict management style prevailing in teams. Our research also indicates that the process to motivate team members may differ depending on their orientation.
The development of electronic commerce has been constrained by the, inability of online consumers to feel, touch, and sample products through Web interfaces, as they are able to do in conventional in-store shopping. Previous academic studies have argued that this limitation could be partly alleviated by providing consumers with virtual product experience (VPE), to enable potential customers to experience products virtually. This paper discusses virtual control, a specific type of VPE implementation, and identifies its two dimensions: visual control and functional control. Visual control enables consumers to manipulate Web product images, to view products from various angles and distances; functional control enables consumers to explore and experience different features and functions of products. The individual and joint effects of visual and functional control were investigated in a laboratory experiment, the results of which indicated that visual and functional control increased the perceived diagnosticity (i.e., the extent to which a consumer believes the shopping experience is helpful to evaluate a product) of their corresponding attribute factors, and that both visual and functional control increased consumer overall perceived diagnosticity and flow.
Web search engines have become an integral part of the daily life of a knowledge worker, who depends on these search engines to retrieve relevant information from the Web or from the company's vast document databases. Current search engines are very fast in terms of their response time to a user query. But their usefulness to the user in terms of retrieval performance leaves a lot to be desired. Typically, the user has to sift through a lot of nonrelevant documents to get only a few relevant ones for the user's information needs. Ranking functions play a very important role in the search engine retrieval performance. In this paper, we describe a methodology using genetic programming to discover new ranking functions for the Web-based information-seeking task. We exploit the content as well as structural information in the Web documents in the discovery process. The discovery process is carried out for both the ad hoc task and the routing task in retrieval. For either of the retrieval tasks, the retrieval performance of these newly discovered ranking functions has been found to be superior to the performance obtained by well-known ranking strategies in the information retrieval literature.
Understanding the successful adoption of information technology is largely based upon understanding the linkages among quality, Satisfaction, and usage. Although the satisfaction and usage constructs have been well studied in the information systems literature, there has been only limited attention to information and system quality over the past decade. To address this shortcoming, we developed a model consisting of nine fundamental determinants of quality in an information technology contexts four under the rubric of information quality (the output of an information system) and five that describe system quality (the information processing system required to produce the output). We then empirically examined the aptness of our model using a sample of 465 data warehouse users from seven different organizations that employed report-based, query based, and analytical business intelligence tools. The results suggest that our determinants are indeed predictive of overall information and system quality in data warehouse environments, and that our model strikes a balance between comprehensiveness and parsimony. We conclude with a discussion of the implications for both theory and the development and implementation of information technology applications in practice.
Information overload often hinders knowledge discovery on the Web. Existing tools lack analysis and visualization capabilities. Search engine displays often overwhelm users with irrelevant information. This research proposes a visual framework for knowledge discovery on the Web. The framework incorporates Web mining, clustering, and visualization techniques to support effective exploration of knowledge. Two new browsing methods were developed and applied to the business intelligence domain: Web community uses a genetic algorithm to organize Web sites into a tree format; knowledge map uses a multidimensional scaling algorithm to place Web sites as points on a screen. Experimental results show that knowledge map outperformed Kartoo, a commercial search engine with graphical display, in terms of effectiveness and efficiency. Web community was found to be more effective, efficient, and usable than result list. Our visual framework thus helps to alleviate information overload on the Web and offers practical implications for search engine developers.
Data mining techniques provide a popular and powerful tool set to generate various data-driven classification systems. In this paper, we investigate the combined use of self-organizing maps (SOM) and nonsmooth nonconvex optimization techniques in order to produce a working case of a data-driven risk classification system. The optimization approach strengthens the validity of SOM results, and the improved classification system increases both the quality of prediction and the homogeneity within the risk groups. Accurate classification of prostate cancer patients into risk groups is important to assist in the identification of appropriate treatment paths. We start with the existing rules and aim to improve classification accuracy by identifying inconsistencies utilizing self-organizing maps as a data visualization tool. Then, we progress to the study of assigning prostate cancer patients into homogenous groups with the aim to support future clinical treatment decisions. Using the case of prostate cancer patients grouping, we demonstrate strong potential of data-driven risk classification schemes for addressing the risk grouping issues in more general organizational settings.
In large-scale negotiation problems and in assessments of complex and uncertain environments, it is vital to analyze the different stakeholders involved and to evaluate their positions in the negotiations. This paper extends a model, which merges previous multi-issue and actor-focused methods, based on power relationships between stakeholders and their ability to bargain in order to increase their utility. The model has already used for assessing a public WLAN landscape. The paper emphasizes the dynamic application of the model we developed for experimenting the negotiation evolution, shifting positions on some issues, and exchanging positions between actors. We also claim that such forecasting analyses of negotiation landscapes can be significantly improved using more appropriate visualization support. We propose new visualization tools for analyzing negotiation outcomes, representing negotiation landscapes, and applying what-if simulations, using passive influence, expected outcome and dissatisfaction, power distribution, proximity, and negotiation maps.
The capabilities of network technologies have facilitated the growth of electronic commerce. Major issues-notably, security and product quality uncertainty-still pose serious challenges to the further adoption of electronic commerce. Traditional market transactions have a long history and well-understood protections for buyers and sellers. In the electronic markets, formal and informal mechanisms such as trusted third parties (TTP) have emerged trying to ensure safe transactions. In this paper, we investigate under what conditions people will stick to the traditional market and face-to-face transactions, and under what conditions electronic transactions will be the convention of the future. Of particular interest is the role of TTPs in facilitating online transactions. Using evolutionary game theory, we present an analytical model of buyer and seller choices and examine which patterns of transactions can be sustained. We further study how the traders' adaptive behavior may influence the outcome of the market evolution. Through this analysis, we demonstrate that the market will show divergence: for commodity products, electronic transactions through TTPs will get established as the convention for market transactions when traders use historical information about other traders' past strategies. For look and feel products, the market evolution depends on the initial distribution of the transaction strategies in the population.
Trust provides the foundation for the successful implementation and operation of a virtual community (VC). Trust is an especially relevant success factor in online health-care communities. A look at existing communities leads to the conclusion that many VCs fail to meet requirements upon which trust is established. Based on the findings in the literature and the researchers' experience, this paper describes how trust-enabling functionalities can be systematically designed and implemented in a VC for cancer patients. Consequently, the outcomes of these design measures are evaluated. The evaluation results show that supporting trust can be achieved following a two-step model. The presented components support the perceived competence and perceived goodwill of the operators and the other members. Perceived goodwill and competence then support the process of creating and sustaining trust between members as well as between members and the operators of the VC and contribute to the successful implementation and maintenance of the community. The paper concludes with a discussion on further trust-supporting components yet to be implemented and gives recommendations for further research in this area.
We draw on the resource-based. theory to examine how information systems (IS) resources and capabilities affect firm performance. A basic premise is that a firm's performance can be explained by how effective the firm is in using information technology (IT) to support and enhance its core competencies. In contrast to past studies that have implicitly assumed that IS assets could have direct effects on firm performance, this study draws from the resource complementarity arguments and posits that it is the targeted use of IS assets that is likely to be rent-yielding. We develop the theoretical underpinnings of this premise and propose a model that interrelates IS resources, IS capabilities, IT support for core competencies, and firm performance. The model is empirically tested using data collected from 129 firms in the United States. The results provide strong support for the research model and suggest that variation in firm performance is explained by the extent to which IT is used to support and enhance a firm's core competencies. The results also support our proposition that an organization's ability to use IT to support its core competencies is dependent on IS functional capabilities, which, in turn, are dependent on the nature of human, technology, and relationship resources of the IS department. These results are interpreted and the implications of this study for IS research and practice are discussed.
Information that is stored digitally is at risk for being irretrievably lost if either the methods used to retrieve the bitstream or the methods used to interpret the bitstream are lost. The Digital Rosetta Stone (DRS) model was developed as a conceptual framework for capturing and maintaining the methods necessary to retrieve digital information stored on obsolete media and to properly interpret it, though the software used to create it may also be obsolete. However, the community of those professionals concerned with this issue had not yet assessed this conceptual model. This study used the Delphi method to explore these issues with those responsible for maintaining access to digital data. Overall, the Delphi group expressed concerns about the practicality of developing the DRS, but agreed that it is an important concept that should be explored further. If found to be technologically feasible and economically desirable, the DRS could well lead to a long-term solution for recovering information that would otherwise be impossible to recover.
The advent of electronic commerce has induced many organizations to develop a Web presence and exploit the opportunities offered by the Internet. In an environment that commoditizes products and allows for easy imitative behavior through instant access to information on competitor's offerings, it is not clear how to build a sustainable competitive advantage. This study endeavors to facilitate an understanding of this complex issue. Electronic commerce competence is posited as a key driver of organizational performance, and it is argued that this effect is mediated by the generation of customer value through Web site functionality. By empirically analyzing primary and secondary data from over 100 companies, the relationship between electronic commerce competence, customer value, and both short- and long-term firm performance is examined. The results show that firms with high electronic commerce competence exhibit superior performance and that customer value generated through Web site functionality partially mediates this relationship. In addition, the results show that companies can enhance short-term performance by providing value to the customer in prepurchase situations. But in order to build customer loyalty and thus long-term performance, companies need to enhance the product ownership experience of customers.
Despite their potential to significantly reduce transaction costs for both buyers and sellers, e-marketplaces have struggled. Recent literature has examined the value propositions of e-marketplaces and proposed conceptual frameworks for their analysis. In this research, we move beyond conceptual analysis by developing a game theoretic model of return-on-investment (ROI)-driven e-marketplace participation growth. This model provides insights into expected e-marketplace growth and participation, and can be used to determine both the viability and expected long-run size of a given e-marketplace. Our results indicate that the pricing policy of the e-marketplace intermediary can affect the rate at which participation grows and, therefore, sentiment about its prospects. We focus on e-marketplaces that add value to buyers and sellers by increasing the efficiency of administrative tasks but also simultaneously add value to buyers and reduce value to sellers by lowering prices for goods purchased. Value to participants in these e-marketplaces is determined by the volume of transactions that can be conducted using the e-marketplace, resulting in a two-sided network effect-buyers reacting to sellers and sellers reacting to buyers. The game theoretic model identifies an e-marketplace equilibrium at which participation growth is predicted to stop.
As products on the Web are continually enhanced through free Web-based services that add to the product purchase experience, it is important to understand how these free services may affect pricing and customer retention strategies of an online vendor. This paper argues that product competition on the Web is not for generic products but, rather, for expected and augmented product bundles. Our findings point out that even in the absence of price premiums, variance in the ability to offer online services can affect pricing strategies and possibly contribute to online price dispersion. We then go on to suggest that online services affect a vendor's customer retention strategy as they influence the design of the augmented product. We characterize an online vendor's selection of augmenting services as a knapsack problem, and recommend that the online vendor should not only periodically reevaluate the set of services offered to satisfy the expected product requirements, but also assess the customer retention ability of his augmented product. A service does not contribute to customer retention when it has either lost its value to the customer or become required as a part of the expected product. Our solution recommends that a vendor should include new services based on the cost-to-value ratio of each service so as to remain above the loyalty threshold of a consumer. The results from our model partially explain the variety in product offerings of many online vendors, whose competency in providing Web-based services allows them to vary the generic product.
This study develops an instrument that may be used as an information systems (IS) functional scorecard (ISFS). It is based on a theoretical input-output model of the IS function's role in supporting business process effectiveness and organizational performance. The research model consists of three system output dimensions-systems performance, information effectiveness, and service performance. The updated paradigm for instrument development was followed to develop and validate the ISFS instrument. Construct validation of the instrument was conducted using responses from 346 systems users in 149 organizations by a combination of exploratory factor analysis and structural equation modeling using LISREL. The process resulted in an instrument that measures 18 unidimensional factors within the three ISFS dimensions. Moreover, a sample of 120 matched-paired responses of separate CIO and user responses was used for nomological validation. The results showed that the ISFS measure reflected by the instrument was positively related to improvements in business processes effectiveness and organizational performance. Consequently, the instrument may be used for assessing IS performance, for guiding information technology investment and sourcing decisions, and as a basis for further research and instrument development.
This paper conceptualizes and develops valid measurements of the key dimensions of information systems development project (ISDP) complexity. A conceptual framework is proposed to define four components of ISDP complexity: structural organizational complexity, structural IT complexity, dynamic organizational complexity, and dynamic IT complexity. Measures of ISDP complexity are generated based on literature review, field interviews, and focus group discussions. The measures are then refined through a systematic process and are tested using confirmatory data analyses with survey responses from 541 ISDP managers. Results support the final measurement model that consists of a second-order factor of ISDP complexity, four distinct first-order factors, and 15 measurement items. The measurement adequately satisfies the criteria for unidimensionality, convergent validity, discriminant validity, reliability, factorial invariance across different types of ISDPs, and nomological validity. Implications of the study results to research and practice as well as limitations of the study and directions for future research are discussed.
Managing electronic trading partner relationships is a key to successful development of an interorganizational systems (IOS) network. Firms often exercise their power and offer reciprocal investments to their trading partners in developing an IOS network. However, limited effort has been made to empirically validate their effects on increasing IOS usage between trading partners. This paper gauges the effects of these two relational factors-power and reciprocal investments-within the context of an electronic data interchange (EDI) network development. Moreover, the role of channel climate in increasing EDI usage is explicated with a particular focus on its determinants and impacts. With insights obtained from social exchange and transaction cost theories, a research model is developed and tested with data collected from 233 suppliers with electronic linkages via EDI with a nationally recognized retailer of home improvement supplies and materials in the United States. The customer's reciprocal investments in the form of EDI-related support are proven to be effective in increasing EDI volume and diversity. However, power exercised is found to be not effective. Suppliers' cooperation with the customer, which is influenced by perceived uncertainty, trust, and transaction-specific investments, is found to have strong effects on EDI volume and diversity. Finally, the reciprocal investments are found to be an even more effective strategy when suppliers desire to keep a more cooperative relationship with the customer.
In recent years, several organizations have implemented nonmandatory information and communication systems that escape the conventional behavioral logic of understanding acceptance and usage from a normative perspective of compliance with the beliefs of others. Because voluntary systems require users' volitional behavior, researchers have traced recent implementation failures to a lack of user commitment. However, gaps in our understanding of volitional usage behavior and user commitment have made it difficult to advance theory, research, and practice on this issue. To validate a proposed research model, cross-sectional, between-subjects, and within-subjects field data were collected from 714 users at the time of initial adoption and after six months of extended use. The model explained between 44.1 percent and 58.5 percent of the variance in adoption and usage behavior based upon direct effects of user commitment. Findings suggest that user commitment plays a critical role in the volitional acceptance and usage of such systems. Affective commitment-that is, internalization and identification based upon personal norms--exhibits a sustained positive influence on usage behavior. In contrast, continuance commitment-that is, compliance based upon social norms-shows a sustained negative influence from initial adoption to extended use. Theory development based upon Kelman's social influence framework offers new empirical insights about system users' commitment and how it affects volitional usage behavior.
The inherent riskiness of large-scale information technology (IT) outsourcing led us to investigate what motivates large-scale IT outsourcing decisions. We employed fixed-effects logistical regressions to examine publicly available data for 51 firms that announced their decisions to outsource all or a large portion of their IT function during the 1993-2001 period. Results suggest that incentives created by CEO stock options and overall compensation mix significantly influence decisions to outsource. We thus provide the first evidence of a relationship between managerial self-interest and IT outsourcing. Additional results suggest that poor overall firm performance, poor cost control, and short-term cash needs also drive large-scale IT outsourcing, but provide no evidence that firms outsource IT to reduce leverage. Overall, we conclude that CEOs consider several personal and firm-level financial factors, including factors unrelated to IT cost and performance, when making large scale IT outsourcing decisions. Our conclusion has numerous implications regarding long-term IT performance, long-term firm performance, CEO contracting and financial reporting.
This study uses Galbraith's information processing theory to examine the fit between information processing needs and information processing capability in an interorganizational supply chain context and to examine its effect on performance. Information processing needs are assessed based on various characteristics of the product and procurement environment and information processing capabilities are assessed by the level of information technology support for various activities in the procurement life cycle. A taxonomy of information processing needs and information processing capabilities is developed. The effect of the fit between information processing needs and capabilities on procurement performance is examined. The study collected data on 142 products through personal interviews and surveys, used cluster analytic techniques to develop taxonomies, and analysis of variance (ANOVA) to test the fit between needs and capability, modeled as an interaction effect. The results reveal two clusters for information processing needs and three clusters for information processing capability. ANOVA results show that the interactive effect of information needs and capability has a significant effect on performance, supporting our fit theory.
This paper addresses the understudied issue of how individually held expertise in information systems development (ISD) teams results in creativity at the team level during the development process. We develop the idea that team creativity results primarily from integration of individually held expertise of team members at the team level. We further propose the quality of intrateam relationships and knowledge complementarities that align the work of individual team members at the project level influence creativity primarily through the process of expertise integration. We use data from a field study of 142 participants in 42 ISD projects to test the proposed model. The paper makes three new contributions to the IS literature. Its key contribution lies in developing an expertise integration view of team creativity. We demonstrate the centrality of integrating individually held tacit and explicit knowledge about the problem domain and the technology at the team level in achieving team creativity. The use of a process-focused conceptualization of team creativity is especially noteworthy here. The second contribution of the paper lies in conceptually developing and operationalizing the concept of expertise integration, a mechanism by which individually held knowledge is integratively applied at the project level. Although the importance of knowledge in the ISD process is widely recognized in prior research, this is the first study to develop the concept in a operationally meaningful way. The third key contribution lies in showing that the compositional and relational attributes of ISD project teams--diverse specialized knowledge in a team, the quality of intratearn working relationships, and members' cross-domain absorptive capacity--do not engender creativity by themselves; they do so primarily because they enhance integration of individual knowledge at the project level. We offer empirical evidence for such full mediation. These findings have important theoretical and practical implications, which are discussed in the paper.
This paper develops a conceptual framework to explain employees' technology usage within organizations. Much of the prior information systems literature has assumed an underlying relationship between facilitating conditions for information technology (IT) adoption (e.g., user training, technical support, resource availability) and employees' technology use. Although these facilitating conditions are important, they do not provide a complete explanation of employees' IT usage. The reality of working in organizational settings suggests a different model of IT adoption and usage. Drawing from research on social information processing theory, and acknowledging the role of other individuals within the work context that shapes employees' learning, values, and behavior, we propose a framework to explain employees' adoption of IT and their level of usage within organizations, featuring both individual level factors and factors related to the social information processing influence of coworkers. Our results show that an employee's coworkers exert an important influence on IT usage, whereas individual-level factors exhibit more modest effects.
The risks associated with outsourcing have been the principal limitation on the growth of business process outsourcing, especially cross-border outsourcing. In addition to technological improvements in risk management, it is possible to reduce the risk of opportunistic behavior faced by the buyer by redesigning work flows and dividing work among multiple vendors, increasing the range of tasks that are now appropriate candidates for outsourcing. We provide a taxonomy of risks associated with the outsourcing of business processes. We focus on strategic risks and identify the components of this risk and the means by which it can be mitigated.
We explore daily patterns of Internet pricing for the two major retailers, Amazon.com and Barnes and Noble (BN), using data on 377 books collected over a 449-day period in 2003-4. We frame this investigation in terms of a key question: How rigid are prices on the Internet? Are there reasons to suggest that prior predictions of more flexible prices on the Internet may not have been founded on the appropriate theoretical knowledge? We find that Internet retailers, in contrast with traditional firms, adjust prices any day of the week throughout the year. Yet firms' price adjustments for books occur much less frequently than daily-every 90 days on average. For most observers of Internet-based selling, this is surprising, because most expect more frequent price adjustments-based on the quality of technological environment that supports price-setting. In fact, our results show that price-change activity appears to vary by book category, from a high of one change, on average, every 61 days for best sellers to a low of one change every 184 days, on average, for steady sellers. In addition, we learned that individual firms exhibited different patterns for their price changes: Amazon changed book prices every 222 days, whereas BN changed its book prices more frequently, every 56 days on average.
Vertical IS standards prescribe data structures and definitions, document formats, and business processes for particular industries, in contrast to generic information technology (IT) standards, which concern IT characteristics applicable to many industries. This paper explores the potential industry structure effects of vertical information systems (IS) standards through a case study of the U.S. home mortgage industry. We review theoretical arguments about the potential industry structure effects of standards for interorganizational coordination, and we compare the characteristics of XML-based vertical IS standards with those of electronic data interchange (EDI) to gauge the applicability of prior literature. We argue that the lower costs and wider accessibility of XML-based standards that use the Internet can result in significant changes to the structure of the mortgage industry. However, the nature of industry change will depend on the specific ways in which standards are implemented by organizations in the industry-there are many patterns of implementation with potentially different effects at the industry level of analysis. We illustrate these theoretical arguments with data from our case.
We present a model to investigate the competitive implications of electronic secondary markets that promote concurrent selling of new and used goods on a supply chain. In secondary markets where suppliers cannot directly utilize used goods for practicing intertemporal price discrimination and where transaction costs of resales are negligible, the threat of cannibalization of new goods by used goods becomes significant. We examine conditions under which it is optimal for suppliers to operate in such markets, explaining why these markets may not always be detrimental for them. Intuitively, secondary markets provide an active outlet for some high-valuation consumers to sell their used goods. The potential for such resales leads to an increase in consumers' valuation for a new good, leading them to buy an additional new good. Given sufficient heterogeneity in consumers' affinity across multiple suppliers' products, the market expansion effect accruing from consumers' cross-product purchase affinity can mitigate the losses incurred by suppliers from the direct cannibalization effect. We also highlight the strategic role that the used goods commission set by the retailer plays in determining profits for suppliers. We conclude the paper by empirically testing some implications of our model using a unique data set from the online book industry, which has a flourishing secondary market.
Convergence (i.e., mutual understanding) between an organization's CEO and CIO is critical to its efforts to successfully exploit information technology. Communication theory predicts that greater communication frequency and channel richness lead to more such convergence. A postal survey of 202 pairs of CEOs and CIOs investigated the effect of communication frequency and channel richness on CEO/CIO convergence, as well as the effect of convergence on the financial contribution of information systems (IS) to the organization. Convergence was operationalized in terms of the current and future roles of information technology (IT) as defined by the strategic grid. Rigorous validation confirmed the current role as composed of one factor and the future role as composed of three factors (i.e., managerial support, differentiation, and enhancement). More frequent communication predicted convergence about the current role, differentiation future role, and enhancement future role. The use of richer channels predicted convergence about the differentiation future role. Convergence about the current role predicted IS financial contribution. From a research perspective, the study extended theory about communication frequency, media richness, convergence, and the role of IT in organizations. From a managerial perspective, it provided direction for CEOs and CIOs interested in increasing their mutual understanding of the role of IT.
During the past two decades, both business managers and academic researchers have shown considerable interest in understanding how information technologies (IT) help to create competitive advantage for a firm. While recently the idea of competitive differentiation through IT has been challenged, this study contrasts the traditional thinking about competitive advantage with the resource-based view. Specifically, it is argued that by demarcating specific types of capabilities, we can contribute to better understanding of the sources of IT-based competitive advantage. Conceptually, we distinguish here between value, competitive, and dynamic capabilities as three distinct types of capabilities. Within each type, we identify specific capabilities, such as quality of the IT infrastructure, IT business experience, relationship infrastructure, and intensity of organizational learning, and present a model that describes relationships between these capabilities and competitive advantage. We then empirically test the model using data collected via a national mail survey from chief IT executives from 202 manufacturing firms. While the quality of the IT infrastructure is hypothesized as a value capability and expectedly did not have any significant effect on competitive advantage, the quality of IT business expertise and the relationship infrastructure (competitive capabilities) did. The results of the study also indicate that the intensity of organizational learning (dynamic capability) was significantly related to all of the capabilities. These results point to the importance of delineating capabilities such as relationship infrastructure that can facilitate differentiation in the marketplace, and dynamic capabilities such as organizational learning as an important antecedent to IT capability building.
Software developers face a constant barrage of innovations designed to improve the development environment. Yet stress/strain among software developers has been steadily increasing and is at an all-time high, while their productivity is often questioned. Why, if these innovations are meant to improve the environment, are developers more stressed and less productive than they should be? Using a combination of cognitive style and person-environment fit theories as the theoretical lens, this study examines one potential source of stress/strain and productivity impediment among software developers. Specifically, this paper examines the fit between the preferred cognitive style of a software developer and his or her perception of the cognitive style required by the job environment, and the effect of that fit on stress/strain and performance. Data collected from a field study of 123 (object-oriented) software developers suggest that performance decreases and stress increases as this gap between cognitive styles becomes wider. Using surface response methodology, the precise fit relationship is modeled. The interaction of the developer and the environment provides explanatory power above and beyond either of the factors separately, suggesting that studies examining strain and performance of developers should explicitly consider and measure the cognitive style fit between the software developer and the software development environment. In practice, managers can use the results to help recognize misfit, its consequences, and the appropriate interventions (such as training or person/task matching).
This research provides empirical evidence of the role of information technology (IT) as an enabler of growth in firms. We postulate that as a firm grows larger, a superior IT infrastructure increases the productivity of other inputs by controlling the complexity-related costs that the firm incurs as it increases in size. In the empirical analysis to support this view, we find that firms with high-growth expectations increase their IT spending as their free cash flow increases, whereas low-growth firms maintain a constant level of IT spending, irrespective of their free cash flow. Further, we analyze the effect of IT investments in one period on growth-related metrics in subsequent periods. We find that a superior IT infrastructure significantly reduces the cost of operations for high-growth firms in subsequent periods. In summary, the theory and empirical evidence presented in this paper point to an indirect but important contribution of the IT infrastructure to a firm's growth.
Strategic sourcing, defined as a firm's key business process to identify, evaluate, configure, and negotiate purchases in important spend categories while managing long-term supplier relationships, is playing a significant role in sourcing strategies. The adoption of e-sourcing, defined as the use of business software (for example, using application service providers to conduct online procurement auctions) to automate or augment the aforementioned key business process, has been growing rapidly in recent years. One often-cited benefit of e-sourcing is the predicted savings, which is appealing, given the increasing pressure on cost competitiveness faced by firms. Using queuing techniques, this paper develops an economic model that captures fundamental trade-offs in a firm's e-sourcing business process as characterized by communication complexity, frequency of use, and cost of delay. This allows comparisons of two widely adopted structures for e-sourcing: the centralized structure versus the decentralized structure. Conditions under which the centralized structure is favored over the decentralized structure and vice versa are identified and illustrated with numerical examples and case evidence. These findings are robust in other settings. The paper concludes with a discussion of managerial implications.
Many argue that offshoring is an inexorable trend, since a variety of information technology (IT) skills have become global commodities and they are vastly cheaper in other parts of the world. According to this view, most IT work would be drained from the United States to overseas locations. However, the loss of jobs to offshoring has increased pressure to impose restrictions. On the supply side, as IT salaries in outsourcing vendor nations increase, they become less attractive for offshoring. The literature identifies multiple factors-some enhancing, others inhibiting-that affect the growth of offshoring. In this paper, we attempt to add to that knowledge by asking, What are the mechanics by which these factors interact to produce the observed growth in IT offshoring? We use the system dynamics methodology to build a two-country simulation model of offshoring growth that captures individual cause-effect relationships generated by its supply and demand drivers. Examined as a whole, these individual relationships reveal larger feedback loops that constitute the mechanism underlying offshoring growth between the two countries. Simulation experiments show how the dynamic behavior of offshoring is likely to evolve beyond the current high-growth period. The model contributes to our understanding of offshoring by offering a causal foundation for its growth pattern. It can also be used to computationally study different scenarios of offshoring growth.
This paper uses modified economic growth theory to compare and contrast two currently available ways of digital content distribution: the client-server model and the peer-to-peer (P2P) model. We describe a monopolistic pricing scheme for distributing digital content over P2P networks that rewards peer users who actively participate in the distribution process. Our results show that digital distribution through a P2P network is more profitable and more efficient than in the corresponding client-server setting, if the pricing mechanism used provides strong incentives to users to share content. The basic results hold when the model is extended to include time-variant preferences across generations of consumers, and when the monopolist performs price discrimination based on generations. Some practical implications from the theoretical analysis are also discussed.
The basic premise of the extant literature related to electronic integration has been that the higher the integration, the higher will be the organizational performance. However, excessive electronic integration can be dysfunctional too. We make a conceptual argument that more is not always better and that the fit between contextual factors and electronic information sharing should be achieved to seek optimal channel performance. We empirically examine the fit between electronic information transfer (EIT) and contextual factors of a supply channel, our specific contribution being the assessment of fit in terms of multivariate congruence. The data required for this field study was collected from 124 managers/buyers responsible for supplier relationships in six multinational enterprises in two different industries (automobile and heavy shipbuilding) headquartered in Korea. The results ratify our hypothesis that multivariate congruence between EIT components and supply-channel contextual factors indeed exists. Follow-up drill-down analysis indicates that the monitoring component of EIT has a significant influence on demand uncertainty, and complexity-in-use is influenced by the coordination aspect of EIT. However, both the coordination and monitoring aspects of EIT are significantly relevant to interdependence of partners in a supply channel. A post hoc exploratory analysis suggests that the supply-channel performance is influenced by the fit between the contextual factors and the channel design factors. An inference of practical value that emerges from our findings is that more or less electronic integration is not the real issue. What is critical is the fit between supply-channel context and the level of electronic integration.
This research explores how multimedia vividness and the use of computer-based social cues can influence involvement with technology and decision-making outcomes. An experiment is conducted that examines the effect that increased levels of vividness (text, voice, and animation) and decision aid personality have on decision-making involvement. In addition, the influence of two individual differences, gender and computer playfulness, on decision aid involvement are investigated. The cost-benefit framework of decision making and related research on consumer information processing provide the theoretical foundation for the study and suggest how increased involvement may influence decision making. Several decision-making outcomes are measured, including decision effort, decision quality, satisfaction with the decision aid, and understanding of the decision aid. Findings indicate that personality similarity (between the user and the decision aid) and computer playfulness result in increased involvement with the decision aid. In addition, women report higher levels of involvement with the decision aid. Increased levels of multimedia vividness are found to have a contradictory effect, with animation actually reducing involvement with the decision aid. The findings are discussed in terms of theoretical contributions and practical interface design implications.
Knowledge repositories are commonly used by technical Support analysts in call center environments as a way of capturing and reusing solutions to common problems, and are generally expected to improve service quality, reduce costs, and enhance analyst learning. This study investigates why technical support analysts seek out and access knowledge from these repositories, as opposed to more traditional sources of such knowledge-col leagues and manuals. Focusing on the demand for-rather than supply of-knowledge in organizations, our research elaborates the role played by analysts' learning orientation, perceived work demands, and risk aversion in predicting their knowledge sourcing behavior. Our results include several counterintuitive findings that suggest there is not very much learning going on via technical support knowledge repositories. Analysts seem to be focused on finding recipes for solving customers' problems rather than building a better understanding of the products they support. Implications for research and practice highlight the need for more effective technologies to speed searches, the utility of a formal and visible mechanism for validating knowledge, and the inherent tension between efficiency and learning in these environments.
Investments in information technology (IT) have become crucial for firms to improve the quality of their products and services. Typically, IT cost for the same performance level declines over time. In a competitive market, a decline in IT cost over time provides a cost advantage to the later entrant, making the early entrant's investment decision problem challenging. In this paper, we study the problem of strategic IT investments in the declining cost scenario using a sequential duopoly model. Our results show that declining IT cost intensifies or relaxes competition between firms depending on whether they are serving quality- or price-sensitive markets. In both cases, the average price per unit quality decreases when the IT cost declines, which benefits consumers. We also show that if the first entrant is uncertain about the extent of its cost disadvantage, the first entrant overinvests (underinvests) in a price-sensitive (quality-sensitive) market as the degree of uncertainty increases.
Although there has been a great deal of research on impression formation, little application of that research has been made to electronic commerce. A research model was constructed that hypothesized errors, poor style, and incompleteness to be inversely related to the users' level of perceived quality of an online store. Further, this perceived quality of the online store's Web site would be directly related to users' trust in the store and, ultimately, to users' intentions to purchase from the store. An experimental study with 272 undergraduate and graduate student volunteers supported all the hypotheses. In addition, it was found that the relationship between the factors and perceived quality was mediated by the perception of the flaws. The perception of flaws rather than the actual flaws influenced users' perception of quality. Supplemental analysis also seemed to indicate a pattern of diminishing effects with each subsequent flaw.
This paper reports the results of an experiment investigating the differences between budget negotiations conducted on an electronic negotiation support system (NSS) and those conducted face-to-face. The negotiation setting consisted of a supervisor and a subordinate negotiating a performance budget for the subordinate. Results revealed that when supervisor performance expectations were incongruent with subordinate capability, face-to-face negotiations hit impasse at a significantly higher rate than NSS negotiations. These results held regardless of the amount of concession needed to reach consensus, and they support the contention that single-issue distributive negotiations, such as budget negotiations, can benefit from the use of an NSS. In a secondary analysis of subordinate performance after the budget negotiation, we found that NSS subordinates perceived more task conflict, which positively influenced postnegotiation performance, whereas face-to-face subordinates perceived less relational conflict, which worked through satisfaction to positively influence postnegotiation performance. This result adds to the literature by clarifying the roles that communication mode plays in a negotiation and a negotiation's aftermath.
This paper presents an experiment investigating the impact of context-relevant graphics on a knowledge sharing task in a technology-mediated collaborative (TMC) environment. The Cognitive Theory of Multimedia Learning (CTML) is introduced as the theoretical base for the hypotheses. The principles of multimedia and coherence from the CTML are used to hypothesize about the effectiveness of graphics embedded in TMC environments. Comprehension and transfer are used as dependent measures. Three TMC interface treatments were considered (no graphic, irrelevant graphic, relevant graphic). Hierarchical analysis of covariance (HANCOVA) comparing TMC treatments indicated no significant differences in comprehension; however, transfer scores for the TMC teams with context-relevant graphics were significantly higher than the other TMC teams. Although adding graphics to the collaborative interface improves the level of understanding developed within a group, the graphics need to be context relevant to be effective. These findings support the coherence and multimedia principles and provide guidance for designers of TMC environments.
From the social network perspective, this study explores the ontological structure of knowledge sharing activities engaged in by researchers in the field of information systems (IS) over the past three decades. We construct a knowledge network based on coauthorship patterns extracted from four major journals in the IS field in order to analyze the distinctive characteristics of each subfield and to assess the amount of internal and external knowledge exchange that has taken place among IS researchers. This study also tests the role of different types of social capital that influence the academic impact of researchers. Our results indicate that the proportion of coauthored IS articles in the four journals has doubled over the past 25 years, from merely 40 percent in 1978 to over 80 percent in 2002. However, a significant variation exists in terms of the shape, density, and centralization of knowledge exchange networks across the four subfields of IS - namely, behavioral science, organizational science, computer science, and economic science. For example, the behavioral science subgroup, in terms of internal cohesion among researchers, tends to develop the most dense collaborative relationships, whereas the computer science subgroup is the most fragmented. Moreover, external collaboration across these subfields appears to be limited and severely unbalanced. Across the four subfields, on average, less than 20 percent of the research collaboration ties involved researchers from different subdisciplines. Finally, the regression analysis reveals that knowledge capital derived from a network rich in structural holes has a positive influence on an individual researcher's academic performance.
Knowledge management to facilitate the creation, storage, transfer, and application of knowledge in organizations has received wide attention in practice and research in the past several years. Often cited as a significant challenge in knowledge management practices is the issue of organizational culture. Although many studies raise the issue of organizational culture's influence on knowledge management success, few investigate the way in which this influence manifests itself. This paper aims to explore how organizational culture influences knowledge management practices. Using a case study method, we examine the cultural values and knowledge management approaches within a large global information services company and one of its knowledge communities. The findings highlight the influence of culture on the use of knowledge management technologies and the outcomes of such use.
The technology acceptance model (TAM) is one of the most widely used models of information technology (IT) adoption. According to TAM, IT adoption is influenced by two perceptions: usefulness and ease of use. In this study, we extend TAM to the mobile commerce context. We categorize the tasks performed on wireless handheld devices into three categories: (1) general tasks that do not involve transactions and gaming, (2) gaming tasks, and (3) transactional tasks. We propose a unified conceptual model for wireless technology adoption. In this model, task type moderates the effects of four possible determinants: perceived usefulness, perceived ease of use, perceived playfulness, and perceived security. We postulate that, under the mobile context, user intention to perform general tasks that do not involve transactions and gaming is influenced by perceived usefulness and perceived ease of use, user intention to play games is affected by perceived playfulness, and user intention transact is influenced by perceived usefulness and perceived security. A survey was conducted to collect data about user perception of 12 tasks that could be performed on wireless handheld devices and user intention to use wireless technology. Multiple regression analyses supported the proposed research model.
Knowledge represents a critical resource in the modern enterprise. But it is dynamic and distributed unevenly. Capitalizing on this dynamic resource for enterprise performance depends upon its rapid and reliable flows across people, organizations, locations, and times of application. From a technological perspective, this points immediately to the design of information systems to enhance knowledge flows. The problem is, the design of information systems to enhance knowledge flows requires new understanding. The research described in this paper concentrates on understanding the dynamics of knowledge phenomenologically and on developing and applying techniques for modeling and visualizing dynamic knowledge flows and stocks. We draw key, theoretical concepts from multiple literatures, and we build upon integrative modeling work that composes a parsimonious, multidimensional, analytical framework for representing and visualizing dynamic knowledge. We then conduct field research to learn how this theoretical framework may be used to model knowledge flows in practice. By focusing this empirical work on an extreme organization and processes that involve and rely upon tacit knowledge, we illustrate how dynamic knowledge patterns can inform design in new ways. New chunks of kernel theory deriving from this fieldwork are articulated in terms of a propositional model, which provides a basis for the development of testable design theory hypotheses.
Identifying attribute correspondences across heterogeneous databases is a critical and time-consuming step in integrating the databases. Past research has applied correlation analysis techniques to explore correspondences between attributes. These techniques, however, are appropriate for numeric attributes that are linearly related. This paper proposes an information-theoretic approach to exploring correspondences between attributes in heterogeneous databases. The proposed approach is applicable to character attributes, as well as to numeric attributes, regardless whether or not they are linearly related. It overcomes some serious shortcomings of previous approaches based on correlation analysis and has much broader applicability. The proposed procedure samples both matching and nonmatching pairs of records from the databases under consideration, applies matching functions to compare pairs of attributes, and then uses the mutual information to measure the dependency between a matching function as applied to a pair of attributes and the class (i.e., matching or nonmatching) of a pair of records. A high mutual information index implies a potential attribute correspondence, which is presented to the analyst for further evaluation. The paper also presents some empirical results demonstrating the utility of the proposed approach.
eXtreme Programming (XP) is a well-known agile software development method. While a number of reports have been published on explaining the XP methodology and the perceived benefits when using XP for system development in recent years, less is known about the actual operationalization of the XP principles. This paper presents an action research study reporting on the experiences of implementing the XP methodology in a development project for a Web-based, distributed information system. The goal of this research was to increase the understanding of how to effectively operationalize XP techniques so that the system being developed catered to today's fast-paced technological environment by allowing the developers to respond quickly to innovative and changing requirements. ;Overall, the research indicates that most of the XP principles could be effectively implemented; however, three of the principles required modification (i.e., testing, pair programming, customer collocation). Several benefits resulted from the usage of XP. The rapid prototyping enabled information technology developers and users to clarify system requirements, communicate openly, quickly build rapport, and create an interface that was easy to use and learn. Further, the research found that where the technology was new or foreign to the development team and the user, the XP process was flexible enough to support several iterations of technology and produce prototypes in a timely manner. Pair programming appeared to work effectively and offer value; however, it is not always practically feasible.
The great potential of speech recognition systems in freeing users' hands while interacting with computers has inspired a variety of promising applications. However, given the performance of the state-of-the-art speech recognition technology today, widespread acceptance of speech recognition technology would not be realistic without designing and developing new approaches to detecting and correcting recognition errors effectively. In seeking solutions to the above problem, identifying cues to error detection (CERD) is central. Our survey of the extant literature on the detection and correction of speech recognition errors reveals that the system-initiated, data-driven approach is dominant, but that heuristics from human users have been largely overlooked. This may have hindered the advance of speech technology. In this research, we propose a user-centered approach to discovering CERD. User studies are carried out to implement the approach. Content analysis of the collected verbal protocols lends itself to a taxonomy of CERD. The CERD discovered in this study can improve our knowledge on CERD by not only validating CERD from a user's perspective but also suggesting promising new CERD for detecting speech recognition errors. Moreover, the analysis of CERD in relation to error types and other CERD provides new insights into the context where specific CERD are effective. The findings of this study can be used to not only improve speech recognition output but also to provide context-aware support for error detection. This will help break the barrier for mainstream adoption of speech technology in a variety of information systems and applications.
In order to build a digital inclusive society, both government and nongovernment organizations in countries such as China, Japan, Korea, Singapore, Taiwan, the United Kingdom, and the United States have been offering training programs to the general public and establishing communitywide public access computer facilities in recent years. However, offering training programs and enabling access to facilities are not sufficient on their own if, due to other reasons, the socially disadvantaged groups do not choose to make use of the facilities. As an exploratory investigation, this study focuses on the voluntary adoption of these facilities (typified by the Internet) by one such disadvantaged group-older adults. In particular, this study investigates the role of Internet self-efficacy and Outcome expectations in older adults' usage of the Internet through a three-part longitudinal study, involving almost 1,000 participants. A theoretical model based on social cognitive theory was developed and empirically tested through both surveys and lab experiments. Behavioral modeling training courses were offered to adults age 55 or older in the study over a one-year period. Questionnaire surveys and cognitive knowledge assessments were conducted. In general, the findings in the longitudinal study (including three repeated measures) validated the affects of Internet self-efficacy and outcome expectations on usage intention, and the important roles of support and encouragement in the formation of self-efficacy and outcome expectations. Limitations and implications are discussed.
Advances in information and communications technology have made possible collaborative activities in virtual settings. Virtual settings can significantly expand the knowledge resources available, yet they also create additional challenges to the already difficult activities of collaborating. The purpose of this research is to provide a better understanding of how collaborative activities in virtual settings enable the different parties to achieve their desired objectives by examining them from a knowledge management perspective. Three aspects of knowledge management-knowledge transfer, knowledge discovery, and knowledge creation-are examined in the context of telemedicine projects. The findings indicate that an association exists between the types of collaborative activities engaged in virtual settings and the effects such projects are perceived as having. While this research focuses only on virtual collaborative activities in health care, it is likely that these findings are applicable to other industries engaged in such activities in virtual settings.
Historically, information systems (IS) researchers have questioned which research paradigms, activities, and methods IS research should follow. In this paper, we argue that different research methods and activities may interact with each other, different research paradigms may complement each other due to such interactions, and therefore, a multimethodological, cross-paradigm research approach may result in better IS research than a singular approach. Three existing multimethodological IS research frameworks are reviewed and summarized into an integrated approach. Two types of interactions between different research methods across system evaluation and theory testing research activities are identified. A three-year research study about a computer-based training system for deception detection (Agent99 Trainer) provides a concrete example to demonstrate the existence and research benefits of these two types of interactions, as well as the benefits of a multimethodological, cross-paradigm IS research approach.
Conventional approaches to business-to-business (B2B) negotiation use primary negotiation terms (PNTs) such as price or order quantity for modeling and analysis, but pay little attention to Such secondary negotiation terms (SNTs) as resource availability and corporate culture. This paper argues that SNTs also contribute to good negotiation decisions because PNTs and SNTs are closely interlinked in the form of causal relationships. Moreover, B2B negotiation demands a practical and useful framework that can reuse past negotiation knowledge and perform what-if analysis. This paper proposes a framework that consists of formalization, reuse, and problem-solving phases. The framework first formalizes TAKBN (tacit knowledge about B2B negotiation) with both PNTs and SNTs using a cognitive map and case-based reasoning, then stores them in case bases as cases that can be retrieved for later use and problem solving. This framework provides a platform with which decision makers can study past B2B negotiation cases, apply them to current B2B negotiation problems, and simulate different negotiation situations before making decisions. The framework has been tested using two practical scenarios. A structured, 13-item questionnaire was rigorously developed and applied to evaluate the validity of the proposed framework based on 16 B2B negotiation experts' judgments. Statistical tests proved that the proposed framework could improve decision performance significantly in B2B negotiations.
This study develops an alternative methodology for the risk analysis of information systems security (ISS), an evidential reasoning approach under the Dempster-Shafer theory of belief functions. The approach has the following important dimensions. First, the evidential reasoning approach provides a rigorous, structured manner to incorporate relevant ISS risk factors, related countermeasures, and their interrelationships when estimating ISS risk. Second, the methodology employs the belief function definition of risk-that is, ISS risk is the plausibility of ISS failures. The proposed approach has other appealing features, such as facilitating cost-benefit analyses to help promote efficient ISS risk management. The paper elaborates the theoretical concepts and provides operational guidance for implementing the method. The method is illustrated using a hypothetical example from the perspective of management and a real-world example from the perspective of external assurance providers. Sensitivity analyses are performed to evaluate the impact of important parameters on the model's results.
This paper examines how information technology (IT) transforrns relations across fields of practice within organizations. Drawing on Bourdieu's practice theory, we argue that the production of any practice involves varying degrees of embodiment (i.e., relying on personal relationships) and objectification (i.e., relying on the exchange of objects). We subsequently characterize boundary-spanning practices according to their relative degrees of embodiment and objectification. We distinguish between market-like boundary-spanning practices, which rely primarily on an objectified mode of practice production, from community-like practices, which involve mostly the embodied mode of practice production. IT is then conceptualized as a medium for sharing objects in the production of practices. As such, IT use allows for the sharing of objects without relying on embodied relationships. ;We use data from an in-depth ethnographic case study to investigate how IT was used to transform community-like boundary-spanning practices within an organization into market-like ones. Moreover, we demonstrate how, as IT was used to support the exchange and combination of depersonalized objects, other aspects of the practice (such as the roles of intermediaries and the nature of meetings) also changed. The related changes in these diverse aspects of a boundary-spanning practice supported the trend toward greater objectification. IT use also increased visibility of the terms associated with object exchange. This increased visibility exposed the inequity of the exchange and encouraged the disadvantaged party to renegotiate the relationship.
Despite the fact that several event studies have investigated the market's reaction to information technology (IT) investment announcements, little is known about how specific transactional risks influence the market value of a firm. This study examines stock market data to assess investors' responses to various transactional risks associated with IT outsourcing. More specifically, we develop and test several hypotheses to understand how transactional risks that arise due to a range of factors (i.e., the size of outsourcing contracts, difficulties in performance monitoring, asset specificity of IT resources, vendor capability, and the lack of cultural similarity between client and vendor firms) influence investors' reactions to IT outsourcing announcements. Our results indicate that most of these factors indeed significantly influence investors' perceptions of the risks involved in IT outsourcing. We discuss these findings in a larger organizational context and offer implications for both research and practice. In particular, our study offers a theoretical rationale for why negative reactions to IT outsourcing announcements may occur, while providing practitioners with several means by which they can increase the informational value of outsourcing arrangements.
The integration of heterogeneous information systems has always been problematic in health-care organizations, as it is associated with the delivery of key services and has high operational costs. Therefore, health-care organizations are looking for new means to increase their functional capabilities and reduce integration cost. In addressing this need, enterprise application integration (EAI) technology has emerged to facilitate systems integration, enhance the quality of services, and reduce integration costs. Despite the application of EAI in other sectors, its adoption in health care is slow. In seeking to build on the limited normative research surrounding EAI, the authors of this paper focus on the evaluation of factors that influence EAI adoption in the health-care sector. In doing so, using fuzzy cognitive mapping as a technique to identify causal interrelationships among the EAI adoption factors. This approach will enhance the quality of the evaluation process and emphasizes the importance of each factor and its interrelationship with other factors. The outcomes shown in this paper will support health-care organizations' decision makers in exploring the implications surrounding EAI adoption.
In this paper, we use concepts from actor-network theory (ANT) to interpret the sequence of events that led to business process change (BPC) failure at a telecommunications company in the United States. Through our intensive examination of the BPC initiative, we find that a number of issues suggested by ANT, such as errors in problematization, parallel translation, betrayal, and irreversible inscription of interests, contributed significantly to the failure. We provide nine abstraction statements capturing the essence of our findings in a concrete form. The larger implication of our study is that, for sociotechnical phenomena such as BPC with significant political components, an ANT-informed understanding can enable practitioners to better anticipate and cope with emergent complexities.
As online information dissemination and e-commerce transactions become globally popular, understanding the cultural aspects of Web site documents will gain critical importance. Hidden cultural dimensions could facilitate or inhibit the usability and communication effectiveness of Web sites. However, few studies have investigated the existence of cultural dimensions in Web sites. This study identifies cultural signifiers of Web documents as they relate to the masculinity-femininity dimension. We adopt an interpretive approach for investigating, identifying, and categorizing masculinity-femininity signifiers. Comparing and contrasting Web sites aimed predominantly at either men or women, we use grounded theory for constant comparison and categorization of data. The interpretive analysis is carried out within a framework of hermeneutics. Drawing from the literature of signs (semiology), we identify the signifiers and myths for the masculinity and femininity of Web documents, and report on the possible presence of masculine and feminine androgyny. Following the dictum of grounded theory, we present support for our results from theories and findings in diverse fields of study. We then report on the contributions of our research in three ways. First, the knowledge of cultural signifiers raises managers' and researchers' awareness of cultural contents of Web documents, and may lead to improvement in the clarity and communication effectiveness of Web documents. Second, our work brings forth contrasts and contradictions inherent in masculine and feminine modes of Web document development, raising questions about cultural messages within Web documents that could distort communication and promote cultural values not shared by members of the targeted community. Third, we introduce the concept of androgyny as playing a possible role in reducing such distortions.
We analyze an e-market design that allows multiple market segments to be served simultaneously with a single generalized combinatorial auction. The mechanism uses rule-based bids designed to accommodate various kinds of bidders, such as those more sensitive to price or those more restricted in their requirements. We demonstrate experimentally-using agent-based simulation of the actual market for television advertising slots-that the rule-based approach effectively handles the wide range of market segments, while maintaining buyer and seller surplus and efficiently allocating goods.
This paper suggests and empirically supports the propositions that a link between two organizations' Web sites will have simultaneous effects on trust in both the link sender and the link recipient, and that these effects result from interactions among the reputation of the link recipient, trust in the link sender, and the perceived relationship of the linked organizations. The study finds that the perceived relationship caused by a link leads to positive effects for the less reputable of the linked organizations, but negative effects for the more reputable organization. These effects are exaggerated or attenuated depending on the reputation of the organization that sends the link. The effect of presenting the link as an advertisement or a link to a partner was also examined, but no effect was uncovered, raising the question of how organizations may effectively differentiate links on their Web sites.
Advances in online technologies and bandwidth availability have opened new vistas for online distribution of digital goods, but potential benefits for consumers are juxtaposed against challenges for retailers. Here, we investigate one type of digital experience good-music-whose market environment includes the very real presence of online piracy. Although arguments abound for and against online distribution of such digital goods, little research exists in this area. We develop a model of consumer search for such an experience good, and study different emerging market environments for retailers, where consumers can pirate music online. Retailer cost to publishers is modeled using a variety of licensing schemas. Survey results, together with data from online sharing networks, are utilized to validate a key assumption. Finally, computational analysis is used to develop insights that cannot be obtained analytically. Our results indicate that decreasing piracy is not necessarily equivalent to increasing profit, and online selling strategies can provide additional profits for a traditional retailer even in the presence of piracy. We show that leading strategies for business in such goods should include pricing options, provision of efficient search tools, and new licensing structures.
There are many benefits of enterprise resource planning (ERP) systems, but their implementation is both complicated and difficult because the product spans functional silos and involves many internal and external entities. An ERP system is the outcome of social processes, and different ERP systems can embody distinct social arrangements when developed in different cultural contexts. Such social arrangements are difficult to change due the closure effect of technology stabilization. This leads to various misfit problems, both during and after ERP implementation, causing adverse effects on delivered ERP quality. With a survey of 85 ERP implementation cases in Taiwan, this study derives and empirically tests the main as well as the interaction effects of the country of origin of the ERP package, consultant quality, top management support, and user support of the ERP system quality as perceived by the client after implementation. The results demonstrate the important role of the country of origin of the ERP package and consultant quality in configuring a high-quality ERP system and alleviating the negative effect of misfit problems.
A new model of competition, where competition is among supply chain networks rather than individual firms, is transforming traditional market-based buyer-supplier relations to one of competition among cooperative sets. In order to integrate and realize performance gains from participating in cooperative supply networks, the importance of information sharing across the supply chain has been emphasized in different literature streams. In this study, we examine the relational antecedents of this critical aspect of supply chain integration-that is, information flow integration. ;Our objective is to investigate the relationship between relational orientation of the focal firm, as characterized by (1) long-term orientation of its supply chain relationships, (2) asset specificity, and (3) interaction routines and the information flow integration between a firm and its supply chain partners. A research model was developed and data were collected from 110 supply chain and logistics managers in manufacturing and retail organizations. Our results suggest that tangible and intangible resources invested in supply chain relationships enable the integration of information flows with supply chain partners. Specifically, formal and informal interaction routines that take time and effort to develop enable integration of informational flows across a firm's supply chain. Investments in relation-specific assets and long-term orientation in relationships enable the development of these interaction routines.
The Internet offers several tools such as shopping bots and search engines that help potential buyers search for lower prices. This paper defines buyers' online search strategy as using one or more of these tools to search for lower prices, and empirically investigates the validity of economics of information search theory in explaining buyers' choice of a particular online search strategy. We find that buyers' attitudes toward the price offered by their preferred online seller, their perception of online price dispersion, and their awareness of shopping agents have a significant effect on their choice of online search strategy. An understanding of buyers' choice of online search strategies can help an online seller to estimate its expected probability of making an online sale, optimize its online pricing, and improve its online promotional and advertising activities.
As real options analysis (ROA) is being applied to increasingly complex information technology (IT) investment problems, a concern arises over the use of heuristic ROA models that are simpler to apply but can produce overvaluations. A good example is the application of a heuristic nested variation of the Black-Scholes (BS) model to the evaluation of interrelated IT investments as nested options. This particular heuristic BS model could overvalue by more than 100 percent. Using a binomial model that is custom-tailored to a generic IT investment embedding nested options as the baseline, we identify conditions under which the degree of overvaluation of this heuristic BS model is severe and unpredictable. Moreover, upon examining the structure of the custom-tailored binomial model, we identify the reason for overvaluation and derive a more accurate nested variation of the BS model. These findings should serve as a cautionary message about the use of untested heuristic ROA models.
We examined the relationship between information technology (IT) and organizational performance in the U.S. life/health insurance industry by applying and testing Galbraith's information processing theory and strategic contingency theory. Rather than focusing on resource allocations in IT, we instead examined the manner in which IT is deployed in organizations through information processing design choices. Our results suggest that while some information processing design choices are generally related to organizational performance, others should be matched to a specific strategic posture. For example, we found that all organizations benefit from using IT to increase cost-effectiveness. However, other uses of IT should be more closely aligned with an organization's strategy. In particular, domain offensive organizations should align their IT systems to focus on the front end of their operations and understanding customer needs, which will spur further innovation.
What is the role of information technology (IT) in enabling the outsourcing of manufacturing plant production processes? Do plant strategies influence production outsourcing? Does production process outsourcing influence plant performance? This research addresses these questions by investigating the role of IT and plant strategies as antecedents of production outsourcing, and evaluating the impact of production outsourcing and IT investments on plant cost and quality. We develop a theoretical framework for the antecedents and performance outcomes of production outsourcing at the plant level. We validate this theoretical framework using cross-sectional survey data from U.S. manufacturing plants. Our analysis suggests that plants with greater IT investments are more likely to outsource their production processes, and that IT investments and production outsourcing are associated with lower plant cost of goods sold and higher product quality improvement. Our research provides an integrated model for studying the effects of IT and production outsourcing on plant performance.
Previous studies surrounding the DeLone and McLean model of information systems (IS) success have called for future research and further examination of its measure in different contexts. We draw from the literature on strategic IS planning and organizational culture to contextualize the DeLone and McLean model. There is some evidence that a high-quality information technology (IT) plan leads to system success; therefore, we empirically examine the inclusion of the IT plan quality construct as an antecedent to IS success. We also empirically examine the relationships among constructs in the model of IS success in the context of different corporate cultural types-entrepreneurial and formal. The results provide strong support for the research model and suggest that variations in IS success are explained by the quality of the IT plan and the corporate culture exhibited by a firm. We discuss implications related to our finding that IT plan quality has a greater impact on IS success in organizations that exhibit an entrepreneurial corporate culture than in those that exhibit a formal corporate culture. Furthermore, we discuss how the relationships in the DeLone and McLean model of IS success differ in diverse corporate cultural types and the meaning of these differences.
We analyze how online reviews are used to evaluate the effectiveness of product differentiation strategies based on the theories of hyperdifferentiation and resonance marketing. Hyperdifferentiation says that firms can now produce almost anything that appeals to consumers and they can manage the complexity of the increasingly diverse product portfolios that result. Resonance marketing says that informed consumers will purchase products that they actually truly want. When consumers become more informed, firms that provide highly differentiated products should experience higher growth rates than firms with less differentiated offerings. We construct measures of product positioning based on online ratings and find supportive evidence using sales data from the craft beer industry. In particular, we find that the variance of ratings and the strength of the most positive quartile of reviews play a significant role in determining which new products grow fastest in the marketplace. This supports our expectations for resonance marketing.
Information technology (IT) value remains a serious concern of management today, especially how it should be measured and how it is created. Although we have made significant progress at the firm and aggregate levels of analysis, process-level analysis is still in its infancy, and there is a need for a systematic basis for identifying IT effects. We provide such an approach by developing two models: a process performance model of how system characteristics enhance process output and quality and an economic performance model linking process performance to the economic performance of the firm. We apply these models to global trade services in international banking. We obtained estimates for key variables in both models and general support for the approach. We interpret our results and discuss the merits of the process-level approach for the assessment of IT-reliant work systems.
The existence of product complementarities is especially relevant in network-type industries, such as information technology and communications, where systems of complementary components made by different manufacturers have to be assembled. Relying on the characteristics of software markets and drawing on the economic theory of complementarities, this paper investigates how complementarities create value in mergers and acquisitions between software companies. We introduce and empirically validate the software stack as a structure to measure complementarities. In a sample of mergers and acquisitions, in which either the acquirer or the target is a software firm, we find values of abnormal returns consistent with previous results. However, when we use the concept of stack, we find an inverse curvilinear relationship between abnormal returns and the distance between acquirers and targets in various layers of the stack.
Providing profitable online content has been an elusive goal, challenging many companies such as the New York Times, Disney/ABC/ESPN, and Microsoft/ Slate. Charging for content has been hit-or-miss, attributable to a lack of generally applicable models of information value. Previous studies in the management information systems literature emphasized extrinsically motivated content (addressing tangible gains), while many sites target intrinsic goals such as entertainment or education. This study examines potential factors influencing willingness to pay for intrinsically motivated online content. Data from 392 college students indicate that even when analyzing content whose potential rewards are intangible and nonquantifiable, potential consumers focus on expected benefits as the main antecedent for willingness to pay. Other antecedents, such as perceived quality and provider reputation, only affected willingness to pay indirectly through expected benefits. Researchers are offered a baseline model for future study, and practitioners are advised to provide initial visitors a clear message about benefits of use to entice them to pay for content.
As electronic commerce and knowledge economy environments proliferate, both individuals and organizations increasingly generate and consume large amounts of online information, typically available as textual documents. To manage this ever-increasing volume of documents, individuals and organizations frequently organize their documents into categories that facilitate document management and subsequent access and browsing. Document clustering is an intentional act that should reflect individual preferences with regard to the semantic coherency and relevant categorization of documents. Hence, effective document clustering must consider individual preferences and needs to support personalization in document categorization. In this paper, we present an automatic document-clustering approach that incorporates an individual's partial clustering as preferential information. Combining two document representation methods, feature refinement and feature weighting, with two clustering methods, precluster-based hierarchical agglomerative clustering (HAC) and atomic-based HAC, we establish four personalized document-clustering techniques. Using a traditional content-based document-clustering technique as a performance benchmark, we find that the proposed personalized document-clustering techniques improve clustering effectiveness, as measured by cluster precision and cluster recall.
Organizing and maintaining a competent and flexible supply chain is a major challenge to manufacturers in today's increasingly competitive and uncertain environments. Virtual integration represents the substitution of ownership with partnership by integrating a set of suppliers through information technology (IT) for tighter supply-chain collaboration. From the systems and control perspectives, this study develops a theory of virtual integration with an empirical model to examine the role that virtual integration plays in facilitating manufacturers to achieve greater manufacturing flexibility and comparative cost advantage. Based on a survey of Taiwanese manufacturing firms, our results show that environmental uncertainty tends to motivate manufacturers to increase their manufacturing flexibility, with both virtual integration and supplier responsiveness playing a vital enabling role. The results demonstrate the importance of supplier responsiveness for manufacturers to gain manufacturing flexibility and comparative cost advantage in supply-chain operations. Environmental uncertainty, thus, might first appear as a threat to a manufacturer, but with the help of IT and more responsive suppliers, such a threat could be transformed into a competitive edge, as reflected in the manufacturer's higher levels of manufacturing flexibility and comparative cost advantage.
This research investigates the effectiveness of various trust-building strategies to influence actual buying behavior in online shopping environments, particularly for first-time visitors to an Internet store that does not have an established reputation. Drawing from the literature on trust, we developed a model of how trustbuilding strategies could affect trust and the consequences of trust. We investigated two trust-building strategies: portal association (based on reputation categorization and trust transference) and satisfied customer endorsements (based on unit grouping, reputation categorization, and trust transference). ;A series of two studies was conducted at a large public university in Hong Kong. The first study employed a laboratory experiment to test the model in an online bookstore environment, using a real task that involves actual book purchases. Of the two strategies investigated, satisfied customer endorsement by similar peers, but not portal association, was found to increase consumers' trusting beliefs about the store. This, in turn, positively influenced consumers' attitudes toward the store and their willingness to buy from the store, which ultimately led to actual buying behaviors. To gather further insights on the two Web strategies investigated, a second study was conducted using a questionnaire survey approach. Overall, the findings corroborated those in the first study. Specifically, it shows that endorsements by similar (local, nonforeign) peers, but not by dissimilar (foreign) peers, were effective means of developing trust among first-time visitors to online stores.
Technology innovators are faced with the question of whether to license an innovation to other firms, and if so, what type of license it should use. This question takes on paramount importance with information technology innovations that lead to new products and services that exhibit network effects. This paper explores the impact of network effects on the licensing choice. The literature suggests that without network effects, a royalty license is preferred by producer-innovators. We find that a fixed-fee license is optimal with strong network effects. For less intense network effects, the optimal license uses a royalty rate, either alone or in combination with a fee. We further derive the terms of the optimal license and discuss the impact of the investment needed to replicate the innovation and the size of the potential market. Our results provide insights for licensing decisions in industries that exhibit network effects.
The concept of collective efficacy within virtual teams has yet to be studied. This study developed and rigorously validated a domain-specific measure of collective efficacy, entitled virtual team efficacy, within a comprehensive research framework. Over a two-year period we collected field study data from multiple samples of information systems project teams-in all, 52 virtual teams comprising 318 students from the United States, Great Britain, and Hong Kong. As we hypothesized, group potency and computer collective efficacy act as antecedents to virtual team efficacy, and virtual team efficacy is in turn predictive of perceptual and objective measures of performance. Further, consistent with efficacy theory, we also find that virtual team efficacy acts on performance outcomes through specific mediating processes. This paper contributes to the academic and practitioner communities by providing a comprehensive model of virtual team efficacy and performance and by providing validated instrumentation that can be immediately applied during further research in this area.
The continuous growth of e-commerce makes it critical for firms to understand consumers' search behavior so that e-commerce Web sites and the underlying information systems can be designed to better cater to consumers' needs. This paper extends the classic search model to analyze online consumer search behavior. The analytical results suggest how consumers' search depth is influenced by a variety of factors such as search cost, individual consumer difference, and product characteristics. Evidence is provided using clickstream data of online searches and purchases of music CDs, computer hardware, and airline tickets during the period from July 2002 to December 2002 collected by an Internet marketing company, ComScore Inc. Compared with the search depth reported in previous works, this study finds that consumers are searching more intensely before purchasing online. This reflects the evolution of Internet users and the growth of online retail business.
Senior executives continue to be concerned about factors influencing the business effect of information technology (IT). Prior research has argued that business-IT strategic alignment facilitates business effect of IT and that contextual factors affect business-IT alignment. However, the role of knowledge considerations in the relationship between contextual factors and alignment, and the role of IT projects in the relationship between alignment and business effects of IT, have not been explicitly examined. Therefore, this paper pursues the following two research questions: (1) Based on knowledge considerations, how do planning behaviors (specifically, IT managers' participation in business planning and business managers' participation in IT planning) and top management knowledge of IT mediate the effects of two contextual factors-organizational emphasis on knowledge management and centralization of IT decisions-on business-IT strategic alignment? (2) How do aspects of IT projects (specifically, quality of IT project planning and implementation problems in IT projects) mediate the relationship between business-IT strategic alignment and business effects of IT? ;Results from a survey of 274 senior information officers indicate that organizational emphasis on knowledge management and centralization of IT decisions affect top managers' knowledge of IT, which facilitates business managers' participation in strategic IT planning and IT managers' participation in business planning, and both of these planning behaviors affect business-IT strategic alignment. Moreover, the results indicate that quality of IT project planning and implementation problems in IT projects mediate the relationship between business-IT strategic alignment and business effect of IT. These findings highlight the importance of considering the planning and implementation of IT projects when examining the effects of business-IT strategic alignment, and highlight the importance of considering shared domain knowledge (i.e., top managers' knowledge of IT) and planning behaviors when examining the effects of contextual factors on business-IT strategic alignment. Managers can use these results to develop more comprehensive action plans for achieving greater business-IT strategic alignment, and for translating alignment into enhanced IT effects on business performance.
Web Sites are important components of Internet strategy for organizations. This paper develops a theoretical model for understanding the effect of Web site design elements on customer loyalty to a Web site. We show the relevance of the business domain of a Web site to gain a contextual understanding of relative importance of Web site design elements. We use a hierarchical linear modeling approach to model multilevel and cross-level interactions that have not been explicitly considered in previous research. By analyzing, data on more than 12,000 online customer surveys for 43 Web sites in several business domains, we find that the relative importance of different Web site features (e.g., content, functionality) in affecting customer loyalty to a Web site varies depending on the Web site's domain. For example, we find that the relationship between Web site content and customer loyalty is stronger for information-oriented Web sites than for transaction-oriented Web sites. However, the relationship between functionality and customer loyalty is stronger for transaction-oriented Web sites than for information-oriented Web sites. We also find that government Web sites enjoy greater word-of-mouth effect than commercial Web sites. Finally, transaction-oriented Web sites tend to score higher on mean customer loyalty than do information-oriented Web sites.
Increasing globalization and advances in communication technology have fuelled the emergence of global virtual teams (GVTs). There is much potential for conflict in GVTs as members work across cultural, geographical, and time boundaries. This study examines the antecedents of GVT conflict and the circumstances under which conflict affects team performance. An in-depth study of GVT conflict episodes was carried out using interviews, observations, communication logs, and documents. Based on findings from the teams under study interpreted in the light of prior literature, propositions are developed about the antecedents and effects of GVT conflict as stated. Within GVTs, cultural diversity is likely to contribute to both task and relationship conflict while functional diversity may result in task conflict. Large volumes of electronic communication and lack of immediacy of feedback in asynchronous media can contribute to task conflict. Moreover, the relationship between task conflict and team performance is likely to be contingent upon task complexity and conflict resolution approach. The influence of relationship conflict on performance may depend on task interdependence and conflict resolution approach. The conflict resolution approach may in turn be determined by the nature of conflict attribution. These propositions have been synthesized into a model to guide future empirical research and GVT practice.
Web services are interoperable and reusable software components that can be dynamically discovered and integrated over the Internet. Developed on open standards, Web services have become a promising solution to inter- and intra-organization application integration. The supply chain of Web services exhibits two distinct features that are not considered in previous literature on information and physical-good supply chain: the integration of multiple Web services and the cross-network externality effect between Web service vendors and users. In a quest to fill in the research gap, this paper studies the optimal pricing strategies of a monopolistic intermediary in the supply chain of complementary Web services. The Web service intermediary (WSI) provides both technical and aggregation services, and seeks to charge optimal subscription and listing fees. Analytical results show that in a supply chain of complementary Web services exhibiting cross-network effects, the optimal strategy for the WSI is to set the listing fee such that all service providers list on it. On the other hand, the optimal subscription fee depends on the intensity of the cross-network effect, consumers' valuation of value-added services, and the characteristics of the Web services under consideration.
Personalized services are increasingly popular in the Internet world. This study identifies theories related to the use of personalized content services and their effect on user satisfaction. Three major theories have been identified-information overload, uses and gratifications, and user involvement. The information overload theory implies that user satisfaction increases when the recommended content fits user interests (i.e., the recommendation accuracy increases). The uses and gratifications theory indicates that motivations for information access affect user satisfaction. The user involvement theory implies that users prefer content recommended by a process in which they have explicit involvement. In this research, a research model was proposed to integrate these theories and two experiments were conducted to examine the theoretical relationships. Our findings indicate that information overload and uses and gratifications are two major theories for explaining user satisfaction with personalized services. Personalized services can reduce information overload and, hence, increase user satisfaction, but their effects may be moderated by the motivation for information access. The effect is stronger for users whose motivation is in searching for a specific target. This implies that content recommendation would be more useful for knowledge management systems, where users are often looking for specific knowledge, rather than for general purpose Web sites, whose customers often come for scanning. Explicit user involvement in the personalization process may affect a user's perception of customization, but has no significant effect on overall satisfaction.
Organizations increasingly need to build an enterprise-wide capability to leverage technology that is distributed in different business units. Some organizations establish enterprise architecture (EA) standards to enable greater compatibility of information technology (IT) components and integration of applications and data across the enterprise. Through a firm-level survey, we sought to answer two key questions about the use of EA standards: (1) How do different governance mechanisms affect the use of EA standards? and (2) To what extent does the use of EA standards help organizations to improve the sharing and integration of IT resources across the enterprise? We identified four key governance mechanisms for EA standards management and examined how each mechanism affected the use of EA standards. We also examined how the use of EA standards affects the management of IT infrastructure, applications, and data resources across business units. Our empirical results showed that the use of EA standards is effective in helping organizations to better manage their IT resources. Our study also provides detailed insights into how organizations can set up governance mechanisms to facilitate the use of EA standards in achieving enterprisewide goals.
Customer-centric business makes the needs and resources of individual customers the starting point for planning new products and services or improving existing ones. While customer-centricity has received recent attention in the marketing literature, technologies to enable customer-centricity have been largely ignored in research and theory development. In this paper, we describe one enabling technology-wikis. Wiki is a Web-based collaboration technology designed to allow anyone to update any information posted to a wiki-based Web site. As such, wikis can be used to enable customers to not only access but also change the organization's Web presence, creating previously unheard of opportunities for joint content development and 11 peer production of Web content. At the same time, such openness may make the organization vulnerable to Web site defacing, destruction of intellectual property, and general chaos. In this zone of tension-between opportunity and possible failure-an increasing number of organizations are experimenting with the use of wikis and the wiki way to engage customers. Three cases of organizations using wikis to foster customer-centricity are described, with each case representing an ever-increasing level of customer engagement. An examination of the three cases reveals six characteristics that affect customer engagement-comm unity custodianship, goal alignment among contributors, value-adding processes, emerging layers of participation, critical mass of management and monitoring activity, and technologies in which features are matched to assumptions about how the community collaborates. Parallels between our findings and those evolving in studies of the open source software movement are drawn.
We examine a knowledge management (KM) success model that incorporates the quality of available knowledge and KM systems built to share and reuse knowledge such as determinants of users' perception of usefulness and user satisfaction with an organization's KM practices. Perceived usefulness and user satisfaction, in turn, affect knowledge use, which in our model is a measure of how well knowledge sharing and reuse activities are internalized by an organization. Our model includes organizational support structure as a contributing factor to the success of KM system implementation. Data collected from 150 knowledge workers from a variety of organizations confirmed 10 of 13 hypothesized relationships. Notably, the organizational support factors of leadership commitment, supervisor and coworker support, as well as incentives, directly or indirectly supported shared knowledge quality and knowledge use. In line with the proposed model, the study lends support to the argument that, in addition to KM systems quality, firms must pay careful attention to championing and goal setting as well as designing adequate reward systems for the ultimate success of these efforts. This is one of the first studies that encompasses both the supply (knowledge contribution) and demand (knowledge reuse) sides of KM in the same model. It provides more than anecdotal evidence of factors that determine successful KM system implementations. Unlike earlier studies that only deal with knowledge-sharing incentives or quality of shared knowledge, we present and empirically validate an integrated model that includes knowledge sharing and knowledge quality and their links to the desired outcome-namely, knowledge reuse.
Attracted by the promise of greater market exposure and increased revenues, firms across a wide variety of industries have undertaken significant investments in online channels. However, while some firms' entire business models revolve around this initiative, others have made only limited commitments to online channel ventures. What accounts for these marked differences in commitment to online initiatives, and do firms reap the performance benefits of increased levels of commitment? Furthermore, how do firms' internal and external capabilities affect their propensity to establish and succeed with online channel ventures? Drawing on marketing, innovation, and information systems perspectives, along with insights from the resource-based view of the firm, we propose an integrative conceptual framework that helps answer these questions. We ground our hypotheses in the context of retailers' online channel development efforts, and test our conceptual framework with data collected via a Web-based survey of 550 retailers. We find evidence of significant positive returns to investments in online channels. Furthermore, we observe the divergent effects of different sets of capabilities on commitment and performance. Importantly, although we find that the direct effect of firms' information systems capabilities on online performance appears to be negative, the indirect effect (mediated by commitment) is positive. Our study also examines the impact of firms' established distribution channels on levels of commitment to, and performance of, the online channel. We find that firms' established distribution channels act as double-edged swords, with divergent effects on commitment and performance. We also find evidence of diminishing returns to commitment as a function of established distribution presence, thereby suggesting that the rewards of commitment do not accrue equally to all firms.
Majority influence is the attempt by a majority of group members to impose their common position on group dissenters during group decision making. Because of globalization, the use of cross-cultural teams in group tasks is becoming increasingly common. The objective of this study was to investigate how national culture, social presence, and group diversity may affect majority influence in a group decision-making context. A total of 183 groups participated in a large-scale empirical experiment at multiple sites. The results show that the national culture of group minorities has a significant impact on majority influence and that the use of computer-mediated communication can reduce majority influence. The findings have both theoretical and practical implications for improving the outcome and the effectiveness of group decision making in cross-cultural environments.
E-business standards are critical for electronic interorganizational transactions. In many industries, firms develop e-business standards collaboratively in a standard consortium. They can choose to become a leading developer, a passive adopter, or a nonadopter. To capture firms' strategic choices at the development stage and the adoption stage, which are related due to the double-sided interactions between the two stages, we propose an integrated model of consortium-based e-business standardization. We find that firms' payoffs from standard adoption increase with the intrinsic value of the standard, but developers' benefits increase faster than passive adopters' benefits. The model examines the value of passive adopters to the standard development via network externalities, even though passive adopters do not contribute directly in the consortium. We find that passive adopters do not always exist. There are two possible equilibria for the endogenous formation of the developer network and the adopter network, one without passive adopters and one with passive adopters. How external conditions affect the endogenous formation of the consortium depends upon whether there are passive adopters in the equilibrium. Based on our analysis, we recommend strategies to e-business standard consortia to motivate firms' participation and enhance social welfare created by the standard.
The notion of a genre system typically connotes sequences of interrelated communicative genres. This paper suggests that we can find other types of relationships among genres. Data from a field study in a large emergency room illustrate how doctors, nurses, and clerical staff routinely combine document genres not only in sequences but also in various accumulations achieved through proximity and movement. The combinations of genres add flexibility to the emergency room staff's genre use and allow them to employ individual genres for several purposes. The data allow us to explore how organizational members manage the tension between a need for continuity in communicative practices and a need for flexibility in managing a jumble of paper-based and digital information systems. In addition, it demonstrates how end users often tinker with genres' media and form in the process of altering combinations among specific genres.
The management of the information systems (IS) function is a complex task, particularly in the case of multinational corporations (MNCs), where installations dispersed across distance, time, and cultures can lead to diverse and incompatible systems spreading among foreign subsidiaries. The need to globally control and coordinate the IS management function is often met with resistance from local IS managers, who may perceive corporate standards as intrusive. Resource dependence theory (RDT) argues that control is made easier when a subsidiary unit is dependent on corporate headquarters for critical resources. This study examined the IS management relationship and the use of various mechanisms of control (formal and informal) between 54 headquarters-subsidiary pairs spread across 19 countries of varying resource-richness. While RDT appears to be valid when subsidiaries are dependent on MNC headquarters for resources, the expected relationship between the mechanisms and host country IS resource availability was not observed. Although there was a significant relationship with the use of informal mechanisms and IS resources, it was in the opposite direction to what would be expected by RDT.
Ideation is a key step in organizational problem solving, so researchers have developed a variety of technological interventions for improving ideation quality, which we define as the degree to which an ideation activity produces ideas that are helpful in attaining a goal. In this paper, we examine the four measures typically used to evaluate ideation quality, including idea-count, sum-of-quality, average-quality, and good-idea-count, and discuss their validity and potential biases. An experimental study comparing three levels of social comparison was used to illustrate the differences in the ideation quality measures and revealed that research conclusions were dependent on the measure used. Based on our analysis of the measures and experimental results, we recommend that only good-idea-count be used as a measure to evaluate ideation treatments and call into question research that has based its findings on the other measures. Finally, we discuss implications for research and other potential approaches to evaluating ideation quality.
Knowledge of objects, situations, or locations in the environment can be productive, useful, or even life-critical for mobile augmented reality (AR) users. Users may need assistance with (1) dangers, obstacles, or situations requiring attention; (2) visual search; (3) task sequencing; and (4) spatial navigation. The omnidirectional attention funnel is a general purpose AR interface technique that rapidly guides attention to any tracked object, person, or place in the space. The attention funnel dynamically directs user attention with strong bottom-up spatial attention cues. In a study comparing the attention funnel to other attentional techniques such as highlighting and audio cueing, search speed increased by over 50 percent, and perceived cognitive load decreased by 18 percent. The technique is a general three-dimensional cursor in a wide array of applications requiring visual search, emergency warning, and alerts to specific objects or obstacles, or for three-dimensional navigation to objects in space.
In business and government organizations, information systems often handle sensitive data about individuals and other organizations, using various kinds of identifiers. The growing cooperation of organizations results in the need to share and exchange such data. This collection and sharing, however, is affected by privacy concerns, and organizational and technical issues have to be solved and taken into account. This paper describes the results of an exploratory study in the government sector, focusing on the way public organizations manage identity-related data and the sharing of such data, either with other public agencies or with private organizations. Despite significant progress in harmonizing the legal and administrative provisions and technical standards in the European Union, there are still considerable cross-country differences regarding this subject. These differences-together with the growing mobility of goods, persons, and related data within the European Union-cause particular challenges for information systems in digital government in this region. After discussing and defining the key notions and methodology of the study, we present the status quo in 18 out of 25 EU member states and compare it to the results of a prior study by the same network done in 2001. Finally, we draw conclusions about identity management in cross-border contexts.
Firms are increasingly using collaborative systems to enhance supply-chain visibility. A key emphasis of these interorganizational systems (IOS) is to improve the coordination between buyers and suppliers through electronic integration. While such IOS integration is purportedly good, because it tightens linkages in the supply chain, it is not clear whether it is the best configuration under all conditions. A review of literature on adoption and use of electronic data interchange (EDI) systems (a type of IOS) shows that this issue has been examined from multiple theoretic perspectives. Researchers have examined how contingencies related to technology, organization, and environment shape EDI use. Limited attention has been directed toward understanding how conditions under which transactions are conducted impact the use of IOS. We argue that transactional characteristics are important antecedents to IOS integration and propose that demand uncertainty, complexity, market fragmentation, and market volatility capture key characteristics. These factors coupled with an open information-sharing environment are hypothesized to influence 10S integration. Data collected from the electronics industry is used to examine the research model. Results show that firms tend to deploy integrated 10S when complexity of the component is high, market fragmentation is low, and an open information-sharing environment exists. Thus, from a managerial perspective, IOS integration is the appropriate configuration under conditions of high product complexity and open information-sharing environment, but it precludes the firm from participating in the open market and gaining brokerage benefits.
We empirically test the effects of explanation facilities on consumers' initial trusting beliefs concerning online recommendation agents (RAS). RAS provide online shopping advice based on user-specified needs and preferences. The characteristics of RAs that may hamper consumers' trust building in the RAS are identified, and the provision of explanation facilities is proposed as a knowledge-based approach to enhance consumers' trusting beliefs by dealing with these obstacles. This study examines the effects of three types of explanations about an RA and its use-how, why, and trade-off explanations-on consumers' trusting beliefs in an RA's competence, benevolence, and integrity. An RA was built as the experimental platform and a laboratory experiment was conducted. The results confirm the important role of explanation facilities in enhancing consumers' initial trusting beliefs and indicate that consumers' use of different types of explanations enhances different trusting beliefs: the use of how explanations increases their competence and benevolence beliefs, the use of why explanations increases their benevolence beliefs, and the use of trade-off explanations increases their integrity beliefs.
Based on empirical survey data, this paper uses concepts from sociotechnical theory and role theory to explore the effects of stress created by information and computer technology (ICT)-that is, technostress-on role stress and on individual productivity. We first explain different ways in which ICTs can create stress in users and identify factors that create technostress. We next propose three hypotheses: (1) technostress is inversely related to individual productivity, (2) role stress is inversely related to individual productivity, and (3) technostress is directly related to role stress. We then use structural equation modeling on survey data from ICT users in 223 organizations to test the hypotheses. The results show support for them. Theoretically, the paper contributes in three ways. First, the different dimensions of technostress identified here add to existing concepts on stress experienced by individuals in organizations. Second, by showing that technostress inversely affects productivity, the paper reinforces that failure to manage the effects of ICT-induced stress can offset expected increases in productivity. Third, validation of the positive relationship between technostress and role stress adds a new conceptual thread to literature analyzing the relationship between technology and organizational roles and structure. In the practical domain, the paper proposes a diagnostic tool to evaluate the extent to which technostress is present in an organization and suggests that the adverse effects of technostress can be partly countered by strategies that reduce role conflict and role overload.
Although there is substantial research on learning that occurs before adoption of a new information system, there is a dearth of research on postimplementation learning when a new system is assimilated as a routine element of users'work. Hence, during the postimplementation period of a bank's new work flow system, we conducted a longitudinal participant observation study to observe knowledge transfers of users and information technology (IT) professionals assigned to a help desk. We found that although users turned to IT professionals to obtain knowledge related to conceptual understanding and procedures to use the system, they most often turned to other users to obtain knowledge that allowed them to adapt the system to their work. IT professionals, on the other hand, often turned to their colleagues to obtain knowledge that helped them modify the system to emerging innovative uses. These patterns of knowledge transfers can be explained based upon source expertise. Our findings indicate that organizations must sustain designated sources of knowledge such as help desks, but must also establish conduits for users to acquire knowledge from other users and develop innovative uses of the system. A substantial amount of critical knowledge transfers relevant to system adaptation occurred during face-to-face discussions between users and IT professionals, and therefore future research should examine how this would be affected by the outsourcing of technical support functions.
Failures in large-scale information technology implementation are abundantly documented in the practitioner literature. In this study, we examine why some firms benefit more from enterprise resource planning (ERP) implementation than others. We look at ERP implementation from a technological diffusion perspective, and investigate under what contextual conditions the extent of ERP implementation has the greatest effect on business process outcomes. Using empirical data, we find that the extent of ERP implementation influences business process outcomes, and both ERP radicalness and delivery system play moderating roles. For information systems (IS) practice, this study helps managers direct their attention to the most promising factors, provides insights into how to manage their complex interactions, and elaborates on their differential effects on business process outcomes. For IS research, it integrates innovation diffusion theory into our current knowledge of ERP implementation and provides theoretical explanations for ERP implementation failures.
Despite the significant opportunities to transform the way that organizations conduct trading activities, few studies have investigated the impetus for organizational strategic moves toward business-to-business (13213) electronic marketplaces. Drawing on transaction cost theory and institutional theory, this paper identifies two groups of factors-efficiency- and legitimacy-oriented factors, respectively-that can influence organizational buyers'initial adoption of, and the level of participation in, B2B e-marketplaces. The effects of these factors on initial adoption of and participation level in 13213 e-marketplaces are empirically tested with data collected, respectively, from 98 potential adopter and 85 current adopter organizations. The results of a partial least squares analysis of the data indicate that the two groups of factors exhibit different patterns in explaining initial adoption in the preadoption period and participation level in the postadoption period. Specifically, all three of the efficiency-oriented factors investigated in this study-product characteristics, demand uncertainty, and market volatility-and their subconstructs exhibit a significant influence on adoption intent or participation level, or both. The results demonstrate that two legitimacy-oriented factors-mimetic pressures and normative pressures-and their subconstructs have a significant impact on adoption intent, but not on participation level. Our findings also indicate that clearly different patterns exist between the two groups of factors in explaining adoption intent and participation level.
As outsourcing evolves into a competitive necessity, managers must increasingly contend with the decision about which software development projects to outsource. Although a variety of theories have been invoked to study the initial outsourcing decision, much of this work has relied in isolation on one theoretical perspective. Therefore, the relative importance ascribed by managers to the factors from these theories is poorly understood. The majority of this work also masks interesting insights into outsourcing decisions by focusing on the information technology (IT) function rather than individual projects as the unit of analysis, where many of these decisions occur. In contrast, prior research at the project level has focused on predicting development performance in the postoutsourcing-decision phases of projects. The objective of this study is to examine the relative importance that IT managers ascribe to various factors from three complementary theories-transaction cost economics, agency theory, and knowledge-based theory-as they simultaneously consider them in their project outsourcing decisions. A secondary objective is to assess the cross-cultural robustness (United States versus Japan in this study) of such models in predicting project-level IT outsourcing decisions. We develop and test a multitheoretic model using data on 1,008 project-level decisions collected from 33 Japanese and 55 U.S. managers. Overall, our results provide novel insights into the relative importance that managers ascribe to the factors from these three theories, their complementarities and occasional contradictions, and offer new insights into the differences among U.S. and Japanese IT managers. Implications for theory and practice are also discussed.
Information-processing networks (IPNs) denote dynamic network-based information-processing structures that operate as coordination mechanisms that transcend formal hierarchies. Despite growing interest in information technology-enabled IPNs, the literature has been silent in exploring the various ontological structures of IPNs and the structural efficiency embedded in each IPN, especially in the event of radical organizational changes. To fill this gap, this study identifies, from the perspective of graph theory, four ontological IPN archetypes that can serve as blueprints for information processing within and across organizations-random, small world, moderate scale free (MSF), and Barabasi. We then assess how each structure reacts to corporate restructuring (e.g., downsizing) and investigate, based on computer simulation, the extent to which each structure preserves a worker's efficiency and the stability of the network structure in the event of downsizing. Two moderating variables are included in the model-that is, scale of downsizing and the reconnection strategy in the presence of downsizing. In this study, downsizing is viewed not only as the simple elimination of individual workers but also as the elimination of the communication and information-processing conduits necessary for effective communication and coordination. We find that when firms implement a relatively small-scale workforce reduction, centralized coordination structures such as MSF and Barabasi are generally more resilient and facilitate better coordination. However, when the downsizing strategy involves massive and severe layoffs, decentralized coordination structures such as random and small world are more durable, and tend to provide a stronger safety net, irrespective of the strategies employed to create new ties. Although this study focused exclusively on the context of downsizing, the results of the study have important implications for other types of organizational restructuring (e.g., organizational expansion and merger and acquisition) that reconfigure IPNs.
Although research has made significant strides in recent years in evaluating the performance impacts from information technology (IT), a dearth of easily accessible objective measures, particularly at the process level, continues to limit IT research. Suggestions that researchers use perceptual measures instead are met with claims that the biased nature of perceptions renders them imperfect proxies for the true extent of IT impacts. In this paper, we use sensemaking theory to explore this claim. We outline a model relating what executives notice about process-level IT impacts with sensemaking-based perceptions of IT impacts at the firm level, and firm performance as the ultimate arbiter of perceptual accuracy. Estimating the model with survey data from executives in 196 firms, we find that executives' perceptions are more fact than fiction. While perceptions are not a perfect proxy for hard-to-find objective measures, perceptual accuracy should stimulate greater consideration of executives' perceptions in future IT business value research.
Coordination is important in software development because it leads to benefits such as cost savings, shorter development cycles, and better-integrated products. Team cognition research suggests that members coordinate through team knowledge, but this perspective has only been investigated in real-time collocated tasks and we know little about which types of team knowledge best help coordination in the most geographically distributed software work. In this field study, we investigate the coordination needs of software teams, how team knowledge affects coordination, and how this effect is influenced by geographic dispersion. Our findings show that software teams have three distinct types of coordination needs-technical, temporal, and process-and that these needs vary with the members' role; geographic distance has a negative effect on coordination, but is mitigated by shared knowledge of the team and presence awareness; and shared task knowledge is more important for coordination among collocated members. We articulate propositions for future research in this area based on our analysis.
This paper analyzes a software market consisting of a freely available open source software (OSS), the commercial version of this OSS (OSS-SS), and the competing commercial proprietary software (PS). We find that in software markets characterized by low direct network benefits, the PS vendor is better off in the presence of competition from OSS-SS. Furthermore, the OSS-SS vendor in these markets is better off by having lower usability than PS. Therefore, the PS vendor has little incentive to improve the usability of their software in these markets. On the other hand, in software markets characterized by high network benefits, a PS vendor is threatened by the presence of OSS-SS and can survive only if the PS is more usable than the competing OSS-SS.
Intrusion prevention requires effective identification of and response to malicious events. In this paper, we model two important managerial decisions involved in the intrusion prevention process: the configuration of the detection component, and the response by the reaction component. The configuration decision affects the number of alarms the firm has to investigate. It is well known that the traditional intrusion detection system generates too many false alarms. The response decision determines whether alarms are going to be investigated or rejected outright. By jointly optimizing these two decision variables, a firm may apply different strategies in protecting its informational assets: slow but accurate, rapid but inaccurate, or a mixture of the two strategies. We use the optimal control approach to study the problem. Unlike previous literature, which studied the problem with a static model, in our model, the decision on balancing the desire to detect all malicious events with the opportunity costs required to do so is time dependent. Furthermore, we show how the choice of an optimal mixture of reactive and proactive responses depends on the values of cost parameters and investigation rate parameters. We find that in our model, a high damage cost does not immediately translate to a preference of proactive response, or a high false rejection cost does not translate to a preference of reactive response. The dynamics of the problem, such as how fast alarms accumulate and how fast they can be cleared, also affect the decisions.
The advent of the Internet has made the transmission of personally identifiable information more common and often unintended by the user. As personal information becomes more accessible, individuals worry that businesses misuse the information that is collected while they are online. Organizations have tried to mitigate this concern in two ways: (1) by offering privacy policies regarding the handling and use of personal information and (2) by offering benefits such as financial gains or convenience. In this paper, we interpret these actions in the context of the information-processing theory of motivation. Information-processing theories, also known as expectancy theories in the context of motivated behavior, are built on the premise that people process information about behavior-outcome relationships. By doing so, they are forming expectations and making decisions about what behavior to choose. Using an experimental setting, we empirically validate predictions that the means to mitigate privacy concerns are associated with positive valences resulting in an increase in motivational score. In a conjoint analysis exercise, 268 participants from the United States and Singapore face trade-off situations, where an organization may only offer incomplete privacy protection or some benefits. While privacy protections (against secondary use, improper access, and error) are associated with positive valences, we also find that financial gains and convenience can significantly increase individuals' motivational score of registering with a Web site. We find that benefits-monetary reward and future convenience-significantly affect individuals' preferences over Web sites with differing privacy policies. We also quantify the value of Web site privacy protection. Among U.S. subjects, protection against errors, improper access, and secondary use of personal information is worth $30.49-$44.62. Finally, our approach also allows us to identify three distinct segments of Internet users-privacy guardians, information sellers, and convenience seekers.
Information displayed on an e-commerce site can be used not just by the intended customers but also by competitors. While retailers enhance service quality by linking inventory systems to Web servers and making stockout information available in real time, that stockout information could also be used by competitors in determining their prices on current stocks. In this paper, we examine the effect of such proactive use of information in the setting of e-commerce retailing where duopoly retailers set their prices of a commodity that is in short supply. We show that when customer reservation value is relatively high and retailers are differentiated in fill rate, both retailers choose the dynamic pricing strategy in equilibrium. By investing in Web scraping technology, retailers automatically monitor each other's stock status and dynamically adjust prices contingent on rival's stock availability.
eBay's highly visible feedback-based rating system is also highly flawed, contributing to problems for buyers, which in turn creates problems for sellers. The well-known market for lemons phenomenon studied by Akerlof, and the even older Gresham's law effect, are contributing to loss of buyers' confidence in eBay, shrinking sellers' margins, contributing to the erosion of eBay's share price, and, potentially, leading to serious reductions in the value of eBay as an electronic auction site. buySAFE has created an alternative mechanism for reducing buyers' information deficit concerning sellers and their merchandise, involving a third-party certification system and bonding for qualified sellers; the rating is analogous to bond rating services such as Moody's and Standard & Poor's. Analysis of buySAFE's certification and bonding strategies for eBay sellers provides a basis for ongoing theory development related to organizational strategies that recognize the importance of information asymmetries in the digital marketplace and address resolution of consumers' concerns. buySAFE's original business model involves bonding sellers' transactions and protecting consumers for as much as $25,000. This has a number of beneficial effects on the buyers and sellers: it improves the information endowments of the buyers, it increases their willingness to pay for the goods and services offered, and it increases the margins and total revenues of the sellers. Although acceptance of buySAFE has been rapid, it has been slower than anticipated, and slower than theory would suggest. The company's executives are exploring adjusting their approach to the market and finding a way to achieve higher profitability, and working to limit their dependence upon eBay.
In software development, team-based work structures are commonly used to accomplish complex projects. Software project teams must be able to utilize the expertise and knowledge of participants without overwhelming individual members. To efficiently leverage individuals' knowledge and expertise, software project teams develop team cognition structures that facilitate their knowledge activities. This study focuses on the emergence and evolution of team cognition in software project teams, and examines how communication activity and team diversity impact the formation of these structures. A longitudinal study was conducted of 51 database development teams. The results suggest that some forms of communication and team diversity affect the formation of team cognition. Frequency of meetings and phone calls were positively related to the formation of team cognition, while e-mail use had no effect. Gender diversity had a strong and positive effect on the development of team cognition and the effect remained stable over time. Implications for the practical potential and limitations of purposive team construction as a strategy for improving software development team performance are discussed.
A well-defined software process is critical for success in software projects. Software process tailoring refers to the activity of tuning a standardized process to meet the needs of a specific project. We conducted two case studies that address the research question: How is a software process tailored to suit its context? This study identifies process tailoring as a key mechanism to address the challenges faced by a project and develops a model that describes how a process is tailored to resolve these challenges. The model identifies a set of environmental factors, challenges, project goals, process tailoring strategies, and their influences on each other. Specifically, the findings demonstrate the duality of the software process, showing how the project context (i.e., project goals, environmental factors, and challenges) and tailoring decisions dynamically interact with each other and construct the context in which the project is developed and the process is tailored.
Many enterprise resource planning (ERP) implementation projects fail despite huge investments. To explain such failures, we draw on the resource-based view (RBV) of the firm to define various dimensions of information systems (IS) resources. Using resource-picking and capability-building arguments, we examine the relationships between IS resources and ERP capabilities to find out whether they have complementary effects on outcomes. Empirical results from a survey of manufacturing firms that recently implemented ERP systems support the hypothesized model. For IS research, this study further develops the complementary and capability-building roles of IS resources, integrates RBV into our cur-rent knowledge of ERP implementation, and provides theoretical explanations for when or under what conditions building ERP capabilities has the highest impact on business process outcomes. For IS practice, it emphasizes the importance of IS resources in building ERP capabilities, provides preliminary measures for IS resource dimensions, and demonstrates their impact on firms' ERP capabilities and consequent business process outcomes.
Software is available through a number of different licensing models such as the commonly used perpetual licensing model and a relatively new licensing model called software as a service (SaaS). There are several differences between SaaS and perpetual licensing. SaaS licensing offers software using a subscription model, whereas perpetual licensing involves a one-time payment for a perpetual use license and optional additional payments for future upgrades. Prior literature has not considered the impact of these licensing schemes on the publisher's incentive to invest in software quality. We model differences in how new software features are disseminated in SaaS and perpetual licensing. We show that these differences affect the publisher's incentive to invest in product development. We find that the SaaS licensing model leads to greater investment in product development under most conditions. This increased investment leads to higher software quality in equilibrium under SaaS as compared to perpetual licensing. The software publisher earns greater profits and social welfare is higher under SaaS under these conditions.
This field study research evaluates the viability of applying an option-based risk management (OBRiM) framework, and its accompanying theoretical perspective and methodology, to real-world sequential information technology (IT) investment problems. These problems involve alternative investment structures that bear different risk profiles for the firm, and also may improve the payoffs of the associated projects and the organization's performance. We sought to surface the costs, benefits, and risks associated with a complex sequential investment setting that has the key features that OBRiM treats. We combine traditional, purchased real options that subsequently create strategic flexibility for the decision maker, with implicit or embedded real options that are available with no specific investment required provided the decision maker recognizes them. This combination helps the decision maker to both (1) explicitly surface all of his or her strategic choices and (2) accurately value those choices, including ones that require prior enabling investments. The latter permits senior managers to adjust a project's investment trajectory in the face of revealed risk. This normally is important when there are uncertain organizational, technological, competitive, and market conditions. The context of our research is a data mart consolidation project, which was conducted by a major airline firm in association with a data warehousing systems vendor. Field study inquiry and data collection were essential elements in the retrospective analysis of the efficacy of OBRiM as a means to control risk in a large-scale project. We learned that OBRiM's main benefits are (1) the ability to generate meaningftil option-bearing investment structures, (2) simplification of the complexities of real options for the business context, (3) accuracy in analyzing the risks of IT investments, and (4) support for more proactive planning. These issues, which we show are more effectively addressed by OBRiM than the other methods, have become crucial as more corporate finance-style approaches are applied to IT investment and IT services problems. Our evaluative study shows that OBRiM has the potential to add value for managers looking to structure risky IT investments, although some aspects still require refinements.
We examine the question of which services are tradable within a concrete Setting: the outsourcing of information technology (IT) services across a broad cross-section of establishments in the United States. If markets for IT services are local, then we should expect increases in local supply would increase the likelihood of outsourcing by lowering the cost of outsourcing. If markets are not local, then local supply will not affect outsourcing demand. We analyze the outsourcing decisions of a large sample of 99,775 establishments in 2002 and 2004, for two types of IT services-programming and design and hosting. Programming and design projects require communication of detailed user requirements whereas hosting requires less coordination between client and service provider than programming and design. Our empirical results bear out this intuition: the probability of outsourcing programming and design is increasing in the local supply of outsourcing, and this sensitivity to local supply conditions has been increasing over time. This suggests there is some nontradable or local component to programming and design services that cannot be easily removed. In contrast, the decision to outsource hosting is sensitive to local supply only for firms for which network uptime and security concerns are particularly acute.
This paper examines new forms of collaboration between producers and consumers that are emerging in the digital entertainment space. Taking the case of the video game industry, we show how some firms have opened a portion of their proprietary content for transformation by consumers and allowed the development of consumer-designed and consumer-implemented derivative products. By reappropriating these derivatives, video game firms are successfully outsourcing parts of their game design and development process to digital consumer networks. Applying economic analysis, we explore the potential benefits and risks associated with outsourcing to networks of consumers. We also derive the optimal combination of copyright enforcement and consumer compensation. Our results suggest that profit-maximizing producers of video games have incentive to partially open game content to their users and to remunerate the most innovative ones, under the condition that the derivatives constitute complements to, and not substitutes for, the original product. We discuss the implications on firm strategy for innovation.
Although it has been argued that knowledge is an important organizational resource, little research has investigated where individuals go to search for information or knowledge. Prior work has investigated sources in isolation, but in an organizational setting, sources are encountered as an open portfolio instead of in isolation. it is important to understand how individuals perceive the wide array of sources available to them and how those perceptions affect their use of different types of sources. Building on prior work, this research looks at factors underlying the selection of sources that require direct interpersonal contact (relational sources) and those that do not (nonrelational sources) and explores factors that differentially affect the use of these types of sources. A sample of 204 working professionals recruited from graduate business studies was used to test hypotheses regarding the effects of accessibility and quality, as well as comparisons and trade-offs between relational and nonrelational sources. Consistent with prior work, source accessibility and quality significantly affect usage of a source. This relationship, however, is moderated by the type of source with accessibility having less effect on the use of relational sources. Furthermore, use of each type of source was also affected by the perceived accessibility and quality of alternative types of sources. Together these results highlight the importance of simultaneously considering the relational and nonrelational sources available to individuals. These results also have implications for the design and implementation of systems for managing information and knowledge assets.
Word mismatch represents a fundamental information retrieval challenge that has become increasingly important as electronic document repositories (e.g., Web resources, digital libraries) grow in number and sheer volume. In general, word mismatch refers to the phenomenon in which a concept is described by different terms in user queries and in source documents. Query expansion represents a promising avenue to address such problems. Previous research predominantly approaches query expansion on the basis of global or local analysis. However, these approaches emphasize a global perspective rather than taking a topic-specific view of term associations. As a consequence, their effectiveness can be severely constrained when the document corpus spans a diverse set of topics. In this study, we propose a topic-based approach for query expansion and develop and empirically evaluate two novel methods-namely, nonfuzzy and fuzzy topic-based query expansion-to address word mismatch problems. According to our evaluation results, the proposed topic-based approach is more effective than a benchmark global analysis method, particularly when user queries consist of multiple query terms.
Even after a decade of research and discussion, strategic alignment, denoting the fit between information technology (IT) and business strategy, remains an enduring challenge for firms worldwide. In this paper, we go beyond the dominant firm-level alignment paradigm by utilizing a value disciplines perspective on strategic foci to conceptualize alignment at the process level. Theory would then suggest that alignment should be tightest in processes that are considered critical to each firm's strategic focus. Using data from matched surveys of IT and business executives at 241 firms, we detect support for this locus of alignment argument when alignment is identified using profile deviation or moderation. We also find a positive link between alignment and perceived IT business value in each of five primary processes in the value chain. By bringing a process-level view to the study of alignment and its impacts, we go beyond a discussion on the extent of fit-a cornerstone of the literature-to whether firms are pursuing the right type of fit for the particular mix of processes underlying their strategy. In this way, a process-level perspective can foster a deeper and more meaningful understanding of how alignment affects firm performance. Our results also show a need for managers to reconsider the steps taken to align IT and business strategy by looking more closely at how IT can support individual processes rather than at how IT can support an entire strategy.
The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.
Piracy of digital experience goods such as music recordings has received increased attention in the literature. Much of this research has focused on pricing policies, protection against piracy, and governmental policies in the software industry. In this research, we focus on pricing policies of producers of digital experience goods. We consider a heterogeneous consumer market with different segments, each having a different affinity to piracy. We analyze the effect of different producer pricing policies on the revenue of the creator of the product, who may be different than the producer. Our results indicate that the explicit incorporation of these different consumer segments will cause the producer to charge lower prices and, therefore, lead to higher legal product diffusion. We show that the royalty system does not solve the double marginalization problem and is suboptimal from a supply-chain perspective. Also, the creator of the goods prefers-a lower price than the producer's optimal price, and this tendency increases with the creator's per unit royalty.
The literature in general suggests that selling multiple versions is more profitable than selling only a single version. However, how many versions should be offered is not as clear. Classical pricing studies suggest providing as many versions as the number of customer types, whereas some studies in information systems suggest providing only one or two versions. In reality, firms typically provide more than one or two versions, such as three in the case of Goldilocks pricing. This study explains the discrepancies in these results and observations by showing that, although profit increases with more versions, the marginal benefit of an additional version decreases rapidly. Therefore, firms sell few versions even in the presence of very small versioning-related costs such as menu and cognitive costs. This study analyzes the effects of these costs, and shows that cognitive costs have more profound effects on versioning than menu costs.
Knowledge transferred in the open market via a price mechanism enjoys the benefits of avoiding internal competition, learning from external competitors, and accumulating diversified knowledge. In the market, users can access a repository of knowledge for a single price (repository pricing) or knowledge items in the repository can be sold individually (knowledge pricing). However, users have been found to prefer repository pricing but not the knowledge in the repository. This irrationality can cause market failure because users derive a suboptimal level of utility from the knowledge repository, and vendors have contradictory pricing and knowledge strategies. We empirically examine a joint explanation from two competing theoretical perspectives that accounts for this inconsistency nicely: The mental accounting perspective endorses repository pricing because it entices users with the benefits of the whole repository, whereas the transaction decoupling perspective finds expression in individually priced knowledge because it prevents the discrete benefit of knowledge from becoming obscure. By integrating the two theoretical perspectives and considering price, knowledge, and user characteristics simultaneously, the results offer important implications for the market transfer of knowledge. Repository pricing attracts users and is essential to initiate the transfer process, whereas knowledge pricing generates knowledge preference and is thus an effective approach for learning.
Efficiently delivering expected performance from information technology projects remains a critical challenge for many organizations. Improving our understanding of how various factors influence project performance is therefore an important research objective. This study proposes and tests a temporal model of information technology project performance (TMPP). It shows that performance can be better understood by separating risk factors into earlier (a priori) risk factors and later (emergent) risk factors, and modeling the influence of the former on the latter. Project performance, the dependent variable, is measured by considering both process (budget and schedule) and product (outcome) components. The model includes interactions between risk factors, project management practices, and project performance components. The model is tested using partial least squares analysis with data from a survey of 194 project managers. Our results indicate that the TMPP increases explanatory power when compared with models that link risk factors directly to project performance. The results show the importance for active risk management of recognizing, planning for, and managing a priori and emergent risk factors. The finding of a strong relationship between structural risk factors and subsequent volatility shows the need for risk management practice to recognize the interaction of a priori and emergent risk factors. The results confirm the importance of knowledge resources, organizational support, and project management practices, and demonstrate the ways in which they reinforce each other.
Media and network companies are increasingly providing digital media online. We develop a model to examine optimal strategies for media providers to utilize the online channel to distribute digital media. We examine a number of options for media providers. Our results suggest that media companies should sell programs online when content quality is relative high and online access cost is low. When online access cost is relative high, media providers could use the advertising strategy. Overall, companies are better off providing both pricing and advertising options to consumers. We derive the optimal price and advertising level, and analyze the factors that affect the price and advertising decisions. We find that as advertisement revenue rate increases, advertising level should be kept low. In addition, media companies should set online price and advertising level with consideration of the traditional channel in order to avoid channel cannibalization. We also analyze the advertising level in the traditional channel. Our results suggest that as digital video recorder technologies provide more convenience to consumers, media companies should increase, rather than decrease, revenues from advertising.
Advances in information-acquisition technologies and the increasing strategic importance of this information have created a market for consumers' personal and preference information. Behavioral research suggests that consumers engage in a privacy calculus where they trade off their privacy costs from sharing information against their value from personalization. Through a formal economic model of this personalization-for-privacy (p4p) trade-off, we examine welfare implications by characterizing consumption utilities as no-free-disposal functions. We investigate the optimality of four regulatory regimes (through allowance/disallowance of usage-enforcing technologies, and private contracts) by analyzing the strategic interaction between a monopolist who offers personalization services free of charge and two consumer types-privacy and convenience seekers. While many privacy watchdog groups have called for technology restrictions and more regulation, our research broadly suggests that society is better off with assignment of property rights over their information to consumers and full allowance of technological control and contractual abilities for the monopolist. However, when private contracts are proscribed, the regulator should also prevent the deployment of usage-enforcing technologies, particularly when the market is predominantly composed of privacy seekers. Interestingly, unlike traditional price-instrument markets for goods with free disposal, a regulator should not only encourage this market's knowledge of consumers' p4p preferences but also the various uses and benefits of preference information to the vendor.
This study examines the impact of culture on trust determinants in computer-mediated commerce transactions. Adopting trust-building foundations from cross-culture literature and focusing on a set of well-established cultural constructs as groups of culture (Type I and Type 11), this study develops a theoretical model of self-perception-based versus transference-based consumer trust in e-vendors, and empirically tests the model using cross-cultural data. The results show that transference-based trust determinants (i.e., perceived importance of third-party seal and perceived importance of positive referral) are more positively related to consumer trust in e-vendors in a Type II (i.e., collectivist-strong uncertainty avoidance-high long-term orientation-high context) culture than in a Type I (i.e., individualistic-weak uncertainty avoidance-low long-term orientation-low context) culture. Unlike the initial hypothesized expectations, self-perception-based trust determinants (i.e., perceived security protection, perceived privacy concern, and perceived system reliability) do not show stronger roles to consumer trust in e-vendors in a Type I culture than in a Type II culture, although the stronger negative effect of perceived privacy concerns is observed on consumer trust in e-vendors in a Type I culture than in a Type II culture. Theoretical contributions for e-commerce cross-culture literature and implications for multinational online business managers are discussed.
This study explores the process by which trust evolves over time. There have been a number of studies underscoring the importance of trust in the online environment. However, most trust studies have concentrated on the initial trust, and there is little known about how trust beliefs evolve over time. The dynamics of trust are of particular importance in the use of infomediaries (online information providers), among which health infomediaries are the most important for Web consumers in dealing with their wellness and health issues. We investigate the evolution of trust using the case of health infomediaries. The examination of the temporal changes in trust was carried out through two approaches-comparative statics and dynamic analyses. The research method was laboratory experiment and the data were collected for two episodes of encounters. Two comparative statics models and one dynamic model were estimated in order to examine the parameter changes from one episode of encounter to the next as well as the dynamics of belief changes. The results of analysis show that the structure of trust changes over time and information quality becomes the single most important antecedent in infomediary trust building in the later stages of use. Furthermore, our study also indicates that satisfaction plays an important role in changing Web customers' trust beliefs. Contributions as well as research and managerial implications are discussed.
As organizations increasingly utilize Web-based technologies to support customers better, trust in decision support technologies has emerged as an important issue in online environments. In this study, we identify six reasons users trust (or do not trust) a technology in the early stages of its use by extending the theories of trust formation in interpersonal and organizational contexts to that of decision support technologies. We study the particular context of decision support technologies for e-commerce: online recommendation agents (RAs), which facilitate users' decision making by providing advice on what to buy based on user-specified needs and preferences.;A laboratory experiment is conducted using a multimethod approach to collect data. Both quantitative data about participants' trust in RAS and written protocols that explain the reasons for their levels of trust are collected. A content analysis of the written protocols identifies both positive and negative trust attributions that are then mapped to six trust reasons. A structural equation modeling analysis is employed to test the causal strengths of the trust reasons in explaining participants' trust in RAS. The results reveal that in the early stages of trust formation, four positive reasons (i.e., knowledge-based, interactive, calculative, and dispositional) are associated with higher trust in RAS and two negative reasons (i.e., calculative and interactive) are associated with lower trust in RAS. The results also demonstrate some distinctive features of trust formation with respect to decision support technologies. We discuss the research and practical implications of the findings and describe opportunities for future research.
The topic of trust in information technology (IT) artifacts has piqued interest among researchers, but studies of this form of trust are not definitive regarding which factors contribute to it the most. Our study empirically tests a model of trust in IT artifacts that increases our understanding in two ways. First, it sets forth two previously unexamined system quality constructs-navigational structure and visual appeal. We found that both of these system quality constructs significantly predict the extent to which users place trust in mobile commerce technologies. Second, our study considers the effect of culture by comparing the trust of French and American potential users in m-commerce technologies. We found that not only does culture directly affect user trust in IT artifacts but it also moderates the extent to which navigational structure affects this form of trust. These findings show that system quality and culture significantly affect trust in the IT artifact and point to rich possibilities for future research in these areas.
High-quality customer service is an integral part of any successful enterprise, but providing it can be a challenge for online merchants, especially when customers are complaining about each other. This study examines how justice and trust affect user acceptance of e-customer services by conducting an online experiment involving 380 participants. The results suggest that trust in the e-customer service fully mediates the effects of trust in the service representative and procedural justice on intentions to reuse the e-customer service. Furthermore, the effect of distributive justice on trust in the e-customer service was fully mediated by trust in the e-service representative. Finally, the effect of informational justice on user intentions to reuse the e-customer service was partially mediated by trust in the service representative and trust in the e-customer service. Theoretical and practical implications are further discussed.
Trust is particularly important in online markets to facilitate the transfer of sensitive consumer information to online retailers. In electronic markets, various proposals have been made to facilitate these information transfers. We develop analytic models of hidden information to analyze the effectiveness of these regimes to build trust and their efficiency in terms of social welfare. We find that firms' ability to influence consumer beliefs about trust depends on whether firms can send unambiguous signals to consumers regarding their intention of protecting privacy. Ambiguous signals can lead to a breakdown of consumer trust, while the clarity and credibility of the signal under industry self-regulation can lead to enhanced trust and improved social welfare. Our results also indicate that although overarching government regulations can enhance consumer trust, regulation may not be socially optimal in all environments because of lower profit margins for firms and higher prices for consumers.
Trust is a crucial factor in e-commerce. However, consumers are less likely to trust unknown Web sites. This study explores how less-familiar e-commerce Web sites can use branding alliances and Web site quality to increase the likelihood of initial consumer trust. We use the associative network model of memory to explain brand knowledge and to show how the mere exposure effect can be leveraged to improve a Web site's brand image. We also extend information integration theory to explain how branding alliances are able to increase initial trust and transfer positive effects to Web sites. Testing of our model shows that the most important constructs for increasing initial trust in our experimental context are branding and Web site quality. Finally, we discuss future research ideas, limitations, implications, and ideas for practitioners.
We present an agenda for the future research that has the potential to extend the conceptual foundations of trust in online environments and to improve the practice in the domain. The agenda draws on the previous work on trust, the papers included in this Special Issue, and our perspective on the state of the literature. This agenda is structured into four components-nature and role of trust, moderators of trust, antecedents of trust, and empirical methods for examining trust.
This research explores the effect of the introduction of online reverse auctions (ORAs) on interorganizational trust between buyers and suppliers in the retail industry. Building upon the notion of the spirit of the technology and the organizing vision, we shed light on the equivoque nature of ORAs. In an integrative model, we show how the desocialization associated with the introduction to ORAs can lead to distrust. Our findings show specifically the importance of the role played by technical problems and rumors.
This paper focuses on the cultural effect of gender on the relationship of online word of mouth and trust in e-commerce. To encourage online commerce, many online retailers use online word-of-mouth systems, where consumers can rate products offered for sale. To date, how such ratings affect trust and adoption of e-commerce across genders has been relatively unexplored. We assess whether the effect of online trust on intention to shop online is moderated by gender. Our results show that the effect of trust on intention to shop online is stronger for women than for men. In addition, we find that men value their ability to post content online, whereas women value the responsive participation of other consumers to the content they have posted. Finally, we find that online word-of-mouth quality affects online trust differently across genders.
Despite rapidly increasing numbers of diverse online shoppers, the relationship of Web site design to trust, satisfaction, and loyalty has not previously been modeled across cultures. In the current investigation, three components of Web site design (information design, navigation design, and visual design) are considered for their impact on trust and satisfaction. In turn, relationships of trust and satisfaction to online loyalty are evaluated. Utilizing data collected from 571 participants in Canada, Germany, and China, various relationships in the research model are tested using partial least squares analysis for each country separately. In addition, the overall model is tested for all countries combined as a control and verification of earlier research findings, although this time with a mixed country sample. All paths in the overall model are confirmed. Differences are determined for separate country samples concerning whether navigation design, visual design, and information design result in trust, satisfaction, and ultimately loyalty-suggesting design characteristics should be a central consideration in Web site design across cultures.
The availability of data on the Web and new data extraction technologies have made it increasingly easy to reuse existing data to create new databases and provide value-added services. Meanwhile, database creators have been seeking legal protection for their data, such as the European Union's Database Directive. The legislative development shows that there is significant difficulty in finding the right balance between protecting the incentives of creating publicly accessible databases (including semistructured Web sites) and preserving adequate access to factual data for value-creating, activities. We address this issue using an extended spatial competition model that explicitly considers licensing provisions and inefficiencies in policy administration. The results show that, depending on the cost level of database creation, the degree of differentiation of the reuser database, and the efficiency of policy administration, there are different socially beneficial policy choices, such as protecting a legal monopoly, encouraging competition via compulsory licensing, discouraging voluntary licensing, or even allowing free riding. With the appropriate policy in place, both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents.
The Internet provides an additional channel for manufacturers to provide information about and sell their products. The electronic channel has the advantage of reduced search cost and its reach is increasing, but it has limited capability to provide product information. This paper examines how Internet technology affects a monopoly manufacturer's distribution problem in an environment where product information is important for consumers to identify their ideal product. The model suggests that a manufacturer uses the electronic channel in addition to the physical channel when the product information is very valuable and product information is largely about digital attributes, or when the product information is not valuable. The model also suggests that when the manufacturer chooses to sell through both channels, there is an increase in price competition between the two channels such that the manufacturer need not sell through the electronic retailer with the highest reach, Also, when a large proportion of consumers have access to both channels, the manufacturer may sell through only one channel. The paper also examines the case where the manufacturer operates in the electronic channel and the case where the retailers are integrated.
Collaboration in complex and dynamic environments such as hospitals, airlines, and disaster response teams is challenging. High performance requires smooth coordination across multiple groups whose incentives, cultures, and routines call conflict. In this paper, we present an in-depth case study of a hospital's operating room practices to understand challenges associated with multiple group coordination and how information technology may help. We use the concept of trajectory to focus our observations and interviews oil workflow across groups and critical events when coordination breaks down. A careful examination of the sources, coping mechanisms, and consequences of coordination breakdowns suggests three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) information systems integration, and (3) information pooling and learning at the organizational level. We conclude with technological recommendations to promote trajectory awareness and to automate information gathering and monitoring, so as to facilitate multiple group coordination in complex and dynamic task environments.
Information systems can serve as intermediaries between the buyers and the sellers in a market, creating an electronic marketplace that lowers the buyers' cost to acquire information about sellers' prices and product offerings. Although electronic trading systems provide potential to create an efficient market structure, we witness that it $45 trillion fixed-income market still makes little use of these systems. Low penetration of electronic trading systems in the marketplace is at odds with the existing information technology research doctrine. The reason is that the creation of efficient market structure through an-electronic marketplace is based on macro-level interfirm relationships that do not take into account the recurrent micro-level, interpersonal interaction among the market actors. Our empirical investigation, based on face-to-face interviews with 90 fixed-income senior managers and traders from 25 financial institutions, provides a unique insight into the social capital based on social networks of interpersonal relationships in the fixed-income market. Our research findings show that the market structure of embedded interpersonal ties enables participants to take advantage of information asymmetry for profit taking. As a result, imposition of solely electronic trading systems on the present fixed-income market structure is at odds with the present interfirm market norms and business processes enacted for large transactions among market makers and institutional investors.
Information technology (IT) adoption research recognizes theoretical limitations in discerning if and when user behavior results from perceived external influences or from personal volition. A clear understanding of this issue requires a precise distinction between mandatory and volitional behaviors. Consistent with organismic integration theory (OIT), this study situates the locus of user motivations inside the user. Drawing upon an endogenous view of behaviors, this research makes three key contributions. First, it develops the theoretical basis for clearly discerning if and when behavior results from perceived external influences or from personal volition. Specifically, it examines how endogenous psychological feelings of autonomy, freedom, conflict, and external pressure can predict and explain user intentions. Second, it proposes that behavior may result from combinations of perceived external influences and personal volition. Recognizing how such collections of motivations together influence behavior advances our understanding beyond the dichotomy of extrinsic versus intrinsic motivations often adopted in prior research. Third, it proposes that some desired behaviors may be thwarted or impeded by a conflict between perceived external influences and personal volition. The theoretically grounded research model was empirically validated in a field study on Blackboard, a Web-based education platform at a large university. Data collected from a sample of 211 users were tested using structural equation models of initial system adoption and experienced use. Empirical support was found for the proposed model and related hypotheses. The results of this study advance our understanding about user motivations for adopting IT.
Classical negotiation models are weak in supporting real-world business negotiations because these models often assume that the preference information of each negotiator is made public. Although parametric teaming methods have been proposed for acquiring the preference information of negotiation opponents, these methods suffer from the strong assumptions about the specific utility function and negotiation mechanism employed by the opponents. Consequently, it is difficult to apply these learning methods to the heterogeneous negotiation agents participating in e-marketplaces. This paper illustrates the design, development, and evaluation of a nonparametric negotiation knowledge discovery method which is underpinned by the well-known Bayesian learning paradigm. According to our empirical testing, the novel knowledge discovery method can speed up the negotiation processes while maintaining negotiation effectiveness. To the best of our knowledge, this is the first nonparametric negotiation knowledge discovery method developed and evaluated in the context of multi-issue bargaining over e-marketplaces.
This study synthesizes the research findings of 82 empirical studies on user participation in information systems development (ISD). Various ISD outcomes are addressed using a classification scheme involving two broad categories-attitudinal/ behavioral outcomes and productivity outcomes. The results demonstrate that user participation is minimally-to-moderately beneficial to ISD; its effects are comparatively stronger on attitudinal/behavioral outcomes than on productivity outcomes. This attitudinal/behavioral impact may largely be the result of the emphasis that has been placed on user participation by academics and consultants. The results of this analysis are compared to those of a meta-analysis in the broader management context of participation. The results are similar in terms of attitudinal outcomes, but different, and lesser, in terms of productivity outcomes. Since the current status of research in the broad area of participation is that the effects of participation are considered to be problematic, that status and the results of this study suggest that user participation alone may not be sufficient to achieve significantly improved ISD outcomes, and that different strategies should be employed based on the specific goals of ISD projects. If system acceptance is the ultimate goal, user participation should be designed to induce more psychological involvement among potential users. If productivity benefits are the focus, user participation should be designed to provide developers the needed domain knowledge. In sum, user participation should be treated as one of a number of means for ISD projects to be more successful.
Information technology (IT) services providers are exposed to exogenous risks faced by the industry as a whole, and endogenous risks from their Current portfolio of IT contracts. This exposure may lead to cost overruns or legal responsibility for service-level breeches. Providers can leverage information about their risk positions implied by their IT services contract portfolios to gain strategic advantage over their competitors. We build theory in support of a new construct, profit-at-risk, for evaluating the trade-offs between contract profitability and service-level risk, stemming from financial economics theory and models. We simulate an IT services contract portfolio, and show how managers can reduce organizational risk by forgoing profit-maximizing contracts in lieu of more conservative service-level agreements, yet still achieve high returns. Our approach provides decision support for ex ante contract evaluation and negotiation, and a means to conduct ex post efficiency evaluation. It also aligns IT service management with best practices in financial management.
The diffusion of innovation theory is deployed to investigate the global assimilation of collaborative information technologies (CITs). Based on the concepts of IT acquisition and utilization, an assimilation framework is presented to highlight four states (limited, focused, lagging, and pervasive) that capture the assimilation of conferencing and groupware CITs. Data collected from 538 organizations in the United States, Australia, Hong Kong, Norway, and Switzerland are aggregated and analyzed to explore assimilation patterns and the influence of decision-making pattern, functional integration, promotion of collaboration, organization size, and IT function size on the assimilation of CITs. Although most of these factors influence assimilation of CITs from nonadoption to a state of limited assimilation, and from limited assimilation to a state of pervasive assimilation, they may not be critical when assimilation of CITs deviates from the expected path. The implications of our findings are discussed for practice and research on assimilation of CITs.
Online reputation systems are intended to facilitate the propagation of word of mouth as a credibility scoring mechanism for improved trust in electronic marketplaces. However, they experience two problems attributable to anonymity abuse-easy identity changes and reputation manipulation. In this study, we propose the use of stylometric analysis to help identify online traders based on the writing style traces inherentin their posted feedback comments. We incorporated a rich stylistic feature set and developed the Writeprint technique for detection of anonymous trader identities. The technique and extended feature set were evaluated on a test bed encompassing thousands of feedback comments posted by 200 eBay traders. Experiments conducted to assess the scalability (number of traders) and robustness (against intentional obfuscation) of the proposed approach found it to significantly outperform benchmark stylometric techniques. The results indicate that the proposed method may help militate against easy identity changes and reputation manipulation in electronic markets.
Information availability has increased consumers' informedness, the degree to which they know what is available in the marketplace, with precisely which attributes and at precisely what price. This informedness has altered the demand side of market behavior: customers now discount more heavily when comparable products are available from competitors and when products do not meet their wants, needs, cravings, and longings, but they no longer discount as heavily when purchasing unfamiliar products. Changes in the demand side are producing comparable changes in the supply side: firms earn less than their expectations when competing in traditional mass-market fat spots, while earning far more than previously when entering newly created resonance marketing sweet spots. We trace the impact of hyperdifferentiation and resonance marketing on strategy, with a clear progression from a limited number of fat spots, through reliance on line extensions, and ultimately to fully differentiated market sweet spots.
We adapt the event study methodology from research in financial economics to study the impact of government enforcement and economic opportunities on information security attacks. We found limited evidence that domestic enforcement deters attacks within the country. However, we found compelling evidence of a displacement effect: U.S. enforcement substantially increases attacks originating from other countries. We also found strong evidence that attackers are economically motivated in that the number of attacks is increasing in the U.S. unemployment rate. Our findings were robust to differences in the effective time window of enforcement and the measurement of vulnerabilities.
We investigate whether greater market competition improves or inhibits the ability of feedback systems in Internet markets to deliver trust and trustworthiness to the marketplace. Our investigation is grounded in the theory of signaling from information economics. Using methods from experimental economics, we create a laboratory online market where sellers face a moral hazard. We manipulate the level of market competition and the nature of the social network behind the feedback system and study the affect on trust, trustworthiness, and market efficiency. We find that competition in strangers networks, where market encounters are one-shot and reputation information is communicated through outside parties, improves trust, trustworthiness, and market efficiency. The efficiency advantage that partners networks, where a buyer can maintain a repeated relationship with a seller, have over strangers networks largely vanishes with the introduction of competition. This is because the difference in the pattern of social networking largely disappears. Overall, encouraging competition leads to more effective feedback systems in Internet markets. We discuss implications for trader strategy and Internet market design.
Real options analysis is an important but costly tool for valuing many information technology (IT) investments. As a low-cost substitute for real options-based methods, firms often depend on managerial intuition, which sometimes approximates real options-based valuations and sometimes does not. Making good choices about how to value IT investments requires an understanding of why, and therefore when, intuitive judgment is more or less likely to be consistent with real options-based valuations. Field and survey studies have provided ex post observations of systematic variations in consistency by option type, but ex ante hypotheses explaining this variation have been rare. This study uses two behavioral economic theories to predict option-type-specific differences between intuitive judgments and real options prescriptions. Regret theory posits that individuals will value decision outcomes based on both the expected utility of payoffs and on anticipated regret for not having made an alternative decision. As a consequence, intuitive IT investment decisions are less aggressive as uncertainty increases (higher valuation of deferral options, lower valuation of growth options), in contrast to higher normative values for both real option types with higher uncertainty. Consistent with competitive behavior theories that predict overaggressive behavior to contest market behavior, intuitive IT investment decisions are more aggressive in the presence of a potential competitor (lower valuation of deferral and higher valuation of growth options), holding constant the normative value of the options. We present experimental evidence consistent with these predictions. ;An important implication of our results is that future research should not test for general consistency between intuitive judgment and real options theory, but should identify and explain systematic variation in consistency across option types and settings. Such variation is important in practice because it determines when intuitive judgment is and is not likely to be an adequate substitute for costly formal real options valuation. It also determines when training in real options concepts needs to be more intensive to overcome inconsistency with intuitive judgment, and when the outputs of formal real options valuation are likely to be unintuitive and thus not readily acceptable to managers with limited option theory training.
Organizations are faced with a variety of information security threats and implement several information system security countermeasures (ISSCs) to mitigate possible damage due to security attacks. These security countermeasures vary in their ability to deal with different types of security attacks and, hence, are implemented as a portfolio of ISSCs. A key challenge for organizations is to understand the economic consequences of security attacks relative to the ISSC portfolio implemented. This paper combines the risk analysis and disaster recovery perspectives to build an integrated simulation model of ISSC portfolio value. The model incorporates the characteristics of an ISSC portfolio relative to the threat and business environments and includes the type of attack, frequency of attacks, possible damage, and the extent and time of recovery from damage. The simulation experiments provide interesting insights into the interactions between ISSC portfolio components and characteristics of business and threat environments in determining portfolio value.
Firms face many different types of information security risk. Inadvertent disclosure of sensitive business information represents one of the largest classes of recent security breaches. We examine a specific instance of this problem-inadvertent disclosures through peer-to-peer file-sharing networks. We characterize the extent of the security risk for a group of large financial institutions using a direct analysis of leaked documents. We also characterize the threat of loss by examining search patterns in peer-to-peer networks. Our analysis demonstrates both a substantial threat and vulnerability for large financial firms. We find a statistically significant link between leakage and leak sources including the firm employment base and the number of retail accounts. We also find a link between firm visibility and threat activity. Finally, we find that firms with more leaks also experience increased threat.
Information technology (IT) advances often create turmoil and disturb existing industry structures. In the travel industry, electronic distribution has existed for decades via the global distribution systems (GDSs), reservation systems that were introduced in the early 1980s on mainframe platforms. Yet with the Internet, new digital intermediaries have threatened the viability of these legacy GDSs. We examine this transformation of e-travel distribution to test the theory of newly vulnerable markets, which predicts how markets become vulnerable to fundamental changes triggered by IT. The tenets of newly vulnerable markets theory are supported. The GDS market became newly easy to enter due to a decrease in barriers to entry caused by the Internet and other technologies, attractive to attack due to their out-of-date and inefficient pricing mechanisms, which made opportunistic pickoff possible across customer profitability gradients, and difficult to defend due to their lack of vision and strategic inflexibility. We use our findings to expand the application of this theory to newly vulnerable e-markets, in general.
This research investigates the relationship between a manufacturer's use of information technology (IT) (particularly electronic procurement) and the number of suppliers in its supply chain. Will a manufacturer use more or fewer suppliers due to the increasing use of IT? Based on data from a sample of 150 U.S. manufacturers, we find no direct relationship between e-procurement and number of suppliers at the aggregate level. However, when we distinguish the type of goods purchased, we find that the use of electronic procurement is associated with buying from more suppliers for custom goods but from fewer suppliers for standard (or commodity) goods. It is possible that for commodity goods, an efficiently functioning transparent market ensures that a few suppliers are sufficient, whereas for custom goods the need for protection from opportunistic vendor holdup leads to the use of more suppliers. Further, the positive relationship between number of suppliers and electronic procurement for custom goods is negatively moderated by deeper buyer-supplier system integration. This implies that such integration can help buyers obtain better fit for their customized requirements, an alternative to increasing fit by employing more suppliers as proposed in the extant literature.
Firms have been increasing their information technology (IT) security budgets significantly to deal with increased security threats. An examination of current practices reveals that managers view security investment as any other and use traditional decision-theoretic risk management techniques to determine security investments. We argue in this paper that this method is incomplete because of the problem's strategic nature-hackers alter their hacking strategies in response to a firm's investment strategies. We propose game theory for determining IT security investment levels and compare game theory and decision theory approaches on several dimensions such as the investment levels, vulnerability, and payoff from investments. We show that the sequential game results in the maximum payoff to the firm, but requires that the firm move first before the hacker. Even if a simultaneous game is played, the firm enjoys a higher payoff than that in the decision theory approach, except when the firm's estimate of the hacker effort in the decision theory approach is sufficiently close to the actual hacker effort. We also show that if the firm learns from prior observations of hacker effort and uses these to estimate future hacker effort in the decision theory approach, then the gap between the results of decision theory and game theory approaches diminishes over time. The rate of convergence and the extent of loss the firm suffers before convergence depend on the learning model employed by the firm to estimate hacker effort.
Many Internet intermediaries operate two-sided networks, that is, they provide platforms to bring together two types of participants, or sides, such as buyers and sellers. This paper develops a model that characterizes the intermediary's pricing in two-sided networks, the value created by these networks, and the allocation of that value across the two sides. It extends the two-sided networks literature by endogenizing the level of network effects as the result of relevant investments by the intermediary, which determine the design of the network. It shows that under certain assumptions about the available technologies, the design of the two-sided network is highly asymmetric independent of its ownership structure. The paper provides insight into design strategies for Internet platforms, and it discusses their welfare implications.
The use of a well-defined process is a widely recognized approach to increasing quality and productivity in software development. Building software processes from scratch each time is expensive and risky. Therefore, they are often created by tailoring existing processes and standards. Process tailoring is a knowledge-intensive activity. This research explores the link between knowledge support and software process tailoring performance under different levels of tailoring task complexity. It theoretically develops and tests how the fit between knowledge (generalized and contextualized) and software tailoring task complexity influences process tailoring performance. Process tailoring performance is conceptualized in terms of effectiveness and efficiency. The results from an experiment and a protocol analysis show that contextualized knowledge outperforms generalized knowledge in improving tailoring performance, and that such improvement in performance is greater in complex process tailoring tasks when compared to simple tasks.
E-markets have been established in many industries as a sourcing option for buyers. The existing literature focuses on the substitutional effect of e-markets on the traditional supply chain, yet in many situations, e-markets are used by buyers as a benchmarking tool in negotiations with traditional suppliers. This paper examines the role of e-markets in price negotiations and relationship-specific investments. We find that e-markets can be an effective tool to stimulate the traditional supplier's relationship-specific investments, lower the procurement prices, and improve the buyer's profitability and the supply-chain efficiency. Therefore, e-markets can complement rather than substitute for the traditional relationship-based supply chain. When there is quality uncertainty in the e-market offering, two effects of quality uncertainty on e-market adoption are identified. Better quality on average will increase e-market adoption, but surprisingly, increasing quality dispersion of e-markets will also help. Therefore, e-markets should strive to enlist suppliers with better quality products, but do not need to worry too much about the quality dispersion. Having better price transparency will also help in attracting more business from buyers.
Electronic government is being increasingly recognized as a means for transforming public governance. Despite this increasing interest, information systems (IS) literature is mostly silent on what really contributes to the success of e-government Web sites. To fill this gap, this study examines the role of trust in e-government success using the updated Delone and McLean IS Success model as e theoretical framework. The model is tested via a survey of 214 Singapore e-government Web site users. The results show that trust in government, but not trust in technology, is positively related to trust in e-government Web sites. Further, trust in e-government Web sites is positively related to information quality, system quality, and service quality. The quality constructs have different effects on intention to continue using the Web site and satisfaction with the Web site. Post hoc analysis indicates that the nature of usage (active versus passive users) may help us better understand the interrelationships among success variables examined in this study. This result suggests that the DeLone and McLean model can be further extended by examining the nature of IS use. In addition, it is important to consider the role of trust as well as various Web site quality attributes in understanding e-government success.
It is recognized that change management is necessary for information technology implementation success. While there are a growing number of interorganizational systems (IOS) designed for the public, there is little study of and lack of clear guidelines on managing change related to their implementation. This research explores the phenomenon through the case study of a country-wide farecard system implemented in Singapore's public transportation system that involved several organizations and the public. Through the case analysis, we identified critical success factors (CSFs) for change management in IOS for the public and interrelated them using a causal loop diagram (CLD). These factors included refinements of existing CSFs identified from the literature as well as new CSFs from our case study. Our case analysis showed that communication through senior management and cooperation of affected organizations in the system implementation was able to overcome resistance to change in these organizations. We also found that while comprehensive publicity C could initiate change in the public, a critical mass had to be built up for managing public 7 change by coopting public opinion leaders as well. By interrelating CSFs identified in the case via a CLD, this study provides a preliminary theoretical framework for studying change management in IOS for the public and aims to guide practitioners in implementing Such systems.
In this paper, we examine how the motivations and attitudes of open source software (OSS) developers affect their preference among the three common OSS license types-Strong-Copyleft, Weak-Copyleft,and Non-Copyleft. Despite the importance of the license type and developers to OSS projects, there is little understanding in open source literature of the license choice from a developer's perspective. The results from our empirical study of OSS developers reveal that the intrinsic motivation of challenge (problem solving) is associated with the developers' preference for licenses with moderate restrictions, while the extrinsic motivation of status (through peer recognition) is associated with developers' preference for licenses with least restrictions. We also find that when choosing an OSS license, a developer's attitude toward the software redistribution rights conflicts with his or her attitude toward preserving the social benefits of open source. A major implication of our findings is that OSS managers who want to attract a limited number of highly skilled programers to their open source project should choose a restrictive OSS license. Similarly, managers of software projects for social programs could attract more developers by choosing a restrictive OSS license.
Open source software (OSS) is now posing significant competition to proprietary or closed source software (CSS) in several software markets. In this paper, we characterize the response of a firm developing CSS (where the CSS is a revenue earner) to the presence of an OSS in its market. In particular, we look at the firm's choice of resource investments to improve quality and the firm's pricing decisions. We are primarily motivated by the following questions: Would a firm producing CSS produce higher-quality software when it faces competition from ail OSS than when there is no OSS in its market? Would there be a change in the firm's response if the CSS faced competition from another CSS in addition to competition from the OSS? We show that the firm produces lower-quality CSS when it faces competition from an OSS than when it does not. Also. the quality of the CSS decreases as the quality of the OSS increases. This result holds true even if we consider network effects. When we consider competition from another CSS, in addition to competition from the OSS, then the quality of the CSS could increase or decrease as the quality of the OSS increases. The change in quality depends on how closely substitutable the two CSS are. We also extend our base model to consider (1) competition for resources, (2) uncertainty in resources available to the OSS, and (3) uncertainty about the software development process.
The application of real options techniques to information security is significantly different than in the case of general information technology investments due to characteristics unique to information security. Emerging research in the economics of information security has suggested real options analysis (ROA) as a potential technique for assessing the value of information security assets, but has focused primarily on the most effective level of investment and the configuration of intrusion prevention/detection systems. In this paper, we attempt to address significant gaps ill the literature by developing an integrated real options model for information security investments using Bayesian statistics that Incorporates learning and postauditing in the analysis. By using the proposed model with actual data on e-mail and Spain, we demonstrate that ROA with Bayesian postauditing offers a systematic valuation and risk management framework for evaluating information security spending by firms. We also discuss the managerial implications.
In order to better understand the sociopsychological factors involved in employees' adaptation to new technology in organizations, we examine the role that two types of social networks-supportive and informational-play in individual adaptation to IT-induced change in a large financial company. Using survey data from 371 employees working in 133 different branches of the organization, we find that several aspects of the social networks relate to quality of employees' adaptation to the new technology as assessed by the company's departmental directors. Specifically, the size of the support network as well as the strength and density of the information network significantly predict employees' adaptation to the new system. We conclude the paper by discussing theoretical implications for the relevance of social network research for members' adaptation to organizational changes as well as outlining specific implications for practice.
We develop three auction-based pricing and allocation solution methods for the case where a capacity-constrained online service provider offers multiple classes of unique, one-time services with differentiated quality. Consumers desire exactly one of the many service classes offered. We call such a setting a vertically integrated online services market. Examples of these services are webcasting of special events over the Inter-net, provision of video-on-demand, and allocation of grid computing resources. We model the pricing and allocation decision faced by firms in such a setting as a knapsack problem with an added preference elicitation dimension. We present a variety of computational Solution approaches based on adaptations of the traditional g for knapsack problems. The solution approaches vary in efficacy greedy heuristic I depending on whether bidders are restricted to bid in one service class or allowed to bid in multiple service classes, as well as on the overall variability of the demand. In the case bidders can bid in multiple classes but are interested in consuming only one class, a direct application of the heuristics developed for the single service case results in a nonfair allocation. We develop a novel data structure to eliminate the unfair allocation while maintaining the original computation complexity of the simpler setting. The paper contributes by presenting a menu of auction clearing mechanisms for selling vertically integrated online services.
Real-world predictive data mining (classification or regression) problems are often cost sensitive, meaning that different types of prediction errors are not equally costly. While cost-sensitive learning methods for classification problems have been extensively studied recently, cost-sensitive regression has not been adequately addressed in the data mining literature yet. In this paper, we first advocate the use of average misprediction cost as a measure for assessing the performance of a cost-sensitive regression model. We then propose an efficient algorithm for tuning a regression model to further reduce its average misprediction cost. In contrast with previous statistical methods, which are tailored to particular cost functions. this algorithm can deal with any convex cost functions without modifying the underlying regression methods. We have evaluated the algorithm in bank loan charge-off forecasting, where underforecasting is considered much more costly than overforecasting. Our results show that the proposed algorithm significantly reduces the average misprediction costs of models learned with various base regression methods,such as linear regression, model tree, and neural network. The amount of cost reduction increases as the difference between the unit costs of the two types of errors (overprediction and underprediction) increases.
In online shopping environments, the product-advising function originally performed by salespeople is being increasingly taken over by software-based product recommendation agents (PRAs). However, the literature has mostly focused on the functionality design and utilitarian value of such decision support systems, mostly ignoring the potential social influence they could exert on their users. ;The objective of this study is to apply a social relationship perspective to the design of interfaces for PRAs. We investigate the effects of applying anthropomorphic interfaces-namely, humanoid embodiment and voice output-on users' perceived social relationship with a technological and software-based artifact designed for electronic commerce contexts. The findings from a laboratory experiment indicate that using humanoid embodiment and human voice-based communication significantly influences users' perceptions of social presence, which in turn enhances users' trusting beliefs, perceptions of enjoyment, and ultimately, their intentions to use the agent as a decision aid. These results extend the applicability of theories concerning traditional shopper-sales person relationships to customers' interactions with technological artifacts residing on Web sites-that is, the recommendation agent software-and provide practitioners with guidelines on how to design Internet stores with the goal of building social relationships with online shoppers and enhancing their overall shopping experiences.
While many of the changes that information systems professionals encounter are incremental requiring only modifications to an existing mind-set, others represent radical changes that require a fundamental shift in mind-set in order to fully benefit from the change. The goal of this study was to examine how software development concept thinking changes as individuals shift their mind-set to the object-oriented software development approach. The results indicate that the transition may not be as simple as a single aha! moment, but rather a series of cognitive shifts as constructs from the old mind-set are replaced by constructs in the new mind-set. In addition, this study identifies problems that may be encountered during the transition due to a lack of cognitive fit, confusion, and reverting to an inappropriate mind-set. The findings from this study may be used to decrease the learning curve and minimize the number of cognitive shifts experienced during the transition.
Some economic and informational problems associated with organizational information technology (IT) spending may be attributed to managerial rent-seeking. Because of the unavoidable incompleteness of labor contracts, managers with misaligned incentives and budgetary discretion could entrench themselves through their non-value-maximizing adoption decisions. In order to boost their bargaining power in future contract renegotiation, they invest excessively in technologies they manage more effectively than their potential rivals. In addition, they tend to adopt technologies that can create large information asymmetries giving them significant knowledge advantage over their potential rivals ex post. We study the implications and effects of their rent-seeking behavior within the context of organizational IT adoption and management. The efficacies and the limitations of formal incentive contracting are discussed to underscore the need for additional governance mechanisms. While knowledge management may mitigate some of the agency problems associated with entrenchment, managerial self-policing issue remains a challenge. We further explore the incentive provision potential of relational labor contracts in combating entrenchment.
Analysis of prevalent document management practices shows the popular use of categories (e.g., folders) to organize documents for subsequent searches and retrievals. The coherence and distinction of an existing document category can diminish considerably as influxes of new documents arrive over time. The complexity of and effort requirements for document-category management favor an automated approach that can be supported by appropriate document-clustering techniques. A review of the extant literature shows a predominant focus on document content analysis in automated document-category management, which cannot preserve the user's document-grouping preferences. This research develops two advanced evolution-based techniques for preserving user preferences in their management of document categories. The first technique (CE2), which supports the automated evolution of a set of flat (i.e., nonhierarchical) document categories, extends a promising evolution-based technique (category evolution, CE) by addressing its fundamental limitations inherent to the use of holistic measures. The second technique, category hierarchy evolution (CHE), is developed on the basis of CE2 to support scenarios where document categories are organized with a hierarchical structure. Empirical evaluations of the effectiveness of each technique in various category evolution scenarios created using two different document corpora (i.e., news documents from Reuters and research articles from the ACM digital library), as compared with those of associated salient techniques for benchmark purposes, show that CE2 and CHE outperform their respective benchmark techniques. Their performance is reasonably robust and appears more effective when the quality (coherence) of the previously created categories does not deteriorate excessively. According to our results, the evolution-based approach is viable, appealing, and capable of preserving user preferences in automatic reorganizations of document categories.
The ascension of e-business has significantly changed competence requirements of information technology (IT) professionals. In this paper, we derive a competence set that addresses these changes and investigate individual preferences for specific competence components within e-business teams. We connect these preferences and competence valuation with personal characteristics of team members that were found to influence the perception of competence requirements in previous research. To empirically address this issue, we apply a Web-based questionnaire with adaptive conjoint measurement. By cluster analysis, we identify four competence profiles preferred by team members. Data from 176 respondents suggest that experience is related to the preferred profile, whereas expertise is related to overall competence valuation. Our research suggests that immature teams should consider that preferences regarding IT professionals may change with venture maturation, whereas interdisciplinary teams should discuss each member's value contribution. Considering our results, these suggestions could optimize the process of team composition.
A large body of research has examined the performance effects of diversification. However, these results have been mixed, and scholars have called for examining contingencies under which the effects of different types (related, unrelated, geographic) and levels of diversification on performance vary. This study attempts to fill this gap in the literature by arguing that examining the performance effects of diversification is incomplete without taking into consideration the firm's information technology (IT) spending. We posit that IT, by enabling coordination and control in firms, is likely to moderate the relationship between diversification and performance. Combining arguments from both resource-based view and the organizational controls literature, we theorize about the moderating effects of IT spending under different types and levels of diversification. Using data from large U.S. manufacturing firms, we test our research hypotheses. Our results indicate that while IT spending interacts with related diversification to have a positive effect on firm performance, similar interactions with unrelated diversification do not have any effects on firm performance. Furthermore, the interaction between IT spending and geographic diversification is positively associated with performance only when the level of geographic diversification is low. We interpret and discuss these results and highlight the theoretical and practical implications of our findings.
Although the online business-to-consumer (B2C) channel is the primary selling channel for digital content (e.g., videos, images, and music), modern digital technology has made possible the legal dissemination of such content over the consumer-to-consumer (C2C) channel through personal computing devices, such as PCs, mobile phones, and portable media players. This paper investigates the optimal channel structure and the corresponding pricing and service strategies for digital content distribution in order to understand the business value of introducing the C2C channel alongside the prevailing B2C channel. We identify conditions under which it is more profitable to use both B2C and C2C channels simultaneously (i.e., the dual-channel distribution). In such cases, the seller performs price discrimination among consumers but provides them with a higher level of service. Our analysis further characterizes the benefits of service provision. We show that service provision can increase the dual-channel pricing flexibility, reduce the seller's B2C channel dependence, and allow the seller to tolerate higher C2C channel redistribution costs. Finally, in examining the effect of a competitively determined B2C channel price on optimal channel strategy, we find that the seller prefers a dual-channel distribution under higher B2C channel prices.
Prior research into open source software (OSS) developer participation has emphasized individuals' motivations for joining these volunteer communities, but it has failed to explain why people stay or leave in the long run. Building upon Lave and Wenger's theory of legitimate peripheral participation (LPP), this paper offers a longitudinal investigation of one OSS community in which sustained participation is hypothesized to be associated with the coevolution of two major elements of LPP theory: situated learning (the process of acting knowledgeably and purposefully in the world) and identity construction (the process of being identified within the community). To test this hypothesis, data were collected from multiple sources, including online public project documents, electronic mail messages, tracker messages, and log files. Results from qualitative analyses revealed that initial conditions to participate did not effectively predict long-term participation, but that situated learning and identity construction behaviors were positively linked to sustained participation. Furthermore, this study reveals that sustained participants distinguished themselves by consistently engaging in situated learning that both made conceptual (advising others) and practical contributions (improving the code). Implications and future research are discussed.
To ensure that knowledge repositories contain high-quality knowledge, knowledge management research recommends that contributions to a repository undergo stringent validation processes. To date, however, no research has studied the impact of such processes on contributors' repository-related perceptions or behaviors. To address this gap, we develop a model based on signaling theory and reinforcement theory to explain how individuals' perceptions of three primary validation process characteristics (duration, transparency, and restrictiveness) impact their perceptions of repository knowledge quality and their contribution behaviors. Our empirical results confirm the importance of implementing review processes that are transparent and developmentally oriented as a way of encouraging knowledge contribution. More broadly, this study underscores the need to develop integrated theoretical models that draw from a variety of reference theories when attempting to explain knowledge-related behaviors.
We investigate the assimilation of electronic procurement innovations (EPIs) and its impact on procurement productivity in buyer organizations. We identify online reverse auctions, electronic catalog management, electronic order fulfillment, and electronic payment and settlement as moderate complements for the performance of the procurement process. We develop a theoretical model that is informed by the literature on innovation assimilation and by structuration theory to explain the aggregated assimilation of EPIs. Our empirical study is based on survey data collected about EMS from 166 buyer firms. Based on our analysis, we isolate the organizational, technological, and interorganizational factors that shape the meta-structures for the aggregated assimilation of EMS. Our results also provide evidence of a substantial impact of the assimilation of these innovations on procurement productivity. Our post hoc analysis provides insights on differences across stages and across EMS on the factors and meta-structures that enable assimilation.
A new empirical model for the production function of the hospital incorporating two types of information systems (IS) is developed. One type of IS is representative of information technology (IT) used in primary, clinical, value-chain activities, and the other is representative of the IT used in support (administrative) value-chain activities. The model innovation is that it accommodates up to a seven-year lag for each type of IS. The output variables for the production function are hospital output and medical labor productivity. Using data spanning from 1979 to 2006 from several hospitals, it was found that clinical IS improve hospital output in the short run (of two years). Administrative IS were found to be negatively associated with organizational performance in the short run, but positively associated with these performance measures over the long run (over four years). These results highlight the importance of timing IT investments and the sequencing chosen for the implementation of IS presenting various value-chain activities, and the resulting pattern of business value over time. Differential lag length of the types of IS is to be considered in estimating the rate of return of new IT projects.
Process satisfaction is one important determinant of work group collaborative system adoption, continuance, and performance. We explicate the computer-mediated communication (CMC) interactivity model (CMCIM) to explain and predict how interactivity enhances communication quality that results in increased process satisfaction in CMC-supported work groups. We operationalize this model in the challenging context of very large groups using extremely lean CMC. We tested it with a rigorous field experiment and analyzed the results with the latest structural equation modeling techniques. Interactivity and communication quality dramatically improved for very large groups using highly lean CMC (audience response systems) over face-to-face groups. Moreover, CMC groups had fewer negative status effects and higher process satisfaction than face-to-face groups. The practical applications of lean CMC rival theoretical applications in importance because lean CMC is relatively inexpensive and requires minimal training and support compared to other media. The results may aid large global work group continuance, satisfaction, and performance in systems, product and strategy development, and other processes in which status effects and communication issues regularly have negative influences on outcomes.
Knowledge management is essential to modem organizations. Due to the information overload problem, managers are facing critical challenges in utilizing the data in organizations. Although several automated tools have been applied, previous applications often deem knowledge items independent and use solely contents, which may limit their analysis abilities. This study focuses on the process of knowledge evolution and proposes to incorporate this perspective into knowledge management tasks. Using a patent classification task as an example, we represent knowledge evolution processes with patent citations and introduce a labeled citation graph kernel to classify patents under a kernel-based machine learning framework. In the experimental study, our proposed approach shows more than 30 percent improvement in classification accuracy compared to traditional content-based methods. The approach can potentially affect the existing patent management procedures. Moreover, this research ends strong support to considering knowledge evolution processes in other knowledge management tasks.
Ideas competitions appear to be a promising tool for crowdsourcing and open innovation processes, especially for business-to-business software companies. Active participation of potential lead users is the key to success. Yet a look at existing ideas competitions in the software field leads to the conclusion that many information technology (IT)-based ideas competitions fail to meet requirements upon which active participation is established. The paper describes how activation-enabling functionalities can be systematically designed and implemented in an IT-based ideas competition for enterprise resource planning software. We proceeded to evaluate the outcomes of these design measures and found that participation can be supported using a two-step model. The components of the model support incentives and motives of users. Incentives and motives of the users then support the process of activation and consequently participation throughout the ideas competition. This contributes to the successful implementation and maintenance of the ideas competition, thereby providing support for the development of promising innovative ideas. The paper concludes with a discussion of further activation-supporting components yet to be implemented and points to rich possibilities for future research in these areas.
Blogging is becoming increasingly popular as a global phenomenon. Individual blog traffic and blogosphere structure are of interest to academia and practice. Although it is difficult to get a snapshot of the blogosphere with enough blogs over a long enough period to capture the real situation, chaos theory finds underlying order in this apparent random and complex phenomenon. This study provides an overall view of blogging from micro (individual blog traffic dynamics) and macro (blogosphere structure) levels through a chaos theory lens. Key concepts of chaos theory are used to construct an interpretive framework to illustrate blog system behavior dynamics. Blog systems tend to be nonlinear, dynamic, and deterministic, as well as sensitive to initial conditions. The study also demonstrates the feasibility of applying chaos theory thinking to areas such as knowledge management and the recent global financial crisis. Implications for practice and research opportunities are presented.
E-learning has seen tremendous growth in recent years. More and more, university courses are now available online to a potentially global audience. However, a significant shortcoming of e-learning technologies has been poor support for group-oriented learning. We believe that virtual worlds offer a potential solution. Unlike videoconferencing (for instance), virtual worlds provide a shared visual space for students to meet and interact (via avatars). Not only do students share the quasi-realism of a 3D environment where participants can see and hear one another, they also have the capability to manipulate artifacts together. These factors provide a strong sense of group presence, which leads to engaging group learning interactions.
How does self-governance happen in Wikipedia? Through in-depth interviews with 20 individuals who have held a variety of responsibilities in the English-language Wikipedia, we obtained rich descriptions of how various forces produce and regulate social structures on the site. Although Wikipedia is sometimes portrayed as lacking oversight, our analysis describes Wikipedia as an organization with highly refined policies, norms, and a technological architecture that supports organizational ideals of consensus building and discussion. We describe how governance on the site is becoming increasingly decentralized as the community grows and how this is predicted by theories of commons-based governance developed in offline contexts. We also briefly examine local governance structures called WikiProjects through the example of WikiProject Military History, one of the oldest and most prolific projects on the site.
Information systems (IS) research often attempts to examine and explain how technology leads to outcomes through usage of IS. Although extensive research in this area has resulted in a significant number of theories, limited work has been done on integrating these theories. This paper presents adaptive structuration theory (AST) as a meta-theory for examining IS within an organizational context. The two main contributions of the paper are an understanding of meta-theory's role in IS and building a case for using AST as a meta-theory to (1) provide an overarching perspective for understanding and integrating existing literature and theories, (2) provide a template and set of guidelines for creating better context-specific IS models and theories, and (3) provide a deeper understanding of a theory. Along with discussion of the contributions, we provide examples to guide researchers in applying AST as a meta-theory.
Collaboration engineering is an approach for the design and deployment of repeatable collaboration processes that can be executed by practitioners without the support of collaboration professionals such as facilitators. A critical challenge in collaboration engineering concerns how the design activities have to be executed and which design choices have to be made to create a process design. We report on a four-year design science study in which we developed a design approach for collaboration engineering that incorporates existing process design methods, pattern-based design principles, and insights from expert facilitators regarding design challenges and choices. The resulting approach was evaluated and continuously improved in four trials with 37 students. Our findings suggest that this approach is useful to support the design of repeatable collaboration processes. Our study further serves as an example of how a design approach can be developed and improved following a multimethod design science approach.
Almost all attempts to data to monetize Internet applications targetted at individuals have focused on natural extensions of traditional media or traditional retailing, Most are either some form of consumer-focused advertising or of consumer-focused e-commerce And yet the Net is far more powerful than traditional media on one hand. and far more liberating, and thus inappropriate as an alternative to traditional media on the other. There are numerous potential online business models that are not based oil advertising. This paper explores several such areas and divides them into two basic categories, those that sell some product, experience., content, or service and earn revenues from the sale, and those that provide access to consumers and charge for access. It further divides each of these major categories into subcategories, and for each subcategory reviews Current experience, places it in the context of the relevant literature in competitive strategy, and uses that theory to make predictions. The paper concludes with strategic recommendations for corporations as well as with suggestions for further research.
We use a game-theoretic model to examine how information personalization by firms interacts with different dimensions of product differentiation (namely, horizontal and vertical differentiation). We consider the possibility that consumers attach different importance to various types of product differentiation, and report the equilibrium in terms of the quality-fit ratio, which measures the relative strength of preference for quality compared to preference for product fit and is a function of the cost of quality and the cost of product misfit. We also consider how different market structures (whether firms are similar or differentiated on the horizontal dimension ex ante) lead to different equilibriums when firms adopt personalization. We show that personalization by one firm leads to higher profits for both firms if product quality and misfit costs are high and the firms offer similar products ex ante. On the other hand, if firms offer differentiated products, personalization is profitable only if the effectiveness of the personalization technology is high or if both product quality and misfit costs are low. We also highlight conditions under which investments in personalization and product quality can be complements or Substitutes to each other. Finally, we show that a firm can respond to a competitor's personalization by either increasing (aggressive response) or decreasing (defensive response) investments in its own quality. Our results provide insights to managers on when to invest in personalization technologies and how to adjust their investments in product quality after the firm (or its competitor) adopts personalization.
Application service providers (ASP), which host and maintain information technology (IT) applications across the Internet, offer in alternative to traditional models 4 IT service for user firms. We build oil prior literature in transaction cost economics (TCE) to argue that the contract design should address ex post transaction costs that result due to contractual incompleteness and opportunism. We argue that contract design is multidimensional, and that it is necessary to design governance structures that Call protect user firms from shirking and monitoring costs, as well as provide for efficient adaptation when requirements are incompletely specified at the start. of the initiative. Our empirical analysis suggests that factors such as uncertainty in specifying service requirements, interdependence between the ASP application and IT systems in the client organization, and the need for specific investments favor time and materials contracts, whereas fixed prices are desirable when strong incentives are needed for cost reduction. We also find that contracts that are aligned with transaction attributes in a transaction cost-economizing manner are significantly less likely to experience budget overruns and realize better ex post performance than those that are not. These, results hold normative implications for both user and provider firms to assess the performance implications of choosing contracts in line with prescriptions of TCE.
Traditionally, trust has been seen as a result of personal knowledge of an individual's past behavior. In this view, trust develops gradually over time based oil an individuals cognitive assessment of the other person's behavior. However, high levels of trust have been observed among members of virtual teams, who often have little prior history of working together and may never meet each other in person. To integrate these two seemingly contradictory views of trust. this study manipulated team member characteristics and team member behavior to empirically test a two-stage theoretical model of trust formation and the influence of information and communication technologies (ICT) on trust formation. The results indicate that category-based processing of team member characteristics and an individual's own disposition to trust dominated the initial formation of swift trust.. Once individuals accumulated sufficient information to assess a team member's trustworthiness, the effects of swift trust declined and knowledge-based trust. formed using team members' behaviors (perceived ability, integrity, and benevolence) became dominant. The use of ICT increased perceived risk of team failure, which reduced the likelihood that team members Would engage in future trusting behaviors.
We compare alternative information security policies-facilitating end-user precautions and enforcement against attackers. The context is mass and targeted attacks, taking account of strategic interactions between end users and attackers. For both mass and targeted attacks. facilitating end-user precautions reduces, the expected loss of end users. However, the impact of enforcement oil expected loss depends oil the balance between deterrence and Slackening of end-user precautions. Facilitating end-user precautions is more effective than enforcement against. attackers when the cost of precautions and the cost of atacks are lower. With targeted attacks, facilitating end-user precautions is more effective for users with relatively high valuation of information security, while enforcement against attackers is more effective for users with relatively low valuation of security
To increase their firms' competitiveness. information technology (IT) managers are adopting a strategy that many deemed risky in the past. Recent IT advances combined with certain firm and industry characteristics are prompting firms to move toward a unified procurement strategy for enterprise software solutions. Unified procurement Occurs when a firm elects to purchase all compatible products and services from a single vendor. Key benefits of unified procurement involve motivating managers to alter their procurement strategy. At the top of the list of benefits is transferred risk, which includes risks that can be transferred from one participating party to another during a transaction. In the case of unified procurement, they include technology risks and integration costs that are transferred from the procurement firm to its vendor. Firms have been shifting toward a unified procurement strategy for enterprise software solutions. We discuss the evolution of procurement practices in an industry that exemplifies a manifestation of Clemons et al 's [10] move to the middle  hypothesis predictions. The adoption of unified procurement is being driven by changes in IT, firm, and industry structure. Are explore the move to the middle: transaction cost economics. and industry clockspeed theory to explain this phenomenon. We present a series of propositions that extend the prior theory to the more specific setting of enterprise software procurement-an example of middle range theory development-and use mini-cases to validate the various perspectives that we offer.
Because uncertainties around innovative technologies resolve over time, investments in such technologies are often made in stages so that organizations can use the knowledge gained from earlier stages to decide the next step. Previous studies usually assume that once some uncertainty is resolved, it becomes common knowledge within the investing organization. We develop a game-theoretical model to study how different parties within an organization gain and transfer knowledge about new technologies while investing in these technologies, and how the learning process may affect the investment decisions. We show that managers with incentives misaligned with the organization may transfer their knowledge untruthfully and distort the learning process of decision makers. Such behavior may lead to inefficient investment decisions. We also study the effect of uncertainty on the misreporting problem and the investment decisions. Mechanisms to mitigate or prevent untruthful knowledge transfer are also proposed. In particular., powerful incentive schemes may alleviate, but not prevent. the misreporting problem;, punishing managers who are caught mis-reporting may deter the misreporting behavior, but in practice such mechanisms are difficult to implement.
We use an economic learning model to examine how knowledge parameters characterizing a sourcing relationship between it vendor and it client interact with production costs and coordination costs to affect the business value of alternative outsourcing strategies. This Information is then used to determine a fit-in's optimal rate of information technology (IT) Outsourcing. We find that the optimal Outsourcing rate is dependent oil the ability of the Outsourcing client to acquire production Knowledge front its outsourcing vendor and to retain its internal coordination knowledge despite losses of fundamental production skills due to Outsourcing. Specifically, when the Client is unable to acquire sufficient production knowledge from the external vendor, the client's optimal Outsourcing decision is to engage in either one of two extreme strategies-total insourcing or total outsourcing-depending on the rate at which the client's coordination knowledge depreciates. On the other hand, when the client is able to acquire a substantial amount of production knowledge from the external vendor, the firm's optimal decision is to outsource only a portion of its IT services, where the proportion depends on the rate at which the client's coordination knowledge depreciates.
This paper identifies and measures the various business Value benefits that accrue as a result of implementing and integrating large-scale enterprise information systems. Specifically, we look at the integration of electronic medical records for all patients with the radiology information system and a picture archiving and communication system at a regional medical center. Our work is among the first to carefully study and analyze the impact of enterprise information systems at a large-scale service organization that produces intangible outputs-health. It extends the literature on information economics by quantifying the benefits in process dynamics as a source of ongoing firm-level performance improvements. The key dimensions of measurements Include financial revenues, operating lead times, and subjective satisfaction levels by the clinical staff and by the referring physicians. Analyses of longitudinal data suggest that performance levels along a key metric-clinical process lead time-showed a significant improvement immediately after the deployment and integration of the systems. The evidence reveals that performance kept improving for the following 12 months at ail impressive learning rate of 63 percent. Moreover, the reported satisfaction level after installation was higher among referring physicians who actively used the full spectrum of technological functionalities at their own clinics or at the hospital's site. Finally, we present a general framework for capturing the actual tangible and intangible benefits of enterprise information systems installation and integration in a clinical context.
Location-based services (LBS) use positioning technologies to provide individual users with reachability and accessibility that would otherwise not be available in the conventional commercial realm. While LBS confer greater connectivity and personalization on consumers, they also threaten users' information privacy through granular tracking of their preferences, behaviors, and identity. To address privacy concerns in the LBS context, this study extends the privacy calculus model to explore the role of information delivery mechanisms (pull and push) in the efficacy of three privacy intervention approaches (compensation, industry self-regulation, and government regulation) in influencing individual privacy decision making. The research model was tested using data gathered from 528 respondents through a quasi-experimental survey method. Structural equations modeling using partial least squares validated the instrument and the proposed model. Results suggest that the effects of the three privacy intervention approaches on an individual's privacy calculus vary based on the type of information delivery mechanism (pull and push). Results suggest that providing financial compensation for push-based LBS is more important than it is for pull-based LBS. Moreover, this study shows that privacy advocates and government legislators should not treat all types of LBS as undifferentiated but could instead specifically target certain types of services.
Although the choice of control mechanisms in systems development projects has been extensively studied in prior research, differences in such choices across internal and outsourced projects and their effects on systems development performance have not received much attention. This study attempts to address this gap using data on 57 outsourced and 79 internal projects in 136 organizations. Our results reveal a paradoxical overarching pattern: controllers attempt greater use of control mechanisms in outsourced projects relative to internal projects, yet controls enhance systems development performance in internal projects but not in outsourced projects. We introduce a distinction between attempted control and realized control to explain this disconnect, and show how anticipated transaction hazards motivate the former but meeting specific informational and social prerequisites facilitate the latter. ;Our results contribute three new insights to the systems development control literature. First, controllers attempt to use controller-driven control mechanisms to a greater degree in outsourced projects but controllee-driven control mechanisms to a greater degree in internal projects. Second, we establish a hitherto-missing control-performance link. The nuanced differences in internal and outsourced projects simultaneously confirm and refute a pervasive assertion in the information systems controls literature that control enhances performance. Finally, we show how requirements volatility-which can be at odds with control-alters the control-performance relationships. Implications for theory and practice are also discussed.
We use a duopoly model of quality-price competition between a software innovator and an imitator to determine the socially optimal software patent policy and to assess the social welfare implications of alternative patent policies. We find that the optimal patent policy is to grant patent protection for the entire life of the innovative software product (i.e., set infinite patent length) and to set the novelty and nonobviousness requirement for attaining a patent (i.e., patent height) and the scope of patent protection (i.e., patent width) such that both the innovator and imitator produce higher-quality products than they would in the free market. This policy not only maximizes social welfare but also makes the innovator, imitator, and consumers better off than in the free market. However, counter to intuition, we find that firms exhibit lower research and development intensity under the socially optimal patent policy than they do under free market competition. While highly stylized, the model offers a useful framework within which researchers and regulators can think about the economic trade-offs among three patent policy parameters (length, height, and width) simultaneously.
The research question examined in this paper is whether or not product price can be used as a proxy to predict how customers' trust will be influenced by different trust-assuring arguments displayed on a business-to-consumer e-commerce Web site. Drawing from the elaboration likelihood model (ELM) and Toulmin's model of argumentation, we examine the effects on consumer trust of two levels of source and two levels of content factors, under two levels of product price, in a laboratory experiment with 128 subjects. Product price was predicted as a moderating factor that would influence the customer's motivation to scrutinize more closely the content of the trust-assuring arguments. The results suggest that customers are more influenced by the content of trust-assuring arguments when the price of a product is relatively high than when it is relatively low. Presumably, Internet stores employ a third party's trust-assuring arguments because customers are less likely to trust an unknown Internet store's own trust-assuring arguments. However, the results paradoxically may imply that when customers have more at stake (e.g., buying a high-price product), they do not necessarily have to rely only on an independent third-party source to form high trust beliefs about the store. When customers purchase a high-price product, they seem to form trusting beliefs by scrutinizing argument content rather than by depending on heuristic cues (e.g., an independent party's opinion) as the ELM would predict.
Many software products are available free of charge. While the benefits resulting from network externality have been examined in the related literature, the effect of free offer on the diffusion of new software has not been formally analyzed. We show in this study that even if other benefits do not exist, a software firm can still benefit from giving away fully functioning software. This is due to the accelerated diffusion process and subsequently the increased net present value of future sales. By adapting the Bass diffusion model to capture the impact of free software offer, we provide a methodology to determine the optimal number of free adopters. We show that the optimal free offer solution depends on the discount rate, the length of the demand window, and the ratio of low-valuation to high-valuation free adopters. Our methodology is shown to be applicable for both fixed and dynamic pricing strategies.
Cost-efficient and multimedia-rich interaction opportunities offered by the Internet and the existence of online communities have made virtual co-creation a suitable means of creating value and improving the overall success of new products. Information technology enables new forms of producer-consumer collaboration in new product development processes. However, little research exists on consumers' experiences during virtual co-creation tasks. Drawing on the literature on organizational behavior, we introduce the construct of consumer empowerment to describe consumers' perceived influence on product design and decision making. This paper presents the first large-scale empirical study investigating how consumers are empowered through Internet-based co-creation activities. To analyze the impact of applied interaction tools, 727 consumers having taken part in virtual co-creation projects were asked about their experienced tool support, their perceived empowerment, how much they enjoyed the task, and their readiness to participate in future co-creation opportunities. The results show that consumers engaging in co-creation feel more or less empowered. The level of experienced empowerment depends on the design of the applied virtual interaction tool, the related enjoyment of the virtual interaction, the participants' task and product involvement, as well as their creativity and lead-user characteristics. The design of the interaction tool determines to what extent consumers with varying capabilities are able to solve the assigned co-creation task. It determines the consumers' perceived empowerment and experienced enjoyment. Both the levels of perceived empowerment and enjoyment have a strong impact on the consumers' willingness to participate in future virtual new product development projects. These findings contribute to a better understanding of antecedents and consequences of successful consumer co-creation. They provide recommendations on how to design a compelling virtual new product co-creation experience.
The paper uses a game-theoretic setting to examine the interaction between strategic attackers who try to gain unauthorized access to information systems, or targets, and defenders of those targets. Our analysis of the attacker-defender interaction shows that well-protected targets can use signals of their superior level of protection as a deterrence tool. This is due to the fact that, all other things being equal, rational attackers motivated by potential financial gains tend to direct their effort toward less-protected targets. We analyze several scenarios differing in the scope of publicly available information about target parameters and discuss conditions under which greater defenders' ability to signal their security characteristics may improve their welfare. Our results may assist security researchers in devising better defense strategies through the use of deterrence and provide new insight about the efficacy of specific security practices in complex information security environments.
In the work presented here, we develop and apply preference markets in evaluating early stage technology. Partnering with a Fortune 5 company, we developed and implemented two internal preference markets (field experiments). In both cases, nonmonetary (play money) incentives were utilized, but one market provided additional nonmonetary (play money) incentives. Working with the partner company, our investigation started with seven emerging technologies and expanded to a total of 17 emerging technologies. Our results suggest that even a simple form of additional nonmonetary play money incentive yielded greater price convergence, increased spread across final market prices, and greater consistency with a costly expert panel that was set up by the partner company. Based on the outcomes of our analyses, the partner company is investing in developing extended applications of preference markets as a potentially scalable approach for dealing with its ongoing and expanding strategic identification of promising emerging technologies.
Sellers in eBay are often small-business owners whose livelihood depends on fast turnaround of their cash flows. Unlike in traditional auctions, these sellers are content to sell as soon as some target price is reached. While a wealth of literature exists on the final rent of the various stakeholders in a traditional auction setting, what is of interest here is to estimate the time required to reach a certain bid level in ongoing auctions. This paper introduces an analytical model to estimate the time it takes an online auction to reach a prespecified price threshold. The motivation for the research is to avoid unnecessary delays in conducting the transaction. Specifying the right duration would benefit small sellers who would realize the revenue proceeds from the sale faster. To this end, we model the bidding process as an infinite quasi-birth-death process, characterized by bursts of rapid bidding and subsequent lulls. We obtain closed-form solutions for the transient probability distribution in the frequency domain of the bid prices in an ongoing auction, which are then used to compute the transient probability distributions in the time domain. Experienced auctioneers can use these results to estimate expected ending times for their auctions. Sample observations from online auctions indicate that there may be potential room for improvement for sellers in setting their auction ending times. Simulations of the quasi-birth-death processes back up the theoretical observations.
With the move to an information-based economy, financial services has become a key contributor to the U.S. gross domestic product. Even as consolidation reduces the number of banks, small banks with under $100 million in assets continue to report higher profit margins than large banks with over $100 million in assets. Lacking scale, small banks employ a service-oriented business strategy (customer intimacy), whereas large banks focus on productivity and throughput (operational excellence). Information technology (IT) plays a key role in applying each strategy, but as banks move toward customer intimacy in general, the challenge is to grow without undermining service quality. Using a balanced panel data set from 43 U.S. banks, this paper finds that banking strategies are becoming more customer focused. Yet for large banks in particular, IT remains resolutely operations focused. This misalignment could restrict future banking performance. In this way, this paper contributes to the service science literature by using size to dissect banking strategies and performance.
We examine contract choices in the provision of software-as-a-service (SaaS), which is a business innovation that transforms information technology (IT) resources into a continuously provided service. We draw upon agency theory and modularity theory to propose that one of the central challenges in service disaggregation is that of knowledge interdependencies across client and provider organizations. The resulting lack of verifiability of certain tasks results in a multitask agency problem. Our key research questions involve (1) the suitability of high- versus low-powered incentives in SaaS contracts when the outsourced tasks involve business analytics that are difficult to verify, and (2) how such contract choices are affected by the modularity of interfaces between the client and the provider. Analysis of data collected from 154 providers of SaaS offering a range of IT services supports our contention that when contracting for business analytics characterized by knowledge interdependencies across clients and providers, incentives should be low powered. Modularity in the interfaces of the service provider increases the desirability of high-powered incentives in such situations. Our results are robust after accounting for endogeneity issues arising from unobserved matching between service providers and the nature of IT services outsourced by clients. With the increasing importance of information systems in services, this paper suggests that arm's-length relationships and high-powered incentives may be ineffective in incentivizing providers to perform on complex business analytic tasks, unless accompanied by the modularization of interfaces.
In an information technology services outsourcing arrangement, variance in demand volume and individual user preferences pose significant challenges to the provider organization in making resource allocation decisions. Such variations affect service levels, especially under fixed resource constraints. We explore the possible role of periodic demand information sharing and subsequent resource-level adjustments as a means of addressing issues arising from demand variation. As information exchange alters the dynamics of the relationship between the customer and provider organizations, incorporating information sharing in service-level agreements requires modifying current pricing schemes. A pricing heuristic is developed and tested under varying levels of information accuracy and granularity. The heuristic is shown to provide better economic welfare for both participants in comparison to the baseline pricing strategies considered. Also, it is shown that information, even at a coarse level of granularity, is very effective in providing stable service levels a finding that is encouraging for enhanced collaborations between customer and provider organizations in outsourcing arrangements.
An important task for managers in information technology (IT) service settings is the judgment of service performance. The complex and intangible nature of IT services, however, renders this task especially difficult. We use a sample of 85 outsourced software development projects to test for the presence of the input bias, which is defined as the systematic misuse of nondiagnostic input information in forming managerial judgments of outcomes. The service outcome we examine is process performance. The diagnostic inputs are given by objective performance metrics based on the final cost and duration of completed projects, whereas the nondiagnostic inputs are risk anticipations formed by managers prior to the start of the project. We find strong evidence of the input bias, which leads managers' subjective assessments to diverge considerably from objective outcomes, and that it is moderated by contract type. Our study contributes to better service management by improving our understanding of managers' judgments of service performance and how these judgments are formed.
Service-oriented architecture (SOA) is one of the most discussed topics in the information systems (IS) discipline. While most computer scientists agree that the service-oriented paradigm has clear benefits in terms of technical quality attributes, it has been difficult to justify SOA economically. The few studies that have investigated the strategic and economic aspects of SOA are mostly exploratory and lack a more comprehensive framework for understanding the sources of its economic potential. Based on IS and SOA literature, our work goes further in suggesting the SOA economic potential model, which describes the causal relationships between the SOA's style characteristics and value it can provide on the business side. Using this model, we investigate 164 SOA cases published between 2003 and 2008 to explore the economic rationale for adopting SOA. Our findings suggest that SOA's business benefits are currently mainly driven by operational and information technology infrastructural improvements. However, enterprises also realize strategic benefits from SOA; for example, by electronically integrating with their business partners by means of SOA. We use the results of our study to derive propositions and suggest a research model for future studies on SOA's economic potential.
The importance of building relationships with customers and trust in the services provider is well documented in the marketing literature. Conceptually, we extend this logic to the context of internal information technology (IT) services operations through the notion of the service delivery chain. The purpose of the study is to examine how key service mechanisms in operational IT implementation are related to employee perceptions of actual system benefits and trust in the IT services provider. We report on a study with 380 employees of 14 bank affiliates that were recently acquired by a bank holding company. The focus of the study is on postimplementation trust rather than preimplementation or initial trust, and the service provider is viewed as the object of trust rather than the technology. Our findings suggest that training, trial, and social influence are key service mechanisms an IT services provider can use to stimulate trust in the IT services provider and the realization of system benefits.
The computing industry is gradually evolving to cater to the demand for software-as-a-service (SaaS). Two core competencies that have emerged over the past few years are that of the application service providers (ASPs) and the application infrastructure providers (AIPs). The arrangements between them result in system dynamics that is typical in supply chain networks. We examine the performance of an SaaS set up under different coordination strategies between these two players. Our analysis indicates that coordination between the monopoly ASP and the AIP can result in an outcome with the same overall surplus as can be achieved by a central planner. Even though the players have an incentive to deviate, it is possible to create the right incentives so that the economically efficient outcome is also the Nash equilibrium. The results of the analysis have significant implications for the coordination strategies for providers in the burgeoning business model of delivering software services over the Internet.
Service-oriented architecture (SOA) has been promoted as a technology that can enhance information systems agility, interoperability between applications, deployment flexibility, and reusability. As with any new information technology (IT), the decision to adopt SOA cannot be taken lightly, given the nontrivial investment in economic and personnel resources. The complexity associated with industry-wide diffusion, coupled with organization, industry, and environment factors, contributes to a lack of a clear strategy for assessing the business value that SOA provides an organization. This research attempts to shed light on this process of value creation for an organization, using a system dynamics approach. A detailed model of the industry diffusion coupled with organization adoption is presented. After suitable calibration and validation, a series of simulations using the model evaluate the efficacy of SOA under a variety of diverse conditions. The results of the simulations indicate clear benefits of SOA over monolithic ITs when employed under appropriate conditions. Situations where SOA fails to live up to expectations are also identified. The model and accompanying simulations can serve as a practical decision support tool for an organization to help make the strategic decisions of adopting and implementing SOA.
The emergence of new service science approaches to business problems in information technology (IT) services offers new, unusually relevant insights for the senior management of vendors in this business area. This research examines how service-level agreement contract flexibility should be designed when the technological and business market environments result in volatility of demand, based on an understanding of related changes in the cost drivers that underlie IT services contracts. Our approach draws on a blend of well-known methods from financial economics the real option pricing method and the contingent claims analysis method. In particular, our research examines a setting in which a vendor provides IT services to a client according to a prenegotiated IT services contract in the presence of demand volatility. We analyze the motivation of and value consequences for a vendor that offers the client the flexibility to opt out of the contract. For example, the client might switch to another vendor, or backsource and provide its own services internally. Our core results offer important foundational thinking for how to specify various forms of IT service-related flexibility in terms of put and call options from the point of view of an IT services vendor, so that their value and exercise timing can be estimated. We show that the client firm's demand trigger value for deciding when to backsource its IT services varies, and it depends on the degree of demand volatility as well as the usage-based fees charged by the vendor. Working from our modeling approach, we also are able to characterize the extent to which a vendor can benefit from bearing the costs of making a backsourcing flexibility option available to its client.
The increasing importance of information technology (IT) services in the global economy prompts researchers in the field of information systems (IS) to give special attention to the foundations of managerial and technical knowledge in this emerging arena of knowledge. Already we have seen the computer science discipline embrace the challenges of finding new directions in design science toward making services-oriented computing approaches more effective, setting the stage for the development of a new science service science, management, and engineering (SSME). This paper addresses the issues from the point of view of service science as a fundamental area for IS research. We propose a robust framework for evaluating the research on service science, and the likely outcomes and new directions that we expect to see in the coming decade. We emphasize the multiple roles of producers and consumers of services-oriented technology innovations, as well as value-adding seller intermediaries and systems integrators, and standards organizations, user groups, and regulators as monitors. The analysis is cast in multidisciplinary terms, including computer science and IS, economics and finance, marketing, and operations and supply chain management. Evaluating the accomplishments and opportunities for research related to the SSME perspective through a robust framework enables in-depth assessment in the present, as well as an ongoing evaluation of new knowledge in this area, and the advancement of the related management practice capabilities to improve IT services in organizations.
Just as scientists in other disciplines use experience and a small set of frequently occurring problems to structure unfamiliar situations, information strategy and economics provides its own patterns to guide and structure the use of experience in managerial settings. These patterns emerged through case studies, theoretical derivations, and empirical analyses of company, industry, and national data sets. The six most frequently observed patterns identified here are (1) newly vulnerable markets experience opportunistic pickoff; (2) transparency of product attributes increases informedness, enabling resonance marketing and increasing the benefits from offering truly differentiated products and services; (3) changes in transaction costs have changed the boundary of the firm; (4) unique resources endowments can confer or sustain competitive advantage; (5) the geometry of distribution determines power and affects profitability; and (6) network-based advantages can form the basis of platform-envelopment strategies. Finally, prospects for the future of information, strategy, and economics over the coming two decades are reviewed.
We discuss the optimal way for a software vendor to license software: a perpetual license at a posted price, a subscription contract that subscribers receive automatic updates for periodic payment, or a hybrid approach that involves both. By addressing specific issues in the software market such as network effects, quality uncertainty, upgrade compatibility, and the vendor's ability to commit to future prices in a dynamic environment, we demonstrate how a software vendor can manage the trade-offs of perpetual licensing and subscription to optimize profit, as well as the corresponding welfare effect on consumers. Although the subscription model helps the vendor lock in consumers so as to increase profit when there is great uncertainty associated with the next version software, it destroys the path dependence in creating network externalities. Therefore, when the network effect is sufficiently large, it is more profitable for a software vendor to provide both perpetual licensing and subscription.
Phishing has been a major problem for information systems managers and users for several years now. In 2008, it was estimated that phishing resulted in close to $50 billion in damages to U.S. consumers and businesses. Even so, research has yet to explore many of the reasons why Internet users continue to be exploited. The goal of this paper is to better understand the behavioral factors that may increase one's susceptibility for complying with a phisher's request for personal information. Using past research on deception detection, a research model was developed to help explain compliant phishing responses. The model was tested using a field study in which each participant received a phishing e-mail asking for sensitive information. It was found that four behavioral factors were influential as to whether the phishing e-mails were answered with sensitive information. The paper concludes by suggesting that the behavioral aspect of susceptible users be integrated into the current tools and materials used in antiphishing efforts.
We examine the role of network externalities on the use of blogs in an organization. Prior research has considered social influences such as peer pressure, but there is little prior work on how the extent of others' actual usage can influence an individual's use of technology. We also examine how technology usage is influenced by positive feedback from others. Finally, we look at how the relation between technology usage and network effects is moderated by demographic variables such as age and gender. The results of the study show that usage of blogs within an individual's network is associated with an increase in one's own usage. We also show that network effects are stronger for younger generations and that this relation is nonmonotonic with age. This is interesting considering that prior research suggests that social influences are stronger for older employees. Our results also show that network effects are stronger for women than for men. Further, we show that the impact of age on blog usage in not linear. We also find that feedback or appreciation from others is associated with higher blog usage by an individual. Finally, we subdivide the network effects into various subtypes and find that network effects are strongest for relational networks, and that use of blogs by an employee's managers is associated with higher usage by the employee.
Can models of collaboration serve as foundations for development of collaborative technologies in much the same way that engineers use models when developing complex systems? We explore this issue by investigating how eight approaches to understanding or modeling collaboration could be used to improve technologies that support processes used in a large aerospace program. Some modeling approaches are ostensive, defining how collaboration should be achieved or how the technology should be used. These approaches provide ways of documenting, analyzing, simulating, and automating the process. Other approaches are performative, describing actual collaboration behavior and actual technology use. Performative approaches reveal the variability in collaboration and deviations from the intended process. Technologies can benefit from and facilitate both types of modeling approaches by recording collaborative events for later analysis. We conclude by considering ways that modeling collaboration could contribute to requirements analysis, new collaboration capabilities, adoption, and maximizing benefit from technologies.
Decision aids have long been an important source of help in making structured decisions. However, decision support for more complex problems has been much more difficult to create. Decision aids are now being developed for very complex problems, and their effects among low- and high-task-knowledge individuals are still being explored. One such task is credibility assessment, in which message recipients or observers must determine a message's veracity and trustworthiness. Credibility assessment is made difficult by lack of constraints, hidden or incomplete information, and mistaken beliefs of the assessor. ;The theory of technology dominance (TTD) proposes that technology is most effectively applied in intelligent decision aids when an experienced user is paired with a sophisticated decision aid. This work examines TTD in the complex task of credibility assessment. To assist in credibility assessment, we created a decision aid that augments the capabilities of the user-whether novice or professional. Using hypotheses based on TTD, we tested the decision aid using high-stakes deception in recorded interviews and involved both student (novice) and law enforcement (professional) users. Both professionals and novices improved their assessment accuracy by using the decision aid. Consistent with TTD, novices were more reliant on the decision aid than were professionals. However, contrary to TTD, there was no significant difference in the way novices and professionals interacted with the system, and the decision aid was not more beneficial to professionals. Novices and professionals frequently discounted the aid's recommendations, and in many cases professionals did not view explanations when the decision aid contradicted their assessments. Potential reasons for these findings, as well as limitations and future research opportunities, are discussed.
As organizations' information technology (IT) investment goals evolve from improving operational efficiency to enhancing strategic growth, the chief information officer (CIO) is increasingly expected to play not only the traditional supply-side leadership role that focuses on exploiting existing IT competencies to support known business needs but also the demand-side leadership role that focuses on exploring new IT-enabled business opportunities that result in competitive advantage. Using matched CIO business executive responses from 174 firms, we test a staged maturity relationship between CIO supply-side and demand-side leadership and examine three antecedents (CIO human capital, CIO structural power, and organizational support for IT) and two effects (IT contribution to firm efficiency and strategic growth) of CIO leadership. The staged maturity model is supported by our findings and provides insight into how these two stages of CIO leadership influence IT impact within the organization and how they are influenced by these key antecedents.
Follow the sun (FTS) has interesting appeal-hand off work at the end of every day from one site to the next, many time zones away, in order to speed up product development. Although the potential effect on time to market can be profound, at least conceptually, FTS has enjoyed few documented industry successes because it is acknowledged to be extremely difficult to implement. In order to address this FTS challenge, we provide here a conceptual foundation and formal definition of FTS. We then analyze the conditions under which FT'S can be successful in reducing duration in software development. We show that handoff efficiency is paramount to successful FTS practices and that duration can be reduced only when lower within-site coordination and improved personal productivity outweigh the corresponding increase in cross-site coordination. We also develop 12 research propositions based on fundamental issues surrounding FTS, such as calendar efficiency, development method, product architecture and handoff efficiency, within-site coordination, cross-site coordination, and personal productivity. We combine the conceptual analysis with a description of our FTS exploratory comparative field studies and draw out their key findings and learning. The main implication of this paper is that understanding calendar efficiency, handoff efficiency, within-site coordination, and cross-site coordination is necessary to evaluation-if FTS is to be successful in reducing software development duration.
Organizations often look to their information systems (IS) professionals to work with system stakeholders to generate new ideas to solve complex problems and to provide information technology (IT) artifacts to support ideation processes. Much research therefore seeks to increase the number of ideas people generate based on Alex F. Osborn's conjecture that more ideas give rise to more good ideas. Recent research, however, calls the quantity-quality conjecture into question. This paper advances bounded ideation theory (BIT), an explanation for the ideation function-the relationship between the number of good ideas and the number of ideas contributed. BIT posits that boundaries of understanding, attention resources, goal congruence, mental and physical stamina, and the solution space moderate a primary relationship between individual ability and idea quality, yielding an ideation function with an inflected curve. We discuss six strategies for improving ideation and call into question the value of the quantity focus of ideation research in the IS/IT literature, arguing that a quality focus would be more useful.
Social loafing is the tendency of individuals to withhold contributions to a task in a team setting. Team size and dispersion are two primary drivers of social loafing in technology-supported team settings. However, the mechanisms through which these drivers affect social loafing are not well understood. Consequently, the objective of this study is to identify the cognitive mechanisms that mediate the effect of team size and dispersion on social loafing in technology-supported teams. Drawing on the theory of moral disengagement, we posit that three primary cognitive mechanisms-diffusion of responsibility, attribution of blame, and dehumanization-will mediate the effect of team size and dispersion on social loafing. We conducted a laboratory study involving 140 students randomly assigned to 32 teams performing a brainstorming task using group systems software. The results show that diffusion of responsibility, attribution of blame, and dehumanization all mediate (partially) the effects of team size on social loafing. Meanwhile, only dehumanization mediates (fully) the effect of dispersion on social loafing.
Adoption of open source software (OSS) principles to internal software development has gained considerable momentum. Often labeled as internal open source (IOS), several large firms have started to implement these programs. Research to date has mostly focused on facilitating IOS adoption. In the present research, we focus on how IOS affects reuse. Employing a qualitative case study, we examine the IOS program at IBM called Community Source. Analyzing data gathered from multiple sources reveals that IOS adoption facilitates participatory reuse by enhancing information sharing and leveraging of broader community skills. Participatory reuse manifests itself when potential reusers participate in the entire development process leading to the creation of reusable assets. Based on data, we develop a theoretical model to illustrate how IOS affects reuse. While furthering research on IOS and reuse, the model informs managers wishing to foster participatory reuse that they are wise to adopt IOS as a vehicle to promote greater openness of the software development infrastructure for leveraging broader community skills and enhancing information sharing among projects' stakeholders.
Although formal and informal control mechanisms are often simultaneously used to govern systems development projects, considerable disagreement exists about whether the use of one strengthens or diminishes the benefits of the other. In other words, are they complements or substitutes? Competing theoretical perspectives favor both sides of the argument, and neither the information systems (IS) controls literature nor the information technology (IT) outsourcing literature has addressed this issue. This study theoretically develops the idea that these competing perspectives are mutually compatible rather than contradictory because informal and formal control mechanisms can simultaneously be complements and substitutes. Using data from 120 outsourced systems development projects, it is shown that informal control mechanisms strengthen the influence of formal behavior control mechanisms on systems development ambidexterity (complementary effects) but weaken the influence of formal outcome control mechanisms (substitutive effects). The key contribution of the paper therefore lies in exploring interactions among control mechanisms in a project's control portfolio to reconcile the competing theoretical perspectives on whether formal and informal controls are complements or substitutes. The findings provide managers guidance on how to carefully combine formal and informal control mechanisms in a project. Combining informal with formal process-based control mechanisms can simultaneously enhance the fulfillment of project goals and development flexibility. However, combining informal with formal outcome-based control mechanisms can instead impair these objectives.
Online retailers are increasingly using information technologies to provide value-added services to customers. Prominent examples of these services are online recommender systems and consumer feedback mechanisms, both of which serve to reduce consumer search costs and uncertainty associated with the purchase of unfamiliar products. The central question we address is how recommender systems affect sales. We take into consideration the interaction among recommendations, sales, and price. We then develop a robust empirical model that incorporates the indirect effect of recommendations on sales through retailer pricing, potential simultaneity between sales and recommendations, and a comprehensive measure of the strength of recommendations. Applying the model to a panel data set collected from two online retailers, we found that the strength of recommendations has a positive effect on sales. Moreover, this effect is moderated by the recency effect, where more recently released recommended items positively affect the cross-selling efforts of sellers. We also show that recommender systems help to reinforce the long-tail phenomenon of electronic commerce, and obscure recommendations positively affect cross-selling. We also found a positive effect of recommendations on prices. These results suggest that recommendations not only improve sales but they also provide added flexibility to retailers to adjust their prices. A comparative analysis reveals that recommendations have a higher effect on sales than does consumer feedback. Our empirical results show that providing value-added services, such as digital word of mouth and recommendations, allows retailers to charge higher prices while at the same time increasing demand by providing more information regarding the quality and match of products.
Online intermediaries have recently started offering database services to donors and certification services to nonprofit organizations through the Internet. We conceptualize a donor-to-nonprofit (D2N) marketplace as an online intermediary that offers these two services and examine its effect on fund-raising strategies of nonprofit organizations using an analytical model based on spatial competition under incomplete information with donor search. We characterize the signaling equilibria where certification of quality conveys information about organizational effectiveness in generating socially valuable services. Interestingly, the emergence of the D2N marketplace may lead to a drop in total net fund-raising revenues in the market, despite the fact that the intermediary's database service eliminates search costs for some donors. We also explain why such a marketplace may deliberately lower the accuracy of its certification process.
In many domains of increased turbulence and volatility, interorganizational ad hoc collaborations are common. One such domain is homeland security in which security professionals collaborate virtually with individuals outside of their own organizations in response to a security threat. In such a domain, a safe context is needed to ensure that interactions with collaborators not only help to solve the immediate threat but also avoid the improper use by outside parties of information released during these collaborations. We use the heuristic systematic model of information processing to hypothesize that the relationship between different safe context factors and a security professional's perceptions of collaboration success will be contingent on differences in geographic proximity of the collaborating parties differences in proximity that are not related to differences in physical face-to-face contact but to differences in social proximity. Our exploratory empirical investigation finds support for the hypothesized interaction effect: safe contexts that require deeper processing are related to higher levels of perceived success when the parties are geographically proximal (with no differences in face-to-face contact), whereas safe contexts that involve heuristic-based processing are related to success when parties are geographically less proximal. Our findings suggest that the utility of safe context factors is contextualized based on the proximity of interacting parties, that geographical proximity's social space dimension plays a key role independent of differences in physical face-to-face contact, and that, practically, to be successful, ad hoc collaborators should have access to a range of safe context factors, using them in different combinations depending on the proximity of network members.
Privacy is a significant concern of customers in the business-to-consumer online environment. Several technical, economic, and regulatory mechanisms have been proposed to address online privacy. A current market-based mechanism is the privacy seal, under which a third party assures adherence by a vendor to its posted privacy policy. In this paper, we present empirical evidence of the effect of displaying a privacy seal on the product prices of online vendors of electronic books, downloadable audiobooks, and textbooks. Using data collected on these relatively homogeneous products sold by online vendors, we find that while controlling for vendor-specific characteristics, vendors bearing privacy seals charge a premium for such products compared to vendors not bearing a seal. The paper provides empirical evidence of the economic value of privacy assurance from the customers' perspective as measured by the price premium charged for products. The research has implications for researchers and policymakers by providing evidence that privacy is another factor that creates friction in e-commerce, and that prices on the Internet for homogeneous products need not converge.
Whether broadband service providers (BSPs) should be allowed to vertically integrate with content providers is a contentious issue. This is even more so when viewed through the lens of the net neutrality debate, since the vertically integrated firm can prioritize the delivery of its own content at the expense of that of its competitors if net neutrality is not enforced. Using a game-theoretic model, we analyze the issues of vertical integration of content and broadband services surrounding this debate from an economic perspective. Our analysis establishes the various equilibria in the game and shows that the vertically integrated BSP does not have any incentive to abide by the principles of net neutrality. If net neutrality is not enforced, social welfare might, in certain cases, decrease with vertical integration, and in such cases, the BSP's objectives are at odds with that of the social planner. With other ranges of parameter values, social welfare increases with vertical integration at the expense of the competing pure-play content provider. Interestingly, we find that it is not always true that the BSP will always degrade the delivery of the competing content, and in fact will sometimes have the incentive to prioritize the latter over its own. The analysis thus provides crucial inputs to policymakers as they decide on whether to allow vertical integration between a BSP and a content provider in the absence of net neutrality.
User-generated content has been hailed by some as a democratizing force that enables consumers to discuss niche products that were previously ignored by mainstream media. Nevertheless, the extent to which consumers truly prefer to use these new outlets to discuss lesser-known products as opposed to spending most of their energies on discussing widely marketed or already successful products has so far remained an open question. We explore this question by investigating how a population's propensity to contribute postconsumption online reviews for different products of the same category (motion pictures) relates to various indicators of those products' popularity. We discover that, ceteris paribus, consumers prefer to post reviews for products that are less available and less successful in the market. At the same time, however, they are also more likely to contribute reviews for products that many other people have already commented on online. The presence of these two opposite forces leads to a U-shaped relationship between a population's average propensity to review a movie postconsumption and that movie's box office revenues: moviegoers appear to be more likely to contribute reviews for very obscure movies but also for very high-grossing movies. Our findings suggest that online forum designers who wish to increase the contribution of user reviews for lesser-known products should make information about the volume of previously posted reviews a less-prominent feature of their sites.
Information technology (IT) innovations follow a diverse set of diffusion patterns. Early diffusion models explaining technology diffusion patterns assumed that there is a single homogeneous segment of potential adopters. It was later shown that a two-segment model considering two groups of adopters explains variations in diffusion patterns better than the existing one-segment models. While the two-segment model considers a group of adopters promoting adoption by exerting a positive influence on prospective adopters, it does not consider the members of society who aim to inhibit the adoption process by exerting a negative influence on prospective adopters. In fact, most IT innovations face opposition. Yet it is not clear how opposition affects the diffusion process. In this paper, we model the diffusion of an IT innovation through its target population with three types of actors: influentials, who are autonomous in adopting new technology and promote its adoption; opponents, who are opposed to the technology and inhibit its adoption; and imitators, who are information seekers, thus affected by both influentials and opponents. We show that opponents play a crucial role in determining the diffusion path of an innovation. The empirical tests using real as well as simulated data sets demonstrate the ability of our model to fit the data better and to identify the segments of adopters correctly.
The paper presents a model integrating theories from collaboration research (i.e., social presence theory, channel expansion theory, and the task closure model) with a recent theory from technology adoption research (i.e., unified theory of acceptance and use of technology, abbreviated to UTAUT) to explain the adoption and use of collaboration technology. We theorize that collaboration technology characteristics, individual and group characteristics, task characteristics, and situational characteristics are predictors of performance expectancy, effort expectancy, social influence, and facilitating conditions in UTAUT. We further theorize that the UTAUT constructs, in concert with gender, age, and experience, predict intention to use a collaboration technology, which in turn predicts use. We conducted two field studies in Finland among (1) 349 short message service (SMS) users and (2) 447 employees who were potential users of a new collaboration technology in an organization. Our model was supported in both studies. The current work contributes to research by developing and testing a technology-specific model of adoption in the collaboration context.
This paper identifies and analyzes firm-level characteristics that facilitate onshore and offshore business process outsourcing (BPO). We use organizational learning and capabilities to develop a conceptual model. We test the conceptual model with archival data on a broad cross section of U.S. firms. Our empirical findings indicate that firms with experience in onshore information technology (IT) outsourcing and capabilities related to IT coordination applications and process codification are more likely to engage in BPO, and firms with experience in internationalization are more likely to engage in offshore BPO. We also find that IT coordination applications have a greater impact on onshore BPO than on offshore BPO, and the effect of process codification is partly mediated through IT outsourcing.
Organizational use of information and communications technologies (ICT) is increasingly resulting in negative cognitions in individuals, such as information overload and interruptions. Recent literature has encapsulated these cognitions in the concept of technostress, which is stress caused by an inability to cope with the demands of organizational computer usage. Given the critical role of the user in organizational information processing and accomplishing application-enabled workflows, understanding how these cognitions affect users' satisfaction with ICT and their performance in ICT-mediated tasks is an important step in appropriating benefits from current computing environments. The objective of this paper is to (1) understand the negative effects of technostress on the extent to which end users perceive the applications they use to be satisfactory and can utilize them to improve their performance at work and (2) identify mechanisms that can mitigate these effects. Specifically, we draw from the end-user computing and technostress literature to develop and validate a model that analyzes the effects of factors that create technostress on the individual's satisfaction with, and task performance using, ICT. The model also examines how user involvement in ICT development and support mechanisms for innovation can be used to weaken technostress-creating factors and their outcomes. The results, based on survey data analysis from 233 ICT users from two organizations, show that factors that create technostress reduce the satisfaction of individuals with the ICT they use and the extent to which they can utilize ICT for productivity and innovation in their tasks. Mechanisms that facilitate involvement of users, and encourage them to take risks, learn, explore new ideas, and experiment in the context of ICT use, diminish the factors that create technostress and increase satisfaction with the ICT they use. These mechanisms also have a positive effect on users' appropriation of ICT for productivity and innovation in their tasks. The paper contributes to emerging literature on negative outcomes of ICT use by (1) highlighting the influence of technostress on users' satisfaction and performance (i.e., productivity and innovation in ICT-mediated tasks) with ICT, (2) extending the literature on technostress, which has so far looked largely at the general behavioral and psychological domains, to include the domain of end-user computing, and (3) demonstrating the importance of user involvement and innovation support mechanisms in reducing technostress-creating conditions and their ICT use related outcomes.
Over the past few years, open source software (OSS) development has gained a huge popularity and has attracted a large variety of developers. According to software engineering folklore, the architecture and the organization of software depend on the communication patterns of the contributors. Communication patterns among developers influence knowledge sharing among them. Unlike in a formal organization, the communication network structures in an OSS project evolve unrestricted and unplanned. We develop a non-cooperative game-theoretic model to investigate the network formation in an OSS team and to characterize the stable and efficient structures. Developer heterogeneity in the network is incorporated based on their informative value. We find that there may exist several stable structures that are inefficient and there may not always exist a stable structure that is efficient. The tension between the stability and efficiency of structures results from developers acting in their self-interest rather than the group interest. Whenever there is such tension, the stable structure is either underconnected across types or overconnected within type of developers from an efficiency perspective. We further discuss how an administrator can help evolve a stable network into an efficient one. Empirically, we use the latent class model and analyze two real-world OSS projects hosted at Source Forge. For each project, different types of developers and a stable structure are identified, which fits well with the predictions of our model. Overall, our study sheds light on how developer abilities and incentives affect communication network formation in OSS projects.
Application-based pricing is common in telecommunications. Wireless carriers charge consumers more per byte of traffic for text messages than they do for wireless surfing or voice calls. Such pricing is possible because carriers and handset manufacturers have the ability to tag and meter each application. While tagging and metering are possible in the case of closed platforms such as iPhone, they are not in the case of open platforms such as Android. Android is open source with open application programming interfaces, and anyone can develop applications for it. Because the carriers have little control over applications, Android is inherently disruptive of differential pricing across applications. Users and neutrality advocates support Android, believing that it can increase consumer surplus by disrupting differential pricing. However, we show that the equilibrium under differential pricing is different from the equilibrium under open platforms, and it is particularly so with regard to the sets of consumers served and the quantities consumed. With open platforms, certain consumers are either not served or they are served a quantity that is less than what they would be served under differential pricing. Consequently, the consumer surplus and the social surplus are often lower with open platforms. Similarly, firms are expected to prefer differential pricing. We show that this expectation is also not true under certain circumstances in which open platforms and neutral pricing work like a quasi-bundle.
Employee information-seeking behavior shapes the formation of organizational communication networks and affects performance. However, it is not easy to facilitate, particularly through information technology, and its motivations are not well understood. Recognizing two broad categories of information-that is, task and social information-this study investigates and compares the antecedents of task and social information seeking. Deriving from the relational communication perspective, informational and relational motivations are modeled as the two main antecedents of source preference and sourcing frequency in dyadic information seeking. Through a survey of employee dyads, our findings indicate that perceived information relevance is a significant antecedent of source preference for both task and social information seeking, whereas perceived relational benefit is significant in the context of task information. The results also show that perceived relational benefit has a stronger effect on source preference in task information seeking than in social information seeking. Furthermore, preference for a source is a significant antecedent of the frequency of sourcing in both contexts. This study provides an explanation of the formation of organizational communication networks. It suggests that organizational information and communication technologies not only need to support information delivery but must also facilitate relationship management for the seeker.
The design and implementation of online auctions has given rise to a unique set of bidding strategies that has stimulated a growing body of research. We make use of a theoretically grounded, well-understood, and empirically observable bidder behavior-the winner's curse adjustment for the expected number of bidders in an auction-to examine the relationships between bidder experience, bidding patterns, and the winner's curse adjustment in rare coin online auctions. We also examine the impact of uncertainty on the winner's curse adjustment, both by using precise measures of uncertainty and by considering seller and bidder strategies for reducing that uncertainty. We analyze a complete record of all auctions in a three-month period for rare U.S. coins, examining 284,681 bids from 62,625 auctions hosted by eBay, the market leader in online auctions. One of the main contributions of this paper is to demonstrate that the bidding patterns associated with different bidders are strongly related to whether they calculate their bids to take into account the number of competing bidders, as predicted for common-value auctions. This is a substantial extension and empirical confirmation of prior work that has explored the implications of different observed patterns of bidding. We also explore new territory by examining the relationships between bidder experience, bidding patterns observed, and the economic outcomes for bidders. We are able to show that bidders with more domain-specific experience (rather than general auction experience) make better adjustments for the winner's curse, that experience has an effect on the type of bidding strategy, and that the type of bidding strategy has a significant effect on the economic outcomes for the bidders.
Opportunism, or self-interest seeking with guile, is often witnessed in human behavior, and it bedevils human interactions and relationships. Organizations expend considerable effort to reduce opportunism. Agency theory espouses formal contracts as effective constraints on opportunism; however, a consultant's use of tacit knowledge subjects clients to information asymmetry that is not amenable to formal contracts. The principal professional lens was developed to accommodate the presence of tacit knowledge, but it ignores formal contracts and, like agency theory, ignores the existence of principal opportunism. This examination of information systems (IS) consulting notes that when information asymmetry is present, both clients and consultants sometimes behave opportunistically. The level of information asymmetry, the type of knowledge, and the level of contract specificity in an IS consulting engagement determine the mixture of legal and social constraints that are efficacious. Based on these revelations and the inadequacy of other theories, a theoretical model of relationship constraints is developed to explain the interplay between signaling and screening, knowledge type, contract specificity, and the levels of information asymmetry in predicting adopted constraint mechanisms. For researchers, this new model offers a lens to study opportunism from a knowledge-based perspective, whereas for practitioners it offers the possibility of forestalling a decline in markets due to rampant opportunism.
Some digital business models may be so innovative that they overwhelm existing regulatory mechanisms, both legislation and historical jurisprudence, and require extension to or modification of antitrust law. Regulatory policies that were developed in response to nineteenth- or twentieth-century antitrust concerns dealt principally with economies of scale leading to monopoly power and may not be well suited to the issues of network effects or third-party payer online business models such as sponsored search. From the perspective of information systems economics, we investigate if such third-party payer digital systems require intervention as profound as the government's innovative approach to the problems posed by AT&T in the 1913 Kingsbury Commitment, establishing the first private regulated monopoly. Google provides an example of a company whose innovative digital business model is difficult to fit into current regulatory frameworks, and may provide examples of the issues that might require an extension to regulatory policy.
Many countries limit the influence of foreign entertainment products, such as music, film, and television programs, to protect their domestic cultural industry. Commonly observed policy tools include quotas, tariffs, and subsidies. However, advances in digital technology enable consumers to access digital versions of foreign entertainment programs via the Internet, a leakage channel that bypasses government protection methods. This calls for a reexamination of the effectiveness of these traditional tools. We build a unified analytical framework to study the impact of digital technology on cultural protection policies. We find that in the presence of Internet leakage, imposing a quota is the least effective protection policy to maximize the total domestic social welfare, but using either a tariff or subsidy policy is optimal, depending on the quality difference between domestic and foreign entertainment programs via the traditional channel and the Internet. Using quotas remains the least effective policy when we extend the analyses to consider the presence of piracy. In addition to the quality difference between foreign and domestic entertainment, the proportion of unethical consumers and the cost of piracy determine whether using tariffs or subsidies is the optimal policy.
In this paper, we set up a game-theoretic model to examine oligopolistic price competition, considering two features of online search: the existence of a common search ordering and shoppers who have nonpositive search cost. We find that in equilibrium firms set their prices probabilistically rather than deterministically, and different firms follow different price distributions. The equilibrium pricing pattern exhibits an interesting local-competition feature in which direct price competition occurs only between firms adjacent to each other. Further, we incorporate consumers' search strategies into the model so that both search order and stopping rules are determined rationally by consumers. We show that similar patterns may continue to hold in the fully rational framework when consumers have higher inspection costs for inferior positions.
Witnessing both opportunities and challenges in virtual work arrangements, researchers have explored a number of technological, social, and organizational factors in order to improve virtual work effectiveness. However, there is limited understanding of an important element of virtual work the individuals. Our review of the literature indicates that the composition of individual knowledge, skills, and abilities (KSAs) required to work virtually would benefit from further research. In this study, we theoretically and empirically develop the construct of individual virtual competence that captures the key KSAs required to perform effectively in today's virtualized workplace, within a parsimonious nomological network. Substantiated by its explanatory power on individual perceived performance and satisfaction, individual virtual competence contributes to the literature by acknowledging a distinct workplace competency that can be incorporated in future individual-level studies of virtual phenomena. This research provides managers with a lens to understand differences in individual work outcomes and provides a lever to developing individuals' capabilities so as to improve work outcomes.
Despite the extensive research on information technology (IT) outsourcing, our knowledge and understanding of how industry characteristics impact the use of IT outsourcing remain limited. Drawing upon theories from organization behavior and industrial economics, this study identifies four major industry characteristics (i.e., munificence, dynamism, concentration, and capital intensity) and investigates how each of these factors affects the use of IT outsourcing. Specifically, we postulate that the extent of industry munificence is positively related to the utilization of IT outsourcing. Since timely strategic actions are the crucial aspects of leveraging munificent resources, IT outsourcing, which can be implemented in short periods of time, is considered to be a preferred option in such environments. Furthermore, industry dynamism is also positively associated with IT outsourcing, given that firms in dynamically evolving industries tend to look for flexibility and avoid a large amount of fixed investments (e.g., IT development in-house). In contrast to these hypotheses, we predict that industry concentration is negatively related to IT outsourcing. Firms in concentrated industries are likely to develop their own IT infrastructures, as they are not constrained by institutional pressures or cost-driven strategic actions. Finally, because firms in capital-intensive industries tend to conform to long-standing traditional practices, and do not highly value novel and risky practices, they will be less likely to use IT outsourcing than firms in industries with low capital intensity. The data from the U.S. Bureau of Economic Analysis along with Compustat empirically validated all of the proposed hypotheses; however, only marginal support was found for the association between industry concentration and IT outsourcing. Our findings offer business executives and IT service providers strategic and managerial insights into the dynamics and complexities involved in the diverse aspects of industry environments and IT outsourcing decisions.
Social computing technologies typically have multiple features that allow users to reveal their personal information to other users. Such self-disclosure (SD) behavior is generally considered positive and beneficial in interpersonal communication and relationships. Using a newly proposed model based on social exchange theory, this paper investigates and empirically validates the relationships between SD technology use and culture. In particular, we explore the effects of culture on information privacy concerns and the desire for online interpersonal awareness, which influence attitudes toward, intention to use, and actual use of SD technologies. Our model was tested using arguably the strongest social computing technology for online SD instant messaging (IM) with users from China and the United States. Our findings reveal that cross-cultural dimensions are significant predictors of information privacy concerns and desire for online awareness, which are, in turn, found to be predictors of attitude toward, intention to use, and actual use of IM. Overall, our proposed model is applicable to both cultures. Our findings enhance the theoretical understanding of the effects of culture and privacy concerns on SD technologies and provide practical suggestions for developers of SD technologies, such as adding additional control features to applications.
This paper examines how information provided by online reviews influences firms' pricing strategy for repeat purchase products. It is commonly understood that online reviews can reduce consumer uncertainty about product characteristics and, therefore, have the potential to increase product demand and firm profits. However, when considering repeat purchase products, online reviews have an additional effect in that they can alter consumers' propensity to switch among products, which can intensify price competition and lead to lower profits. The strength of these potentially offsetting effects depends on the informativeness of consumer reviews, which is a function of both objective review accuracy and the ability of consumers to obtain information from reviews when their idiosyncratic preferences over product characteristics might differ from the preferences of reviewers. The interplay of these competing effects results in an S-shaped relationship between the quality of reviews and firm profits. There exists an optimal level of consumer informedness from the firms' perspective, and competing firms may have incentives to facilitate consumer reviews in some markets but not in others. Given firms' strategic pricing, consumers may also be worse off as review informativeness increases.
Interactive decision aids (IDAs) typically use concrete, feature based approaches to interact with consumers. Recently, however, interaction designs that focus on communicating abstract consumer needs have been suggested as a promising alternative. This paper investigates how temporal distance moderates the effectiveness of these two competing IDA communication designs by its effect on consumers' mental representation of the product decision problem. Temporal distance is inherently connected to IDAs in two ways. Congruency between consumption timing (immediate versus distant) and IDA communication design (concrete versus abstract, respectively) increases the likelihood to accept the IDA's advice. This effect is also achieved by congruency between IDA process timing (immediate versus delayed delivery of recommendations) and IDA communication design (concrete versus abstract, respectively). We further show that this process is mediated by the perceived transparency of the IDA process. Managers and researchers need to take into account the importance of congruency between the user and the interface through which companies interact with their users and can further optimize IDAs so that they better match consumers' mental representations.
The tendency to remain silent about project-related issues can contribute to suboptimal project performance or project failure. Prior research in offshore outsourcing suggests that client managers should play a critical role to induce offshore vendors' employees not only to report project problems in a timely fashion but also to brainstorm and contribute ideas to a project. Also, the extant research on cross-cultural teams has emphasized the importance of cultural adaptation in the smooth functioning of these teams, but the role of cultural adaptation in silence mitigation has been largely underdeveloped in the literature. In this research, we bring these concepts of vendor silence and cultural adaptation in cross-cultural teams together and develop a process framework that illustrates how vendor silence may be mitigated in offshore outsourcing through various silence mitigation mechanisms. We then develop three propositions for organizational action toward mitigating vendor silence, which highlight the mediating role of cultural adaptation.
The Internet makes it easy to offer large assortments of products, tempting managers to chase the long tail that is, the phenomenon in which niche products gain a significant share of demand among all products. Yet few studies empirically examine the existence and drivers of this long tail phenomenon. This study uses a unique data set with 843,922 purchases from 143,939 customers that a monopolistic video-on-demand operator observed over 111 weeks after its launch of the service. The current analysis centers on the effects of increasing assortment sizes and improved search technologies on measures of the long tail, such as per customer demand, the share of products purchased from the assortment, the distribution of demand across products, and the concentration of demand. Increases in assortment sizes and better assortment quality lead to increases in demand per customer and a longer tail. The length of the tail (i.e., share of purchased products) is also driven by new customers and seasonal effects, such as school vacations, whereas the presence of high-quality blockbuster products shortens the tail. Different search technologies can shift demand toward niche products as well as toward blockbuster products.
In this paper, we build analytical models to examine the impact of network externalities on the competition between open source software (OSS) and proprietary software. We investigate the competing OSS and proprietary software products with comparable functionalities in four different scenarios depending on whether they are compatible with each other and whether the underlying market is fully covered (i.e., all consumers adopt one of the two products). Furthermore, we study which party has the most incentive to make its product compatible with its counterpart. When the market is fully covered, the installed base and the profit of proprietary software increase at the expense of a decreasing user base for OSS in the presence of network externalities. This competitive imbalance becomes more pronounced when OSS and proprietary software are incompatible and the market is partially covered. Finally, we find that in the presence of network externalities, being compatible with its rival is not desirable for the proprietary software, but highly beneficial to the OSS community.
The success of Wikipedia demonstrates that self-organizing production communities can produce high-quality information-based products. Research on Wikipedia has proceeded largely atheoretically, focusing on (I) the diversity in members' knowledge bases as a determinant of Wikipedia's content quality, (2) the task-related conflicts that occur during the collaborative authoring process, and (3) the different roles members play in Wikipedia. We develop a theoretical model that explains how these three factors interact to determine the quality of Wikipedia articles. The results from the empirical study of 96 Wikipedia articles suggest that (I) diversity should be encouraged, as the creative abrasion that is generated when cognitively diverse members engage in task-related conflict leads to higher-quality articles, (2) task conflict should be managed, as conflict notwithstanding its contribution to creative abrasion can negatively affect group output, and (3) groups should maintain a balance of both administrative- and content-oriented members, as both contribute to the collaborative process.
We empirically investigate whether business process reengineering (BPR), which requires substantial investment in information technology to integrate separate tasks into complete cross-functional processes, is associated with enhanced firm productivity and performance. We analyze firm-level panel data covering the period 1987-2008 using fixed effects and first differencing, standard methods that account for unobservable firm-level effects. We find that return on assets drops significantly during the project initiation year. According to fixed effects results, the performance and productivity measures improve in a decreasing manner after project initiation, suggesting that BPR indeed positively affects firm performance on average. We also find that enterprise-wide BPR projects are associated with more negative returns during project initiation than functionally focused projects. However, there is no clear evidence regarding their superiority over functionally focused BPR projects in terms of performance improvements after project initiation, perhaps because grand projects are risky and sometimes lead to grand failures.
Web sites rely on pictures and animation to convey subtle messages that are more effectively communicated nonverbally. We argue that such messages could have strong cultural content, which should be understood in developing Web sites. Hence, this paper explores the cultural content of Web site images and develops a theory for Web-image signifiers. This is done in two phases. Phase I has an interpretive qualitative approach that uses Grounded Theory to identify signifiers and to develop the Web-image signifiers (WIS) theory. Phase II quantitatively tests the WIS theory. Together, these two phases identify and validate signifiers of cultural dimensions in Web site images. More interestingly, the results uncover that cultural dimensions are signified in five categories, of which two, humans and buildings categories, are the most prominent. The contribution of this paper is in developing a comprehensive theory for the cultural content of Web images, identifying 48 signifiers in Web images, discovering new categories of signifiers, and providing insights into the nature of cultural signification by testing the theory. Such knowledge could heighten our sensitivity and awareness of hidden cultural messages in Web site images. The WIS theory could provide a novel approach to the cultural studies of Web images and other artifacts with cultural content. The results of this work have immediate application in the design of Web sites for a multicultural audience.
We examine how the virtuality of work context influences individuals' social networks within and across work groups. Given this purpose, we develop a multilevel research framework that explores the effects of different levels of virtuality on one's intra-group tie strength and extra-group network range based on the computer-mediated communication theory, the proximity theory, and the social network theory. The results of the hierarchical linear modeling indicate that the individual-level virtuality (use of personal and communal communication technologies) significantly influences one's intra-group tie strength and extra-group network range. Moreover, the results show that the effects of individual-level virtuality on social networks vary depending on the group-level virtuality, such as geographic/temporal dispersion and technological support. By illuminating how individuals' social networks can be developed through the appropriate use of personal and communal communication technologies in the context of a virtual group, this study provides useful insights into the mechanics that underlie effective virtual work.
The importance of communication and trust in the context of global virtual teams has been noted and reiterated in the information systems (IS) literature. Yet precisely how communication and trust influence certain outcomes within virtual teams remains unresolved. In this study, we seek to contribute some clarity to the understanding of the theoretical linkages among trust, communication, and member performance in virtual teams. To this end, we identify and test three proposed models (additive, interaction, and mediation) describing the role of trust in its relationship with communication to explain performance. In testing the relationships, we note that the concepts of communication and trust are inherently relational and not properties of individuals. Thus, we argue that a social network approach is potentially more appropriate than attribute-based approaches that have been utilized in prior research. Our results indicate that the mediating model best explains how communication and trust work together to influence performance. Overall, the study contributes to the existing body of knowledge on virtual teams by empirically reconciling conflicting views regarding the interrelationships between key constructs in the literature. Further, the study, through its adoption of the social network analysis approach, provides awareness within the IS research community of the strengths of applying network approaches in examining new organizational forms.
This exploratory paper presents a conceptual model of the factors of governmental support and openness, business and technology investment, and socioeconomic level that are posited to influence technological utilization. The conceptual model and conjectures are developed inductively based on logic and prior research about the relationship among variables related to the factors. Structural equation modeling (SEM) is applied to operationalize and test the model. The SEM analysis tests five points of investigation on a large sample of country data from the World Bank and the World Economic Forum. Findings indicate a critical pathway of associations between the factors of government support and openness, investment in business and technology, socioeconomic level, and technology utilization. The paper presents two country case examples of the model and suggests policy steps for national governments of developed and developing countries to prioritize information and communications technology, create openness, strengthen research and development and technology investment, and enhance education and information technology training.
We have created an automated kiosk that uses embodied intelligent agents to interview individuals and detect changes in arousal, behavior, and cognitive effort by using psychophysiological information systems. In this paper, we describe the system and propose a unique class of intelligent agents, which are described as Special Purpose Embodied Conversational Intelligence with Environmental Sensors (SPECIES). SPECIES agents use heterogeneous sensors to detect human physiology and behavior during interactions, and they affect their environment by influencing human behavior using various embodied states (i.e., gender and demeanor), messages, and recommendations. Based on the SPECIES paradigm, we present three studies that evaluate different portions of the model, and these studies are used as foundational research for the development of the automated kiosk. The first study evaluates human computer interaction and how SPECIES agents can change perceptions of information systems by varying appearance and demeanor. Instantiations that had the agents embodied as males were perceived as more powerful, while female embodied agents were perceived as more likable. Similarly, smiling agents were perceived as more likable than neutral demeanor agents. The second study demonstrated that a single sensor measuring vocal pitch provides SPECIES with environmental awareness of human stress and deception. The final study ties the first two studies together and demonstrates an avatar-based kiosk that asks questions and measures the responses using vocalic measurements.
In the field of collaboration engineering, thinkLets describe reusable and transferable collaborative activities to reproduce known patterns of collaboration. This paper focuses on thinkLets of the pattern Generate, which define collaboration activities to produce and share new contributions by a group. We address the question whether the small number of published Generate thinkLets can adequately represent the various approaches contained in published idea generation techniques. We used a cognitive model to analyze 101 idea generation techniques with regard to the underlying mental principles that stimulate the ideation process by deliberately activating larger areas of the knowledge network. We present three changes of perspective based on these principles, which can be used to formalize the underlying mechanisms of idea generation techniques. The paper shows how these three principles can be used to improve Generate thinkLets and discusses how this formalization can improve the applicability of information systems for ideation processes.
Historically, inaccurate credibility assessments have resulted in tremendous costs to businesses and to society. Recent research offers unobtrusive credibility assessment aids as a solution; however, the accuracy of these decision aids is inadequate, and users often resist accepting the aids' recommendations. We follow the principles of signal detection theory to improve the accuracy of recommendations in computer-aided credibility assessment by combining automated and participatory decision support. We also leverage participation in decision-making theory to explain and predict an increased acceptance of assessment aid recommendations when perceptual cues are elicited from users. Based on these two theories, we design and test a hybrid decision aid to perform automated linguistic analysis and to elicit and analyze perceptual cues from an observer. Results from a laboratory experiment indicate that decision aids that use linguistic and perceptual cues offer more accurate recommendations than aids that use only one type of cue. Automatic analysis of linguistic cues improved both the decision aid's recommendations and the users' credibility assessment accuracy. Challenging the generalizability of past findings, the elicitation of perceptual cues did not improve the decision aid's recommendations or the users' assessment accuracy. Elicitation of perceptual cues, however, did improve user acceptance of the decision aid's recommendations. These findings provide guidance for future development of credibility assessment decision aids.
In response to the rapid changes in users' requirements, a new generation of information systems (IS), namely, agile IS, has emerged. Agile IS, defined as information systems developed using agile methods, are characterized by frequent upgrades with a small number of new features released periodically. The existing research on agile IS has mainly focused on the developers' perspective with little research into end users' responses to these agile IS. Drawing upon the tripartite model of attitude, the status quo and the omission bias theories, and the availability heuristic, we propose a model that utilizes constructs from the unified theory of acceptance and use of technology, the IS continuance model, habit, and individual differences to examine the drivers of user acceptance of agile IS. Further, we investigate not only users' intentions to continue using the agile IS but also their intentions to use new features when they are released, which is a surrogate for the ultimate success of agile IS. Data from 477 users of an agile IS showed that users' level of comfort with constant changes, the facilitating conditions provided, and users' habit are predictors of both types of intentions, with users' level of comfort with constant changes being the strongest predictor. Users' intentions to continue using agile IS are also determined by users' satisfaction with and perceived usefulness of the past upgrades. Finally, users who are innovative are more likely to use future releases of new features. The present work fills a gap in the software engineering literature and contributes a technology acceptance model specific to agile IS, which are becoming a mainstay of companies' IT portfolio in a fast-changing business environment.
We study interindustry information technology (IT) spillover wherein IT investments made by supplier industries increase the productivity of downstream industries. Using data from U.S. manufacturing industries, we find that industries receive significant IT spillover benefits in terms of total factor productivity growth through economic transactions with their respective supplier industries. More importantly, we find that two characteristics of downstream industries, namely, IT intensity and competitiveness, which have been shown to moderate the effect of internal IT investments, play an important role in IT spillovers as well. Our results suggest that IT intensity as well as competitiveness of the downstream industry moderate the effect of IT spillovers industries that are more IT intensive and more competitive benefit more from IT spillovers. Finally, our results suggest that the long-term effects of spillovers are greater than short-term effects, suggesting that learning periods are required to reap the benefits from the IT spillovers.
Effective knowledge management has been increasingly cited as critical for businesses to compete successfully. Knowledge acquisition/capture, the first step in knowledge management, continues to be a bottleneck and is exacerbated when experts are geographically distributed. Furthermore, knowledge from multiple experts is likely to generate inconsistent knowledge for a given problem domain. There is thus a compelling need to generate consensus by resolving inconsistencies and conflicts that may occur among experts during the process of knowledge acquisition. This process is more challenging when dealing with virtual teams of experts. This study addresses task-based or cognitive conflicts among experts. A key objective of this study is to examine the effectiveness of two cognitive techniques the repertory grid (or RepGrid) and Delphi in generating consensus among experts during the knowledge capture process. A field experiment with geographically distributed real-world network experts involving multiple rounds of interaction over an extended period of time was conducted. Findings from this research indicate that, in the short run, Delphi works better than the RepGrid in reducing conflict and generating consensus. However, the RepGrid technique appears to perform better in the long run. We find similar results for satisfaction with the process and outcome. Our findings also indicate that experts using the RepGrid technique elicited more knowledge as well as higher-quality knowledge than experts using the Delphi technique. To sum up, our study indicates that RepGrid is superior to Delphi, and therefore managers should seriously consider the use of RepGrid in capturing knowledge from multiple and distributed experts when dealing with complex real-world issues.
E-business standards are a key infrastructure for electronic commerce. In many industries, they are collaboratively developed by firms in an open and neutral industry consortium. It is imperative to understand what drives firms' resource investments in such consortia, as they are critical for the success of e-business standardization. Based on collective action theory, we propose a research model to investigate the drivers of standard development within consortia. We test the model through a data set of 232 firms from 7 consortia. Consistent with collective action theory, our results demonstrate that firms' interests, resource availability, and consortium management effectiveness jointly determine their resource expenditures within the consortium. However, our exploratory investigation indicates differences between vendors and users, as vendors are more motivated by perceived standard benefits whereas users are more motivated by perceived process benefits. Our research provides a deeper understanding of firms' behaviors within consortia and factors driving their standard making.
There are two popular forms of business-to-business (B2B) marketplaces: public marketplaces and private channels. We study why firms choose either or both of these sourcing channels. Using a framework of decision making under uncertainty, we explain firms' choice of B2B channels as a hedging strategy and as a method of obtaining greater managerial flexibility for the future. We show that greater uncertainty can lead to higher investment with firms more likely to invest in both public and private channels. We find that the level of information technology (IT) capability and spending is an important factor in firms' decision making. When a firm chooses its level of IT investment simultaneously with the decision about which sourcing channels to use, the firm choosing both channels selects the highest level of IT capability and the firm implementing only one channel selects lower levels of IT capability.
Despite evidence that a lack of interoperable information systems results in enormous costs, development, implementation, and effective use of interorganizational systems (IOS) remain an elusive goal for many companies. Lack of interoperability across systems is especially problematic for manufacturers dependent on global supply chains. We develop propositions about the characteristics of IOS that affect information transparency in supply chains. Specifically, we propose that data and process standards are necessary, but not sufficient, to solve such information transparency problems. Instead, standards need to be complemented by hub-type information technology architectures that are shared by organizations participating in an industrial field, not just by the participants in one manufacturer's supply chain. These arguments are supported by an automotive industry case study involving data and process standardization and a shared, cloud-based architecture. We conclude with additional aspects of the case that may be relevant to addressing information transparency problems in global supply chains.
Oftentimes researchers may not only generalize across a population, but may also extrapolate research findings across time. While either assumption can introduce difficulties, generalizing results in one time frame to another time frame may be especially perilous. We study a data exchange, and find that interventions designed to improve exchange features at two points in time have markedly varying effects, from an initial transaction use (time one) to a second transaction occurring two weeks later (time two). Our research objective is to test whether two system design features have the same effects on the intent to continue using an exchange in time two as they had in time one. The two features are control transparency (the availability of information cues) and interim shipping outcome feedback. These effects are mediated, in varying degrees, by perceived information quality. We use social exchange theory and social cognition theory to develop hypotheses regarding changes between time one (the first user transaction) and time two (the second transaction). These are tested using a combined experiment and survey. Supporting the theory, outcome feedback matters at time two even though it did not matter at time one. While control transparency has direct effects on a user's intent to continue use of the exchange in time one, its effects are reduced in time two if negative outcome feedback is communicated to the user. Outcome feedback's effects grow stronger from time one to time two vis-a-vis control transparency's effects. This underscores how critical it is to examine such phenomena at more than one period of time. The study also suggests different strategies for managing data exchanges based on the time frame of use. At the initial transaction use, the exchange should make transparent high-quality information cues to its user. At the next transaction, it should provide feedback showing properly fulfilled orders. These findings have implications for both future research examining effective data exchange design and for professionals who wish to enrich electronic data exchange interactions.
We examine a platform's optimal two-sided pricing strategy while considering seller-side innovation decisions and price competition. We model the innovation race among sellers in both finite and infinite horizons. In the finite case, we analytically show that the platform's optimal seller-side access fee fully extracts the sellers' surplus, and that the optimal buyer-side access fee mitigates price competition among sellers. The platform's optimal strategy may be to charge or subsidize buyers depending on the degree of variation in the buyers' willingness to pay for quality; this optimal strategy induces full participation on both sides. Furthermore, a wider quality gap among sellers' products lowers the optimal buyer-side fee but leads to a higher optimal seller-side fee. In the infinite innovation race, we perform computations to find the stationary Markov equilibrium of sellers' innovation rate. Our results show that when all sellers innovate, there exists a parameterization under which a higher seller-side access fee stimulates innovation.
End users are said to be the weakest link in information systems (IS) security management in the workplace. They often knowingly engage in certain insecure uses of IS and violate security policies without malicious intentions. Few studies, however, have examined end user motivation to engage in such behavior. To fill this research gap, in the present study we propose and test empirically a nonmalicious security violation (NMSV) model with data from a survey of end users at work. The results suggest that utilitarian outcomes (relative advantage for job performance, perceived security risk), normative outcomes (workgroup norms), and self-identity outcomes (perceived identity match) are key determinants of end user intentions to engage in NMSVs. In contrast, the influences of attitudes toward security policy and perceived sanctions are not significant. This study makes several significant contributions to research on security-related behavior by (1) highlighting the importance of job performance goals and security risk perceptions on shaping user attitudes, (2) demonstrating the effect of workgroup norms on both user attitudes and behavioral intentions, (3) introducing and testing the effect of perceived identity match on user attitudes and behavioral intentions, and (4) identifying nonlinear relationships between constructs. This study also informs security management practices on the importance of linking security and business objectives, obtaining user buy-in of security measures, and cultivating a culture of secure behavior at local workgroup levels in organizations.
Digital intermediaries and Internet search technologies have commoditized many products, resulting in intense price competition and channel conflict. Firms use decommoditization strategies to regain control over distribution channels, as well as to implement resonance marketing and hyperdifferentiation, which allows them to improve margins through differentiation. We test two hypotheses: the decommoditization hypothesis and the resonance marketing hypothesis. We use data from an airline with a new a la carte pricing mechanism, which allows consumers to tailor airline ticket bundles to suit their individual preferences. We compare a la carte ticket pricing, whose features can be modified by the purchaser, and fixed (bundled offer) sales, which cannot be modified. We found that a significant number of travelers do use a la carte pricing, which allows the airlines to regain some control over distribution. We find that travelers customized standard bundles when it was possible for them to make a la carte ticket bookings, but mainly for low-feature standard bundles. Frequent-flyer members purchased higher-feature bundles more often when they had the opportunity. The findings support the proposed hypotheses. We discuss the implications for distribution strategy and channel conflict management.
Measuring peer influence in social networks is an important business and policy question that has become increasingly salient with the development of globally interconnected information and communication technology networks. However, in spite of the new data sources available today, researchers still face many of the same measurement challenges that have been present in the literature for over four decades: homophily, reflection and selection problems, identifying the source of influence, and determining preexisting knowledge. The goal of this paper is to develop an empirical approach for measuring information diffusion and discovery in online social networks that have these measurement challenges. We develop such an approach and apply it to data collected from 4,000 users of an online music community. We show that peers on such networks significantly increase music discovery. Moreover, we demonstrate how future research can use this method to measure information discovery and diffusion using data from other online social networks.
Online communities provide a social sphere for people to share information and knowledge. While information sharing is becoming a ubiquitous online phenomenon, how to ensure information quality or induce quality content remains a challenge because of the anonymity of commentators. This paper introduces moderation into reputation systems. We show that moderation directly affects strategic commentators' incentive to generate useful information, and moderation is generally desirable to improve information quality. We find that when being moderated with different probabilities based on their reputations, commentators might display a pattern of reputation oscillation, in which they generate useful content to build up high reputation and then exploit their reputation. As a result, the expected performance from high-reputation commentators can be inferior to that from low-reputation commentators (reverse reputation). We then investigate the optimal moderation resource allocation and conclude that the seemingly abnormal reverse reputation could arise as an optimal result. Our study underscores the importance of moderation and highlights that the frequency of moderation should be properly chosen for better performance of online communities.
This research examines the role of diversification on incumbent firms' response to the threat of new entry. When faced with threats posed by new technologies, incumbent firms in the information technology (IT) industry can either perform research and development (R&D), or acquire the new entrants who are successful at innovating. We use a two-stage game-theoretic framework to model the relation between diversification and the decision to acquire versus perform R&D. We also collect data on financial indicators for firms in the IT industry using the Compustat database to empirically test the propositions from the analytical model. Our results suggest that firms with a higher degree of diversification are more likely to innovate through acquisition than through R&D. Moreover, diversification has a positive effect on investment in acquisitions, as well as a negative effect on investment in R&D.
While integrating information flows between internal organizational functions and across partner firms is widely acknowledged as a contributor to organizational competitiveness, there is little empirical research on the effects of situational factors on the success of information integration. Based on contingency theory, we address the following question: Under what circumstances does information integration contribute to better performance outcomes in supply chain management (SCM)? Our results provide a contingency perspective of information integration, which highlights that the performance outcomes of information integration are contingent on both external environmental conditions and internal operational characteristics. We find that information integration improves firms' ability to perform, particularly when they operate under favorable environmental conditions-a highly munificent and a less uncertain environment-and when they offer durable and complex products. Our findings advance contingency research on the performance outcomes of information integration for SCM. Our study provides managers with empirical insights on the effects of information integration on the cost and customer-oriented operational performance of SCM under favorable and unfavorable environmental conditions.
We demonstrate that two intertwined activities of music piracy, unauthorized obtaining and unauthorized sharing, are differentially influenced by the same social learning environment. We develop a structural model and test it using survey data from a prime demographic set of respondents who engage in music piracy. Considering behavioral heterogeneity, we employ a factor mixture modeling technique to classify respondents into different groups that highlight distinct patterns of social learning influences. We find that the differential effects of social learning factors on obtaining and sharing persist across these groups. We further utilize demographic variables to profile members in each group for segmentation insights. From a theoretical perspective, our findings advance the understanding of music piracy and suggest the importance of separating obtaining from sharing activities when studying piracy. From a managerial perspective, our research provides new avenues for managers and policymakers to design targeted incentives to curtail music piracy.
The alignment of information technology (IT) and business strategy is a perennial challenge for corporate executives. While earlier studies confirm the value of alignment, there is still some question as to how alignment creates value and the level at which value is created. In this research, we use a series of theoretical arguments based on the interconnected structure of the value chain to consider the extended effects of alignment at the process level. Since processes are often linked to create a complex chain of activities, the absence or presence of alignment in any process could have implications for business performance elsewhere in the value chain. Minimally aligned processes can not only disrupt performance within the focal process, but their effects may also be felt further downstream in the form of bottlenecks and a diminution in the business value of IT. Using a simplified form of the value chain and data from matched surveys of business and IT executives at 317 U. S. and EU firms, we examine how the effects of alignment on a given process spill over into processes further downstream, creating higher IT business value in those downstream processes. We also show that these spillover effects continue along the length of the value chain and do not diminish based on distance from the focal process. Our results reinforce the call for firms to improve the fit between business and IT strategy by showing how efforts to improve alignment in a given process can deliver a stream of benefits along the value chain. This research provides a fresh perspective on the value of alignment, facilitating a deeper understanding and appreciation of the link between strategic IT alignment and firm performance.
Organizations seek to differentiate themselves in the marketplace by deploying information technology (IT) to develop dynamic IT capabilities and resist competitors' attempts to imitate or improve these capabilities. While this strategy has been justified on the grounds that dynamic IT capabilities are durably heterogeneous, there does not seem to be empirical evidence supporting or refuting this assumption. This study empirically validates the assumption of durable heterogeneity of dynamic organizational IT capability (ITC) due to path dependence. We capture ITC heterogeneity by introducing a framework in which firms try to achieve ITC leadership in their industry and we propose that durable ITC heterogeneity can be attributed to path dependence, and hence, it can be tested using Heckman's true state dependence of ITC leadership status. Using random and fixed effect dynamic logit models, we investigate true state dependence of ITC leadership on a sample of large U. S. firms. The results, which are robust to alternative sample, dependent, and control variable specifications, show that achieving ITC leadership is a true state-dependent process, suggesting durable heterogeneity of ITC due to path dependence. The study contributes to the dynamic capabilities literature and has important managerial implications. The proposed framework for conceptualizing durable resource heterogeneity due to path dependence is general and versatile, thus providing a foundation for future research on dynamic capabilities. The findings provide empirical evidence to confirm that ITC is durably heterogeneous and should be managed as a potential source of competitive advantage.
Virtual worlds, immersive three-dimensional virtual spaces where users interact with projected identities of other users (avatars) and objects, are becoming increasingly popular and continue to grow as highly interactive, collaborative, and commercial cyberspaces. However, extant research in this context has not paid much attention to usability design of a virtual world and corresponding effects on users' psychological desire to own and control the space and objects within it and subsequent behavior intention. In this study, we apply concepts of Web site usability and psychological ownership to develop a model that illustrates the relationships between seven usability factors (legibility, firmness, coherence, variety, mystery, classic, and expressive visual aesthetics), four antecedents of psychological ownership (cognitive appraisals, perceived control, affective appraisals, and self-investment), psychological ownership, and use intention. A cross-sectional study with 239 Second Life users was conducted. The results demonstrate that designing a usable virtual world that induces strong psychological ownership is crucial to attract users to spend more time, participate in more activities, and revisit the virtual world. This is an important finding for forward-looking e-business managers looking to invest their limited resources in designing a usable virtual world. In addition, by using our model and corresponding survey items, designers can benchmark and evaluate the usability of their current virtual worlds, compare the results to the designs of competitors, and upgrade the offerings of virtual worlds, as needed, by allocating available resources to the most influential design factors to suit their specific needs.
Organizations are increasingly concerned about ensuring that workers have sufficient sense of control over the information technology (IT) that they use. However, we know little about the antecedents of the end user's perceived behavioral control (PBC) with respect to IT. Drawing on Kurt Lewin's field theory, the present study responds to this concern by formulating and testing a model whereby individual, contextual, and social forces influence PBC directly and indirectly via computer anxiety. In order to test the model, a survey was conducted in France with IT end users enrolled in professional training programs. The results show that increasing autonomy, offering appropriate managerial support, reducing work overload, and perceived innovativeness with IT can together reduce computer anxiety and increase PBC. These findings emphasize the forces that managers can manipulate in order to foster users' feelings of control with respect to IT in the workplace. Following this, the paper makes three main contributions to research. First, it increases our knowledge of the nomological net surrounding PBC by shedding light on the joint influences of internal, external, and social forces on this variable. Second, it reveals the role of computer anxiety, emphasizing that it is an important conduit through which these forces influence workers' PBC. Third, the paper shows how Lewin's field theory can help to create richer and less fragmented models in order to capture more fully the determinants of IT adoption and adaptation. The practical implications regarding the actions that managers can take in order to increase workers' PBC are discussed.
Effective management of information technology (IT) and IT-enabled services is becoming increasingly important due to the growing complexity of their context. These services are often delivered by employees who work at widely dispersed locations and interact with each other to constitute knowledge-intensive service delivery networks (KISDNs). This paper contributes to the effective design and management of KISDNs by presenting a mixed-integer programming model that integrates disparate streams of research. This model facilitates analysis and managerial benchmarking of KISDN performance. It captures how the performance of such networks depends on the interaction between workflow decisions, structure of information flow networks (IFNs), and knowledge management decisions. We propose that knowledge about IFNs and worker competence can be effectively used to make workflow decisions. Our results, based on the study of different IFN archetypes, illustrate practices for effective management of KISDNs. Managers can enhance business value by recognizing existing IFNs, increasing randomness in IFNs, nurturing weak or performative ties depending on the archetype, assigning tasks based on effective worker competence, and selectively delaying assignment of tasks to workers. In addition, our results illustrate the impact of training and network density on KISDN performance.
Blogs have emerged as an innovative tool for sharing information and knowledge, and they command significant interest from information technology (IT) users as well as providers. Our study establishes a research framework to provide an understanding of the factors affecting knowledge sharing among bloggers in online social networks. The research results indicate that bloggers' trust, strength of social ties, and reciprocity all have a positive effect on their knowledge-sharing behavior. Further, the impact of each factor on such behavior varies by gender. Our results provide evidence that offline expected social norms tend to persist in the online blogosphere and that gender differences need to be considered as a significant factor in understanding the IT usage behavior in the context of social capital theory. For IT managers and blog service providers, our results also highlight the importance of being gender aware in an effort to elicit participation from all constituent members for the successful adoption and usage of blogs as a knowledge-sharing mechanism.
We explore the issue of seller differentiation in competitive auction environments, where most sellers have a high percentage of positive feedback. Analyzing a set of eBay auction listings for identical products, we find evidence that the use of visibility-enhancing and quality-signaling discretionary auction attributes affects auction outcomes throughout the auction process (i.e., listing views, bids, and price premiums). We also find strong evidence that the number of reputable sellers in an auction marketplace moderates the effects of these discretionary attributes on auction outcomes. Specifically, as auction environments become more competitive, these attributes become more effective tools for differentiation, whereas seller feedback scores become less effective. Furthermore, sellers appear to select their strategies for employing these discretionary attributes based on both their prior experience and the number of experienced reputable sellers in the market. These findings suggest that in addition to relying on feedback scores, online sellers must take a more strategic approach in the selection of discretionary attributes in their auction listings.
Despite the need to better understand how customers of software-as-a-service (SaaS) solutions perceive the quality of these software services and how these perceptions influence SaaS adoption and use, there is no extant measure that comprehensively captures service quality evaluations in SaaS. Based on previous SERVQUAL and SaaS literature, field interviews and focus groups, a card-sorting exercise, and two surveys of SaaS using companies, we develop, refine, and test SaaS-Qual, a zones-of-tolerance (ZOT)-based service quality measurement instrument specifically for SaaS solutions. Besides validating already established service quality dimensions (i.e., rapport, responsiveness, reliability, and features), we identify two new factors (i.e., security and flexibility) that are essential for the evaluation of service quality of SaaS solutions. SaaS-Qual demonstrates strong psychometric properties and shows high nomological validity within a framework that predicts the continued use of SaaS solutions by existing customers. In addition to developing a validated instrument that provides a fine-grained measurement of SaaS service quality, we also enrich existing research models on information systems continuance. Moreover, the SaaS-Qual instrument can be used as a diagnostic tool by SaaS providers and users alike to spot strengths and weaknesses in the service delivery of SaaS solutions.
Open source projects do have requirements; they are, however, mostly informal text descriptions found in requests, forums, and other correspondence. Understanding such requirements provides insight into the nature of open source projects. Unfortunately, manual analysis of natural language requirements is time-consuming, and for large projects, error prone. Automated analysis of natural language requirements, even partial, will be of great benefit. Toward that end, we describe the design and validation of an automated natural language requirements classifier for open source projects. We compare two strategies for recognizing requirements in open forums of software features. Our results suggest that classifying text at the forum post-aggregation and sentence aggregation levels may be effective. Our results suggest that it can reduce the effort required to analyze requirements of open source projects.
Software vulnerabilities have become a serious concern because unpatched software runs the risk of being exploited by hackers. There is a need for software vendors to make software patches available in a timely manner for vulnerabilities in their products. We develop a survival analysis model of software vendors' patch release behavior and test it using a data set compiled from the National Vulnerability Database, United States Computer Emergency Readiness Team, and vendor Web sites. This model helps to understand how factors specific to vulnerabilities, patches, software vendors, and software affect the patch release behavior of software vendors based on their cost structure. This study also analyzes the impact of the presence of multiple vendors and type of vendor on the patch release behavior of software vendors. Our results indicate that vulnerabilities with high confidentiality impact or high integrity impact are patched faster than vulnerabilities with high availability impact. Interesting differences in the patch release behavior of software vendors based on software type (new release versus update) and type of vendor (open source versus proprietary) are found. Our results illustrate that when there are legislative pressures, vendors react faster in patching vulnerabilities. Thus, appropriate regulations can be an important policy tool to influence vendor behavior toward socially desirable security outcomes.
This paper investigates how information technology (IT) facilitates a firm's customer agility and, in turn, competitive activity. Customer agility captures the extent to which a firm is able to sense and respond quickly to customer-based opportunities for innovation and competitive action. Drawing from the dynamic capability and IT business value research streams, we propose that IT plays an important role in facilitating a knowledge creating synergy derived from the interaction between a firm's Web-based customer infrastructure and its analytical ability. This will enhance the firm's ability to sense customer-based opportunities. IT also plays an important role in process enhancing synergy obtained from the interaction between a firm's coordination efforts and its level of information systems integration, which facilitates the firm's ability to respond to those opportunities. We also leverage the competitive dynamics and strategic alignment literature to propose that the alignment between customer-sensing capability and customer-responding capability will impact the firm's competitive activity. We test our model with a two-stage research design in which we survey marketing executives of high-tech firms. Our results show that a Web-based customer infrastructure facilitates a firm's customer-sensing capability; furthermore, analytical ability positively moderates this relationship. We also find that internal systems integration positively moderates the relationship between interfunctional coordination and a firm's customer-responding capability. Finally, our results show that agility alignment affects the efficacy of a firm's competitive actions. In particular, action efficacy is higher when sensing and responding capabilities are both high.
Transactive memory is an effective mechanism for locating and coordinating expertise in small groups and has been shown to hold numerous benefits for groups and organizations. To extend transactive memory beyond the scope of small groups, researchers have proposed the use of information technology (IT). This paper provides an integrated discussion of our knowledge from three studies concerning IT support in transactive memory in organizations. Focusing on meta-memory, which is at the heart of transactive memory systems, we examine what meta-memory is maintained by members of transactive memory systems, whether providing this meta-memory in a technology-mediated environment can lead to transactive memory development, whether IT can realistically provide this meta-memory, and whether different requirements exist for different users and in different stages of transactive memory development. We discuss the implications of these studies to both research and practice.
Business-to-business interactions are increasingly conducted through interorganizational coordination hubs, in which standardized information technology-based platforms provide data and business process interoperability for interactions among the organizations in particular industrial communities. Because the governance of interorganizational arrangements is believed to affect their efficiency and effectiveness, this paper explores how and why interorganizational coordination hubs are governed. Analysis of relevant prior theory and case examples shows that coordination hub governance is designed to balance the sometimes conflicting needs for capital to invest in new technology, for participation of industry members, and for the protection of data resources. Findings suggest that the governance of interorganizational coordination hubs is not the starkly categorical choice between collective (member-owned) and investor-owned forms as suggested by prior theory. Instead, many hybrid arrangements are observed in the five examined cases. Future theoretical development and empirical research are needed to understand the increasingly important phenomenon of coordination hub governance.
Research shows that under certain conditions, groups using collaboration technologies such as group support systems (GSS) can gain substantial improvements in the effectiveness and efficiency of their work processes. GSS, however, have been slow to develop self-sustaining communities of users in the workplace. Organizations that use collaboration technology may require two kinds of support: process support and technology support. Both types of support involve (1) design tasks (e.g., designing a work process and designing the technology to support the process), (2) application tasks (to apply the process and to use the technology), and (3) management tasks (to monitor and control the process and to oversee the maintenance of the technology). This paper explores how these tasks and associated roles can be anchored in organizations, and the relationship of task allocation patterns to the sustained use of collaboration technology in organizations.
The majority of studies of software development processes explore initial development rather than ongoing software maintenance, yet the majority of the systems development budget in many organizations is devoted to maintenance. Software maintenance projects differ significantly from original development projects, indicating a need for more research specifically concerning maintenance processes. This study uses a grounded theory research method to explore how information technology professionals define and select a methodology to maintain existing software. We found that in-use maintenance methodologies are composed of components from multiple formal methodologies. We developed a factor model describing how these components are chosen. The findings contribute to a better understanding of how standard methodologies are applied in software practice and the critical factors used by professionals when choosing an appropriate methodology for software maintenance activities. This research underscores the need for incorporating the full software life cycle in information systems development research and education.
Effective search support is an important tool for helping individuals deal with the problem of information overload. This is particularly true in the field of nanotechnology, where information from patents, grants, and research papers is growing rapidly. Guided by cognitive fit and cognitive load theories, we develop an advanced Web-based system, Nano Mapper, to support users' search and analysis of nanotechnology developments. We perform controlled experiments to evaluate the functions of Nano Mapper. We examine users' search effectiveness, efficiency, and evaluations of system usefulness, ease of use, and satisfaction. Our results demonstrate that Nano Mapper enables more effective and efficient searching, and users consider it to be more useful and easier to use than the benchmark systems. Users are also more satisfied with Nano Mapper and have higher intention to use it in the future. User evaluations of the analysis functions are equally positive.
Decision optimization is widely used in many decision guidance and support systems (DGSS) to support business decisions such as procurement, scheduling, and planning. In spite of rapid changes in users' requirements, the implementation of DGSS is typically rigid, expensive, and not easily extensible, which is in stark contrast to the agile implementation of management information systems (MIS) based on the database management systems (DBMS) and SQL technologies. This paper focuses on the Decision Guidance Query Language (DGQL) designed to (re-)use SQL programs for decision optimization with the goals of making DGSS implementation agile and intuitive and leveraging existing investment in SQL-implemented MIS. The paper addresses two related technical issues with DGQL: (1) how to annotate existing queries to precisely express the optimization semantics, and (2) how to translate the annotated queries into equivalent mathematical programming formulations that can be solved efficiently.
Electronic auctions are increasingly being used to facilitate the procurement of goods and services in organizations. Multiattribute auctions, which allow bids on multiple dimensions of the product and not just price, are information technology-enabled sourcing mechanisms that can increase the efficiency of procurement for configurable goods and services compared to price-only auctions. Given the strategic nature of procurement auctions, the amount of information concerning the buyer's preferences that is disclosed to the suppliers has implications on the profits of the buyer and the suppliers and, consequently, on the long-term relationship between them. This study explores novel feedback schemes for multisourcing multiattribute auctions that require limited exchange of strategic information between the buyer and the suppliers. To study the impact of feedback on the outcomes and dynamics of the auctions, we conduct laboratory experiments wherein we analyze bidder behavior and economic outcomes under three different treatment conditions with different types of information feedback. Our results indicate that, in contrast to winner-take-all multiattribute auctions, multisourcing multiattribute auctions, with potentially multiple winners, allow bidders (i.e., suppliers) to extract more profit when greater transparency in terms of provisional allocations and prices is provided. We develop several insights for mechanism designers toward developing sustainable procurement auctions that efficiently allocate multiple units of an asset with multiple negotiable attributes among multiple suppliers.
Internet-based social virtual world (SVW) services have aroused extensive interest among academicians and practitioners. The success of SVW services depends heavily on customers' continuance usage, a topic not yet adequately investigated in information systems research. It is unclear to what extent, and how, the existing theories can be extended to explain the continuance usage of such services. In consideration of the distinctive features of these services, this study adapts the dedication-constraint framework of commitment and develops a model of SVW continuance, which is assessed empirically using data collected from 438 experienced users of Second Life, a typical SVW service. Results indicate that SVW customers' continuance intention is jointly determined by two mechanisms: affective commitment (being attracted to) and calculative commitment (being locked in), with the former playing a more central role. Perceived utilitarian value, hedonic value, and relational capital promote affective commitment directly and indirectly through satisfaction, while service-specific investments in personalization and relational capital increase calculative commitment. Theoretical and practical implications and future research directions are subsequently discussed.
This paper focuses on employees' e-learning processes during online job training. A new categorization of self-regulated learning strategies, that is, personal versus social learning strategies, is proposed, and measurement scales are developed. The new measures were tested using data collected from employees in a large company. Our approach provides context-relevant insights into online training providers and employees themselves. The results suggest that learners adopt different self-regulated learning strategies resulting in different e-learning outcomes. Furthermore, the use of self-regulated learning strategies is influenced by individual factors such as virtual competence and goal orientation, and job and contextual factors such as intellectual demand and cooperative norms. The findings can (1) help e-learners obtain better learning outcomes through their active use of varied learning strategies, (2) provide useful information for organizations that are currently using or plan to use e-learning for training, and (3) inform software designers to integrate self-regulated learning strategy support in e-learning system design and development.
Recent research emphasizing the need for more business knowledge in information technology (IT) units and more technical knowledge in line functions largely overlooks the question of when maintaining either form of such peripheral knowledge-a costly endeavor-is valuable. Further application and process novelty are increasingly unavoidable in systems development projects but remain largely overlooked in theory. It is plausible that one type of peripheral knowledge is valuable under one type of novelty but not the other. I develop the idea that discriminating alignment between project novelty and peripheral knowledge is needed for them to enhance systems development performance. Thus, the valuable type of peripheral knowledge depends on whether a project involves novelty in the project concept or in its development processes. Further, we lack an explanation for how such discriminating alignment translates into improved project performance. ;I develop and test a middle-range theory built around two ideas to address these gaps. First, alignment between project novelty and peripheral knowledge must be discriminating to enhance systems development performance. Second, such discriminating alignment accelerates design convergence, which in turn enhances systems development performance. Tests using data from 159 projects support the proposed ideas. The primary contribution of this paper is therefore explaining when and how alignment between project novelty and peripheral knowledge in IT and client departments enhances systems development performance. The key implication is that greater application domain knowledge in the IT unit (technical knowledge in the client department) enhances performance in projects involving greater application novelty (process novelty).
Although control theory has often been invoked to explain the coordination between client and vendor for information systems development (ISD), insights into its moderating effects for explicating ISD contract performance, especially in the offshore context, is rather limited. Such insights would en able better understanding of variables that have complementary or substitutive effects on performance. Further, the control literature talks about different control modes (e.g., formal and informal control modes classified as behavior, outcome, clan, and self-control modes) without adequately distinguishing among the different control mechanisms enacting each of the control modes. In this research, by explicitly classifying the distinctions that exist within each of the control modes, we uncover the key role played by mechanistic governance in outsourced ISD. Grounding our arguments in the information requirement for performance evaluation, the study theorizes the moderating influence of mechanistic governance on the relationships of contract specificity and relational governance with ISD quality and cost performance. We test the theorized model in a field study comprising 160 offshore ISD projects executed by Indian vendors. Our results establish the significant complementary role of mechanistic governance on the relationships of contract specificity with both cost and quality performance variables. Further, mechanistic governance substitutes the impact of relational governance on cost performance. Thus, the study theoretically as well as empirically establishes the need for conceptualizing mechanistic governance as a viable and significant governance mechanism for offshore ISD contracts. The study also teases out the distinctions between the two prime contract types in vogue for managing offshore ISD contracts, namely, fixed price and time and materials contracts. The study thus contributes not only to control theory but also to the stream of literature examining offshore ISD contracts. Further, the study provides insights to managers on having well-specified contracts and acknowledging the role of mechanistic governance for better performance.
We examine the effect that investments in information technology (IT) have on downside risk profiles of companies that made public announcements of their investments in technology. Given the limitations of financial and decision theory perspectives on risk, we adopt the strategic management perspective that stresses downside risk as an important alternative measure of firm performance. We examine whether different types of IT investments have a differential impact on firm downside risk. Drawing on the resource-based view of the firm and the real options perspective, we find evidence that IT investments and their timing influence organizational downside risk. Transformational and informational IT investments lead to a reduction in downside risk only if they lead to strategic IT investments in the industry. For competitive necessities such as IT investments that automate business functions, a reduction in downside risk is realized by investing in parity with industry participants. Our study contributes to the literature by offering an alternative perspective on the benefits of IT investments, particularly where no apparent incremental financial results may be evident. It also generates insights on IT investment strategies that may help firms keep up with or stay ahead of the competition.
Given the pervasive use of teams in organizations coupled with high levels of investment in collaboration technology, there is increasing interest in identifying factors that affect the exploration and use of a broader scope of system features so that firms can benefit from the use of such technology. Prior research has called for a deeper understanding of how managers can encourage greater innovation with technology in the workplace. Drawing on the team climate and technology use literatures, we identify team learning climate and team empowerment climate as key factors that affect employees' propensity to explore a new system's features. We develop and test our multilevel model on team climate, team technology exploration, and team technology use in a field study involving 268 employees embedded in 56 work teams. Three main findings come out of this research. First, the results reveal that the two types of team climate differ in their cross-level effects on individual intention to explore, such that team learning climate promotes greater intention to explore, whereas team empowerment climate reduces employees' intention to explore the technology. In addition, we find that team learning climate and team empowerment climate interact in shaping individual intention to explore, such that the presence of a strong learning climate is more effective in promoting intention to explore when teams also have a strong empowerment climate. Second, the findings show that men and women are affected differently by team climate. We find that for men, team empowerment climate has no influence on intention to explore, whereas for women there is a significant negative cross-level effect. Finally, we find that intention to explore has a positive effect on usage scope, suggesting an important link between team climate, individual cognition, and the scope of features used by employees in team settings. Taken together, the model and results highlight the important role of team climate and gender and the interplay between them as drivers of technology feature exploration. Our findings, especially those related to team empowerment climate, are counterintuitive when compared to prior literature and offer useful insights for managers. On the one hand, managers should consider leveraging team learning climate to intrinsically stimulate employees to engage in exploration of technology. On the other hand, managers should be cautious and guard against saddling employees with too many additional responsibilities during the stages of exploration and experimentation with system features. It is possible that through an expanded set of responsibilities and expectations fostered by team empowerment climate, employees may be experiencing work overload, thus reducing their likelihood of exploring a broader set of technology features. Managers should be especially attentive to this based on the gender composition of their teams.
Designing business models that take into consideration the role of advertising support is critical to the success of online services. In this paper, we address the challenges of these business model strategies and compare different ad revenue models. We use game theory to model vertical differentiation in both monopoly and duopoly settings, in which online service providers may offer an ad-free service, an ad-supported service, or a combination of these services. Offering both ad-free and ad-supported services is the optimal strategy for a monopolist because ad revenues compensate for the cannibalistic effect of vertical differentiation. In a duopoly equilibrium, exactly one firm offers both services when the ad revenue rate is sufficiently high. Furthermore, we find that a higher ad revenue rate may lead to lower service prices. Consistently across both monopoly and duopoly settings, such price reductions are more severe in the cost-per-thousand-impressions model than in the cost-per-click model. Our findings emphasize the role of advertising revenues in vertical differentiation and offer strategic guidance for monetizing online services.
Software project escalation is a costly problem that leads to significant financial losses. Prior research suggests that setting a publicly announced limit on resources can make individuals less willing to escalate their commitment to a failing course of action. However, the relationship between initial budget and schedule goals and software project escalation remains unexplored. Drawing on goal setting theory as well as sunk cost and mental budgeting perspectives, we explore the effect of goal difficulty and goal specificity on software project escalation. The findings from a laboratory experiment with 349 information technology professionals suggest that both very difficult and very specific goals for budget and schedule can limit software project escalation. Further, the level of commitment to a budget and schedule goal directly affects software project escalation and also interacts with goal difficulty and goal specificity to affect software project escalation. This study makes a theoretical contribution to the existing body of knowledge on software project management by establishing a connection between goal setting theory and software project escalation. The study also contributes to practice by highlighting the potential negative consequences that can result from the nature of initial budget and schedule goals that are established at the outset of a project.
Because of the unbalanced class and skewed profit distribution in customer purchase data, the unknown and variant costs of false negative errors are a common problem for predicting the high-value customers in marketing operations. Incorporating cost-sensitive learning into forecasting models can improve the return on investment under resource constraint. This study proposes a cost-sensitive learning algorithm via priority sampling that gives greater weight to the high-value customers. We apply the method to three data sets and compare its performance with that of competing solutions. The results suggest that priority sampling compares favorably with the alternative methods in augmenting profitability. The learning algorithm can be implemented in decision support systems to assist marketing operations and to strengthen the strategic competitiveness of organizations.
Despite the importance of online product recommendations (OPRs) in e-commerce transactions, there is still little understanding about how different recommendation sources affect consumers' beliefs and behavior, and whether these effects are additive, complementary, or rivals for different types of products. This study investigates the differential effects of provider recommendations (PRs) and consumer reviews (CRs) on the instrumental, affective, and trusting dimensions of consumer beliefs and shows how these beliefs ultimately influence continued OPR usage and product purchase intentions. This study tests a conceptual model linking PRs and CRs to four consumer beliefs (perceived usefulness, perceived ease of use, perceived affective quality, and trust) in two different product settings (search products versus experience products). Results of an experimental study show that users of PRs express significantly higher perceived usefulness and perceived ease of use than users of CRs, while users of CRs express higher trusting beliefs and perceived affective quality than users of PRs, resulting in different effect mechanisms toward OPR reuse and purchase intentions in e-commerce transactions. Further, CRs were found to elicit higher perceived usefulness, trusting beliefs, and perceived affective quality for experience goods, while PRs were found to unfold higher effects on all of these variables for search goods.
We study the effect of collaboration network structure on the contribution behavior of participating editors in Wikipedia. Collaboration in Wikipedia is organized around articles, and any two editors co-editing an article have a collaborative relationship. Based on the economic theories about network games and social role theory, we propose that an editor's position in the collaboration network influences the editor's decisions about her total contribution as well as the allocation of her efforts. By leveraging panel data collected from the Chinese language version of Wikipedia and a natural experiment resulting from blocking it in mainland China, we find strong support for the proposed effect of network position on contribution behavior. Our analysis further reveals that different aspects of an individual's network position have distinct implications. This research enhances our understanding about how collaboration network structure shapes individual behavior in online mass collaboration platforms.
This paper examines two ways to create business value of information technology (BVIT): resource structuring and capability building. We develop a research model positing that IT resources and IT capabilities enhance a firm's performance by providing support to its competitive strategies and core competencies, and the strengths of these supports vary in accord with environmental dynamism. The model is empirically tested using data collected from 296 firms in China. It is found that IT resources generate more business effects in stable environments than in dynamic environments, while IT capabilities generate more business effects in dynamic environments than in stable environments. The results suggest that the BVIT creation mechanism in stable environments is primarily resource structuring while the mechanism in dynamic environments is primarily capability building.
This study examines the incentives for content contribution in social media. We propose that exposure and reputation are the major incentives for contributors. Besides, as more and more social media Web sites offer advertising-revenue sharing with some of their contributors, shared revenue provides an extra incentive for contributors who have joined revenue-sharing programs. We develop a dynamic structural model to identify a contributor's underlying utility function from observed contribution behavior. We recognize the dynamic nature of the content-contribution decision-that contributors are forward-looking, anticipating how their decisions affect future rewards. Using data collected from YouTube, we show that content contribution is driven by a contributor's desire for exposure, revenue sharing, and reputation and that the contributor makes decisions dynamically.
We examined 335 business process outsourcing (BPO) ventures to understand the effect of contractual and relational governance factors on BPO satisfaction from the client's perspective. While both contractual and relational factors explain significant variance in BPO satisfaction, relational factors dominate. By examining interactions between key contractual and relational mechanisms, we found that elements of the two governance approaches operate as substitutes with respect to BPO satisfaction. Specifically, the relational mechanism, trust, was found to substitute for contractually specified activity expectations, goal expectations, and contractual flexibility. Similarly, the relational mechanism, information exchange, was found to substitute for contractually specified activity expectations and goal expectations. Finally, the relational mechanism, conflict resolution, was found to substitute for contractually specified goal expectations. Our results can be applied to more effectively realize controls in outsourcing contexts and to design governance systems that integrate contractual and relational governance mechanisms based on the characteristics of client-vendor relationships.
Web site navigability refers to the degree to which a visitor can follow a Web site's hyperlink structure to successfully find information with efficiency and ease. In this study, we take a data-driven approach to measure Web site navigability using Web data readily available in organizations. Guided by information foraging and information-processing theories, we identify fundamental navigability dimensions that should be emphasized in metric development. Accordingly, we propose three data-driven metrics-namely, power, efficiency, and directness-that consider Web structure, usage, and content data to measure a Web site's navigability. We also develop a Web mining-based method that processes Web data to enable the calculation of the proposed metrics. We further implement a prototype system based on the Web mining-based method and use it to assess the navigability of two sizable, real-world Web sites with the metrics. To examine the analysis results by the metrics, we perform an evaluation study that involves these two sites and 248 voluntary participants. The evaluation results show that user performance and assessments are consistent with the analysis results revealed by our metrics. Our study demonstrates the viability and practical value of data-driven metrics for measuring Web site navigability, which can be used for evaluative, diagnostic, or predictive purposes.
The market for security software has witnessed an unprecedented growth in recent years. A closer examination of this market reveals certain idiosyncrasies that are not observed in a traditional market. For example, it is a highly competitive market with over 80 vendors. Yet the market coverage is relatively low. Prior research has not attempted to explain what makes this market so different. In this paper, we develop an economic model to find possible answers to this question. Our model uses existing classification of different types of attacks and models their resulting network effects. We find that the negative network effect from indirect attacks, which is further enhanced by value-based targeted attacks, provides a possible explanation for the unique structure of this market. Overall, our results highlight the unique nature of the security software market, furnish rigorous arguments for several counterintuitive observations in the real world, and provide managerial insights for vendors on market competition.
In today's dynamic business environment, companies are under tremendous pressure to become more innovative and maintain a steady stream of ideas that can lead to new and improved products and services. Companies have begun to explore the possibility of capturing consumers' collective intelligence by establishing firm-sponsored online brainstorming sites where individuals can share their ideas and offer comments on the ideas contributed by others. We term these sites Company-Sponsored Online Co-Creation Brainstorming (COCB). The value of this open and voluntary co-creation depends largely on members' contribution levels, the quality of the contributions, and sustained participation. In this paper, utilizing Zwass's taxonomy of co-creation value as a base, we structure a taxonomic framework of COCBs and an accompanying basic model of COCBs. We then present a series of hypotheses concerning the relationships between the model's various factors and specific COCB activities. We validate the model using empirical data collected over two and a half years, starting from the initiation of a pioneering company-sponsored online brainstorming site. Our analyses demonstrate that the level of peer feedback and the responsiveness (speed) of sponsor company feedback have significant influences on both members' contribution levels and duration of active participation. The sponsoring company's feedback, however, seems to influence only the quality of member's contribution level. On the practical side, the outcomes suggest that sponsoring companies should develop efficient processes for reviewing and responding to submitted ideas. Regarding theory, our findings provide an initial piece of contextualized research that offers implications for theory building in the COCB context, most notably the identification of key relationships between feedback (both peer and company) and participant activity levels and duration of participation.
Web-based portals extend many convenient and collaborative capabilities to consumers and are being adopted by small firms with ever greater frequency, especially in the context of health care. The early adoption of patient portals by ambulatory-care clinics (outpatient health providers) presents a unique opportunity to more fully understand the characteristics of supply-side adopters in a context where firms (ambulatory-care clinics) are extending digital services to consumers (patients). Using diffusion of innovations literature and contingency theory as the theoretical base, we expand upon the firm characteristics traditionally considered to be predictors of adoption (e.g., firm size, slack resources, competition, capabilities, and management support) and examine how demand contingencies, service contingencies, and learning externality contingencies affect patient portal adoption by ambulatory-care clinics in the United States. We find that early adopters often have a structure in place that provides support for innovations (e.g., part of integrated delivery systems), as would be predicted by diffusion of innovation theory. We also find, though, that service contingencies associated with continuity of care, learning externality contingencies associated with local influences, and select demand contingencies associated with the local market significantly influence patient portal adoption decisions. Our findings suggest that the adoption and diffusion of patient portals may be affected by more than traditionally considered dominant firm characteristics and provide insights into how such customer-facing systems may be affected by contingent factors.
Despite the fact that about 90 percent of information transactions in hospitals are communications between patients, doctors, nurses, and other staff, little research has addressed the role that information technology (IT) plays in improving the efficiency and effectiveness of these communications-based transactions. Addressing this research gap is important considering that a substantial number of adverse hospital events stem from communication failures. Furthermore, effective communication is a major driver of patient satisfaction in hospitals. Using a structure-process-outcome (SPO) framework and guided by the strategic role of IT literature, we develop a model that includes structure, operationalized as organizational characteristics and two different categories of IT;  process, two different communication-based processes; and  outcomes, quantified as case-mix adjusted mortality, patient loyalty, and patient ratings. Specifically, we hypothesize that a subset of clinical IT (cardiology IT) will affect technical protocols of patient care, which in turn affects mortality, while administrative IT will affect interpersonal patient care, which relates to patient loyalty and ratings. Thus, IT can serve as a double-edged sword affecting both technical and interpersonal processes of care, but possibly independently and differentially. We test our hypotheses on 2,179 hospitals using data collected and matched from three different sources. Our findings suggest that different types of IT differentially affect hospital processes and these same processes influence performance metrics such as mortality and patient satisfaction. For example, cardiology IT has a greater effect on objective patient health status through improvements in the technical protocols of care. Surprisingly, administrative IT was shown to adversely affect interpersonal care processes. It could be true that the IT is intrusive and interferes in the doctor-patient relationship; however, a post hoc analysis suggests the possibility of curvilinear impacts. Thus, managers should recognize that over- and underinvestment in IT can potentially have negative effects on performance and these results vary by IT type. Both technical and interpersonal processes yielded significant relationships to their respective outcomes and some cross-outcome effects were found, further suggesting that the mediating role of processes is an important link between IT and value.
Technology is an important factor underlying the value propositions of intermediary platforms in two-sided markets. Here, we address two key questions related to the effect of technology in platform markets. First, how does technology asymmetry affect competition between platforms? Second, how does it affect the incentives for platforms to collaborate? Using a game-theoretic model of a two-sided market where technology strongly influences network value, we show that small asymmetries in platform technologies can translate into large differences in their profitability. We find that technology improvements by the inferior platform do not significantly increase its profits, but can reduce opportunities for fruitful cooperation, since collaboration is less likely in markets with closely matched competitors. We also show that collaboration is most profitable when it takes the form of direct network interconnection. Interestingly, collaboration may provide incentives for a dominant platform to accommodate entry, where it would not otherwise do so.
Advances in information technologies enable firms to collect detailed consumer data and target individual consumers with tailored ads. Consumer data are among the most valuable assets that firms own. An interesting phenomenon is that competing firms often trade their consumer data with each other. Based on a common-value all-pay auction framework, this paper studies the advertising competition between two firms that target the same consumer but are asymmetrically informed about the consumer value. We characterize firms' equilibrium competition strategies. The results show that better consumer information does not help the better-informed firm save the advertising expenditure but does enable it to reap a higher expected profit in competition. Sharing individual-level consumer data may soften the competition even though firms compete head-to-head for the same consumer. We also find that the better-informed firm may sell its data to its competitor but never voluntarily shares it with its competitor.
With the growth of e-commerce and e-markets, there is an increasing potential for the use of software agents to negotiate business tasks with human negotiators. Guided by design science methodology, this research prescribes and validates a win-win seeking negotiation agent using strategies of simultaneous-equivalent offers and delayed acceptance and compares their effects against the use of conventional sequential-single offer and immediate acceptance strategies. To evaluate the alternate strategies, a negotiation agent system was implemented and an experiment was conducted in which 110 agent-human dyads negotiated over a four-issue online purchase task. Our results indicate that the proposed agent strategies can enhance the economic performance of the negotiated outcome (counterpart agreement ratio, individual utility, joint utility, and the distance to Pareto-efficient frontier) and maintain the human counterparts' positive perceptions toward the outcome and the agent. The findings confirm the efficacy of the proposed design and showcase an innovative system to facilitate e-commerce transactions.
Team network structure has been shown to be an important determinant of both team and individual performance outcomes, yet few studies have investigated the relationship between team network structure and technology usage behaviors. Drawing from social network and technology use literature, we examine how the structure of a team's advice-seeking network affects individual use of a newly implemented information technology. We develop cross-level hypotheses related to the effects of the structure of mutually interconnected ties within the team (i.e., internal closure) as well as the structure of nonredundant ties outside the team boundaries (i.e., external bridging). The hypotheses are tested in a field study of 265 employees working in 44 teams in a large financial services institution. Results show that internal closure has a U-shaped effect on individual use such that individual usage of the system is higher when the number of internaladvice-seeking ties within the team is low or high, suggesting that medium levels of internal closure are the least desirable network configurations because in such instances teams neither realize the benefits of high closure information sharing nor are they able to avoid in-group biases associated with low closure conditions. Our results also reveal that in addition to having a direct positive effect on individual use, external bridging interacts with internal closure in a complex manner. The U-shaped effect of closure is dominant when bridging is high but assumes an inverted U-shaped pattern when bridging is low. Several implications for managers follow from these findings. First, in order to increase usage of technology, in teams characterized by low internal closure, managers should encourage the development of ties across team boundaries. Second, managers should maximize within-team interconnections in order to facilitate the circulation of external knowledge within team boundaries. Finally, managers should be aware that maximizing internal closure by facilitating interconnections among team members could be dangerous if not accompanied by mechanisms for external bridging.
Sociomateriality (or sociomaterialism) allows us to approach the information technology (IT) capability research from an angle that has been rarely visited by information systems scholars. While relevant studies presume that humans and materials are distinct and largely independent, sociomateriality emphasizes agency that represents the relational, emergent, and shifting capacity realized through the association of actors (both humans and materials). The objective of this paper is to explore the value of conducting IT capability research through the theoretical lens of sociomaterialism. For this, we expand the imbrication metaphor introduced in an early study to explain the formation and advancement of a firm's IT capability from the sociomaterial perspective. Then, the key building blocks of IT capability of an organization are conceptualized based on the combination of existing studies and the expanded imbrication metaphor. Lastly, the effectiveness of formulating IT capability as a third-order construct that substantiates the entanglement concept of sociomaterialism is examined in comparison with that of traditional modeling approaches. We confirm the value of sociomaterialism in conceptualizing IT capability and subsequently in unraveling the true contribution of IT capability toward strengthening business performance. The findings also have practical implications in which IT capability is a function of IT management capability as well as IT personnel capability and IT infrastructure capability.
The adoption of an organization-wide system, such as an enterprise system (ES), has often been mandated by organizational management, which may not necessarily motivate users to proactively explore the system's features and subsequently apply pertinent features that best support their job tasks. Anchoring on self-determination theory, this research investigates the antecedents and consequences of users' intrinsic motivation to explore ES features. We propose two organizational levers (i.e., autonomous job design and socialization tactics) that the management could exercise to trigger intrinsic motivation, thereby leading to improved ES feature exploration. Intrinsic motivation is manifested by hedonic motivation and normative motivation, whereas ES feature exploration is conceptualized as a dual-dimensional outcome reflected by cognitive behavior (exploratory usage) and positive affect (exploration satisfaction). Through a two-stage survey of 127 organizational users in China, we find general support for our research model. We further observe significant moderating effects of prevention focus on the association between organizational levers and intrinsic motivations. Beyond demonstrating how organizational users respond to different organizational levers, this research examines a broader, enduring challenge, which is to determine how organizational users can be induced to be intrinsically inspired to innovatively harness implemented information systems.
The rapid growth of computer networks has led to a proliferation of information security standards. To meet these security standards, some organizations outsource security protection to a managed security service provider (MSSP). However, this may give rise to system interdependency risks. This paper analyzes how such system interdependency risks interact with a mandatory security requirement to affect the equilibrium behaviors of an MSSP and its clients. We show that a mandatory security requirement will increase the MSSP's effort and motivate it to serve more clients. Although more clients can benefit from the MSSP's protection, they are also subjected to greater system interdependency risks. Social welfare will decrease if the mandatory security requirement is high, and imposing verifiability may exacerbate social welfare losses. Our results imply that recent initiatives such as issuing certification to enforce computer security protection, or encouraging auditing of managed security services, may not be advisable.
Current research argues that the most prominent models of group development (the linear stage model and the punctuated equilibrium model) are simply different lenses for studying the same phenomenon. We argue that the two models are distinct (groups do not simultaneously follow both models) and that the key to understanding their use lies in routines. We studied six newly formed groups whose members came from the same organization that worked on similar projects over a seven-week period. Three groups worked nonmediated and three groups used a collaboration technology that was new to them. The three nonmediated groups followed the punctuated equilibrium model and the three collaboration technology groups followed the stage model. We argue that groups that enact the shared routines common in their organizations will experience a different group development path than those groups whose shared routines are disrupted and which must adapt to a new technology. When group members enact shared routines (which they may share due to having a common organizational culture), they can quickly begin work, and group development follows the punctuated equilibrium model. When groups cannot enact shared routines, they must first negotiate how they will work before work can begin, so group development follows the stage model. Thus, the introduction of new collaboration technology (or any new technology or work process) influences how group development occurs.
For an organization to gain maximum benefits from a new information system (IS), individual users in the organization must use it effectively and extensively. To do so, users need to overcome many problems associated with their system use in order to integrate the new IS into their work routines. Much remains to be learned about the types of problems that users encounter in using the new system, in particular, the root causes of system use problems and how they relate to and co-evolve with the problems over time. In this study, we seek to develop a comprehensive and dynamic view of system use problems in organizations. Using a combined method of revealed causal mapping and in-depth network analysis, we analyze nine-month archival data on user-reported problems with a new business intelligence application in a large organization. Our data analysis revealed seven emergent constructs of system use problems and causes, including reporting, data, workflow, role authorization, users' lack of knowledge, system error, and user-system interaction. The seven constructs were found to interact differentially across two usage phases (initial versus continued) and between two types of users (regular versus power user). This study contributes to advancing our theoretical understanding of postadoptive IS use by focusing on its problematic aspect. This study also suggests useful methods for organizations to effectively monitor users' system use problems over time and thus guides organizations to effectively target mechanisms to promote the use of new technologies.
Companies' information security efforts are often threatened by employee negligence and insider breach. To deal with these insider issues, this study draws on the compliance theory and the general deterrence theory to propose a research model in which the relations among coercive control, which has been advocated by scholars and widely practiced by companies; remunerative control, which is generally missing in both research and practice; and certainty of control are studied. A Web-based field experiment involving real-world employees in their natural settings was used to empirically test the model. While lending further support to the general deterrence theory, our findings highlight that reward enforcement, a remunerative control mechanism in the information systems security context, could be an alternative for organizations where sanctions do not successfully prevent violation. The significant interactions between punishment and reward found in the study further indicate a need for a more comprehensive enforcement system that should include a reward enforcement scheme through which the organizational moral standards and values are established or reemphasized. The findings of this study can potentially be used to guide the design of more effective security enforcement systems that encompass remunerative control mechanisms.
Originally designed as a tool to alleviate bottlenecks associated with knowledge management, the suitability of wikis for corporate settings has been questioned given the inherent tensions between wiki affordances and the realities of organizational life. Drawing on regulatory focus theory and social cognitive theory, we developed and tested a model of the motivational dynamics underlying corporate wikis. We examined leaders (owners) and users of 187 wiki-based projects within a large multinational firm. Our findings revealed two countervailing motivational forces, one oriented toward accomplishment and achievement (promotion focus) and one oriented toward safety and security (prevention focus), that not only predicted owners' participation but also the overall level of engagement within the wiki groups. Our primary contribution is in showing that, notwithstanding the potential benefits to users, wikis can trigger risk-avoidance motives that potentially impede engagement. Practically, our findings call for an alignment between organizational procedures surrounding wiki deployment and the technology's affordances.
With the proliferation of available electronic service channels for information systems (IS) users such as mobile or intranet services in companies, service interactions between IS users and IS professionals have become an increasingly important factor for organizational business-IT alignment. Despite the increasing relevance of such interactions, the implications of agreement or disagreement on the fulfillment of critical service quality factors for successful alignment and higher user satisfaction are far from being well understood. While prior research has extensively studied the question of matching different viewpoints on IS service quality in organizations, little or no attention has been paid to the role of perceptual congruence or incongruence in the dyadic relationship between IS professionals and users in forming user satisfaction with the IS function. Drawing on cognitive dissonance theory, prospect theory, and perceptual congruence research, this study examines survey responses from 169 matching pairs of IS professionals and users in different organizations and explains how perceptual fit patterns affect user satisfaction with the IS function. The paper demonstrates that perceptual congruence can, in and of itself, have an impact on user satisfaction, which goes beyond what was found before. Moreover, the results of the study reveal the relevance of nonlinear and asymmetric effect mechanisms arising from perceptual (in)congruence that may affect user satisfaction. This study extends our theoretical understanding of the role of perceptual alignment or misalignment on IS service quality factors in forming user satisfaction and lays the foundation for further study of the interplay between perceptions in the dyadic relationship between IS professionals and IS users. Managers who seek to encourage particular behaviors by the IS professionals or IS users may use the results of this study to reconcile the often troubled business-IT relationship.
Access policy violations by organizational insiders are a major security concern for organizations because these violations commonly result in fraud, unauthorized disclosure, theft of intellectual property, and other abuses. Given the operational demands of dynamic organizations, current approaches to curbing access policy violations are insufficient. This study presents a new approach for reducing access policy violations, introducing both the theory of accountability and the factorial survey to the information systems field. We identify four system mechanisms that heighten an individual's perception of accountability: identifiability, awareness of logging, awareness of audit, and electronic presence. These accountability mechanisms substantially reduce intentions to commit access policy violations. These results not only point to several avenues for future research on access policy violations but also suggest highly practical design-artifact solutions that can be easily implemented with minimal impact on organizational insiders.
As a new communication paradigm, social media has promoted information dissemination in social networks. Previous research has identified several content-related features as well as user and network characteristics that may drive information diffusion. However, little research has focused on the relationship between emotions and information diffusion in a social media setting. In this paper, we examine whether sentiment occurring in social media content is associated with a user's information sharing behavior. We carry out our research in the context of political communication on Twitter. Based on two data sets of more than 165,000 tweets in total, we find that emotionally charged Twitter messages tend to be retweeted more often and more quickly compared to neutral ones. As a practical implication, companies should pay more attention to the analysis of sentiment related to their brands and products in social media communication as well as in designing advertising content that triggers emotions.
In 1992, DeLone and McLean suggested that the dependent variable for information systems (IS) research is IS Success. Their research resulted in the widely cited DeLone and McLean (D&M) IS Success Model, in which System Quality, Information Quality, Use, User Satisfaction, Individual Impact, and Organizational Impact are distinct, but related dimensions of IS success. Since the original IS Success Model was published, research has developed a better understanding of IS success. Meanwhile, comprehensive and integrative research on the variables that influence IS success has been lacking. Therefore, we examine the literature on the independent variables that affect IS success. After examining over 600 articles, we focused our attention on integrating the findings of over 140 studies. In this research, we identify 43 specific variables posited to influence the different dimensions of IS success, and we organize these success factors into five categories based on the Leavitt Diamond of Organizational Change: task characteristics, user characteristics, social characteristics, project characteristics, and organizational characteristics. Next, we identify 15 success factors that have consistently been found to influence IS success: Enjoyment, Trust, User Expectations, Extrinsic Motivation, IT Infrastructure, Task Compatibility, Task Difficulty, Attitudes Toward Technology, Organizational Role, User Involvement, Relationship with Developers, Domain Expert Knowledge, Management Support, Management Processes, and Organizational Competence. Finally, we highlight gaps in our knowledge of success factors and propose a road map for future research.
The provision of services has become an increasingly important component of the economy of industrialized countries and the revenue stream for many traditional product companies. This is especially true for companies that offer information technology (IT) products. This paper examines factors that are associated with the extent to which IT product companies are able to develop service revenue, which we refer to as service expansion of IT product companies. We identify the characteristics of the product portfolio-specifically, the composition and scope of firm offerings among hardware, application software, and infrastructure software-as key to successful service expansion. We also propose that this relationship is moderated by prior performance of the product business and industry characteristics such as concentration and maturity. Data from IT product vendors spanning five years are used to test the proposed relationships. Overall, this research provides a theoretical foundation for understanding service expansion and diversification in the IT industry as well as practical guidance for IT product companies considering expansion to services.
Despite the improving accuracy of agent-based expert systems, human expert users aided by these systems have not improved their accuracy. Self-affirmation theory suggests that human expert users could be experiencing threat, causing them to act defensively and ignore the system's conflicting recommendations. Previous research has demonstrated that affirming an individual in an unrelated area reduces defensiveness and increases objectivity to conflicting information. Using an affirmation manipulation prior to a credibility assessment task, this study investigated if experts are threatened by counterattitudinal expert system recommendations. For our study, 178 credibility assessment experts from the American Polygraph Association (n = 134) and the European Union's border security agency Frontex (n = 44) interacted with a deception detection expert system to make a deception judgment that was immediately contradicted. Reducing the threat prior to making their judgments did not improve accuracy, but did improve objectivity toward the system. This study demonstrates that human experts are threatened by advanced expert systems that contradict their expertise. As more and more systems increase integration of artificial intelligence and inadvertently assail the expertise and abilities of users, threat and self-evaluative concerns will become an impediment to technology acceptance.
To derive more business value from existing organizational knowledge, many organizations seek to rely on strategically aligned knowledge management systems (KMS). However, as documented in prior studies, they often underestimate the challenges about social interactions and users' perceptions in response to new information systems. Based on an interpretive case study, this paper examines the implementation of a KMS to show how social representations of four groups of users resulted in the misalignment of the KMS with the organizational strategy. The social representation lens allows us to interpret strategic alignment in terms of dynamic processes of anchoring and objectification that aid individuals and groups to make sense of KMS initiatives. The groups studied developed different cognitive views of the KMS that ultimately led to a strategic misalignment. The key implication is that social interactions within and among groups shape KMS alignment with organizational strategy, thus elucidating the nature of system use.
Much of human behavior involves subconscious cognition that can be manipulated through priming-the presentation of a stimulus designed to subconsciously implant a concept in working memory that alters subsequent behavior. Priming is a well-known phenomenon for individual behavior, but we do not know whether priming can be used to influence group behavior. We developed a Web-based computer game that was designed to improve creativity through priming. Participants were exposed to a priming game and then worked as members of a group using electronic brainstorming (EBS) to generate ideas on a creativity task. Our results show that when users played the game, designed to improve performance, their groups generated significantly more ideas that were more creative than when they were exposed to neutral priming. Our findings extend the literature by providing evidence that individual priming substantially affects group idea generation performance. Avenues for future research include designing EBS software that optimizes group ideation through priming, examining the conditions under which priming has the most substantial impact on ideation performance, and examining whether priming can be used to enhance other group processes (e.g., convergence tasks).
The potential benefits of collaboration technologies are typically realized only in groups led by collaboration experts. This raises the facilitator-in-the-box challenge: Can collaboration expertise be packaged with collaboration technology in a form that nonexperts can reuse with no training on either tools or techniques? We address that challenge with process support applications (PSAs). We describe a collaboration support system (CSS) that combines a computer-assisted collaboration engineering platform for creating PSAs with a process support system runtime platform for executing PSAs. We show that the CSS meets its design goals: (1) to reduce development cycles for collaboration systems, (2) to allow nonprogrammers to design and develop PSAs, and (3) to package enough expertise in the tools that nonexperts could execute a well-designed collaborative work process without training.
The interdependency of information security risks often induces firms to invest inefficiently in information technology security management. Cyberinsurance has been proposed as a promising solution to help firms optimize security spending. However, cyberinsurance is ineffective in addressing the investment inefficiency caused by risk interdependency. In this paper, we examine two alternative risk management approaches: risk pooling arrangements (RPAs) and managed security services (MSSs). We show that firms can use an RPA as a complement to cyberinsurance to address the overinvestment issue caused by negative externalities of security investments; however, the adoption of an RPA is not incentive-compatible for firms when the security investments generate positive externalities. We then show that the MSS provider serving multiple firms can internalize the externalities of security investments and mitigate the security investment inefficiency. As a result of risk interdependency, collective outsourcing arises as an equilibrium only when the total number of firms is small.
Firms face significant risk when they adopt digital supply chain systems to transact and coordinate with their partners. Drawn upon modular systems theory, this study proposes that system modularity mitigates the risk of adopting digital supply chain systems and therefore motivates firms to digitize more of their supply chain operations. The study theorizes how the risk-mitigating effect of system modularity can be enhanced by the allocation of decision rights to the IT (information technology) unit. The main logic is that IT managers with more domain IT knowledge can better utilize their knowledge in decision making to achieve effective system modularity. We tested these theoretical propositions using a survey study of Chinese companies and found empirical support. We also found that the allocation of decision rights to the IT unit does not directly mitigate the perceived risk of digital supply chain systems, which highlights the role of decision allocation to the IT unit as a key moderator in risk mitigation. The study generates theoretical and practical implications on how IT governance and system modularity may jointly mitigate risk and foster supply chain digitization.
Does a firm get any extra value from investing resources in sponsoring its own virtual community above and beyond the value that could be created for the firm, indirectly, via customer-initiated communities? If so, what explains the extra value derived from a firm-sponsored virtual community and how might this understanding inform managers about appropriate strategies for leveraging virtual communities as part of a value-creating strategy for the firm? We test two models of virtual community to help shed light on the answers to these questions. We hypothesize that in customer-initiated virtual communities, three attributes of member-generated information (MGI) drive value, while in firm-sponsored virtual communities, a sponsoring firm's efforts, as well as MGI, drive value. Drawing on information search and processing theories, and developing new measures of three attributes of MGI (consensus, consistency, and distinctiveness), we surveyed 465 consumers across numerous communities. We find that value can emerge via both models, but that in a firm-sponsored model, a sponsor's efforts are more powerful than MGI and have a positive, direct effect on the trust-building process. Our results suggest a continuum of value creation whereby firms extract greater value as they migrate toward the firm-sponsored model.
Information systems development (ISD) projects are prone to high levels of failure. One of the major reasons attributed to these failures is the inability to harmonize values held by a diverse set of participants in an environment that is characterized by uncertainty due to changing requirements. In this paper, we focus on a relational approach to achieve congruence between a project manager and a team member with respect to influence tactics. Constructs of perceptual congruence and communication congruence that reflect a level of agreement and degree of shared understanding between the project manager and team members are described. A congruence model is constructed and tied to an intermediate outcome variable of control loss. One hundred and thirteen dyadic pairs of project managers and team members are surveyed in order to test the model. The results indicate that having strong relational equity and common understanding can minimize control loss. It is important to consider the perspectives of both the project manager and a team member while formulating and assessing monitoring strategies to promote the success of an ISD project. Especially, encouraging team members to discuss disagreements constructively can motivate them to perform better and keep things under control. Finally, it is critical to address the performance problems as they occur rather than wait until the completion of the project.
Online whistle-blowing reporting systems (WBRS) have become increasingly prevalent channels for reporting organizational failures. The Sarbanes-Oxley Act and similar international laws now require firms to establish whistle-blowing (WB) procedures and WBRSs, increasing the importance of WB research and applications. Although the literature has addressed conventional WB behavior, it has not explained or measured the use of WBRSs in online contexts that could significantly alter elements of anonymity, trust, and risk for those using such reporting tools. This study proposes the WBRS model (WBRS-M). Using actual working professionals in an online experiment of hypothetical scenarios, we empirically tested the WBRS-M for reporting computer abuse and find that anonymity, trust, and risk are highly salient in the WBRS context. Our findings suggest that we have an improved WB model with increased explanatory power. Organizations can make WB less of a professional taboo by enhancing WBRS users' perceptions of trust and anonymity. We also demonstrate that anonymity means more than the mere lack of identification, which is not as important in this context as other elements of anonymity.
This study investigates the development and sustainability of a firm's information technology (IT) capability reputation from an IT executive's standpoint. Building on institutional theory, we argue that IT executives will try to achieve external legitimacy (i.e., project an image of superior IT capability to external stakeholders) in the hope that the top management team and board members will reciprocate by elevating the internal legitimacy of IT executives. Firms that develop such a culture of reciprocity with their IT executives are more likely to sustain their IT capability reputation. Econometric results based on panel data for 1,326 large U. S. firms from a wide spectrum of industries over a 13-year period (1997-2009) validate these predictions. More specifically, we find that IT executives with greater structural power (e. g., higher job titles) or IT-related expert power (e. g., IT-related education or experience) are more likely to attract public recognition for their firm's IT capability. Firms that build such an IT capability reputation are more likely to promote their IT executives, and IT executives who are promoted are more likely to stay longer with their firms. This continuity in IT strategic leadership is positively associated with the firm's ability to sustain its IT capability reputation. Our findings have important practical implications related to a firm's IT reputation strategy as well as the motivation and career of IT executives. Firms wanting to develop and sustain their IT capability reputation would do well to foster the creation of a cycle of positive reciprocity with their IT executives. IT executives hoping to increase their power within their firm's top management team and improve the legitimacy of the firm's IT organization need to project an image of IT superiority to external stakeholders.
Despite advances in software development practices, organizations struggle to implement methodologies that match the risk in a project environment with needed coordination capabilities. Plan-driven and agile software development methodologies each have strengths and risks. However, most project environments cannot be classified as entirely risky or stable, suggesting the need for hybrid approaches. We leverage a design science approach to implement a novel hybrid methodology based on concepts from the service-oriented paradigm. We motivate the approach using theory on interdependence and coordination, and design the methodology using theory on modularity and service-dominant logic. We also examine the effects of its adoption at a large electrical power company over a three-year period. The results imply that service-oriented theory should be applied to the human processes involved in systems development in order to achieve better fit between project risk, interdependencies, and the selected methodology(ies) in order to improve overall project performance.
The paper empirically examines the effects of social capital of the relationship between the chief information officer (CIO) and top management team (TMT) on organizational value creation based on responses from CIOs and matched TMT respondents from 81 hospitals in the United States. Specifically, we theorize how the three dimensions of social capital-structural, cognitive, and relational social capital-facilitate knowledge exchange and combination between the CIO and TMT resulting in the alignment between the organization's information systems (IS) strategy and business strategy. Results show that IS alignment significantly influences the firm's financial performance and mediates the relationship between CIO-TMT social capital and performance. The findings also indicate that cognitive and relational social capital influence information systems strategic alignment but that structural social capital exerts its influence through its effects on cognitive social capital. Recommendations are provided as to how organizations can develop CIO-TMT structural, cognitive, and relational social capital to positively influence firm performance via IS strategic alignment.
Online reviews play a significant role in forming and shaping perceptions about a product. With the credibility of online reviewers a frequent question, this research investigates how potential buyers assess the credibility of anonymous reviewers. Technology separates the reviewer from the review, and potential buyers are left to rely on characteristics of the review itself to determine the credibility of the reviewer. By extending the language expectancy theory to the online setting, we develop hypotheses about how expectancy violations of lexical complexity, two-sidedness (highlighting positive and negative aspects of a product), and affect intensity influence credibility attributions. We present an experiment in which favorable experimental reviews were generated based on actual reviews for a digital camera. The results indicate that two-sidedness caused a positive expectancy violation resulting in greater credibility attribution. High affect intensity caused a negative expectancy violation resulting in lower credibility attribution. Finally, high reviewer credibility significantly improved perceptions of product quality. Our results demonstrate the importance of expectancies and violations when attributing credibility to anonymous individuals. Even small expectancy violations can meaningfully influence reviewer credibility and perceptions of products.
Previous research has found that information technology (IT) investment is associated with significant productivity gains for developed countries but not for developing countries. Yet developing countries have continued to increase their investment in IT rapidly. Given this apparent disconnect, there is a need for new research to study whether the investment has begun to pay off in greater productivity for developing countries. We analyze new data on IT investment and productivity for 45 countries from 1994 to 2007, and compare the results with earlier research. We find that upper-income developing countries have achieved positive and significant productivity gains from IT investment in the more recent period as they have increased their IT capital stocks and gained experience with the use of IT. We also find that the productivity effects of IT are moderated by country factors, including human resources, openness to foreign investment, and the quality and cost of the telecommunications infrastructure. The academic implication is that the effect of IT on productivity is expanding from the richest countries into a large group of developing countries. The policy implication is that lower-tier developing countries can also expect productivity gains from IT investments, particularly through policies that support IT use, such as greater openness to foreign investment, increased investment in tertiary education, and reduced telecommunications costs.
In recent years, there has been stellar growth of location-based/enabled social networks in which people can check in to physical venues they are visiting and share with friends. In this paper, we hypothesize that the check-ins made by friends help users learn the potential payoff of visiting a venue. We argue that this learning-in-a-network process differs from the classic observational learning model in a subtle yet important way: Rather than from anonymous others, the agents learn from their network friends, about whose tastes in experience goods the agents are better informed. The empirical analyses are conducted on a unique data set in which we observe both the explicit interpersonal relationships and their ensuing check-ins. The key result is that the proportion of checked-in friends is not positively associated with the likelihood of a new visit, rejecting the prediction of the conventional observational learning model. Drawing on the literature in sociology and computer science, we show that weighting the friends' check-ins by a parsimonious proximity measure can yield a more intuitive result than the plain proportion does. Repeated check-ins by friends are found to have a pronounced effect. Our empirical result calls for the revisiting of observational learning in a social network setting. It also suggests that practitioners should incorporate network proximity when designing social recommendation products and conducting promotional campaigns in a social network.
The use of telemedicine to improve patients' health has been evolving rapidly over the past few years. Initially, our clinical research focus was on the development of effective ways for treating chronically ill patients, mostly those suffering from neurological disorders. While we identified the medical benefits of this information technology, there remains a salient strategic question addressing its competitive impact. In this paper, we analyze the impact of introducing telemedicine on the market share of the specialty hospital deploying this technology and on the competing hospitals in the region. Our analytical results prove that, contrary to earlier expectations, the value of telemedicine relative to in-person visits is not always increasing with the distance of the patient from the hospital. This result explains why patients located far from the specialty hospital may not prefer telemedicine care. We prove that telemedicine, unlike numerous other e-commerce applications, does not lead to the winner takes all phenomenon. We found that the advent of telemedicine changes the competitive equilibrium between specialty hospitals and community hospitals. Both hospital types will significantly benefit from delivering complementary care to chronic patients, rather than continuing to compete with each other.
It is becoming increasingly important for firms to know when to take steps to reduce buyers' uncertainty about products and services. This paper focuses on investments that firms can make to reduce buyers' uncertainty about taste-related product attributes. Using an analytical model, we show that firms should disclose more taste-related information when the customer segment they directly target represents a larger share of the overall market. We further show that there are practical ways by which managers can decide if such disclosure investments are financially beneficial to their firms. Specifically, we show that the variance of consumer reviews can guide such decisions. The paper's main contribution to the extant literature is to show that firms must consider the variance, but not the mean, of buyer reviews, to determine the need to invest in reducing consumer uncertainty about taste-related attributes. The papers's findings are managerially important due to the ubiquity of consumer reviews. They are novel because most of the previous literature views the mean of the review as the key indicator. Finally, they are general in their applicability since they are independent of any assumptions about heuristics that buyers may use to ascertain product quality from the reviews of previous buyers.
Consumer buzz in the form of user-generated reviews, recommendations, and blogs signals that consumer attitude and advocacy can influence firm value. Web traffic also affects brand awareness and customer acquisition, and is a predictor of the performance of a firm's stock in the market. The information systems and accounting literature have treated buzz and traffic separately in studying their relationships with firm performance. We consider the interactions between buzz and traffic as well as competitive effects that have been overlooked heretofore. To study the relationship between user-initiated Web activities and firm performance, we collected a unique data set with metrics for consumer buzz, Web traffic, and firm value. We employed a vector autoregression with exogenous variables model that captures the evolution and interdependence between the time series of dependent variables. This model enables us to examine a series of questions that have been raised but not fully explored to date, such as dynamic effects, interaction effects, and market competition effects. Our results support the dynamic relationships of buzz and traffic with firm value as well as the related mediation effects of buzz and traffic. They also reveal significant market competition effects, including effects of both a firm's own and its rivals' buzz and traffic. The findings also provide insights for e-commerce managers regarding Web site design, customer relation management, and how to best respond to competitors' strategic moves.
This study identifies how security performance and compliance influence each other and how security resources contribute to two security outcomes: data protection and regulatory compliance. Using simultaneous equation models and data from 243 hospitals, we find that the effects of security resources vary for data breaches and perceived compliance and that security operational maturity plays an important role in the outcomes. In operationally mature organizations, breach occurrences hurt compliance, but, surprisingly, compliance does not affect actual security. In operationally immature organizations, breach occurrences do not affect compliance, whereas compliance significantly improves actual security. The results imply that operationally mature organizations are more likely to be motivated by actual security than compliance, whereas operationally immature organizations are more likely to be motivated by compliance than actual security. Our findings provide policy insights on effective security programs in complex health-care environments.
As information technology (IT) becomes more accessible, sustaining any competitive advantage from it becomes challenging. This has caused some critics to dismiss IT as a less valuable resource. We argue that, in addition to being able to generate strategic advantage, IT should also be viewed as a strategic necessity that prevents competitive disadvantage in rapidly changing business environments. We test a set of hypotheses on strategic advantage and strategic necessity in the context of Internet banking investments for the population of U. S. Federal Deposit Insurance Corporation (FDIC) banks from 2003 to 2005. We seek to understand whether their IT investments were made as a strategic choice or as a result of strategic necessity. Our econometric analysis suggests that IT investments (1) were made to complement firm strategy for strategic advantage as well as due to strategic necessity, and (2) paid off by enhancing firm performance and addressing the issue of strategic necessity. In addition, our analysis reveals the simultaneous relationship between performance and IT investments: high-performing banking firms appear to have been more likely to invest in IT. The econometric analysis methods that we employ made it possible for us to state all of our quantitative findings for the FDIC data to be stated after adjusting for this endogeneity through simultaneity.
We consider an online market where consumers may obtain digital goods from two mutually exclusive channels: a legitimate channel consisting of many law-abiding retailers and a piracy channel consisting of many piracy services. We analyze consumer choice, retailer strategy, and piracy control using a sequential-search approach where information acquisition is costly for some consumers (nonshoppers), yet costless for others (shoppers). First, we show that a nonshopper's channel choice is determined by a simple comparison of two reservation prices. Second, we analyze how piracy threats affect in-channel pricing among retailers. If the in-channel competition intensity among retailers is high, piracy does not affect retailer pricing. If the intensity is medium, retailers respond to piracy by giving up some shoppers and, surprisingly, raising prices. If the intensity is low, the legitimate channel loses some shoppers as well as some nonshoppers to the piracy channel. Third, we consider several mechanisms for fighting piracy and analyze their effects on firm profit and consumer surplus. Reducing piracy quality and increasing piracy search costs are both effective in controlling piracy, yet they affect consumer surplus differently. Reducing the number of piracy services is less effective in controlling piracy.
The usefulness of a software product becomes obvious to consumers only after they get to experience it and, upon experiencing it, they may reach different conclusions regarding its true value. We examine the problem of designing free software trials under a general learning function. Our analyses lead to several new findings. We find that a time-locked trial is optimal only when the rate of learning is sufficiently large. It is not optimal in other situations, even when it has an overall positive effect on consumers' valuations. We also find that positive network effects have a minimal impact on this optimality. Interestingly, we find that neither the optimal trial period nor the optimal price is monotonically increasing in the rate of learning. At moderate rates, the software manufacturer pursues a dual strategy of offering a longer trial as well as a lower price. At higher rates of learning, the manufacturer does the opposite. Our results are robust, and incorporating possibilities such as a trial providing a signal of quality or learning being correlated with prior valuation has little impact on their applicability.
How does the adoption of cloud computing by a firm affect the organizational structure of its information technology (IT) department? To analyze this question, we consider an IT department that procures IT services from a cloud computing vendor and enhances these services for consuming units within the firm. Our model incorporates the competitive environment faced by the cloud vendor, which affects the price of the cloud vendor. We find that when the cloud vendor faces intense competition, the cost-center organizational model is preferred over the profit-center model. Infrastructure services such as basic storage, e-mail, and raw computing face intense competition, and our results suggest that such services be offered as a free corporate resource under the cost-center organizational structure. When the cloud vendor has pricing power, a profit-center organizational structure is likely to be preferred. Our results suggest that highly differentiated services such as cloud-based enterprise-wide enterprise resource planning or business intelligence be offered under the profit-center structure. Finally, the profit-center structure provides greater internal quality enhancement to cloud-based IT services than the cost center.
Drawing on the notion of channel capability, we develop a theoretical framework for understanding the interactions between mobile and traditional online channels for products with different characteristics. Specifically, we identify two channel capabilities-access and search capabilities-that differentiate mobile and online channels, and two product characteristics that are directly related to the channel capabilities-time criticality and information intensity. Based on this framework, we generate a set of predictions on the differential effects of mobile channel introduction across different product categories. We test the predictions by applying a counterfactual analysis based on vector autoregression to a large panel data set from a leading e-market in Korea that covers a 28-month period and contains all of the transactions made through the online and mobile channels before and after the mobile channel introduction. Consistent with our theoretical predictions, our results suggest that the performance impact of the mobile channel depends on the two product characteristics and the resulting product-channel fit. We discuss implications for theory and multichannel strategy.
This study addresses a major gap in our knowledge about the allocation of information technology (IT) decision rights between business and IT units at the application level, including the governance of applications delivered on-premise versus those delivered with a software-as-a-service (SaaS) model. Building on the findings from a multicase qualitative study of organizations that had adopted the same SaaS application, we draw on three theoretical lenses (agency theory, transaction cost economics, and knowledge-based view) to develop a theoretically grounded model with three organization-level factors, three application-level factors, and application-level IT governance. Hypotheses derived from the model, as well as a set of differential hypotheses about factor influences due to on-premise versus SaaS delivery, are tested with survey responses from 207 firms in which application-level governance is operationalized with two dimensions: decision control rights (decision authority) and decision management rights (task responsibility). Three antecedents (origin of the application initiative, scope of application use, business knowledge of the IT unit) were significantly associated with application governance postimplementation, and the on-premise/SaaS subgroup analyses provide preliminary evidence for the mode of application delivery as a moderator of these relationships. Overall, this study contributes to a growing body of research that takes a more modular approach to studying IT governance and provides theoretical explanations for differing application-level governance designs.
This paper applies the theory of real options to analyze how the value of information-based flexibility should affect the decision to centralize or decentralize data management under low and high uncertainty. This study makes two main contributions. First, we show that in the presence of low uncertainty, centralization of data management decisions creates more total surplus for the firm as the similarity of business units increases. In contrast, in the presence of high uncertainty, centralization creates more total surplus as the dissimilarity of business units increases. The pivoting distinction trades the benefit of reduction of uncertainty from dissimilar businesses for centralization (with cost saving) against the benefit of flexibility from decentralization. Second, the framework helps senior management evaluate the trade-offs in data centralization that drive different business models of the firm. We illustrate the application of these propositions formally using an analytical model and informally using case vignettes and simulation.
In recent years, chief information officers have begun to report exponential increases in the amounts of raw data captured and retained across the organization. Managing extreme amounts of data can be complex and challenging at a time when information is increasingly viewed as a strategic resource. Since the dominant focus of the information technology (IT) governance literature has been on how firms govern physical IT artifacts (hardware, software, networks), the goal of this study is to extend the theory of IT governance by uncovering the structures and practices used to govern information artifacts. Through detailed interviews with 37 executives in 30 organizations across 17 industries, we discover a range of structural, procedural, and relational practices used to govern information within a nomological net that includes the antecedents of these practices and their effects on firm performance. While some antecedents enable the speedy adoption of information governance, others can delay or limit the adoption of information governance practices. Once adopted, however, information governance can help to boost firm performance. By incorporating these results into an extended theory of IT governance, we note how information governance practices can unlock value from the ever-expanding mountains of data currently held within organizations.
Reputation systems have been recognized as successful online review communities and word-of-mouth channels. Our study draws upon the elaboration likelihood model to analyze the extent that the characteristics of reviewers and their early reviews reduce or worsen the bias of subsequent online reviews. Investigating the sources of this bias and ways to mitigate it is of considerable importance given the previously established significant impact of online reviews on consumers' purchasing decisions and on businesses' profitability. Based on a panel data set of 744 individual consumers collected from Yelp, we used the Markov chain Monte Carlo simulation method to develop and empirically test a system of simultaneous models of consumer review behavior. Our results reveal that male reviewers or those who lack experience, geographic mobility, or social connectedness are more prone to being influenced by prior reviews. We also found that longer and more frequent reviews can reduce online reviews' biases. This paper is among the first to examine the moderating effects of reviewer and review characteristics on the relationship between prior reviews and subsequent reviews. Practically, this study offers businesses effective customer relationship management strategies to improve their reputations and expand their clientele.
Giving away trial software is a common practice for software developers to maximize the exposure of their products to potential consumers and to minimize the consumers' uncertainty about software quality. There are two types of free trials: (1) freeware, which consists of very basic features of focal software without a time lock, and (2) trialware, which has the full functionality of focal software with a time lock. In this paper, we study what factors make some free-trial software attract more potential adopters than others. Our empirical model under the traditional Bass-type diffusion examines the effects of the different types of free-trial software and ratings on consumer software sampling and reveals the dynamics of sampling over time. Using free-trial software downloading data on Download.com, we observe that the consumer software sampling process can be described by the theory of information diffusion. We find that user ratings affect sampling performance positively and that third-party ratings need to be positive to be effective. Finally, our results do not show any discernible differences between freeware and trialware with regard to their impact on sampling performance. This study contributes to the understanding of software free-trial practice from the perspective of consumer sampling growth of different types of free trials. Our findings can help design free-trial strategies to extrapolate the extent of consumer awareness of focal software and effectively convey its quality information to potential customers.
This paper develops a process model of how and why complementarity and substitution form over time between contractual and relational governance in the context of information systems outsourcing. Our analysis identifies four distinct process patterns that explain this formation as the outcome of interaction processes between key elements of both contractual and relational governance. These patterns unveil the dynamic nature of complementarity and substitution. In particular, we show that the relationship between contractual and relational governance oscillates between complementarity and substitution. Those oscillations are triggered mainly by three types of contextual events (goal fuzziness, goal conflict, and goal misalignment). Surprisingly, substitution of informal control did not occur as an immediate reaction to external events but emerged as a consequence of preceding complementarity. Thus, our study challenges the prevailing view of an either/or dichotomy of complementarity and substitution by showing that they are causally connected over time.
This work anchors on the theories of cognitive fit and schema congruity to advance a review-product congruity proposition. The proposition states that the effects of product review content (either attribute or experience based) on the product review comprehension (reflected by perceived cognitive effort and review comprehension time) and assessment (manifested by perceived review helpfulness) of a consumer are contingent on the assessed product type (either search or experience product). The results of our first experiment support the proposition by revealing that the two matching conditions, (1) attribute-based reviews describing a search product and (2) experience-based reviews describing an experience product, could lead consumers to perceive higher review helpfulness and lower cognitive effort (subjective measure) to comprehend the reviews. However, the subjective evaluation of cognitive effort is not reinforced by the resulting review comprehension time (an objective assessment of comprehension effort), which suggests that consumers spend significantly more time processing reviews in the presence of the two matching conditions. A second experiment was conducted using the think-aloud method to gain further insights into the effects. We found that under the review-product matching conditions, consumers engage in deeper-level comprehension and expend more time in comprehension without realizing it, compared to consumers under the mismatching conditions. This research extends our current understanding of how review content and product reviews jointly influence the comprehension and assessment behavior of the consumer, and provides guidelines on the identification and the presentation of reviews to facilitate the judgment and decision making of potential consumers.
A comprehensive set of governance mechanisms and dimensions were investigated to identify combinations of mechanisms that are effectively used together in on-going volunteer-based open source software (OSS) projects. Three configurations were identified: Defined Community, Open Community, and Authoritarian Community. Notably, Defined Community governance had the strongest coordination and project climate and had the most extensive use of outcome, behavior, and clan control mechanisms (controller driven). The controls in the Defined Community governance configuration appear to effectively enable open, coordinated contribution and participation from a wide variety of talented developers (one of the virtues of open source development) while managing the development process and outcomes. The results add to our theoretical understanding of control in different types of information systems projects, as the combination of control modes found in OSS projects is different from those found in previous research for internal or outsourced information systems development projects. This could be due to unique features of OSS projects, such as volunteer participation and the controller being part of the development team. The results provide guidance for practitioners about how to combine 19 identified governance mechanisms into effective project governance that stimulates productive participation.
Two types of information technology (IT) outsourcing governance-contractual and relational-are commonly employed to address different goals in IT service management in outsourcing arrangements. Contractual governance helps improve efficiency in an outsourcing relationship, whereas relational governance facilitates satisfying changing business needs. Past literature argues that both forms of governance are important and that an appropriate balance between them is necessary. This study finds that these two forms of governance often conflict with one another. We contribute to the research on IT outsourcing governance by opening the black box of the evolutionary process of achieving ambidexterity in this context. Organizations shift their focus between contractual and relational forms of governance in an attempt to develop practices that address conflicts between the two forms. We present the findings from a qualitative study of an organization that outsourced its IT services. Our findings reveal how a balance between contractual and relational governance can be achieved through a process we call the ambidexterity pendulum.
Traders and investors are aware that emotional processes can have material consequences on their financial decision performance. However, typical learning approaches for debiasing fail to overcome emotionally driven financial dispositions, mostly because of subjects' limited capacity for self-monitoring. Our research aims at improving decision makers' performance by (1) boosting their awareness to their emotional state and (2) improving their skills for effective emotion regulation. To that end, we designed and implemented a serious game-based NeuroIS tool that continuously displays the player's individual emotional state, via biofeedback, and adapts the difficulty of the decision environment to this emotional state. The design artifact was then evaluated in two laboratory experiments. Taken together, our study demonstrates how information systems design science research can contribute to improving financial decision making by integrating physiological data into information technology artifacts. Moreover, we provide specific design guidelines for how biofeedback can be integrated into information systems.
Interoperability is a crucial organizational capability that enables firms to manage information systems (IS) from heterogeneous trading partners in a value network. While interoperability has been discussed conceptually in the IS literature, few comprehensive empirical studies have been conducted to conceptualize this construct and examine it in depth. For instance, it is unclear how interoperability is formed and whether it can improve organizational performance. To fill the gap, we argue that interorganizational systems (IOS) standards are a key information technology infrastructure facilitating formation of interoperability. As an organizational ability to work with external trading partners, interoperability's development depends not only on capability building within firm boundaries but also on community readiness across firm boundaries. Using data collected from 194 organizations in the geospatial industry, we empirically confirm that interoperability is formed via these two different paths. Furthermore, our results show that interoperability acts as a mediator by enabling firms to achieve performance gains from IOS standards adoption. Our study sheds new light on formation mechanisms as well as the business value of interoperability.
Neuroscience provides a new lens through which to study information systems (IS). These NeuroIS studies investigate the neurophysiological effects related to the design, use, and impact of IS. A major advantage of this new methodology is its ability to examine human behavior at the underlying neurophysiological level, which was not possible before, and to reduce self-reporting bias in behavior research. Previous studies that have revisited important IS concepts such as trust and distrust have challenged and extended our knowledge. An increasing number of neuroscience studies in IS have given researchers, editors, reviewers, and readers new challenges in terms of determining what makes a good NeuroIS study. While earlier papers focused on how to apply specific methods (e. g., functional magnetic resonance imaging), this paper takes an IS perspective in deriving six phases for conducting NeuroIS research and offers five guidelines for planning and evaluating NeuroIS studies: to advance IS research, to apply the standards of neuroscience, to justify the choice of a neuroscience strategy of inquiry, to map IS concepts to bio-data, and to relate the experimental setting to IS-authentic situations. The guidelines provide guidance for authors, reviewers, and readers of NeuroIS studies, and thus help to capitalize on the potential of neuroscience in IS research.
Avatars, as virtual humans possessing realistic faces, are increasingly used for social and economic interaction on the Internet. Research has already determined that avatar-based communication may increase perceived interpersonal trust in anonymous online environments. Despite this trust-inducing potential of avatars, however, we hypothesize that in trust situations, people will perceive human faces differently than they will perceive avatar faces. This prediction is based on evolution theory, because throughout human history the majority of interaction among people has taken place in face-to-face settings. Therefore, unlike perception of an avatar face, perception of a human face and the related trustworthiness discrimination abilities must be part of the genetic makeup of humans. Against this background, we conducted a functional magnetic resonance imaging experiment based on a multiround trust game to gain insight into the differences and similarities of interactions between humans versus human interaction with avatars. Our results indicate that (1) people are better able to predict the trustworthiness of humans than the trustworthiness of avatars; (2) decision making about whether or not to trust another actor activates the medial frontal cortex significantly more during interaction with humans, if compared to interaction with avatars; this brain area is of paramount importance for the prediction of other individuals' thoughts and intentions (mentalizing), a notably important ability in trust situations; and (3) the trustworthiness learning rate is similar, whether interacting with humans or avatars. Thus, the major implication of this study is that although interaction on the Internet may have benefits, the lack of real human faces in communication may serve to reduce these benefits, in turn leading to reduced levels of collaboration effectiveness.
This paper examines the effect of a social network on prediction markets using a controlled laboratory experiment that allows us to identify causal relationships between a social network and the performance of an individual participant, as well as the performance of the prediction market as a whole. Through a randomized experiment, we first confirm the theoretical predictions that participants with more social connections are less likely to invest in information acquisition from outside information sources, but perform significantly better than other participants in prediction markets. We further show that when the cost of information acquisition is low, a social network-embedded prediction market outperforms a nonnetworked prediction market. We find strong support for peer effects in prediction accuracy among participants. These results have direct managerial implications for the business practice of prediction markets and are critical to understanding how to use social networks to improve the performance of prediction markets.
Virtual teams are increasingly common in today's organizations, yet they often make poor decisions. Teams that interact using text-based collaboration technology typically exchange more information than when they perform the same task face-to-face, but past results suggest that team members are more likely to ignore information they receive from others. Collaboration technology makes unique demands on individual cognitive resources that may change how individual team members process information in virtual settings compared to face-to-face settings. This experiment uses electroencephalography, electrodermal activity, and facial electromyography to investigate how team members process information received from text-based collaboration during a team decision-making process. Our findings show that information that challenges an individual's prediscussion decision preference is processed similarly to irrelevant information, while information that supports an individual's prediscussion decision preference is processed more thoroughly. Our results present neurological evidence for the underlying processes of confirmation bias in information processing during online team discussions.
Product sales via sponsored keyword advertising on search engines rely on an effective selection of keywords that describe the offerings. In this study, we consider both the direct sales of the advertised products and indirect sales (i.e., cross-selling) of other products, and examine how specific keywords and general keywords influence these two types of sales differently. We also examine how the cross-selling effects may vary across different types of products (main products and accessories). Our results suggest that the use of specific keywords leans toward improving the direct sales of advertised products, while the use of general keywords leans toward improving the indirect sales of other products. The contribution of keywords to indirect sales is influenced by product type. For main products, the use of specific keywords generates a higher marginal contribution to indirect sales than that of general keywords. For accessory products, the use of general keywords generates a higher marginal contribution to indirect sales than that of specific keywords. The key implication of this study is that sellers focusing on different types of sales (direct or indirect sales) or products (main or accessory products) should consider using different types of keywords in search engine advertising to drive sales.
User-game engagement is vital for building and retaining a customer base for software games. However, few studies have investigated such engagement during gameplay and the impact of gaming elements on engagement. Drawing on the theoretical foundation of engagement, we meticulously deduced two cognitive-related gaming elements of a software game, namely, game complexity and game familiarity, and argued that these elements have individual and joint effects on user-game engagement. This research adopted multimethod empirical investigations to validate our conceptions. The first investigation used electroencephalography and a self-report survey to study quantitatively the cognitive activities of user-game engagement. The second investigation adopted the qualitative interview method to triangulate the findings from the quantitative data. This research contributes to theory in two ways, namely, conceptualizing and empirically examining user-game engagement as well as theorizing and demonstrating how the two gaming elements affect user-game engagement. This work contributes to the gaming practice by providing a set of design principles for gaming elements.
This study examines two types of information commonly used by group-buying sites to induce purchasing. The first study indicates the number of people who have bought a deal (buy information). The second one indicates Facebook friends who like a deal (like information). The effects of the group-buying information on opinions (attitude and intention) and emotions were examined using a controlled experiment. Our results show that positive and negative buy information has an asymmetric influence on attitude and intention, whereas like information has a positive influence on intention. The presence of buy information is associated with EEG activity that is generally linked to negative emotions. However, the addition of like information is associated with EEG activity that is generally linked to positive emotions. The different effects of the two types of group-buying information can be explained by the different social influences exerted by the information.
Human emotions' role in phenomena related to information systems ( IS) is increasingly of interest to research and practice, and is now informed by a burgeoning literature in neuroscience. This study develops a nomological network with an overarching view of relationships among emotions and other constructs of interest in IS research. The resulting 3-emotion systems' nomological network includes three interacting emotion systems: language, physiology, and behavior. Two laboratory experiments were conducted to test the nomological network, with six online travel service Web pages used as stimuli. The first study used paper-based self-report measures and qualitative comments, whereas the second included both self-reports and electroencephalography (EEG) measures. An outcome measure of e-loyalty was included in each study. The results of both studies showed positive and negative emotion-inducing stimuli were related to positive and negative emotions when viewing the Web sites as indicated by both self-reports and EEG data. In turn, positive and negative emotions as measured by both self-reports and EEG measures were linked to e-loyalty to some degree. This research is novel and significant because it is possibly the first in-depth study to link the study of emotions in IS with a sound theory base and multiple measurement approaches, including neuroscience measures. It shows that an EEG measure has some predictive power for an outcome such as e-loyalty. Implications of the research are that IS studies should distinguish between the different emotion systems of language and physiology, choose emotion measures carefully, and also recognize the intertwining of the emotion systems and cognitive processing.
Behavioral beliefs-perceived usefulness and perceived ease of use-have been identified as the most influential antecedents of individuals' information systems use intentions and behaviors within the technology acceptance model. However, little research has been aimed at investigating the implicit (automatic or unconscious) determinants of such cognitive beliefs, and more importantly, the potential nonlinear relationships of such antecedents with explicit (perceptual) ones. As such, this paper theorizes that implicit neurophysiological states-memory load and distraction- and explicit-engagement and frustration-antecedents interact in the formation of perceived usefulness and perceived ease of use. To test the study's hypotheses, we conducted an experiment that measured neurophysiological states while individuals worked on instrumental and hedonic tasks using technology. The results show that, as theorized, implicit and explicit constructs interact together, and thus have a nonlinear effect on behavioral beliefs. Specifically, when engagement is high, neurophysiological distraction does not statistically significantly affect perceived usefulness, whereas when engagement is low, neurophysiological distraction has a negative and significant effect on usefulness. The results also show that when frustration is high, neurophysiological memory load has a negative effect on perceived ease of use, whereas when it is low, neurophysiological memory load has a positive effect on perceived ease of use. This study makes several contributions to acceptance research and the emerging field of NeuroIS, including demonstration of the importance of emotional perceptions for moderating the effects of neurophysiological states on behavioral beliefs.
Human emotions' role in phenomena related to information systems ( IS) is increasingly of interest to research and practice, and is now informed by a burgeoning literature in neuroscience. This study develops a nomological network with an overarching view of relationships among emotions and other constructs of interest in IS research. The resulting 3-emotion systems' nomological network includes three interacting emotion systems: language, physiology, and behavior. Two laboratory experiments were conducted to test the nomological network, with six online travel service Web pages used as stimuli. The first study used paper-based self-report measures and qualitative comments, whereas the second included both self-reports and electroencephalography (EEG) measures. An outcome measure of e-loyalty was included in each study. The results of both studies showed positive and negative emotion-inducing stimuli were related to positive and negative emotions when viewing the Web sites as indicated by both self-reports and EEG data. In turn, positive and negative emotions as measured by both self-reports and EEG measures were linked to e-loyalty to some degree. This research is novel and significant because it is possibly the first in-depth study to link the study of emotions in IS with a sound theory base and multiple measurement approaches, including neuroscience measures. It shows that an EEG measure has some predictive power for an outcome such as e-loyalty. Implications of the research are that IS studies should distinguish between the different emotion systems of language and physiology, choose emotion measures carefully, and also recognize the intertwining of the emotion systems and cognitive processing.
Virtual teams are increasingly common in today's organizations, yet they often make poor decisions. Teams that interact using text-based collaboration technology typically exchange more information than when they perform the same task face-to-face, but past results suggest that team members are more likely to ignore information they receive from others. Collaboration technology makes unique demands on individual cognitive resources that may change how individual team members process information in virtual settings compared to face-to-face settings. This experiment uses electroencephalography, electrodermal activity, and facial electromyography to investigate how team members process information received from text-based collaboration during a team decision-making process. Our findings show that information that challenges an individual's prediscussion decision preference is processed similarly to irrelevant information, while information that supports an individual's prediscussion decision preference is processed more thoroughly. Our results present neurological evidence for the underlying processes of confirmation bias in information processing during online team discussions.
Avatars, as virtual humans possessing realistic faces, are increasingly used for social and economic interaction on the Internet. Research has already determined that avatar-based communication may increase perceived interpersonal trust in anonymous online environments. Despite this trust-inducing potential of avatars, however, we hypothesize that in trust situations, people will perceive human faces differently than they will perceive avatar faces. This prediction is based on evolution theory, because throughout human history the majority of interaction among people has taken place in face-to-face settings. Therefore, unlike perception of an avatar face, perception of a human face and the related trustworthiness discrimination abilities must be part of the genetic makeup of humans. Against this background, we conducted a functional magnetic resonance imaging experiment based on a multiround trust game to gain insight into the differences and similarities of interactions between humans versus human interaction with avatars. Our results indicate that (1) people are better able to predict the trustworthiness of humans than the trustworthiness of avatars; (2) decision making about whether or not to trust another actor activates the medial frontal cortex significantly more during interaction with humans, if compared to interaction with avatars; this brain area is of paramount importance for the prediction of other individuals' thoughts and intentions (mentalizing), a notably important ability in trust situations; and (3) the trustworthiness learning rate is similar, whether interacting with humans or avatars. Thus, the major implication of this study is that although interaction on the Internet may have benefits, the lack of real human faces in communication may serve to reduce these benefits, in turn leading to reduced levels of collaboration effectiveness.
User-game engagement is vital for building and retaining a customer base for software games. However, few studies have investigated such engagement during gameplay and the impact of gaming elements on engagement. Drawing on the theoretical foundation of engagement, we meticulously deduced two cognitive-related gaming elements of a software game, namely, game complexity and game familiarity, and argued that these elements have individual and joint effects on user-game engagement. This research adopted multimethod empirical investigations to validate our conceptions. The first investigation used electroencephalography and a self-report survey to study quantitatively the cognitive activities of user-game engagement. The second investigation adopted the qualitative interview method to triangulate the findings from the quantitative data. This research contributes to theory in two ways, namely, conceptualizing and empirically examining user-game engagement as well as theorizing and demonstrating how the two gaming elements affect user-game engagement. This work contributes to the gaming practice by providing a set of design principles for gaming elements.
This study examines two types of information commonly used by group-buying sites to induce purchasing. The first study indicates the number of people who have bought a deal (buy information). The second one indicates Facebook friends who like a deal (like information). The effects of the group-buying information on opinions (attitude and intention) and emotions were examined using a controlled experiment. Our results show that positive and negative buy information has an asymmetric influence on attitude and intention, whereas like information has a positive influence on intention. The presence of buy information is associated with EEG activity that is generally linked to negative emotions. However, the addition of like information is associated with EEG activity that is generally linked to positive emotions. The different effects of the two types of group-buying information can be explained by the different social influences exerted by the information.
Behavioral beliefs-perceived usefulness and perceived ease of use-have been identified as the most influential antecedents of individuals' information systems use intentions and behaviors within the technology acceptance model. However, little research has been aimed at investigating the implicit (automatic or unconscious) determinants of such cognitive beliefs, and more importantly, the potential nonlinear relationships of such antecedents with explicit (perceptual) ones. As such, this paper theorizes that implicit neurophysiological states-memory load and distraction- and explicit-engagement and frustration-antecedents interact in the formation of perceived usefulness and perceived ease of use. To test the study's hypotheses, we conducted an experiment that measured neurophysiological states while individuals worked on instrumental and hedonic tasks using technology. The results show that, as theorized, implicit and explicit constructs interact together, and thus have a nonlinear effect on behavioral beliefs. Specifically, when engagement is high, neurophysiological distraction does not statistically significantly affect perceived usefulness, whereas when engagement is low, neurophysiological distraction has a negative and significant effect on usefulness. The results also show that when frustration is high, neurophysiological memory load has a negative effect on perceived ease of use, whereas when it is low, neurophysiological memory load has a positive effect on perceived ease of use. This study makes several contributions to acceptance research and the emerging field of NeuroIS, including demonstration of the importance of emotional perceptions for moderating the effects of neurophysiological states on behavioral beliefs.
Neuroscience provides a new lens through which to study information systems (IS). These NeuroIS studies investigate the neurophysiological effects related to the design, use, and impact of IS. A major advantage of this new methodology is its ability to examine human behavior at the underlying neurophysiological level, which was not possible before, and to reduce self-reporting bias in behavior research. Previous studies that have revisited important IS concepts such as trust and distrust have challenged and extended our knowledge. An increasing number of neuroscience studies in IS have given researchers, editors, reviewers, and readers new challenges in terms of determining what makes a good NeuroIS study. While earlier papers focused on how to apply specific methods (e. g., functional magnetic resonance imaging), this paper takes an IS perspective in deriving six phases for conducting NeuroIS research and offers five guidelines for planning and evaluating NeuroIS studies: to advance IS research, to apply the standards of neuroscience, to justify the choice of a neuroscience strategy of inquiry, to map IS concepts to bio-data, and to relate the experimental setting to IS-authentic situations. The guidelines provide guidance for authors, reviewers, and readers of NeuroIS studies, and thus help to capitalize on the potential of neuroscience in IS research.
This paper examines the effect of a social network on prediction markets using a controlled laboratory experiment that allows us to identify causal relationships between a social network and the performance of an individual participant, as well as the performance of the prediction market as a whole. Through a randomized experiment, we first confirm the theoretical predictions that participants with more social connections are less likely to invest in information acquisition from outside information sources, but perform significantly better than other participants in prediction markets. We further show that when the cost of information acquisition is low, a social network-embedded prediction market outperforms a nonnetworked prediction market. We find strong support for peer effects in prediction accuracy among participants. These results have direct managerial implications for the business practice of prediction markets and are critical to understanding how to use social networks to improve the performance of prediction markets.
Interoperability is a crucial organizational capability that enables firms to manage information systems (IS) from heterogeneous trading partners in a value network. While interoperability has been discussed conceptually in the IS literature, few comprehensive empirical studies have been conducted to conceptualize this construct and examine it in depth. For instance, it is unclear how interoperability is formed and whether it can improve organizational performance. To fill the gap, we argue that interorganizational systems (IOS) standards are a key information technology infrastructure facilitating formation of interoperability. As an organizational ability to work with external trading partners, interoperability's development depends not only on capability building within firm boundaries but also on community readiness across firm boundaries. Using data collected from 194 organizations in the geospatial industry, we empirically confirm that interoperability is formed via these two different paths. Furthermore, our results show that interoperability acts as a mediator by enabling firms to achieve performance gains from IOS standards adoption. Our study sheds new light on formation mechanisms as well as the business value of interoperability.
Product sales via sponsored keyword advertising on search engines rely on an effective selection of keywords that describe the offerings. In this study, we consider both the direct sales of the advertised products and indirect sales (i.e., cross-selling) of other products, and examine how specific keywords and general keywords influence these two types of sales differently. We also examine how the cross-selling effects may vary across different types of products (main products and accessories). Our results suggest that the use of specific keywords leans toward improving the direct sales of advertised products, while the use of general keywords leans toward improving the indirect sales of other products. The contribution of keywords to indirect sales is influenced by product type. For main products, the use of specific keywords generates a higher marginal contribution to indirect sales than that of general keywords. For accessory products, the use of general keywords generates a higher marginal contribution to indirect sales than that of specific keywords. The key implication of this study is that sellers focusing on different types of sales (direct or indirect sales) or products (main or accessory products) should consider using different types of keywords in search engine advertising to drive sales.
