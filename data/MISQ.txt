The establishment of strong alignment between information technology (IT) and organizational objectives has consistently been reported as one of the key concerns of information systems managers. This paper presents findings from a study which investigated the influence of several factors on the social dimension of alignment within 10 business units in the Canadian life insurance industry. The social dimension of alignment refers to the state in which business and IT executives understand and are committed to the business and IT mission, objectives, and plans. The research model included four factors that would potentially influence alignment: (1) shared domain knowledge between business and IT executives, (2) IT implementation success, (3) communication between business and IT executives, and (4) connections between business and IT planning processes. The outcome, alignment, was operationalized in two ways: the degree of mutual understanding of current objectives (short-term alignment) and the congruence of IT vision (long-term alignment) between business and IT executives. A total of 57 semi-structured interviews were held with 45 informants. Written business and IT strategic plans, minutes from IT steering committee meetings, and other strategy documents were collected and analyzed from each of the 10 business units. All four factors in the model (shared domain knowledge, IT implementation success, communication between business and IT executives, and connections between business and IT planning) were found to influence short-term alignment. Only shared domain knowledge was found to influence long-term alignment. A new factor, strategic business plans, was found to influence both short and long-term alignment. The findings suggest that both practitioners and researchers should direct significant effort toward understanding shared domain knowledge, the factor which had the strongest influence on the alignment between IT and business executives. There is also a call for further research into the creation of an IT vision.
This research considers whether interpretive techniques can be used to enhance our understanding of computer-mediated discussions. The case study considered in this research is the use of a group support system (GSS) to support employee discussions about gender equity in a university. Transcripts of the four discussions were analyzed using two analysis techniques: a positivist approach, which was focused on the GSS sessions themselves, and an interpretive approach which broadened the scope to include contextual considerations as well. What emerged from the positivist analysis was the conclusion of effective group behavior directed toward consensus around alternative solution scenarios. What emerged from the interpretive analysis was evidence of multiple, rich types of information at three levels: cognitive, affective, and behavioral. The interpretive analysis also uncovered the absence of shared consciousness about the issue and imbalanced participation in the sessions. Comparison of the results of both approaches showed that, while the positivist analysis provided useful information, the interpretive analysis provided a different understanding of the same evidence and new information not found in the positivist analysis of the group discussions. This research adds to the body of knowledge concerning the effects of virtual group meetings on the type of information that is shared and the value of a combination of positivist and interpretive analyses of GSS data.
The concept of work exhaustion (or job burnout) from the management and psychology research literature is examined in the context of technology professionals. Data were collected from 270 IT professionals and managers in various industries across the United States. Through structural equation modeling, work exhaustion was shown to partially mediate the effects of workplace factors on turnover intention. In addition, the results of the study revealed that: (1) technology professionals experiencing higher levels of exhaustion reported higher intentions to leave the job and, (2) of the variables expected to influence exhaustion (work overload, role ambiguity and conflict, lack of autonomy and lack of rewards), work overload was the strongest contributor to exhaustion in the technology workers. Moreover, exhausted IT professionals identified insufficient staff and resources as a primary cause of work overload and exhaustion. Implications for practice and future research are discussed.
The resource-based view of the firm attributes superior financial performance to organizational resources and capabilities. This paper develops the concept of IT as an organizational capability and empirically examines the association between IT capability and firm performance. Firm specific IT resources are classified as IT infrastructure, human IT resources, and IT-enabled intangibles. A matched-sample comparison group methodology and publicly available ratings are used to assess IT capability and firm performance. Results indicate that firms with high IT capability tend to outperform a control sample of firms on a variety of profit and cost-based performance measures.
Information systems research has traditionally focused on information as an object that serves as input to decision making. Such a perspective attends mainly to the use of information. Increasingly, however, organizations are concerned about the production of information. This paper focuses on the work of producing informational objects, an activity central to knowledge work. Based on data collected during an eight-month ethnographic study of three groups of knowledge workers-computer system administrators, competitive intelligence analysts, and librarians - I explore the informing practices they relied upon. These are identified as ex-pressing, monitoring, and translating. Common to these informing practices is the knowledge workers’ endeavor to balance subjectivity and objectivity, where subjectivity is a necessary part of doing value adding work and objectivity promises workers authority and a sense of security. Recognizing that researchers are knowledge workers too, I draw on my own experiences as an ethnographic researcher to identify parallels between my informing practices and those of the knowledge workers I studied. These parallels are intended to challenge the taken-for-granted assumptions underlying scientific practice. I adopt a confessional genre of representation for this purpose.
Using the Technology Acceptance Model (TAM), this research investigated gender differences in the overlooked context of individual adoption and sustained usage of technology in the workplace. User reactions and technology usage behavior were studied over a five-month period among 342 workers being introduced to a new software system. At all three points of measurement, compared to women, men's technology usage decisions were more strongly influenced by their perceptions of usefulness. In contrast, women were more strongly influenced by perceptions of ease of use and subjective norm, although the effect of subjective norm diminished over time. These findings were robust even after statistically controlling for key confounding variables identified in prior organizational behavior research (i.e., income, occupation, and education levels), and another possible confound from technology research, prior experience with computers in general. Thus, in addition to identifying key boundary conditions in the role of the original TAM constructs (perceived usefulness and perceived ease of use), this research provides the basis for the integration of subjective norm into the model. In light of these findings, implications for theory and practice are discussed.
The application of real options analysis to information technology investment evaluation problems recently has been proposed in the IS literature (Chalasani et al. 1997; Dos Santos 1991; Kambil et al. 1993; Kumar 1996; Taudes 1998). The research reported on in this paper illustrates the value of applying real options analysis in the context of a case study involving the deployment of point-of-sale (POS) debit services by the Yankee 24 shared electronic banking network of New England. In the course of so doing, the paper also attempts to operationalize real options analysis concepts by examining claimed strengths of this analysis approach and balancing them against methodological difficulties that this approach is believed to involve. The research employs a version of the Black-Scholes option pricing model that is adjusted for risk-averse investors, showing how it is possible to obtain reliable values for Yankee 24’s “investment timing option,” even in the absence of a market to price it. To gather evidence for the existence of the timing option, basic scenario assumptions, and the parameters of the adjusted Black-Scholes model, a structured interview format was developed. The results obtained using real options analysis enabled the network’s senior management to identify conditions for which entry into the POS debit market would be profitable. These results also indicated that, in the absence of formal evaluation of the timing option, traditional approaches for evaluating information technology investments would have produced the wrong recommendations.
Many calls for reengineering suggest that information technology (IT) can be employed to enable significant organizational change. However, organizational inertia typically inhibits such change, resulting in IT development that reflects the organizational status quo. Of interest, then, are strategies and techniques that can be employed to support IT-enabled reengineering. Key to such reengineering is creativity. Therefore, this paper attempts to better understand creativity during IT requirements and logical design phases, at which time reengineering can be devised. A creativity model is adapted from the organizational literature and used to interpret the requirements gathering and logical design experiences of a firm attempting to reengineer its operations through the use of imaging technology. A comparison of creativity and traditional IT development viewpoints reveals significant differences as well as similarities. Insight resulting from this exercise is offered to help managers and researchers identify important variables and relationships in the IT development creativity management process. The model, and future associated research, can help researchers and managers: (1) determine to what degree creative IT requirements and logical design is feasible in a given organizational context and (2) plan and execute a creative IT requirements and logical design process.
Increasingly, business leaders are demanding that IT play the role of a business partner and a strategic enabler. In such an environment, IT human capital has assumed considerable significance. Insightful IT leaders recognize that the greatest impediments to success are often related to people rather than to information, technology, and systems. What is not quite clear to IT leaders, however, is exactly how to develop and leverage this human capital in support of business needs. The transformation of IT from a back-office support role to a strategic business partner requires new roles and competencies for IT leaders and professionals. Key challenges for IT leaders are to envision these roles and competencies and to develop and implement programs to translate this vision to reality. This paper describes the IT human resource vision that is guiding such a transformation at 3M—a large multi-product, diversified manufacturing firm (1998 sales: $15 billion)—and focuses on the implementation of its leadership initiative. This initiative was instrumental in not only allowing 3M to develop needed skills and behaviors among its IT professionals, it also helped 3M evade an industry-wide recruitment and retention trend. The major conceptual models guiding the leadership initiative as well as implementation details are presented. Challenges encountered on the way and the lessons learned from the journey are discussed. 3M’s experiences provide opportunities for managers in other organizations to develop valuable insights regarding the management of human capital in IT.
Aging information systems are expensive to maintain and most are eventually retired and replaced. But what determines (in the choices made by managers) whether and when a system reaches end-of-life? What shapes managers’ judgements about a system’s remaining life expectancy and do these judgments influence the maintenance effort itself? System maintenance and prospective replacement are examined here in new terms, positing that managers “equilibrate” (balance) their allocation of maintenance effort with their expectations of a system’s remaining life. Drawing from data on 758 systems among 54 organizations, support is found for an exploratory structural equation model in which the relationship between maintenance effort and remaining life expectancy is newly explained. A “portfolio effect,” reflecting a system’s familial complexity, is also found to be directly and positively related to the maintenance effort. A further finding is that a system’s size is directly and positively associated with its remaining life expectancy. Notwithstanding normative research suggesting the contrary, larger systems may tend to be longer-lived than smaller systems. Practically, the suggestion is made that better documented and monitored portfolios, together with regular, periodic performance assessments, can lead to better management of systems’ life cycles.
In recent years, the use of option pricing models to support IT investment decisions has been proposed in the MIS literature. In this paper, we discuss the practical advantages of such techniques for the selection of a software platform. First, we argue that traditional quantitative approaches to a cost-benefit analysis give only a partial picture of such decision situations: due to the long planning horizon required because of the time-consuming and resource-intensive implementation process, it is not possible to exactly predict which applications will, in fact, run on the system over time. Thus, the investor is faced with the problem of valuing "implementation opportunities." We then compare different valuation techniques for this task and discuss their respective advantages and drawbacks. The practical advantages of employing such models are demonstrated by describing a real-life case study where option pricing models were used for deciding whether to continue employing SAP R/2 or to switch to SAP R/3.
One of the most challenging decisions that a manager must confront is whether to continue or abandon a troubled project. Published studies suggest that failing software projects are often allowed to continue for too long before appropriate management action is taken to discontinue or redirect the efforts. The level of sunk cost associated with such projects has been offered as one explanation for this escalation of commitment behavior. What prior studies fail to consider is how concepts from risk-taking theory (such as risk propensity and risk perception) affect decision makers’ willingness to continue a project under conditions of sunk cost. To better understand factors that may cause decision makers to continue such projects, this study examines the level of sunk cost together with the risk propensity and risk perception of decision makers. These factors are assessed for cross-cultural robustness using matching laboratory experiments carried out in three cultures (Finland, the Netherlands, and Singapore). With a wider set of explanatory factors than prior studies, we could account for a higher amount of variance in decision makers’ willingness to continue a project. The level of sunk cost and the risk perception of decision makers contributed significantly to their willingness to continue a project. Moreover, the risk propensity of decision makers was inversely related to risk perception. This inverse relationship was significantly stronger in Singapore (a low uncertainty avoidance culture) than in Finland and the Netherlands (high uncertainty avoidance cultures). These results reveal that some factors behind decision makers’ willingness to continue a project are consistent across cultures while others may be culture-sensitive. Implications of these results for further research and practice are discussed.
This paper utilizes a qualitative methodology, revealed causal mapping (RCM), to investigate the phenomenon of software operations support expertise. Software operations support is a large portion of the IS work performed in organizations. While we as researchers have access to generalized theories and frameworks of expertise, very little is known about expertise in this critical area. To understand software operations support expertise, a mid-range theory is evoked from interviews with experts and the construction of RCMs from those interviews. The results of this study indicate that software operation support expertise is comprised of five major constructs: personal competencies, environmental factors, support personnel motivation, IS policies, and support personnel outcomes. Additionally, this study revealed that these constructs interact differently in contexts where software support is the main activity versus contexts where the focus is development. This study demonstrates that the use of the RCM methodology yields constructs of software operations support expertise that are not suggested by generalized theory. In addition, the use of RCM as an evocative, qualitative methodology reveals the interaction and linkages between these constructs. This paper also provides a history of and tutorial to the RCM methodology for use by the research community.
This paper reports on a survey of North American IS programs and secondary data assessing the supply and demand of Information Systems (IS) doctorates. The data document a large and growing lack of supply to meet current and future demand. Demographic factors—including the number of university students, their selection of majors, and retirements among IS faculty—favor a probable scenario for continuing strong demand for IS faculty in the longer term. We argue that the severe imbalance will continue if the current state of the economy and businesses’ need for technically-savvy managers continues. Implications and recommendations are presented for ensuring the long-term health of the IS discipline in addressing this imbalance.
GDSS has enjoyed about a decade and a half of vigorous research activity. Throughout this time, a problem that has occupied the research community is the inconsistent research results that have been obtained. The purpose of this paper is to assess whether the reason for these inconsistencies is rooted in the epistemological mode that has prevailed and to offer an alternative epistemological lens that might help advance our understanding of GDSS use and research. Using qualitative research methods and a symbolic interactionist theoretical basis, this paper examines how a particular group used a GDSS over two meetings. The findings indicate that GDSS use may result in reactions from its users that are difficult to capture using conventional methodological assumptions, thereby helping explain why past results have not been consistent. Based on these findings, a shift in focus is advocated from an emphasis on the technology to an emphasis on human interaction, one that embraces the reasons underlying past inconsistencies rather than attempting to overcome them.
Project failure in the information technology area is a costly problem, and troubled projects are not uncommon. In many cases, these projects seem to take on a life of their own, continuing to absorb valuable resources, while failing to deliver any real business value. While prior research has shown that managers can easily become locked into a cycle of escalating commitment to a failing course of action, there has been comparatively little research on de-escalation, or the process of breaking such a cycle. Through de-escalation, troubled projects may be successfully turned around or sensibly abandoned. This study seeks to understand the process of de-escalation and to establish a model for turning around troubled projects that has both theoretical and practical significance. Through a longitudinal case study of the IT-based baggage handling system at Denver International Airport (DIA), we gathered qualitative data on the de-escalation of commitment to a failing course of action, allowing us to inductively develop a model of the de-escalation process as it unfolded at DIA. The model reveals de-escalation as a four-phase process: (1) problem recognition, (2) re-examination of prior course of action, (3) search for alternative course of action, and (4) implementing an exit strategy. For each phase of the model, we identified key activities that may enable de-escalation to move forward. Implications of this model for both research and practice are discussed.
With the advent of multimedia and intranet technologies, managers and information systems designers face a new challenge: how to capture and present information using a variety of representation formats (text, graphics, audio, video, and animations) so that members of an organization can make better sense out of the information available. In this study, we develop a task-representation fit model to generate several predictions about the potential of multimedia to alleviate the limitations of text-based information in the context of individual decision makers utilizing organizational data and test them in a laboratory experiment. Results support the task-representation fit relationships predicted. For analyzable tasks, text-based representation and multimedia representation are equally effective in reducing perceived equivocality levels. For less-analyzable tasks, only multimedia representation was instrumental in reducing perceived equivocality levels.
We identify top management leadership, a sophisticated management infrastructure, process management efficacy, and stakeholder participation as important elements of a quality-oriented organizational system for software development. A model interrelating these constructs and quality performance is proposed. Data collected through a national survey of IS executives in Fortune 1000 companies and government agencies was used to test the model using a Partial Least Squares analysis methodology. Our results suggest that software quality goals are best attained when top management creates a management infrastructure that promotes improvements in process design and encourages stakeholders to evolve the design of the development processes. Our results also suggest that all elements of the organizational system need to be developed in order to attain quality goals and that piecemeal adoption of select quality management practices are unlikely to be effective. Implications of this research for IS theory and practice are discussed.
The adaptation process for new technology is not yet well understood. This study analyzes how an inter-organizational virtual team, tasked with creating a highly innovative product over a 10month period, adapted the use of a collaborative technology and successfully achieved its challenging objectives. The study of such a virtual team is especially useful for extending our understanding of the adaptation process as virtual teams have more malleable structures than typical organizational units and controlled group experiments. Data were obtained from observations of weekly virtual meetings, electronic log files, interviews, and weekly questionnaires administered to team members. We found that the team initially experienced significant misalignments among the pre-existing organizational environment, group, and technology structures. To resolve these misalignments, the team modified the organizationalenvironment and group structures, leaving the technology structure intact. However, as the team proceeded, a series of events unfolded that caused the team to reevaluate and further modify its structures. This final set of modifications involved reverting back to the pre-existing organizational environment, while new technology and group structures emerged as different from both the pre-existing and the initial ones. A new model of the adaptation process—one that integrates these findings and those of several previous models—is proposed.
This research investigated how the use of a spatial decision support system (SDSS)—a type ofgeographic information system (GIS)—influenced the accuracy and efficiency of different types of problem solvers (i.e., professionals versus students) completing problems of varied complexity. This research—first to simultaneously study these variables—examined subjects who completed a problem involving spatially-referenced information. The experiment was guided by a research model synthesized from various perspectives, including the theory of cognitive fit, prior research on map reading and interpretation, and research examining subject expertise and experience. The results are largely supportive of the research model and demonstrate that SDSS, an increasingly important class of management decision-making technology, increased the efficiency of users working on more complex problems. Professionals were found to be more accurate but less efficient than students; however, professionals who used the SDSS were no more accurate than professionals using paper maps. Need for cognition, a construct that focuses on an individual’s willingness to engage in problem solving tasks, was found to be marginally related to accuracy. The implications of these findings for researchers and practitioners are presented and discussed.
From 1990 through 1998, First American Corporation (FAC) changed its corporate strategy from a traditional banking approach to a customer relationship-oriented strategy that placed FAC.s customers at the center of all aspects of the company’s operations. The transformation made FAC’s an innovative leader in the financial services industry. This case study describes FAC’s transformation and the way in which a data warehouse called VISION helped make it happen. FAC’s experiences suggest lessons for managers who plan to use technology to support changes that are designed to significantly improve organizational performance. In addition, they raise interesting questions about the means by which information technology can be used to gaincompetitive advantage.
Software projects can often spiral out of control to become “runaway systems” that far exceed original budget and schedule projections. The behavior that underlies many runaway systems can best be characterized as “escalation of commitment to a failing course of action.” The objectives of this study were to: (1) understand the extent to which IS projects are prone to escalate, (2) compare the outcomes of projects that escalate with those that do not, and (3) test whether constructs associated with different theories of escalation can be used to discriminate between projects that escalate and those that do not. A survey was administered to IS audit and control professionals and, to establish a baseline for comparison, the survey was designed to gather data on projects that did not escalate as well as those that did escalate.The results of our research suggest that between 30% and 40% of all IS projects exhibit some degree of escalation. Projects that escalated had project outcomes that were significantly worse in terms of perceived implementation performance and perceived budget/schedule performance, as compared to projects that did not escalate. Using constructs from theories that have been used to explain the escalation phenomenon, we were able to test various logistic regression models for their ability to discriminate between projects that escalate and those that do not. To construct our models, we explored constructs derived from self-justification theory, prospect theory, agency theory, and approach avoidance theory. While constructs derived from all four theories were significant in logistic regression models, the completion effect construct derived from approach avoidance theory provided the best classification of projects, correctly classifying over 70% of both escalated and non-escalated projects.
Extant explanations of why users behave in particular ways toward information technologies have tended to focus predominantly on instrumental beliefs as drivers of individual usage intentions. Prior work in individual psychology, however, suggests that holistic experiences with technology as captured in constructs such as enjoyment and flow are potentially important explanatory variables in technology acceptance theories. In this paper, we describe a multi-dimensional construct labeled cognitive absorption and defined as a state of deep involvement with software. Cognitive absorption, theorized as being exhibited through the five dimensions oftemporal dissociation, focused immersion, heightened enjoyment, control, and curiosity, is posited to be a proximal antecedent of two important beliefs about technology use: perceived usefulness and perceived ease of use. In addition, we propose that the individual traits of playfulness and personal innovativeness are important determinants of cognitive absorption. Based on the conceptual definition of this construct, operational measures for each dimension are developed. Using the World Wide Web as the target technology, scale validation indicates that the operational measures have acceptable psychometric properties and confirmatory factor analysis supports the proposed multi-dimensional structure. Structural equation analysis provides evidence for the theorized nomological net of cognitive absorption. Theoretical and practical implications are offered
While information technology (IT) has been transforming the business landscape for a long time now, it is only recently that empirical evidence demonstrating the positive impact of IT on firm performance has begun to accumulate. The strategic importance of a firm’s IT capabilities is prompting an increasing number of companies to appoint chief information officers (CIOs) to effectively manage these assets. Such moves are reflective of changes in top management thinking and policy regarding the role of IT and firms’ approaches to IT governance. This paper uses the event study methodology to examine market reactions to announcements of new CIO positions. Findings strongly support the notion that, for firms competing in industries undergoing IT-driven transformation, announcements of newly created CIO positions do indeed provoke positive reactions from the marketplace.
The IT implementation literature suggests that various implementation factors play critical roles in the success of an information system; however, there is little empirical research about the implementation of data warehousing projects. Data warehousing has unique characteristics that may impact the importance of factors that apply to it. In this study, a cross-sectional survey investigated a model of data warehousing success. Data warehousing managers and data suppliers from 111 organizations completed paired mail questionnaires on implementation factors and the success of the warehouse. The results from a Partial Least Squares analysis of the data identified significant relationships between the system quality and data quality factors and perceived net benefits. It was found that management support and resources help to address organizational issues that arise during warehouse implementations; resources, user participation, and highly-skilled project team members increase the likelihood that warehousing projects will finish on-time, on-budget, with the right functionality; and diverse, unstandardized source systems and poor development technology will increase the technical issues that project teams must overcome. The implementation’s success with organizational and project issues, in turn, influence the system quality of the data warehouse; however, data quality is best explained by factors not included in the research model.
Over 10 years ago, the issue of whether IS researchers were rigorously validating their quantitative, positivist instruments was raised (Straub 1989). In the years that have passed since that time, the profession has undergone many changes. Novel technologies and management trends have come and gone. New professional societies have been formed and grown in prominence and new demands have been placed on the field’s research and teaching obligations. But the issue of rigor in IS research has persisted throughout all such changes. Without solid validation of the instruments that are used to gather data upon which findings and interpretations are based, the very scientific basis of positivist, quantitative research is threatened. As a retrospective on the Straub article, this research seeks to determine if and how the field has advanced in instrument validation. As evidence of the change, we coded positivist, quantitative research articles in five major journals over a recent three year period for use of validation techniques. Findings suggest that the field has advanced in many areas, but, overall, it appears that a majority of published studies are still not sufficiently validating their instruments. Based on these findings, approaches are suggested for reinvigorating the quest for validation in IS research via content/construct validity, reliability, and manipulation validity.
Knowledge is a broad and abstract notion that has defined epistemological debate in western philosophy since the classical Greek era. In the past few years, however, there has been a growing interest in treating knowledge as a significant organizational resource. Consistent with the interest in organizational knowledge and knowledge management (KM), IS researchers have begun promoting a class of information systems, referred to as knowledge management systems (KMS). The objective of KMS is to support creation, transfer, and application of knowledge in organizations. Knowledge and knowledge management are complex and multi-faceted concepts. Thus, effective development and implementation of KMS requires a foundation in several rich literatures.
While technology adoption in the workplace has been studied extensively, drivers of adoption in homes have been largely overlooked. This paper presents the results of a nation-wide, two-wave, longitudinal investigation of the factors driving personal computer (PC) adoption in American homes. The findings revealed that the decisions driving adoption and non-adoption were significantly different. Adopters were driven by utilitarian outcomes, hedonic outcomes (i.e., fun), and social outcomes (i.e., status) from adoption. Non-adopters, on the other hand, were influenced primarily by rapid changes in technology and the consequent fear of obsolescence. A second wave of data collection conducted six months after the initial survey indicated an asymmetrical relationship between intent and behavior, with those who did not intend to adopt a PC following more closely with their intent than those who intended to adopt one. We present important implications for research on adoption of technologies in homes and the workplace, and also discuss challenges facing the PC industry.
We argue that because of important epistemological differences between the fields of information technology and organization studies, much can be gained from greater interaction between them. In particular, we argue that information technology research can benefit from incorporating institutional analysis from organization studies, while organization studies can benefit even more by following the lead of information technology research in taking the material properties of technologies into account. We further suggest that the transformations currently occurring in the nature of work and organizing cannot be understood without considering both the technological changes and the institutional contexts that are reshaping economic and organizational activity. Thus, greater interaction between the fields of information technology and organization studies should be viewed as more than a matter of enrichment. In the intellectual engagement of these two fields lies the potential for an important fusion of perspectives, a fusion more carefully attuned to explaining the nature and consequences of the techno-social phenomena that increasingly pervade our lives.
Many previous papers have lamented the fact that the findings of past GSS research have been inconsistent. This paper develops a new model for interpreting GSS effects on performance (a Fit-Appropriation Model), which argues that GSS performance is affected by two factors. The first is the fit between the task and the GSS structures selected for use (i.e., communication support and information processing support). The second is the appropriation support the group receives in the form of training, facilitation, and software restrictiveness to help them effectively incorporate the selected GSS structures into their meeting process. A meta-analysis using this model to organize and classify past research found that when used appropriately (i.e., there is a fit between the GSS structures and the task, and the group receives appropriation support), GSS use increased the number of ideas generated, took less time, and led to more satisfied participants than if the group worked without the GSS. Fitting the GSS to the task had the most impact on outcome effectiveness (decision quality and ideas), while appropriation support had the most impact on the process (time required and process satisfaction). We conclude that when using this theoretical lens, the results of GSS research do not appear inconsistent.
Researchers from a wide range of management areas agree that conflicts are an important part of organizational life and that their study is important. Yet, interpersonal conflict is a neglected topic in information system development (ISD). Based on definitional properties of interpersonal conflict identified in the management and organizational behavior literatures, this paper tests a model of how individuals participating in ISD projects perceive interpersonal conflict and examines the relationships between interpersonal conflict, management of the conflict, and ISD outcomes. Questionnaire data was obtained from 265 IS staff and 272 users working on 162 ISD projects. Results indicated that the construct of interpersonal conflict was reflected by three key dimensions: disagreement, interference, and negative emotion. While conflict management was found to have positive effects on ISD outcomes, it did not substantially mitigate the negative effects of interpersonal conflict on these outcomes. In other words, the impact of interpersonal conflict was perceived to be negative, regardless of how it was managed or resolved.
There are several theories available to describe how managers choose a medium for communication. However, current technology can affect not only how we communicate but also what we communicate. As a result, the issue for designers of communication support systems has become broader: how should technology be designed to make communication more effective by changing the medium and the attributes of the message itself? The answer to this question requires a shift from current preoccupations with the medium of communication to a view that assesses the balance between medium and message form. There is also a need to look more closely at the process of communication in order to identify more precisely any potential areas of computer support.This paper provides the spadework for a new model of organizational communication, and uses it to review existing research, as well as to suggest directions for future research and development. Beginning with the crucial aspects of action, relationship, and choice, an integrated model of how people communicate is developed. This model incorporates three basic factors: (1) inputs to the communication process (task, sender-receiver distance, and values and norms of communication with a particular emphasis on inter-cultural communication); (2) a cognitive-affective process of communication; and (3) the communication impact on action and relationship. The glue that bonds these factors together is a set of communication strategies aimed at reducing the complexity of communication. The model provides a balance between relationship and action, between cognition and affect, and between message and medium. Such a balance has been lacking in previous work, and we believe it reflects a more realistic picture of communication behavior in organizations. A set of propositions generated from the model sets an agenda for studying the communication process as well as its inputs and outputs. Furthermore, this knowledge of the mechanisms that guide behavior is used to demonstrate the potential for developing design principles for future communication support systems.
This paper describes how a unique type of virtual team, deploying a computer-mediated collaborative technology, developed a radically new product. The uniqueness of the team—what we call VC3 teams, for Virtual Cross-value-chain, Creative Collaborative Teams—stemmed from the fact that it was inter-organizational and virtual, and had to compete for the attention of team members who also belong to collocated teams within their own organizations. Existing research on virtual teams does not fully address the challenges of such VC3 teams. Using the case of Boeing-Rocketdyne, we describe the behavior of members of a VC3 team to derive implications for research on virtual teaming, especially for studying teams within emerging contexts such as the one we observed. The data we collected also allowed us to identify successful managerial practices and develop recommendations for managers responsible for such teams.
Organizations deploy advanced communication media such as audio and videoconferencing to enhance and extend group communication interactions. However, established groups (i.e., groups with a history of working together) can view and use the same technology differently from groups without any past experiences of working together. This study examines the relative influences of media condition and group cohesion on social presence, task participation, and group consensus. Results from a controlled laboratory experiment with 45 triads of college students working on a decision-making task showed that media condition (audio conferencing vs. desktop videoconferencing) has significantly smaller influences on social presence and task participation than group cohesion in established groups. The study found that influence of group cohesion over social presence is additive, rather than substitutive, to that of media condition. The study also established that task participation played a more important role than social presence in determining the degree of consensus among group members in computer-mediated communication environments.
Organizations have significantly increased their use of contracting in information systems (IS), hiring contractors to work with permanent professionals. Based on theories of social exchange and social comparison, we hypothesize differences in work attitudes, behaviors, and performance across the two groups, and evaluate our hypotheses with a sequential mixed-methods design. Our first study surveys contract and permanent professionals on software development teams in a large transportation company. Our second study involves in-depth interviews with contract and permanent IS professionals in three organizations. We find support for many of our hypotheses but also some surprising results. Contrary to our predictions, contractors perceive a more favorable work environment than permanent professionals but exhibit lower in-role and extra-role behaviors than their permanent counterparts. Supervisors perceive their contract subordinates as lower-performing and less loyal, obedient, and trustworthy. In-depth interviews help to explain these findings. Job design emerges as an important factor influencing contractors’ work attitudes, behaviors, and performance. Supervisors restrict the scope of contractors’ jobs, limiting their job behaviors and performance. To compensate, permanent professionals are assigned considerably enlarged job scopes, leading to their lower perceptions of the work environment. We propose a theoretical model that embraces job design in explaining differences in work outcomes for contract versus permanent professionals on software development teams. The results from our study imply that organizations should carefully design and balance the jobs of their contractors and permanent employees to improve attitudes, behaviors, and workplace performance.
This paper examines cognitive beliefs and affect influencing one’s intention to continue using (continuance) information systems (IS). Expectation-confirmation theory is adapted from the consumer behavior literature and integrated with theoretical and empirical findings from prior IS usage research to theorize a model of IS continuance. Five research hypotheses derived from this model are empirically validated using a field survey of online banking users. The results suggest that users’ continuance intention is determined by their satisfaction with IS use and perceived usefulness of continued IS use. User satisfaction, in turn, is influenced by their confirmation of expectation from prior IS use and perceived usefulness. Post-acceptance perceived usefulness is influenced by users’ confirmation level. This study draws attention to the substantive differences between acceptance and continuance behaviors, theorizes and validates one of the earliest theoretical models of IS continuance, integrates confirmation and user satisfaction constructs within our current understanding of IS use, conceptualizes and creates an initial scale for measuring IS continuance, and offers an initial explanation for the acceptance-discontinuance anomaly.
The extent of organizational innovation with information technology, an important construct in the IT innovation literature, has been measured in many different ways. Some measures have a narrow focus while others aggregate innovative behaviors across a set of innovations or stages in the assimilation lifecycle. There appear to be some significant tradeoffs involving aggregation: more aggregated measures can be more robust and generalizable and can promote stronger predictive validity, while less aggregated measures allow more context-specific investigations and can preserve clearer theoretical interpretations. This article begins with a conceptual analysis that identifies the circumstances when these tradeoffs are most likely to favor aggregated measures. It is found that aggregation should be favorable when: (1) the researcher's interest is in general innovation or a model that generalizes to a class of innovations, (2) antecedents have effects in the same direction in all assimilation stages, (3) characteristics of organizations can be treated as constant across the innovations in the study, (4) characteristics of innovations can not be treated as constant across organizations in the study, (5) the set of innovations being aggregated includes substitutes or moderate complements, and (6) sources of noise in the measurement of innovation may be present. The article then presents an empirical study using data on the adoption of software process technologies by 608 U.S. based corporations. This study—which had circumstances quite favorable to aggregation— found that aggregating across three innovations within a technology class more than doubled the variance explained compared to single innovation models. Aggregating across assimilation stages also had a slight positive effect on predictive validity. Taken together, these results provide initial confirmation of the conclusions from the conceptual analysis regarding the circumstances favoring aggregation.
This paper examines the changes engendered when moving from a structured to an object-oriented systems development approach and reconciles the differing views concerning whether this represents an evolutionary or revolutionary change. Author co-citation analysis is used to elucidate the ideational and conceptual relationships between the two approaches. The difference in conceptual distance at the analysis and design level compared to that at the programming level is explained using Henderson’s framework for organizational change. The conceptual shift during analysis and design is considered architectural, whereas for programming it is deemed merely incremental. The managerial implications of these findings are discussed and suggestions for improving the likelihood of success in the adoption of object-oriented systems development methods are provided.
Internet technologies are having a significant impact on the learning industry. For-profit organizations and traditional institutions of higher education have developed and are using web-based courses, but little is known about their effectiveness compared to traditional classroom education. Our work focuses on the effectiveness of a web-based virtual learning environment (VLE) in the context of basic information technology skills training.This article provides three main contributions. First, it introduces and defines the concept of VLE, discussing how a VLE differs from the traditional classroom and differentiating it from the related, but narrower, concept of computer aided instruction (CAI). Second, it presents a framework of VLE effectiveness, grounded in the technology-mediated learning literature, which frames the VLE research domain, and addresses the relationship between the main constructs. Finally, it focuses on one essential VLE design variable, learner control, and compares a web-based VLE to a traditional classroom through a longitudinal experimental design.Our results indicate that, in the context of IT basic skills training in undergraduate education, there are no significant differences in performance between students enrolled in the two environments. However, the VLE leads to higher reported computer self-efficacy, while participants report being less satisfied with the learning process.
Recent studies have confirmed the importance of understanding the cognition of users and information systems (IS) professionals. These works agree that organizational cognition is far too critical to be ignored as it can impact on IS outcomes. While cognition has been considered in a variety of IS contexts, no specific methodology has dominated. A theory and method suitable to the study of cognition—defined as personal constructs that individuals use to understand IT in organizations—is Kelly’s (1955) personal construct theory and its cognitive mapping tool known as the repertory grid (RepGrid). This article expounds on the potential of this technique to IS researchers by considering the variety of ways the RepGrid may be employed. The flexibility of the RepGrid is illustrated by examining published studies in IS. The diagnostic qualities of the RepGrid and its mapping outcomes can be used for practical intervention at the individual and organizational levels.
The conventional wisdom amongst information systems (IS) researchers is that information systems is an applied discipline drawing upon other, more fundamental, reference disciplines. These reference disciplines are seen as having foundational value for IS. We believe that it is time to question the conventional wisdom. We agree that many disciplines are relevant for IS researchers, but we suggest a re-think of the idea of “reference disciplines” for IS. In a sense, IS has come of age. Perhaps the time has come for IS to become a reference discipline for others.
This paper develops a perspective of interorganizational relationships based on the concept of exploitation of expertise. Insights from marketing channel theory and resource-based views of the firm are integrated to test the effects of expertise exploitation capabilities in electronic channels. The distinctiveness of this study is based on the role of information and computer technology in creating advantage through differential expertise. A model of IT-induced quasi-integration was developed and tested on a sample of 117 travel agencies targeted by American Airlines using the Sabre system and SMARTS. We find that while the degree of quasi-integration is moderately explained by the Sabre link, it is more significantly explained by American Airlines’ use of an expertise exploitation capability using SMARTS. These results show the necessity of extending the theoretical perspectives on IT-induced interorganizational relationships from an efficiency perspective to an expertise point of view.
data contained in computer-based systems, they become vulnerable to strategic information manipulation. That is, they become susceptible to situations where their decision-making behaviors can be influenced by others able to access and manipulate this data.This paper describes the results of a field experiment that examines the effects of alternative interventions aimed at inducing sensitivity to the possibility of manipulated data on professionals’ task-related decision behaviors: deception detection, false alarms, and task accuracy. While traditional training had no effect on detection success or the issuance of false alarms, warnings about data quality resulted in better detection success. Warnings combined with just-in-time training resulted in better detection success but at the cost of an increased number of false alarms. Higher levels of detection success increased task accuracy and the time spent solving each problem. A higher number of false alarms was associated with lower levels of task accuracy.
This paper presents a new representation methodology, dependency network diagrams (DNDs), which enables the essential elements governing organizational relations to be captured, communicated, and evaluated under changing conditions. By depicting important features of organizational relations, information systems can be designed explicitly for control and coordination of organizational activities. The rules and construction algorithm for DNDs are presented and applied to a case study of a Canadian automobile insurance company. Analysis of the case reveals how IT was used to create strategic change within the Canadian vehicle repair market.
There has been much debate as of late over the use of the SERVQUAL instrument to measure Information Systems service quality. Detractors argue that the difference score leads to unreliable measures and that the dimensionality and validity is erratic. Proponents argue for the diagnostic power of the gap between expectations and perceived delivery while demonstrating some empirical stability and reliability. To extend the discussion requires the examination of the instrument from the viewpoint of the information system professional. Importantly, a large variety of samples must view the instrument and measures in the same light for the instrument to have applicability. Likewise, analysis of differences between users and providers requires that both populations have similar structural views of the instrument. Empirical evidence collected from information system professionals demonstrated a structure similar to previously published studies with adequate reliability, convergent validity, and discriminant validity. The structure is the same as is found for a gap between users and IS professionals.
The global reach of the Web technological platform, along with the range of services that it supports, makes it a powerful business resource. However, realization of operational and strategic benefits is contingent on effective assimilation of this type III IS innovation. This paper draws upon institutional theory and the conceptual lens of structuring and metastructuring actions to explain the importance of three factors—top management championship, strategic investment rationale, and extent of coordination—in achieving higher levels of Web assimilation within an organization. Survey data are utilized to test a nomological network of relationships among these factors and the extent of organizational assimilation of Web technologies.
In information systems, most research on knowledge management assumes that knowledge has positive implications for organizations. However, knowledge is a double-edged sword: while too little might result in expensive mistakes, too much might result in unwanted accountability. The purpose of this paper is to highlight the lack of attention paid to the unintended consequences of managing organizational knowledge and thereby to broaden the scope of IS-based knowledge management research. To this end, this paper analyzes the IS literature on knowledge management. Using a framework developed by Deetz (1996), research articles published between 1990 and 2000 in six IS journals are classified into one of four scientific discourses. These discourses are the normative, the interpretive, the critical, and the dialogic. For each of these discourses, we identify the research focus, the metaphors of knowledge, the theoretical foundations, and the implications apparent in the articles representing it. The metaphors of knowledge that emerge from this analysis are knowledge as object, asset, mind, commodity, and discipline. Furthermore, we present a paper that is exemplary of each discourse. Our objective with this analysis is to raise IS researchers’ awareness of the potential and the implications of the different discourses in the study of knowledge and knowledge management.
This paper addresses the design problem of providing IT support for emerging knowledge processes (EKPs). EKPs are organizational activity patterns that exhibit three characteristics in combination: an emergent process of deliberations with no best structure or sequence; requirements for knowledge that are complex (both general and situational), distributed across people, and evolving dynamically; and an actor set that is unpredictable in terms of job roles or prior knowledge. Examples of EKPs include basic research, new product development, strategic business planning, and organization design. EKPs differ qualitatively from semi-structured decision making processes; therefore, they have unique requirements that are not all thoroughly supported by familiar classes of systems, such as executive information systems, expert systems, electronic communication systems, organizational memory systems, or repositories. Further, the development literature on familiar classes of systems does not provide adequate guidance on how to build systems that support EKPs. Consequently, EKPs require a new IS design theory, as explicated by Walls et al. (1992).We created such a theory while designing and deploying a system for the EKP of organization design. The system was demonstrated through subsequent empirical analysis to be successful in supporting the process. Abstracting from the experience of building this system, we developed an IS design theory for EKP support systems. This new IS design theory is an important theoretical contribution, because it both provides guidance to developers and sets an agenda for academic research. EKP design theory makes the development process more tractable for developers by restricting the range of effective features (or rules for selecting features) and the range of effective development practices to a more manageable set. EKP design theory also sets an agenda for academic research by articulating theory-based principles that are subject to empirical, as well as practical, validation.
From 1994 through 2000, Nortel Networks transformed itself from a technology-focused to an opportunity/customer-focused company. By 2000, Nortel was a profitable, innovative leader in the telecommunications industry. The change was the result of an ambitious effort to redesign its entire new product development (NPD) process such that time-to-market was significantly reduced. NPD is highly knowledge-intensive work based on the individual and collective expertise of employees. The primary focus of this case study is on Nortel’s efforts to reengineer the front-end of its NPD process and capitalize on knowledge assets. This effort was built around a process-oriented knowledge management (KM) strategy, involving a tripartite and systematic focus on process, people, and technology. Through our case analysis we develop a model of KM success by exploring Nortel’s KM strategy and the managerial, resource, and environmental factors that influenced Nortel’s success. Nortel’s experiences suggest lessons for other firms attempting to manage knowledge assets in core business processes.
Despite the wide use of reputational mechanisms such as eBay’s Feedback Forum to promote trust, empirical studies have shown conflicting results as to whether online feedback mechanisms induce trust and lead to higher auction prices. This study examines the extent to which trust can be induced by proper feedback mechanisms in electronic markets, and how some risk factors play a role in trust formation. Drawing from economic, sociological, and marketing theories and using data from both an online experiment and an online auction market, we demonstrate that appropriate feedback mechanisms can induce calculus-based credibility trust without repeated interactions between two transacting parties. Trust can mitigate information asymmetry by reducing transaction-specific risks, therefore generating price premiums for reputable sellers. In addition, the research also examines the role that trust plays in mitigating the risks inherent in transactions that involve very expensive products.
This paper focuses on cross-cultural software production and use, which is increasingly common in today’s more globalized world. A theoretical basis for analysis is developed, using concepts drawn from structuration theory. The theory is illustrated using two cross-cultural case studies. It is argued that structurational analysis provides a deeper examination of cross-cultural working and IS than is found in the current literature, which is dominated by Hofstede-type studies. In particular, the theoretical approach can be used to analyze cross-cultural conflict and contradiction, cultural heterogeneity, detailed work patterns, and the dynamic nature of culture. The paper contributes to the growing body of literature that emphasizes the essential role of cross-cultural understanding in contemporary society.
Prior theoretical research has established that many software products are subject to network effects and exhibit the characteristics of two-sided markets. However, despite the importance of the software industry to the world economy, few studies have attempted to empirically examine these characteristics, or several others which theory suggests impact software price. This study develops and tests a research-grounded model of two-sided software markets that accounts for several key factors influencing software pricing, including network externalities, cross-market complementarities, standards, mindshare, and trialability. Applying the model to the context of the market for Web server software, several key findings are offered. First, a positive market share to price relationship is identified, offering support for the network externalities hypothesis even though the market examined is based on open standards. Second, the results suggest that the market under study behaves as a two-sided market in that firms able to capture market share for one product enjoy benefits in terms of both market share and price for the complement. Third, the positive price benefits of securing consumer mindshare, of supporting dominant standards, and from offering a trial product are demonstrated. Last, a negative price shock is also identified in the period after a well-known, free-pricing rival has entered the market. Nonetheless, network effects continued to remain significant during the period. These findings enhance our understanding of software markets, offer new techniques for examining such markets, and suggest the wisdom of allocating resources to develop advantages in the factors studied.
This study uses a metatriangulation approach to explore the relationships between power and information technology impacts, development or deployment, and management or use in a sample of 82 articles from 12 management and MIS journals published between 1980 and 1999. We explore the multiple paradigms underlying this research by applying two sets of lenses to examine the major findings from our sample. The technological imperative, organizational imperative, and emergent perspectives (Markus and Robey 1988) are used as one set of lenses to better understand researchers’ views regarding the causal structure between IT and organizational power. A second set of lenses, which includes the rational, pluralist, interpretive, and radical perspectives (Bradshaw-Camball and Murray 1991), is used to focus on researchers’ views of the role of power and different IT outcomes. We apply each lens separately to describe patterns emerging from the previous power and IT studies. In addition, we discuss the similarities and differences that occur when the two sets of lenses are simultaneously applied. We draw from this discussion to develop metaconjectures, (i.e., propositions that can be interpreted from multiple perspectives), and to suggest guidelines for studying power in future research.
Requirements determination (RD) during information systems delivery (ISD) is a complex organizational endeavor, involving political, sensemaking, and communicative processes. This research draws on the analytic concept of technology frames of reference to develop a socio-cognitive process model of how frames and shifts in frame salience influence sensemaking during requirements determination. The model provides a theoretical and conceptual perspective that deepens our understanding of requirements processes in organizations and of the socio-cognitive basis of power in ISD. The paper reports on a longitudinal case study, in which four technology frame domains were identified and the influence of frames on project participants' understanding of requirements was traced through eight RD episodes. Repeated shifts in the salience of the business value of IT and IT delivery strategies frames disrupted project participants' understanding of requirements and contributed to a turbulent RD process. Analysis of frames and framing helped explain how interpretive power was exercised, yet constrained, in this project. Implications for further research and for practice are considered.
To better understand how individual differences influence the use of information technology (IT), this study models and tests relationships among dynamic, IT-specific individual differences (i.e., computer self-efficacy and computer anxiety), stable, situation-specific traits (i.e., personal innovativeness in IT) and stable, broad traits (i.e., trait anxiety and negative affectivity). When compared to broad traits, the model suggests that situation-specific traits exert a more pervasive influence on IT situation-specific individual differences. Further, the model suggests that computer anxiety mediates the influence of situation-specific traits (i.e., personal innovativeness) on computer self-efficacy. Results provide support for many of the hypothesized relationships. From a theoretical perspective, the findings help to further our understanding of the nomological network among individual differences that lead to computer self-efficacy. From a practical perspective, the findings may help IT managers design training programs that more effectively increase the computer self-efficacy of users with different dispositional characteristics.
In order to develop and bring to fruition strategic information systems (SIS) projects, chief information officers (CIOs) must be able to effectively influence their peers. This research examines the relationship between CIO influence behaviors and the successfulness of influence outcomes, utilizing a revised model initially developed by Yukl (1994). Focused interviews were first conducted with CIOs and their peers to gain insights into the phenomenon. A survey instrument was then developed and distributed to a sample of CIO and peer executive pairs to gather data with which to test a research model. A total of 69 pairs of surveys were eventually used for data analysis. The research model was found to be generally meaningful in the CIO–top management context. Furthermore, the influence behaviors rational persuasion and personal appeal exhibited significant relationships with peer commitment, whereas exchange and pressure were significantly related to peer resistance. These results provide useful guidance to CIOs who wish to propose strategic information systems to peers.
A separate and distinct interaction with both the actual e-vendor and with its IT Web site interface is at the heart of online shopping. Previous research has established, accordingly, that online purchase intentions are the product of both consumer assessments of the IT itself—specifically its perceived usefulness and ease-of-use (TAM)—and trust in the e-vendor. But these perspectives have been examined independently by IS researchers. Integrating these two perspectives and examining the factors that build online trust in an environment that lacks the typical human interaction that often leads to trust in other circumstances advances our understanding of these constructs and their linkages to behavior.Our research on experienced repeat online shoppers shows that consumer trust is as important to online commerce as the widely accepted TAM use-antecedents, perceived usefulness and perceived ease of use. Together these variable sets explain a considerable proportion of variance in intended behavior. The study also provides evidence that online trust is built through (1) a belief that the vendor has nothing to gain by cheating, (2) a belief that there are safety mechanisms built into the Web site, and (3) by having a typical interface, (4) one that is, moreover, easy to use.
This study used institutional theory as a lens to understand the factors that enable the adoption of interorganizational systems. It posits that mimetic, coercive, and normative pressures existing in an institutionalized environment could influence organizational predisposition toward an information technology-based interorganizational linkage. Survey-based research was carried out to test this theory. Following questionnaire development, validation, and pretest with a pilot study, data were collected from the CEO, the CFO, and the CIO to measure the institutional pressures they faced and their intentions to adopt financial electronic data interchange (FEDI). A firm-level structural model was developed based on the CEO’s, the CFO’s, and the CIO’s data. LISREL and PLS were used for testing the measurement and structural models respectively. Results showed that all three institutional pressures— mimetic pressures, coercive pressures, and normative pressures—had a significant influence on organizational intention to adopt FEDI. Except for perceived extent of adoption among suppliers, all other subconstructs were significant in the model. These results provide strong support for institutional-based variables as predictors of adoption intention for interorganizational linkages. These findings indicate that organizations are embedded in institutional networks and call for greater attention to be directed at understanding institutional pressures when investigating information technology innovations adoption.
Baskerville and Myers (2002) recently suggested that the information systems (IS) field has “come of age” and that it can now serve as a reference discipline for other fields. In this article, the discourse about their vision is extended by considering the potential for the IS field to contribute to new product development (NPD) research. It is argued that the rapid infusion of information technology (IT) along four dimensions of product development—process management, project management, information and knowledge management, and collaboration and communication—raises several important NPD research issues. These issues could be addressed by drawing from extant theories and models in the IS field. By employing NPD as the context, other issues that underlie the new role envisioned for the IS field are also identified.
In spite of the promise and potential of improving the way organizations develop, operate and maintain information technology (IT) applications, application service providers (ASPs) have fared poorly in terms of attracting a large client base. Anecdotal evidence in the business press points to limited satisfaction among users of ASP, which calls for an assessment of determinants of satisfaction with ASP. In this paper, we draw upon the consumer satisfaction paradigm widely employed in marketing literature to analyze post-usage satisfaction with ASP services. We develop a conceptual model of satisfaction with ASP and empirically test the predictions using data from 256 firms using ASP services. Expectations about ASP service have a significant influence on the performance evaluation of ASPs, and experience-based norms have only limited significance in explaining satisfaction with ASP. We also find empirical support for the influence of performance and disconfirmation on the satisfaction with ASP. Implications for both ASPs and organizations adopting ASP services are discussed.
The resource-based view has been proposed to investigate the impact of information technology (IT) investments on firm performance. Researchers have shown that a firm’s ability to effectively leverage its IT investments by developing a strong IT capability can result in improved firm performance. We test the robustness of this approach and examine several related issues. Our results indicate that firms with superior IT capability indeed exhibit superior current and sustained firm performance when compared to average industry performance, even after adjusting for effects of prior firm performance. However, the differences in the results from various analyses suggest that the impact of “halo effects” and prior financial performance of firms must be taken into consideration in future tests of IT capability. Further, it is critical to develop theoretically derived multidimensional measures of IT capability in order to continue to apply the RBV approach to assess the impact of IT investments on firm performance.
Information technology can facilitate the dissemination of knowledge across the organization— even to the point of making virtual teams a viable alternative to face-to-face work. However, unless managed, the combination of information technology and virtual work may serve to change the distribution of different types of knowledge across individuals, teams, and the organization. Implications include the possibility that information technology plays the role of a jealous mistress when it comes to the development and ownership of valuable knowledge in organizations; that is, information technology may destabilize the relationship between organizations and their employees when it comes to the transfer of knowledge. The paper advances theory and informs practice by illustrating the dynamics of knowledge development and transfer in more and less virtual teams.
This paper reports the results of a field study of six medical project teams that worked together in meetings over a seven-week period to develop plans to improve customer service within a hospital. Half the teams used a group support system (GSS), while the other half used traditional processes that were the habitual norms for this organization. In the teams using traditional project team processes, the leaders defined the teams’ project goal, directed discussions, recorded and controlled the teams’ notes, assigned tasks to team members, and prepared and presented the teams’ report. In the GSS teams, the leaders faced leadership challenges or abdicated, regular members participated to a greater extent, the project goal emerged from team discussion, and the teams’ notes were open and widely distributed. In short, processes in the GSS teams were more participatory and democratic. At first, teams found the GSS-based meeting processes very uncomfortable and returned to traditional verbal discussion-based processes. Once they returned to these traditional processes, however, they found them uncomfortable and moved back to include more electronic communication-based processes. Participants’ attitudes (satisfaction, perceived effectiveness, and cohesiveness) were initially lower in GSS teams, but gradually increased, until they equaled those of the traditional teams. There were significant differences in overall project outcomes: traditional teams developed conservative projects that met the unstated project agenda perceived by the team leaders. In contrast, GSS teams developed projects more closely aligned to the interests of team members.
We are concerned that the IS research community is making the discipline’s central identity ambiguous by, all too frequently, under-investigating phenomena intimately associated with IT-based systems and over-investigating phenomena distantly associated with IT-based systems. In this commentary, we begin by discussing why establishing an identity for the IS field is important. We then describe what such an identity may look like by proposing a core set of properties, i.e., concepts and phenomena, that define the IS field. Next, we discuss research by IS scholars that either fails to address this core set of properties (labeled as error of exclusion) or that addresses concepts/phenomena falling outside this core set (labeled as error of inclusion). We conclude by offering suggestions for redirecting IS scholarship toward the concepts and phenomena that we argue define the core of the IS discipline.
Agility is vital to the innovation and competitive performance of firms in contemporary business environments. Firms are increasingly relying on information technologies, including process, knowledge, and communication technologies, to enhance their agility. The purpose of this paper is to broaden understanding about the strategic role of IT by examining the nomological network of influences through which IT impacts firm performance. By drawing upon recent thinking in the strategy, entrepreneurship, and IT management literatures, this paper uses a multitheoretic lens to argue that information technology investments and capabilities influence firm performance through three significant organizational capabilities (agility, digital options, and entrepreneurial alertness) and strategic processes (capability-building, entrepreneurial action, and coevolutionary adaptation). We also propose that these dynamic capabilities and strategic processes impact the ability of firms to launch many and varied competitive actions and that, in turn, these competitive actions are a significant antecedent of firm performance. Through our theorizing, we draw attention to a significant and reframed role of IT as a digital options generator in contemporary firms.
A concept of the user is fundamental to much of the research and practice of information systems design, development, and evaluation. User-centered information studies have relied on individualistic cognitive models to carefully examine the criteria that influence the selection of information and communication technologies (ICTs) that people make. In many ways, these studies have improved our understanding of how a good information resource fits the people who use it. However, research approaches based on an individualistic user concept are limited. In this paper, we examine the theoretical constructs that shape this user concept and contrast these with alternative views that help to reconceptualize the user as a social actor. Despite pervasive ICT use, social actors are not primarily users of ICTs. Most people who use ICT applications utilize multiple applications, in various roles, and as part of their efforts to produce goods and services while interacting with a variety of other people, and often in multiple social contexts. Moreover, the socially thin user construct limits our understanding of information selection, manipulation, communication, and exchange within complex social contexts. Using analyses from a recent study of online information service use, we develop an institutionalist concept of a social actor whose everyday interactions are infused with ICT use. We then encourage a shift from the user concept to a concept of the social actor in IS research. We suggest that such a shift will sharpen perceptions of how organizational contexts shape ICT-related practices, and at the same time will help researchers more accurately portray the complex and multiple roles that people fulfill while adopting, adapting, and using information systems.
Managers in modern organizations are confronted with ever-increasing volumes of information that they must evaluate when making a decision. Data warehousing and data mining technologies have given managers a number of valuable tools that can help them store, retrieve, and analyze information contained in large databases; however, maximizing user performance with these tools remains a challenge for information systems professionals. One important and under-explored aspect of the effectiveness of these tools is the design of the query interface. In this study, we compared the use of visual and text-based interfaces on both low and high complexity tasks. Results demonstrated that decision maker performance was more accurate using the text-based interface when task complexity was low; however, decision makers using the visual interface performed better when task complexity was high. In addition, decision makers’ subjective mental workload was significantly lower when using the visual interface, regardless of task complexity. In contrast to expectations, less time was needed to make a decision on low complexity tasks when using the visual interface, but those results were reversed under conditions of high task complexity. These results have important implications for the design of managerial decision-making systems, particularly in complex decision-making environments.
To date, most research on information technology (IT) outsourcing concludes that firms decide to outsource IT services because they believe that outside vendors possess production cost advantages. Yet it is not clear whether vendors can provide production cost advantages, particularly to large firms who may be able to replicate vendors’ production cost advantages in-house. Mixed outsourcing success in the past decade calls for a closer examination of the IT outsourcing vendor’s value proposition. While the client’s sourcing decisions and the client-vendor relationship have been examined in IT outsourcing literature, the vendor’s perspective has hardly been explored. In this paper, we conduct a close examination of vendor strategy and practices in one long-term successful applications management outsourcing engagement. Our analysis indicates that the vendor’s efficiency was based on the economic benefits derived from the ability to develop a complementary set of core competencies. This ability, in turn, was based on the centralization of decision rights from a variety and multitude of IT projects controlled by the vendor. The vendor was enticed to share the value with the client through formal and informal relationship management structures. We use the economic concept of complementarity in organizational design, along with prior findings from studies of client-vendor relationships, to explain the IT vendors’ value proposition. We further explain how vendors can offer benefits that cannot be readily replicated internally by client firms.
One result of the increasing sophistication and complexity of MIS theory and research is the number of studies hypothesizing and testing for moderation effects. A review of the MIS and broader management literatures suggests researchers investigating moderated relationships often commit one or more errors falling into three broad categories: inappropriate use or interpretation of statistics, misalignment of research design with phenomena of interest, and measurement or scaling issues. Examples of nine common errors are presented. Commission of these errors is expected to yield literatures characterized by mixed results at best, and thoroughly erroneous results at worse. Procedures representing examples of best practice and reporting guidelines are provided to help MIS investigators avoid or minimize these errors.
Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.
This article reports the findings of a longitudinal study of temporary virtual teams and explores the role of behavior control on trust decline. We conducted an experiment involving 51 temporary virtual teams. Half of the teams were required to comply with behavior control mechanisms traditionally used in colocated teams. Their counterparts were allowed to self-direct.Our analysis shows that the behavior control mechanisms typically used in traditional teams have a significant negative effect on trust in virtual teams. In-depth analysis of the communication logs of selected teams reveals that trust decline in virtual teams is rooted in instances of reneging and incongruence. Behavior control mechanisms increase vigilance and make instances when individuals perceive team members to have failed to uphold their obligations (i.e., reneging and incongruence) salient. Heightened vigilance and salience increase the likelihood that team members’ failure to fulfill their obligations will be detected, thus contributing to trust decline.
Case research has commanded respect in the information systems (IS) discipline for at least a decade. Notwithstanding the relevance and potential value of case studies, this methodological approach was once considered to be one of the least systematic. Toward the end of the 1980s, the issue of whether IS case research was rigorously conducted was first raised. Researchers from our field (e.g., Benbasat et al. 1987; Lee 1989) and from other disciplines (e.g., Eisenhardt 1989; Yin 1994) called for more rigor in case research and, through their recommendations, contributed to the advancement of the case study methodology. Considering these contributions, the present study seeks to determine the extent to which the field of IS has advanced in its operational use of case study method. Precisely, it investigates the level of methodological rigor in positivist IS case research conducted over the past decade. To fulfill this objective, we identified and coded 183 case articles from seven major IS journals. Evaluation attributes or criteria considered in the present review focus on three main areas, namely, design issues, data collection, and data analysis. While the level of methodological rigor has experienced modest progress with respect to some specific attributes, the overall assessed rigor is somewhat equivocal and there are still significant areas for improvement. One of the keys is to include better documentation particularly regarding issues related to the data collection and analysis processes.
Imagine yourself spending years conducting a research project and having it published as an article in a refereed journal, only to see a plagiarized copy of the article later published in another journal. Then imagine yourself being left to fight for your rights alone, and eventually finding out that it would be very difficult to hold the plagiarist accountable for what he or she did. The recent decision by the Association of Information Systems to create a standing committee on member misconduct suggests that while this type of situation may sound outrageous, it is likely to become uncomfortably frequent in the information systems research community if proper measures are not taken by a community-backed organization. In this article, we discuss factors that can drive plagiarism, as well as potential measures to prevent it. Our goal is to discuss alternative ways in which plagiarism can be prevented and dealt with when it arises. We hope to start a debate that provides the basis on which broader mechanisms to deal with plagiarism can be established, which we envision as being associated with and complementary to the committee created by the Association for Information Systems.
Information technology (IT) innovation can be defined as the creation and new organizational application of digital computer and communication technologies. The paper suggests that IT innovation theory needs to be expanded to analyze IT innovations in kind that exhibit atypical discontinuities in IT innovation behaviors by studying two questions. First, can a model of disruptive IT innovations be created to understand qualitative changes in IT development processes and their outcomes so that they can be related to architectural discontinuities in computing capability? Second, to what extent can the observed turmoil among systems development organizations that has been spawned by Internet computing be understood as a disruptive IT innovation?To address the first question, a model of disruptive IT innovation is developed. The model defines a disruptive IT innovation as an architectural innovation originating in the information technology base that has subsequent pervasive and radical impacts on development processes and their outcomes. These base innovations establish necessary but not sufficient conditions for subsequent innovation behaviors. To address the second question, the impact of Internet computing on eight leading-edge systems development organizations in the United States and Finland is investigated. The study shows that the adoption of Internet computing in these firms has radically impacted their IT innovation both in development processes and services.
Management support is considered to be a critical factor in the successful implementation of information systems innovations. The literature suggests a complex relationship between management support and implementation success. However, the empirical literature typically hypothesizes and tests a simple main-effects model. Drawing upon the role of the institutional context and metastructuration actions, we propose a contingent model in which task interdependence moderates the effect of management support on implementation success. A meta-analysis of the empirical literature provides strong support for the model and begins to explain the wide variance in empirical findings. Implications for theory and practice are discussed.
In this paper, we examine the influence of IT strategic role to extend the findings of Im et al. (2001), Chatterjee et al. (2002) and Dos Santos et al. (1993). Specifically, we demonstrate that IT strategic role can explain how IT investments in each of the IT strategic roles might affect the firm’s competitive position and ultimately firm value. We find positive, abnormal returns to announcements of IT investments by firms making transformative IT investments, and with membership in industries with transform IT strategic roles. The results of previous research are not found to be significant when IT strategic role is included as an explanatory variable. These results provide support for the value of capturing the IT strategic role of a firm’s IT-related competitive maneuvering in studies striving to understand the conditions under which IT investments are likely to produce out-of-the-ordinary, positive returns.
Individual beliefs about technology use have been shown to have a profound impact on subsequent behaviors toward information technology (IT). This research note builds upon and extends prior research examining factors that influence key individual beliefs about technology use. It is argued that individuals form beliefs about their use of information technologies within a broad milieu of influences emanating from the individual, institutional, and social contexts in which they interact with IT. We examine the simultaneous effects of these three sets of influences on beliefs about usefulness and ease of use in the context of a contemporary technology targeted at autonomous knowledge workers. Our findings suggest that beliefs about technology use can be influenced by top management commitment to new technology and the individual factors of personal innovativeness and self-efficacy. Surprisingly, social influences from multiple sources exhibited no significant effects. Theoretical and practical implications are offered.
Supply chain management systems (SCMS) championed by network leaders in their supplier networks are now ubiquitous. While prior studies have examined the benefits to network leaders from these systems, little attention has been paid to the benefits to supplier firms. This study draws from organizational theories of learning and action and transaction cost theory to propose a model relating suppliers’ use of SCMS to benefits. It proposes that two patterns of SCMS use by suppliers—exploitation and exploration—create contexts for suppliers to make relationship-specific investments in business processes and domain knowledge. These, in turn, enable suppliers to both create value and retain a portion of the value created by the use of these systems in interfirm relationships.Data from 131 suppliers using an SCMS implemented by one large retailer support hypotheses that relationship-specific intangible investments play a mediating role linking SCMS use to benefits. Evidence that patterns of information technology use are significant determinants of relationship-specific investments in business processes and domain expertise provides a finer-grained explanation of the logic of IT-enabled electronic integration. The results support the vendors-to-partners thesis that IT deployments in supply chains lead to closer buyer-supplier relationships (Bakos and Brynjyolfsson 1993). The results also suggest the complementarity of the transaction-cost and resource-based views, elaborating the logic by which specialized assets can also be strategic assets.
This paper argues that much of the past research on electronic brainstorming has been somewhat myopic. Much as Sony focused on the quality of the picture on its Beta format, we as IS researchers have focused on the number of ideas generated as the dominant measure of electronic brainstorming effectiveness. When VHS killed Beta, Sony discovered that image quality was a secondary consideration for most VCR users. Despite the compelling research on its performance benefits, electronic brainstorming has not yet displaced—or even joined—verbal brainstorming as a widely used idea generation technique. This paper presents arguments that users may not be primarily concerned with the number of ideas generated when planning a brainstorming session, but rather may equally desire group well being and member support. We present theoretical arguments and empirical evidence suggesting that electronic brainstorming is not as effective as verbal brainstorming at providing group well being and member support. We believe that these arguments may also apply to other group and individual research areas and may also call for a reevaluation of the technology acceptance model (TAM). Finally, we suggest further research that may help electronic brainstorming avoid the fate of the Beta format.
While traditional information systems research emphasizes understanding of end users from perspectives such as cognitive fit and technology acceptance, it fails to consider the economic dimensions of their interactions with a system. When viewed as economic agents who participate in electronic markets, it is easy to see that users’ preferences, behaviors, personalities, and ultimately their economic welfare are intricately linked to the design of information systems. We use a data-driven, inductive approach to develop a taxonomy of bidding behavior in online auctions. Our analysis indicates significant heterogeneity exists in the user base of these representative electronic markets. Using online auction data from 1999 and 2000, we find a stable taxonomy of bidder behavior containing five types of bidding strategies. Bidders pursue different bidding strategies that, in aggregate, realize different winning likelihoods and consumer surplus. We find that technological evolution has an impact on bidders’ strategies. We demonstrate how the taxonomy of bidder behavior can be used to enhance the design of some types of information systems. These enhancements include developing user-centric bidding agents, inferring bidders’ underlying valuations to facilitate real-time auction calibration, and creating low-risk computational platforms for decision making.
Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.
Information systems researchers have a long tradition of drawing on theories from disciplines such as economics, computer science, psychology, and general management and using them in their own research. Because of this, the information systems field has become a rich tapestry of theoretical and conceptual foundations. As new theories are brought into the field, particularly theories that have become dominant in other areas, there may be a benefit in pausing to assess their use and contribution in an IS context. The purpose of this paper is to explore and critically evaluate use of the resource-based view of the firm (RBV) by IS researchers. The paper provides a brief review of resource-based theory and then suggests extensions to make the RBV more useful for empirical IS research. First, a typology of key IS resources is presented, and these are then described using six traditional resource attributes. Second, we emphasize the particular importance of looking at both resource complementarity and moderating factors when studying IS resource effects on firm performance. Finally, we discuss three considerations that IS researchers need to address when using the RBV empirically. Eight sets of propositions are advanced to help guide future research.
User beliefs and attitudes are key perceptions driving information technology usage. These perceptions, however, may change with time as users gain first-hand experience with IT usage, which, in turn, may change their subsequent IT usage behavior. This paper elaborates how users’ be liefs and attitudes change during the course of their IT usage, defines emergent constructs driving such change, and proposes a temporal model of belief and attitude change by drawing on expectation-disconfirmation theory and the extant IT usage literature. Student data from two longitudinal studies in end-user computing (computer-based training system usage) and system development (rapid application development software usage) contexts provided empirical support for the hypothesized model, demonstrated its generalizability across technologies and usage contexts, and allowed us to probe context-specific differences. Content analysis of qualitative data validated some of our quantitative results. We report that emergent factors such as disconfirmation and satisfaction are critical to understanding changes in IT users’ beliefs and attitudes and recommend that they be included in future process models of IT usage.
Despite the importance to researchers, managers, and policy makers of how information technology (IT) contributes to organizational performance, there is uncertainty and debate about what we know and don’t know. A review of the literature reveals that studies examining the association between information technology and organizational performance are divergent in how they conceptualize key constructs and their interrelationships. We develop a model of IT business value based on the resource-based view of the firm that integrates the various strands of research into a single framework. We apply the integrative model to synthesize what is known about IT business value and guide future research by developing propositions and suggesting a research agenda. A principal finding is that IT is valuable, but the extent and dimensions are dependent upon internal and external factors, including complementary organizational resources of the firm and its trading partners, as well as the competitive and macro environment. Our analysis provides a blueprint to guide future research and facilitate knowledge accumulation and creation concerning the organizational performance impacts of information technology.
Most of the recent research in data visualization has focused on technical and aesthetic issues involved in the manipulation of graphs, specifically on features that facilitate data exploration to make graphs interactive and dynamic. The present research identifies a gap in the existing knowledge of graph construction, namely potential problems in both 3D and 2D graphs that will impede comprehension of information when three or more variables are used in a graphical representation. Based on theories regarding perceptual issues of graph construction (Bertin 1981; Pinker 1991), we evaluate specific cases where 3D graphs may outperform 2D graphs, and vice-versa. Two experiments have been conducted to test these hypotheses, and 3D graphs have been found to consistently outperform 2D graphs in all of our experimental scenarios. A third experiment has been conducted to identify situations where 2D graphs might perform at least as well as 3D graphs, but its results suggest that 3D graphs outperform 2D graphs even for simple tasks, thus leading to the conclusion that 3D graphs perform better than 2D graphs under all task conditions with more than two variables.
This article examines the relationship between interpersonal trust and virtual collaborative relationship (VCR) performance. Findings from a study of 10 operational telemedicine projects in health care delivery systems are presented. The results presented here confirm, extend, and apparently contradict prior studies of interpersonal trust. Four types of interpersonal trust—calculative, competence, relational, and integrated—are identified and operationalized as a single construct. We found support for an association between calculative, competence, and relational interpersonal trust and performance. Our finding of a positive association between integrated interpersonal trust and performance not only yields the strongest support for a relationship between trust and VCR performance but also contradicts prior research. Our findings indicate that the different types of trust are interrelated in that positive assessments of all three types of trust are necessary if VCRs are to have strongly positive performance. The study also established that if any one type of trust is negative, then it is very likely that VCR performance will not be positive. Our findings indicate that integrated types of interpersonal trust are interdependent, and the various patterns of interaction among them are such that they are mutually reinforcing. These interrelationships and interdependencies of the different types of interpersonal trust must be taken into account by researchers as they attempt to understand the impact of trust on virtual collaborative relationship performance.
In this paper we argue that a large gray area of information systems research exists, whose relevance to the information technology artifact is subject to significant debate even among IS scholars who support the essential role of the IT artifact. As we explain, not explicitly addressing this gray area can have negative, although often inadvertent, effects on the innovative nature of IS research; we explore this danger through three pitfalls. We then propose a stance of strategic ambiguity to deal with the gray area. Strategic ambiguity calls for deliberately withholding judgment on the relevance of research in the gray area and acceptance of gray-area research provided it meets the excellence required by professional journals. We believe that strategic ambiguity benefits innovative IS research without harming the essential role of the IT artifact.
Customer-centric Web-based systems, such as e-commerce Web sites, or sites that support customer relationship management (CRM) activities, are themselves information systems, but their design and maintenance need to follow vastly different approaches from the traditional systems lifecycle approach. Based on marketing frameworks that are applicable to the online world, and following design science principles, we develop a model to guide the design and the continuous management of such sites. The model makes extensive use of current technologies for tracking the customers and their behaviors, and combines elements of data mining and statistical analyses. A case study based on a financial services Web site is used to provide a preliminary validation and design evaluation of our approach. The case study showed considerable measured improvement in the effectiveness of the company’s Web site. In addition, it also highlighted an important benefit of the our approach: the identification of previously unknown or unexpected segments of visitors. This finding can lead to promising new business opportunities.
Past literature recognizes the power of information technology (IT) to establish greater transparency and in turn the potential for greater control. Theoretical perspectives such as informating and agency theory describe situations whereby legitimized management authority can control goal divergence by implementing information systems to better monitor agents’ behavior and outcomes. But what happens when the principal does not possess legitimacy to impose an agent’s use of information and/or behavioral conformance? This study investigates this situation. Through an action research project, a physicians’ profiling system (PPS) was used to monitor and benchmark physicians’ clinical practices and outcomes resulting in changed practice behaviors in closer congruence with management’s goals.The PPS project represents a successful attempt of a hospital’s management (principal) to "informate the clan" of physicians (agents) to reduce clinical procedural costs and adopt practices benchmarked to produce better outcomes. This research moves beyond directly controlling informated workers through legitimized managerial authority to a better understanding of how to informate autonomous professionals. Emerging insights suggest that a clan can be informated if the principal can improve the perceived legitimacy of the information (the message), legitimize the technical messenger (customized user interface), legitimize the human messenger (boundary spanners and influential clan members), and facilitate an environment where clan-based discussion, using the information provided by the principal, is incorporated into the process of concertive control.
Our paper is motivated by one simple question: Why do so many action research efforts fail to persist over time? We approach this question, the problem of sustainability, building on a perspective on action research identifying the pivotal importance of networks. More precisely, local action research interventions need to be conceptualized and approached as but one element in a larger network of action in order to ensure sustainability. A vital aspect of our perspective is that local interventions depend heavily on the support of similar action research efforts in other locations. This is essential for the necessary processes of learning and experience sharing. We suggest that the scaling (i.e., spreading) of intervention is a prerequisite, not a luxury, for sustainable action research. Empirically, we base our analysis on an ongoing, large-scale action research project within the health care sector (called HISP) in a number of developing countries. HISP provides a fruitful occasion to investigate key criteria for our approach to action research, namely sustainability, scalability, and capacity to be politically relevant to the participants. We contribute to three discourses: (1) models of action research, (2) lessons for health information systems in developing countries, and (3) more generally, IS implementations that are dispersed, large-scale, and have scarce resources.
In dialogical action research, the scientific researcher does not “speak science” or otherwise attempt to teach scientific theory to the real-world practitioner, but instead attempts to speak the language of the practitioner and accepts him as the expert on his organization and its problems. Recognizing the difficulty that a practitioner and a scientific researcher can have in communicating across the world of science and the world of practice, dialogical action research offers, as its centerpiece, reflective one-on-one dialogues between the practitioner and the scientific researcher, taking place periodically in a setting removed from the practitioner’s organization. The dialogue itself serves as the interface between the world of science, marked by theoria and the scientific attitude, and the world of the practitioner, marked by praxis and the natural attitude of everyday life. The dialogue attempts to address knowledge heterogeneity, which refers to the different forms that knowledge takes in the world of science and the world of practice, and knowledge contextuality, which refers to the dependence of the meaning of knowledge, such as a scientific theory or professional expertise, on its context. In successive dialogues, the scientific researcher and the practitioner build a mutual understanding, including an understanding of the organization and its problems. The scientific researcher, based on one or more of the scientific theories in her discipline, formulates and suggests one or more actions for the practitioner to take in order to solve or remedy a problem in his organization. Dialogical action research recognizes that the practitioner’s experience, expertise, and tacit knowledge, or praxis, largely shapes how he understands the suggested actions and appropriates them as his own. Upon returning to his organization, he takes one or more of the suggested actions, depending on his reading of the situation at hand. The reactions or responses of the problem to the actions or stimuli of the practitioner would embody, in the practitioner’s eyes, success or failure in solving or remedying the problem and, in the scientific researcher’s eyes, evidence confirming or disconfirming the theory on which the action was based. The scientific researcher may then suggest, based on her theories, additional actions, hence initiating another cycle of action and learning. To illustrate dialogical action research, this paper reconstructs some dialogues between an information systems researcher and a managing director at a European company called Omega Corporation.
While many large businesses start out as a small enterprise, remarkably little is known about how an organization actually changes internally during the periods of growth. Small business growth is known to strain internal communication processes, for example, which likely limits growth opportunities. Information systems are often called upon to remedy such deficiencies. Through a participatory action research project, we investigated the ways in which a small business management team developed an IS-enabled solution to address their growth needs. During the progression of the project, a new outcome of organizational effectiveness, internal transparency, was identified and developed. Adopting a punctuated equilibrium perspective, a theoretical process model is proposed that sheds light on a relationship between internal transparency, small business growth, and IS. The paper concludes with observations that internal transparency may well be a concept that offers significant potential for MIS research as well as a discussion about the applicability and credibility of participatory action research for this project.
Even though the literature on competence in organizations recognizes the need to align organization level core competence with individual level job competence, it does not consider the role of information technology in managing competence across the macro and micro levels. To address this shortcoming, we embarked on an action research study that develops and tests design principles for competence management systems. This research develops an integrative model of competence that not only outlines the interaction between organizational and individual level competence and the role of technology in this process, but also incorporates a typology of competence (competence-in-stock, competence-in-use, and competence-in-the-making). Six Swedish organizations participated in our research project, which took 30 months and consisted of two action research cycles involving numerous data collection strategies and interventions such as prototypes. In addition to developing a set of design principles and considering their implications for both research and practice, this article includes a self-assessment of the study by evaluating it according to the criteria for canonical action research.
Many software organizations engage in software process improvement (SPI) initiatives to increase their capability to develop quality solutions at a competitive level. Such efforts, however, are complex and very demanding. A variety of risks makes it difficult to develop and implement new processes.We studied SPI in its organizational context through collaborative practice research (CPR), a particular form of action research. The CPR program involved close collaboration between practitioners and researchers over a three-year period to understand and improve SPI initiatives in four Danish software organizations. The problem of understanding and managing risks in SPI teams emerged in one of the participating organizations and led to this research. We draw upon insights from the literature on SPI and software risk management as well as practical lessons learned from managing SPI risks in the participating software organizations.Our research offers two contributions. First, we contribute to knowledge on SPI by proposing an approach to understand and manage risks in SPI teams. This risk management approach consists of a framework for understanding risk areas and risk resolution strategies within SPI and a related process for managing SPI risks. Second, we contribute to knowledge on risk management within the information systems and software engineering disciplines. We propose an approach to tailor risk management to specific contexts. This approach consists of a framework for understanding and choosing between different forms of risk management and a process to tailor risk management to specific contexts.
This interpretive case study examines knowledge brokering as an aspect of the work of information technology professionals. The purpose of this exploratory study is to understand knowledge brokering from the perspective of IT professionals as they reflect upon their work practice. As knowledge brokers, IT professionals see themselves as facilitating the flow of knowledge about both IT and business practices across the boundaries that separate work units within organizations. A qualitative analysis of interviews conducted with 23 IT professionals and business users in a large manufacturing and distribution company is summarized in a conceptual framework showing the conditions, practices, and consequences of knowledge brokering by IT professionals. The framework suggests that brokering practices are conditioned by structural conditions, including decentralization and a federated IT management organization, and by technical conditions, specifically shared IT systems that serve as boundary objects. Brokering practices include gaining permission to cross organizational boundaries, surfacing and challenging assumptions made by IT users, translation and interpretation, and relinquishing ownership of knowledge. Consequences of brokering are the transfer of both business and IT knowledge across units in the organization.
This research aims at improving our understanding of the concept of business competence of information technology professionals and at exploring the contribution of this competence to the development of partnerships between IT professionals and their business clients. Business competence focuses on the areas of knowledge that are not specifically IT-related. At a broad level, it comprises the organization-specific knowledge and the interpersonal and management knowledge possessed by IT professionals. Each of these categories is in turn inclusive of more specific areas of knowledge. Organizational overview, organizational unit, organizational responsibility, and IT–business integration form the organization-specific knowledge, while interpersonal communication, leadership, and knowledge networking form the interpersonal and management knowledge. Such competence is hypothesized to be instrumental in increasing the intentions of IT professionals to develop and strengthen the relationship with their clients. The first step in the study was to develop a scale to measure business competence of IT professionals. The scale was validated, and then used to test the model that relates competence to intentions to form IT-business partnerships. The results support the suggested structure for business competence and indicate that business competence significantly influences the intentions of IT professionals to develop partnerships with their business clients.
Many traditional organizations have undertaken major initiatives to leverage the Internet to transform how they coordinate value activities with customers, suppliers, and other business partners with the objective of improving firm performance. This paper addresses processes through which business value is created through such Internet-enabled value chain activities. Relying on the resource-based view of the firm, we propose a model positing that a firm’s abilities to coordinate and exploit firm resources (processes, information technology, and readiness of customers and suppliers) create online informational capabilities (a higher order resource) which then leads to improved operational and financial performance. The outcome of a firm’s online informational capabilities is reflected in superior operational performance through customer and supplier-side digitization efforts, which reflect the extent to which transactions and external interactions occur electronically. We also hypothesize that increased customer and supplier-side digitization leads to better financial performance. The model is tested with data from over 1,000 firms in the manufacturing, retail, and wholesale sectors. The analysis suggests that while most firms are lagging in their supplier-side initiatives relative to the customer-side, supplier-side digitization has a strong positive impact on customer-side digitization, which, in turn, leads to better financial performance. Further, both customer and supplier readiness to engage in digital interactions are shown to be as important as a firm’s internal digitization initiatives, implying that a firm’s transformation-related decisions should include its customers’ and suppliers’ resources and incentives.
Electronic brainstorming (EBS) applications and their methodologies may have achieved the benchmark of enabling interactive users to perform as well as nominal groups. The current challenge is to view this as a plateau and not an endpoint, and to seek ways of improving EBS performance. In this study, we apply theory from cognitive psychology and adopt the individual, rather than the group, as the unit of analysis. We present a model of idea generation cognition based on Hintzman’s MINERVA2 global matching model of memory cognition and additional literature from cognitive psychology on cueing and categorization. Based on this model, we present a technique called cause cueing, which directs subjects’ attention to the causes of the target problem that they themselves have identified, and hypothesize that this will increase the number of ideas that an individual generates. Also based on the model— and consistent with current views on production blocking—we hypothesize that receiving input from others during brainstorming will reduce the number of ideas that an individual generates. Following is an EBS-based study that offers an empirical examination of (1) the effects of cueing attention to natural categories (causes) during idea generation and (2) the effects of cueing attention to ad hoc categories, represented by input from others, during idea generation. A total of 82 subjects were randomly assigned to one of four conditions in a 2 × 2 factorial design ANOVA experiment. Results indicate strong support for our model of idea generation as memory cognition. Cueing participant attention to natural self-generated search categories via the cause cueing technique greatly increased the generation of ideas and high quality ideas, whether or not participants were also cued to attend to ad hoc categories (input from others). Cueing attention to input from others was detrimental to the generation of ideas and the number of high quality ideas, clearly diminishing the positive effects of cueing to natural categories. We explain how our theorizing and results are consistent with and extend earlier production blocking orientations in EBS research. We also examine limitations of current EBS designs and suggest how prevailing methodologies can be modified to better support idea generation cognition.
The short history of Information Systems suggests persistent anxiety about the field’s purported lack of academic legitimacy. A common refrain in the anxiety discourse is that legitimacy can be obtained only by creating a strong theoretic core for the field. This essay takes exception with this view, attributing the anxiety to the field’s relative youth, its focus on technology in a technophobic institutional environment, and academic ethnocentrism within and without the field. While developing stronger theory might be helpful, it is more important that the IS field pushes back against the hegemony of IS critics outside the field whose arguments masquerade as concerns about academic quality. The anxiety discourse should be replaced by the IS field’s aggressive pursuit of new instructional and research opportunities that cross traditional institutional barriers and the pursuit of excellence on academic criteria deemed important by the field itself.
Although organizational innovation with information technology is often carefully considered, bandwagon phenomena indicate that much innovative behavior may nevertheless be of the “me too” variety. In this essay, we explore such differences in innovative behavior. Adopting a perspective that is both institutional and cognitive, we introduce the notion of mindful innovation with IT. A mindful firm attends to an IT innovation with reasoning grounded in its own organizational facts and specifics. We contrast this with mindless innovation, where a firm’s actions betray an absence of such attention and grounding. We develop these concepts by drawing on the recent appearance of the idea of mindfulness in the organizational literature, and adapting it for application to IT innovation. We then bring mindfulness and mindlessness together in a larger theoretical synthesis in which these apparent opposites are seen to interact in ways that help to shape the overall landscape of opportunity for organizational innovation with IT. We conclude by suggesting several promising new research directions.
This paper studies the differences in user acceptance models for productivity-oriented (or utilitarian) and pleasure-oriented (or hedonic) information systems. Hedonic information systems aim to provide self-fulfilling rather than instrumental value to the user, are strongly connected to home and leisure activities, focus on the fun-aspect of using information systems, and encourage prolonged rather than productive use. The paper reports a cross-sectional survey on the usage intentions for one hedonic information system. Analysis of this sample supports the hypotheses that perceived enjoyment and perceived ease of use are stronger determinants of intentions to use than perceived usefulness. The paper concludes that the hedonic nature of an information system is an important boundary condition to the validity of the technology acceptance model. Specifically, perceived usefulness loses its dominant predictive value in favor of ease of use and enjoyment.
Electronic networks of practice are computer-mediated discussion forums focused on problems of practice that enable individuals to exchange advice and ideas with others based on common interests. However, why individuals help strangers in these electronic networks is not well understood: there is no immediate benefit to the contributor, and free-riders are able to acquire the same knowledge as everyone else. To understand this paradox, we apply theories of collective action to examine how individual motivations and social capital influence knowledge contribution in electronic networks. This study reports on the activities of one electronic network supporting a professional legal association. Using archival, network, survey, and content analysis data, we empirically test a model of knowledge contribution. We find that people contribute their knowledge when they perceive that it enhances their professional reputations, when they have the experience to share, and when they are structurally embedded in the network. Surprisingly, contributions occur without regard to expectations of reciprocity from others or high levels of commitment to the network.
Organizations are attempting to leverage their knowledge resources by employing knowledge management (KM) systems, a key form of which are electronic knowledge repositories (EKRs). A large number of KM initiatives fail due to the reluctance of employees to share knowledge through these systems. Motivated by such concerns, this study formulates and tests a theoretical model to explain EKR usage by knowledge contributors. The model employs social exchange theory to identify cost and benefit factors affecting EKR usage, and social capital theory to account for the moderating influence of contextual factors. The model is validated through a large-scale survey of public sector organizations. The results reveal that knowledge self-efficacy and enjoyment in helping others significantly impact EKR usage by knowledge contributors. Contextual factors (generalized trust, pro-sharing norms, and identification) moderate the impact of codification effort, reciprocity, and organizational reward on EKR usage, respectively. It can be seen that extrinsic benefits (reciprocity and organizational reward) impact EKR usage contingent on particular contextual factors whereas the effects of intrinsic benefits (knowledge self-efficacy and enjoyment in helping others) on EKR usage are not moderated by contextual factors. The loss of knowledge power and image do not appear to impact EKR usage by knowledge contributors. Besides contributing to theory building in KM, the results of this study inform KM practice.
We adopt a systems perspective to explore the challenges that organizations face in harnessing knowledge. Such a perspective draws attention to mutually causal processes that have the potential to generate both vicious and virtuous circles. Based on a longitudinal study at Infosys Technologies, we conclude that knowledge management involves more than just the sponsorship of initiatives at and across different organizational levels. It also involves an active process of steering around and out of vicious circles that will inevitably emerge.
The need for continual value innovation is driving supply chains to evolve from a pure transactional focus to leveraging interorganizational partner ships for sharing information and, ultimately, market knowledge creation. Supply chain partners are (1) engaging in interlinked processes that enable rich (broad-ranging, high quality, and privileged) information sharing, and (2) building information technology infrastructures that allow them to process information obtained from their partners to create new knowledge. This study uncovers and examines the variety of supply chain partnership configurations that exist based on differences in capability platforms, reflecting varying processes and information systems. We use the absorptive capacity lens to build a conceptual framework that links these configurations with partner-enabled market knowledge creation. Absorptive capacity refers to the set of organizational routines and processes by which organizations acquire, assimilate, transform, and exploit knowledge to produce dynamic organizational capabilities.Through an exploratory field study conducted in the context of the RosettaNet consortium effort in the IT industry supply chain, we use cluster analysis to uncover and characterize five supply chain partnership configurations (collectors, connectors, crunchers, coercers, and collaborators). We compare their partner-enabled knowledge creation and operational efficiency, as well as the shortcomings in their capability platforms and the nature of information exchange. Through the characterization of each of the configurations, we are able to derive research propositions focused on enterprise absorptive capacity elements. These propositions provide insight into how partner-enabled market knowledge creation and operational efficiency can be affected, and highlight the interconnected roles of coordination information and rich information. The paper concludes by drawing implications for research and practice from the uncovering of these configurations and the resultant research propositions. It also highlights fertile opportunities for advances in research on knowledge management through the study of supply chain contexts and other interorganizational partnering arrangements.
Individuals’ knowledge does not transform easily into organizational knowledge even with the implementation of knowledge repositories. Rather, individuals tend to hoard knowledge for various reasons. The aim of this study is to develop an integrative understanding of the factors supporting or inhibiting individuals’ knowledge-sharing intentions. We employ as our theoretical framework the theory of reasoned action (TRA), and augment it with extrinsic motivators, social-psychological forces and organizational climate factors that are believed to influence individuals’ knowledge- sharing intentions.Through a field survey of 154 managers from 27 Korean organizations, we confirm our hypothesis that attitudes toward and subjective norms with regard to knowledge sharing as well as organizational climate affect individuals’ intentions to share knowledge. Additionally, we find that anticipated reciprocal relationships affect individuals’ attitudes toward knowledge sharing while both sense of self-worth and organizational climate affect subjective norms. Contrary to common belief, we find anticipated extrinsic rewards exert a negative effect on individuals’ knowledge-sharing attitudes.
Enterprise resource planning (ERP) systems and other complex information systems represent critical organizational resources. For such systems, firms typically use consultants to aid in the implementation process. Client firms expect consultants to transfer their implementation knowledge to their employees so that they can contribute to successful implementations and learn to maintain the systems independent of the consultants. This study examines the antecedents of knowledge transfer in the context of such an interfirm complex information systems implementation environment. Drawing from the knowledge transfer, information systems, and communication literatures, an integrated theoretical model is developed that posits that knowledge transfer is influenced by knowledge-related, motivational, and communication-related factors. Data were collected from consultant-and-client matched-pair samples from 96 ERP implementation projects. Unlike most prior studies, a behavioral measure of knowledge transfer that incorporates the application of knowledge was used. The analysis suggests that all three groups of factors influence knowledge transfer, and provides support for 9 of the 13 hypotheses. The analysis also confirms two mediating relationships. These results (1) adapt prior research, primarily done in non-IS contexts, to the ERP implementation context, (2) enhance prior findings by confirming the significance of an antecedent that has previously shown mixed results, and (3) incorporate new IS-related constructs and measures in developing an integrated model that should be broadly applicable to the interfirm IS implementation context and other IS situations. Managerial and research implications are discussed.
This paper investigates how an organizational competence in boundary spanning emerges in practice by drawing on the concepts of boundaryspanner and boundary object. Using data from two qualitative field studies, we argue that in order for boundary spanning to emerge a new joint field of practice must be produced. Our data illustrate that some agents partially transform their practices in local settings so as to accommodate the interests of their counterparts. While negotiating the new joint field, these agents become what we call boundary spanners-in-practice who produce and use objects which become locally useful and which acquire a common identity—hence, boundary objects-in-use. Moreover, we show how boundary spanners-in-practice use various organizational and professional resources including the influence that comes with being nominated to boundary spanners’ roles to create the new joint field. The conditions necessary for boundary spanners-in-practice to emerge are outlined and discussed, as are important implications for IS implementation and use.
Business value of information technology is an enduring research question. The elusive link between IT and financial firm performance calls for further research into intermediate organizational variables through which IT may influence firm performance. This study proposes that knowledge management (KM) is a critical organizational capability through which IT influences firm performance. In the context of multibusiness firms, the study examines how the IT resources of a firm should be organized and managed to enhance the firm’s KM capability, and whether and how KM capability influences firm performance. The study develops two hypothesizes: (1) IT relatedness of the firm’s business units enhances cross-unit KM capability; (2) KM capability, in turn, leads to superior firm performance. Data from 250 Fortune 1000 firms provide empirical support for these hypotheses. IT relatedness of business units enhances the cross-unit KM capability of the firm. The KM capability creates and exploits cross-unit synergies from the product, customer, and managerial knowledge resources of the firm. These synergies increase the financial performance of the firm. IT relatedness also has significant indirect effects on firm performance through the mediation of KM capability.
Knowledge management systems (KMSs) facilitate the efficient and effective sharing of a firm’s intellectual resources. However, sifting through the myriad of content available through KMSs can be challenging, and knowledge workers may be overwhelmed when trying to find the content most relevant for completing a new task. To address this problem, KMS designers often include content rating schemes (i.e., users of the KMS submit ratings to indicate the quality of specific content used) and credibility indicators (indicators describing the validity of the content and/or the ratings) to improve users’ search and evaluation of KMS content. This study examines how content ratings and credibility indicators affect KMS users’ search and evaluation processes and decision performance (how well and how quickly users selected alternatives offered by the KMS). Four interrelated laboratory experiments provide evidence that ratings have a strong influence on KMS search and evaluation processes, which in turn affects decision performance. Finally, this study demonstrates that certain credibility indicators can moderate the relationship between rating validity and KMS content search and evaluation processes.
Advances in information technologies and the growth of a knowledge-based service economy are transforming the basis of technological innovation and corporate competition. This transformation requires taking a broader, institutional and political view of information technology and knowledge management. To succeed, firms are advised to focus on building their distinctive competencies, outsource the rest, and become nodes in value chain networks. This shifts the level of competition from between individual firms to between networks of firms. In these networks, individual firms or entrepreneurs seldom have the resources, power, or legitimacy to produce change alone. As a result, “running in packs” is often more successful than “going it alone” to develop and commercialize knowledge-intensive technologies. Many different actors in public and private sectors make important contributions. These actors do not play impartial roles; instead, they are active participants who become embroiled in diverse, partisan, and embedded issues of innovation development. In this setting, success requires not only technical and rational competence, but also political savvy to understand and mobilize the interests of other players with stakes in an emerging industry.
An enterprise information portal (EIP) is viewed as a knowledge community. Activity theory provides a framework to study such a community: members of an EIP conduct specific tasks that are assigned through a division of labor. Each member of an enterprise information portal can undergo three distinct types of learning processes: learning-by-investment, learning-by-doing, and learning-from-others. Through these three types of learning processes, each member achieves specialized knowledge that is related to his or her own task. Cumulative knowledge resulting from the learning processes is considered in terms of two distinct attributes: depth and breadth of knowledge. This paper formulates a mathematical model and defines the goal of an EIP member as maximizing the net benefits of knowledge resulting from individual investment and effort. Numerical examples are provided to analyze patterns of optimal investment and effort plans as well as the resulting accumulated knowledge. The results provide useful managerial implications. In business conditions characterized by high interest rates or high internal rate of returns, it is preferable for members to delay spending their resources for learning. Intensive investment and efforts to obtain knowledge are preferable when the discount rate of costs is high, when knowledge is durable, when the value of knowledge is high, when the initial level of knowledge is high, when the productivity of the learning process is high, and when sufficient knowledge is transferred from other members. On the other hand, the size of the EIP has a positive or negative effect depending on the attribute of knowledge and the productivity of learning processes. Further properties of the optimal decisions and learning processes are analyzed and discussed.
To maintain competitive advantage, a firm's investment decisions related to knowledge creation are likely to be strategic in nature. However, strategic investments usually have an element of risk linked to uncertain and deferred investment benefits. To date, such investment decisions relating to knowledge workers have not been extensively researched. In this paper, we explore the following research question: How do we strategically assess knowledge creation over time giving consideration to complex decision criteria in order to improve organizational value? We develop a model based on economic and organization theory for assessing organizational value with regard to knowledge creation investments. Our model prototype provides managers with a learning tool relating to the timing and selection of knowledge creation investments. Our own use of the tool in simulation experiments yielded several insights which suggest that the decisions typically made by managers may dilute knowledge creation investments. Our results demonstrate that the organizational benefit of knowledge creation processes should be well aligned with near-term tasks. Under instances of high knowledge depreciation, however, it is unlikely that individual workers can optimize knowledge creation process decisions without organizational involvement in matching skills to task complexities. The organizational benefits of consistent and frequent knowledge creation process participation increase over time as the match of skills and task complexities improve.
The shift to more distributed forms of organizations and the prevalence of interorganizational relationships have led to an increase in the transfer of knowledge between parties with asymmetric and incomplete information about each other. Because of this asymmetry and incompleteness, parties seeking knowledge may not be able to identify qualified knowledge providers, and the appropriate experts may fail to be motivated to engage in knowledge transfer.We propose a sender–receiver framework for studying knowledge transfer under asymmetric and/or incomplete information. We outline four types of information structures for knowledge transfer, and focus on the sender-advantage asymmetric information structure and the symmetric incomplete information structure. We develop formal game-theoretical models, show how information incompleteness and asymmetry may negatively influence knowledge transfer, and propose solutions to alleviate these negative impacts. Implications for knowledge transfer research and practice are also discussed.
Individual adoption of technology has been studied extensively in the workplace. Far less attention has been paid to adoption of technology in the household. In this paper, we performed the first quantitative test of the recently developed model of adoption of technology in households (MATH). Further, we proposed and tested a theoretical extension of MATH by arguing that key demographic characteristics that vary across different life cycle stages would play moderating roles. Survey responses were collected from 746 U.S. households that had not yet adopted a personal computer. The results showed that the integrated model, including MATH constructs and life cycle characteristics, explained 74 percent of the variance in intention to adopt a PC for home use, a significant increase over baseline MATH that explained 50 percent of the variance. Finally, we compared the importance of various factors across household life cycle stages and gained a more refined understanding of the moderating role of household life cycle stage.
This paper defines user adaptation as the cognitive and behavioral efforts performed by users to cope with significant information technology events that occur in their work environment. Drawing on coping theory, we posit that users choose different adaptation strategies based on a combination of primary appraisal (i.e., a user’s assessment of the expected consequences of an IT event) and secondary appraisal (i.e., a user’s assessment of his/her control over the situation). On that basis, we identify four adaptation strategies (benefits maximizing, benefits satisficing, disturbance handling, and self-preservation) which are hypothesized to result in three different individual-level outcomes: restoring emotional stability, minimizing the perceived threats of the technology, and improving user effectiveness and efficiency. A study of the adaptation behaviors of six account managers in two large North American banks provides preliminary support for our model. By explaining adaptation patterns based on users’ initial appraisal and subsequent responses to an IT event, our model offers predictive power while retaining an agency view of user adaptation. Also, by focusing on user cognitive and behavioral adaptation responses related to the technology, the work system, and the self, our model accounts for a wide range of user behaviors such as technology appropriation, avoidance, and resistance.
This paper presents an alternative view of the Information Systems identity crisis described recently by Benbasat and Zmud (2003). We agree with many of their observations, but we are concerned with their prescription for IS research. We critique their discussion of errors of inclusion and exclusion in IS research and highlight the potential misinterpretations that are possible from a literal reading of their comments. Our conclusion is that following Benbasat and Zmud’s nomological net will result in a micro focus for IS research. The results of such a focus are potentially dangerous for the field. They could result in the elimination of IS from many academic programs. We present an alternative set of heuristics that can be used to assess what lies within the domain of IS scholarship. We argue that the IS community has a powerful story to tell about the transformational impact of information technology. We believe that a significant portion of our research should be macro studies of the impact of IT. It is important for academic colleagues, deans, and managers to understand the transformational power of the technology. As IS researchers with deep knowledge of the underlying artifact, we are best positioned to do such research.
For the last 25 years, organizations have invested heavily in information technology to support their work processes. In today’s organizations, intra- and interorganizational work systems are increasingly IT-enabled. Available evidence, however, suggests the functional potential of these installed IT applications is underutilized. Most IT users apply a narrow band of features, operate at low levels of feature use, and rarely initiate extensions of the available features. We argue that organizations need aggressive tactics to encourage users to expand their use of installed IT-enabled work systems. This article strives to accomplish three primary research objectives. First, we offer a comprehensive research model aimed both at coalescing existing research on post-adoptive IT use behaviors and at directing future research on those factors that influence users to (continuously) exploit and extend the functionality built into IT applications. Second, in developing this comprehensive research model, we provide a window (for researchers across a variety of scientific disciplines interested in technology management) into the rich body of research regarding IT adoption, use, and diffusion. Finally, we discuss implications and recommend guidelines for research and practice.
We present a model of the organizational impacts of enterprise resource planning (ERP) systems once the system has gone live and the “shake-out” phase has occurred. Organizational information processing theory states that performance is influenced by the level of fit between information processing mechanisms and organizational context. Two important elements of this context are interdependence and differentiation among subunits of the organization. Because ERP systems include data and process integration, the theory suggests that ERP will be a relatively better fit when interdependence is high and differentiation is low. Our model focuses at the subunit level of the organization (business function or location, such as a manufacturing plant) and includes intermediate benefits through which ERP’s overall subunit impact occurs (in our case at the plant level). ERP customization and the amount of time since ERP implementation are also included in the model. The resulting causal model is tested using a questionnaire survey of 111 manufacturing plants. The data support the key assertions in the model.
Grounded in the theory of trying, this study examines the influence of the work environment and gender on trying to innovate with information technology. The study extends the innovation diffusion literature by offering a theory-driven explanation for examining trying to innovate with IT and a parsimonious measure for this construct. Drawing on the theory of reasoned action, we argue that work environment impediments render intentions inadequate for examining post-adoption IT use. Instead of examining intentions, we introduce the goal-based construct of trying to innovate with IT as an appropriate dependent variable for examining post-adoption IT use. Statistical analysis supports the reliability and validity of a parsimonious measure of trying to innovate with IT. The study focuses on two research questions. First, do perceptions of the work environment such as overload and autonomy influence individuals’ trying to innovate with IT? Second, does gender influence the relationship between perceptions of the environment and trying to innovate with IT?The model articulates how perceptions of the environment moderated by gender may influence trying to innovate with IT. Results provide evidence that overload and autonomy are antecedents to trying to innovate with information technology. Further, findings confirm that autonomy interacts with overload to determine trying to innovate with IT and that these relationships vary by gender. Implications for research and practice are offered.
To better explain resistance to information technology implementation, we used a multilevel, longitudinal approach. We first assessed extant models of resistance to IT. Using semantic analysis, we identified five basic components of resistance: behaviors, object, subject, threats, and initial conditions. We further examined extant models to (1) carry out a preliminary specification of the nature of the relationships between these components and (2) refine our understanding of the multilevel nature of the phenomenon. Using analytic induction, we examined data from three case studies of clinical information systems implementations in hospital settings, focusing on physicians’ resistance behaviors. The resulting mixed-determinants model suggests that group resistance behaviors vary during implementation. When a system is introduced, users in a group will first assess it in terms of the interplay between its features and individual and/or organizational-level initial conditions. They then make projections about the consequences of its use. If expected consequences are threatening, resistance behaviors will result. During implementation, should some trigger occur to either modify or activate an initial condition involving the balance of power between the group and other user groups, it will also modify the object of resistance, from system to system significance. If the relevant initial conditions pertain to the power of the resisting group vis-à-vis the system advocates, the object of resistance will also be modified, from system significance to system advocates. Resistance behaviors will follow if threats are perceived from the interaction between the object of resistance and initial conditions. We also found that the bottom-up process by which group resistance behaviors emerge from individual behaviors is not the same in early versus late implementation. In early implementation, the emergence process is one of compilation, described as a combination of independent, individual behaviors. In later stages of implementation, if group level initial conditions have become active, the emergence process is one of composition, described as the convergence of individual behaviors.
This paper examines the question of how intellectual property rights in the software created during information technology outsourcing relationships should be divided. This paper expands on the property rights approach developed by Grossman, Hart, and Moore by recognizing that, with respect to software, it is possible to separate excludability rights from usability rights. These rights are modeled and the contractually optimal distribution is determined. This model is then modified to account for the possibility of cannibalization of the client’s benefit when multiple others are allowed use of the software. The results show that the best contractual structure depends strongly on the environment.
It has long been recognized that client learning is an important factor in the successful development of information systems. While there is little question that clients should learn, there is less clarity about how best to facilitate client learning during developer-client meetings. In this study, we suggest that a cooperative learning strategy called collaborative elaboration developed by educational psychologists provides a theoretical and practical basis for stimulating client learning during an IS design process. The problem with assessing the effects of collaborative elaboration, however, is in controlling for the many other factors that might affect client learning and outcomes of an IS design phase. In a unique research opportunity, we were able to measure the use of collaborative elaboration among 85 developers and clients involved in 17 projects over a semester-long IS design process. The projects were homogeneous with respect to key contextual variables. Our PLS analysis suggested that teams using more collaborative elaboration had more client learning and teams with more client learning achieved better IS design-phase outcomes. This suggests that theories about collaborative elaboration have significant potential for helping IS researchers identify new approaches for stimulating client learning early in the IS design process.
In this essay, we argue that industry receives little attention in information systems research and theory, despite its increasingly important influence on IS activities. This is evident both in the narrow range of industries examined in IS research and the infrequent consideration of industry in theory. We base these observations on an analysis of IS publications in two top-tier journals (MIS Quarterly and Information Systems Research) over eight years. Drawing from institutional theory, we consider various ways industry can be addressed and assess how industry influences IS activities. We conclude that industry provides an important contextual “space” to build new IS theory and to evaluate the boundaries of existing IS theory. We outline a range of strategies for incorporating industry into IS research.
The expectation norm of Information Systems SERVQUAL has been challenged on both conceptual and empirical grounds, drawing into question the instrument’s practical value. To address the criticism that the original IS SERVQUAL’s expectation measure is ambiguous, we test a new set of scales that posits that service expectations exist at two levels that IS customers use as a basis to assess IS service quality: (1) desired service: the level of IS service desired, and (2) adequate service: the minimum level of IS service customers are willing to accept. Defining these two levels is a “zone of tolerance” (ZOT) that represents the range of IS service performance a customer would consider satisfactory. In other words, IS customer service expectations are characterized by a range of levels, rather than a single expectation point. This research note adapts the ZOT and the generic operational definition from marketing to the IS field, assessing its psychometric properties. Our findings conclude that the instrument shows validity of a four-dimension IS ZOT SERVQUAL measure for desired, adequate, and perceived service quality levels, identifying 18 commonly applicable question items. This measure addresses past criticism while offering a practical diagnostic tool.
Delivering quality customer service has emerged as a strategic imperative, one that is increasingly tied to a firm’s information technology resources and capabilities. This paper presents an empirical study that examines the extent to which IT impacts customer service. More specifically, this study investigates the differential effects of various IT resources and capabilities on the performance of the customer service process across firms that compete in the North American life and health insurance industry. The paper builds on (1) information systems work that suggests that the effects of IT are best documented at the level of processes within a firm, (2) information systems work that suggests that the performance effects of IT are likely to be contingent in nature, and (3) developments in the resource-based view, which describes the kinds of IT resources and capabilities that are likely to enable a process in one firm to outperform the same process in competing firms. The findings suggest that tacit, socially complex, firm-specific resources explain variation in process performance across firms and that IT resources and capabilities without these attributes do not. Of particular interest to IS scholars, it is found that shared knowledge between IT and customer service units—an important driver of how IT is implemented and used in the customer service process—is a key IT capability that affects customer service process performance and moderates the impacts of explicit IT resources such as the generic information technologies used in the process and IT spending, which—consistent with resource-based predictions—were not found to be directly and positively associated with relative process performance. The implications of the findings for research and practice are discussed.
The role of information systems in the creation and appropriation of economic value has a long tradition of research, within which falls the literature on the sustainability of IT-dependent competitive advantage. In this article, we formally define the notion of IT-dependent strategic initiative and use it to frame a review of the literature on the sustain- ability of competitive advantage rooted in information systems use. We offer a framework that articulates both the dynamic approach to IT-dependent strategic advantage currently receiving attention in the literature and the underlying drivers of sustainability. This framework models how and why the characteristics of the IT-dependent strategic initiative enable sustained competitive advantage, and how the determinants of sustainability are developed and strengthened over time. Such explanation facilitates the pre-implementation analysis of planned initiatives by innovators, as well as the post-implementation evaluation of existing initiatives so as to identify the basis of their sustainability.In carrying out this study, we examined the interdisciplinary literature on strategic information systems. Using a structured methodology, we reviewed the titles and abstracts of 648 articles drawn from information systems, strategic management, and marketing literature. We then examined and individually coded a relevant subset of 117 articles. The literature has identified four barriers to erosion of competitive advantage for IT-dependent strategic initiatives and has surfaced the structural determinants of their magnitude. Previous work has also begun to theorize about the process by which these barriers to erosion evolve over time. Our review reveals that significant exploratory research and theoretical development have occurred in this area, but there is a paucity of research providing rigorous tests of theoretical propositions.Our work makes three principal contributions. First, it formalizes the definition of IT-dependent strategic initiative. Second, it organizes the extant interdisciplinary research around an integrative framework that should prove useful to both research and practice. This framework offers an explanation of how and why IT-dependent strategic initiatives contribute to sustained competitive advantage, and explains the process by which they evolve over time. Finally, our review and analysis of the literature offers the basis for future research directions.
We attempt to use general systems theory (GST) to understand why the resources of Texaco’s corporate information technology function consistently did not match its task during its 40-year lifetime. Our interpretation uses mechanistic, organic, and colonial systems metaphors, each with three components. The first is an analysis of a management action system made up of organizational indicators such as Texaco’s revenues, profits, employee numbers, IT budgets, and IT personnel numbers. The second is a narrative of performance versus resource needs, which shows a gap between the resources and expanding responsibilities of Texaco’s IT function. The third is a management perception system, which offers reasons why top management continually misinterpreted IT’s performance as inferior. Our results show that the mechanistic, organic, and colonial interpretations converge. In addition, our GST-based interpretations show how top management might have remedied the situation.
As competition in business-to-consumer e-commerce becomes fiercer, Web-based stores are attempting to attract consumers’ attention by exploiting state-of-the-art technologies. Virtual reality (VR) on the Internet has been gaining prominence recently because it enables consumers to experience products realistically over the Internet, there by mitigating the problems associated with consumers’ lack of physical contact with products. However, while the employment of VR has increased in B2C e-commerce, its impact has not been explored extensively by research in the IS field.This study investigates whether and under what circumstances VR enhances consumer learning about products. In general, VR enables consumers to learn about products thoroughly by providing high-quality three-dimensional images of products, interactivity with the products, and increased telepresence. In addition, congruent with the theory of cognitive fit, the effects of VR are more pronounced when it exhibits products whose salient attributes are completely apparent through visual and auditory cues (because most VR on desktop computers uses only those two sensory modalities to deliver information). Based on these attributes, we distinguish between two types of products—namely, virtually high experiential (VHE) and virtually low experiential (VLE) products—in terms of the sensory modalities that are used and required for product inspection. Hypotheses arising from the distinctions expressed by these terms were tested via a laboratory experiment. The results support the predictions that VR interfaces increase overall consumer learning about products and that these effects extend to VHE products more significantly than to VLE products.
Recently, researchers have begun investigating an emerging, technology-enabled innovation that involves the use of intelligent software agents in enterprise supply chains. Software agents combine and integrate capabilities of several information technology classes in a novel manner that enables supply chain management and decision making in modes not supported previously by IT and not reported previously in the information systems literature. Indeed, federations and swarms of software agents today are moving the boundaries of computer-aided decision making more generally. Such moving boundaries highlight promising new opportunities for competitive advantage in business, in addition to novel theoretical insights. But they also call for shifting research thrusts in information systems. The stream of research associated with this article is taking some first steps to address such issues by examining experimentally the capabilities, limitations, and boundaries of agent technology for computer-based decision support and automation in the procurement domain. Procurement represents an area of particular potential for agent-based process innovation, as well as reflecting some of the greatest technological advances in terms of agents emerging from the laboratory. Procurement is imbued with considerable ambiguity in its task environment, ambiguity that presents a fundamental limitation to IT-based automation of decision making and knowledge work. By investigating the comparative performance of human and software agents across varying levels of ambiguity in the procurement domain, the experimentation described in this article helps to elucidate some new boundaries of computer-based decision making quite broadly. We seek in particular to learn from this domain and to help inform computer-based decision making, agent technological design, and IS research more generally.
Unlike technologies that are applicable in a few specific industries, information technologies have a wide range of applicability across almost all industries. The fundamental principles of good IT management are also applicable in many industries. Thus, firms whose business units operate in different industries have an opportunity to exploit cross-unit IT synergies by applying their IT resources and management processes across multiple units. This study examines sources of cross-unit IT synergy and the conditions under which cross-unit IT synergies improve the performance of multibusiness firms. Building on the resource-based view of diversification and the economic theory of complementarities, the study identifies the relatedness and complementarity of IT resources as two major sources of cross-unit IT synergy. It argues that IT relatedness—the use of common IT infrastructure technologies and common IT management processes across business units—creates sub-additive cost synergies, whereas complementarities among IT infrastructure technologies and IT management processes create super-additive value synergies. In a sample of 356 multibusiness Fortune 1000 firms, the study finds that sub-additive cost synergies arising from the use of related IT resources or management processes do not have any effects on corporate performance, whereas the super-additive value synergies arising from the use of a complementary set of IT resources and management processes have significant effects on corporate performance. The diversification level of the firm moderates the relationship between IT synergies and corporate performance. As the diversification level increases, the performance effects of IT synergies remain positive, but they become weaker. The IT governance mode of the firm (centralized, decentralized, hybrid) does not make a difference in the performance effects of IT synergies.
Software piracy costs the software industry billions of dollars each year. To better understand piracy, we propose a model of ethical decision making that is an adaptation of the four-component model of morality. This model defines four internal processes that result in external moral behavior: recognition, judgment, intention, and behavior. We test our model with a sample of Information Systems students in Hong Kong who provided measures of self-reported behavior regarding levels of buying and using pirated software. Using partial least squares, we investigated the causal pathways of the model and the effects of age and gender. We find that use is determined by buying, buying is determined by intention, and intention is determined by judgment. Although respondents recognized software piracy as an infringement of intellectual property rights, this fact did not affect their judgment of the morality of the act. Significant differences are also found in the ethical decision-making process based on age but only limited differences based on gender. The implications of these results, including the development of a professional ethics program, are discussed.
Explanation facilities are considered essential in facilitating user interaction with knowledge-based systems (KBS). Research on explanation provision and the impact on KBS users has shown that the domain expertise affects the type of explanations selected by the user and the basis for seeking such explanations. The prior literature has been limited, however, by the use of simulated KBS that generally provide only feedback explanations (i.e., ex post to the recommendation of the KBS being presented to the user). The purpose of this study is to examine the way users with varying levels of expertise use alternative types of KBS explanations and the impact of that use on decision making. A total of 64 partner/ manager-level and 82 senior/staff-level insolvency professionals participated in an experiment involving the use of a fully functioning KBS to complete a complex judgment task. In addition to feedback explanations, the KBS also provided feedforward explanations (i.e., general explanations during user input about the relationships between information cues in the KBS) and included definition type explanations (i.e., declarative-level knowledge). The results show that users were more likely to adhere to recommendations of the KBS when an explanation facility was available. Choice patterns in using explanations indicated that novices used feedforward explanations more than experts did, while experts were more likely than novices to use feedback explanations. Novices also used more declarative knowledge and initial problem solving type explanations, while experts used more procedural knowledge explanations. Finally, use of feedback explanations led to greater adherence to the KBS recommendation by experts—a condition that was even more prevalent as the use of feedback explanations increased. The results have several implications for the design and use of KBS in a professional decision-making environment.
We draw from and extend Nonaka’s (1994) theory of knowledge creation to develop a model of media selection and use in the knowledge conversion (KC) process. KC is a process wherein an individual is affected by the experiences of another. The outcomes of KC—transferred and transformed knowledge—hinge on the development of understanding. The KC process is enabled via various communicative and noncommunicative media. Because the KC process occurs over time, it possesses a temporal fabric or structure. We explore the practical realities of KC as a dynamic, time- and experience-dependent process. We consider how the temporal fabric of KC creates an evolving reciprocal relationship among perceived media utility, selection, and use of media, as well as switching and/or combining media. We propose and discuss two key factors as determinants of perceived media utility use in the KC process: (1) the temporal behavior of individuals engaged in the KC process and (2) individual and joint experience-based factors. We also discuss the role of contextual factors as antecedents. Finally, we offer and illustrate two primary temporal structures for KC media selection and use: (1) monophasic, wherein KC participants use a single medium at a time, and (2) polyphasic, wherein KC participants deploy multiple media simultaneously. We conclude with a discussion of the implications for the design of KC-enabling systems and directions for future research.
Firms today use information about customers to improve service and design personalized offerings. To do this successfully, however, firms must collect consumer information. This study enhances awareness about a central paradox for firms investing in personalization; namely, that consumers who value information transparency are also less likely to participate in personalization. We examine the relationship between information technology features, specifically information transparency features, and consumer willingness to share information for online personalization. Based on a survey of over 400 online consumers, we examine the question of whether customer perceived information transparency is associated with consumer willingness to be profiled online. Our results indicate that customers who desire greater information transparency are less willing to be profiled. This result poses a dilemma for firms, as the consumers that value information transparency features most are also the consumers who are less willing to be profiled online. In order to manage this dilemma, we suggest that firms adopt a strategy of providing features that address the needs of consumers who are more willing to partake in personalization, therefore accepting that the privacy sensitive minority of consumers are unwilling to participate in personalization, despite additional privacy features.
What constitutes excellence in information systems research for promotion and tenure? This is a question that is regularly addressed by members of promotion and tenure committees and those called upon to write external letters. While there are many elements to this question, one major element is the quality and quantity of an individual’s research publications. An informal survey of senior Information Systems faculty members at 49 leading U.S. and Canadian universities found 86 percent to expect three or more articles in elite journals. In contrast, an analysis of publication performance of Ph.D. graduates between the years of 1992 and 2004 found that approximately three individuals in each graduating year of Ph.D.s (about 2 percent) published 3 or more articles in a set of 20 elite journals within 6 years of graduation. Only 15 individuals from each graduating year (11 percent) published one or more articles. As a discipline, we publish elite journal articles at a lower rate than Accounting, yet our promotion and tenure standards are higher, similar to those of Management, Marketing, and Finance. Thus, there is a growing divergence between research performance and research standards within the Information Systems discipline. As such, unless we make major changes, these differences will perpetuate a vicious cycle of increasing faculty turnover, declining influence on university affairs, and lower research productivity. We believe that we must act now to create a new future, and offer recommendations that focus on the use of more appropriate standards for promotion and tenure and ways to increase the number of articles published.
Although there is a long tradition of empirical studies of software developers, few studies have focused on software maintenance. Prior work is predicated on the belief that higher levels of software comprehension are associated with higher levels of performance on modification tasks. This study provides a more complete understanding of the relationship between software comprehension and modification. We conceptualize software maintenance as interlinking comprehension and modification, and argue that the relationship between the two is moderated by cognitive fit. Specifically, cognitive fit exists when the software maintainer’s dominant mental representation of the software and their mental representation of the modification task emphasize the same type of knowledge. We hypothesize that when cognitive fit exists, greater improvements in comprehension are associated with higher levels of performance on a modification task. When cognitive fit does not exist, however, the software maintainer’s mental representations of the software and of the modification task do not emphasize the same type of knowledge, which may mean that attention is devoted to comprehension at the expense of modification, resulting in lower performance on the modification task. In these circumstances, comprehension and modification tasks may interfere with each other, an effect known as dual-task interference. We therefore hypothesize that performance on a modification task is moderated by the fit between the mental representation of the software and that of the modification task.We tested our theory by varying cognitive fit to create matched and mismatched conditions in a single experiment that used IT professionals as subjects. Our findings support our theory: cognitive fit moderates the relationship between comprehension and modification. Specifically, changes in software comprehension and modification performance are positively related when cognitive fit exists and negatively related when cognitive fit does not exist. Our findings demonstrate the need to examine more complex relationships among the numerous types of tasks involved in software development rather than examining software comprehension alone.
This paper extends Ajzen’s (1991) theory of planned behavior (TPB) to explain and predict the process of e-commerce adoption by consumers. The process is captured through two online consumer behaviors: (1) getting information and (2) purchasing a product from a Web vendor. First, we simultaneously model the association between these two contingent online behaviors and their respective intentions by appealing to consumer behavior theories and the theory of implementation intentions, respectively. Second, following TPB, we derive for each behavior its intention, attitude, subjective norm, and perceived behavioral control (PBC). Third, we elicit and test a comprehensive set of salient beliefs for each behavior. A longitudinal study with online consumers supports the proposed e-commerce adoption model, validating the predictive power of TPB and the proposed conceptualization of PBC as a higher-order factor formed by self-efficacy and controllability. Our findings stress the importance of trust and technology adoption variables (perceived usefulness and ease of use) as salient beliefs for predicting e-commerce adoption, justifying the integration of trust and technology adoption variables within the TPB framework. In addition, technological characteristics (download delay, Website navigability, and information protection), consumer skills, time and monetary resources, and product characteristics (product diagnosticity and product value) add to the explanatory and predictive power of our model. Implications for Information Systems, e-commerce, TPB, and the study of trust are discussed.
Recent research has presented a conceptualization, metric, and instrument based on Microsoft Usability Guidelines (MUG; see Agarwal and Venkatesh 2002). In this paper, we use MUG to further our understanding of web and wireless site use. We conducted two empirical studies among over 1,000 participants. In study 1, conducted in both the United States and Finland, we establish the generalizability of the MUG conceptualization, metric, and associated instrument from the United States to Finland. In study 2, which involved longitudinal data collection in Finland, we delved into an examination of differences in factors important in determining web versus wireless site usability. Also, in study 2, based on a follow-up survey about site use conducted 3 months after the initial survey, we found support for a model of site use that employs the MUG categories and subcategories as predictors. The MUG-based model outperformed the widely employed technology acceptance model both in terms of richness and variance explained (about 70 percent compared to 50 percent).
Due to the vast amount of user data tracked online, the use of data-based analytical methods is becoming increasingly common for e-businesses. Recently the term analytical eCRM has been used to refer to the use of such methods in the online world. A characteristic of most of the current approaches in eCRM is that they use data collected about users’ activities at a single site only and, as we argue in this paper, this can present an incomplete picture of user activity. However, it is possible to obtain a complete picture of user activity from across-site data on users. Such data is expensive, but can be obtained by firms directly from their users or from market data vendors. A critical question is whether such data is worth obtaining, an issue that little prior research has addressed. In this paper, using a data mining approach, we present an empirical analysis of the modeling benefits that can be obtained by having complete information. Our results suggest that the magnitudes of gains that can be obtained from complete data range from a few percentage points to 50 percent, depending on the problem for which it is used and the performance metrics considered. Qualitatively we find that variables related to customer loyalty and browsing intensity are particularly important and these variables are difficult to derive from data collected at a single site. More importantly, we find that a firm has to collect a reasonably large amount of complete data before any benefits can be reaped and caution against acquiring too little data.
We examine the case of software reuse as a disruptive information technology innovation (i.e., one that requires changes in the architecture of work processes) in software development organizations. Using theories of conflict, coordination, and learning, we develop a model to explain peer-to-peer conflicts that are likely to accompany the introduction of disruptive technologies and how appropriately devised managerial interventions (e.g., coordination mechanisms and organizational learning practices) can lessen these conflicts. A study of software reuse programs in four organizations was conducted to assess the validity of the model. Qualitative and quantitative analyses of the data obtained showed that companies that had implemented such managerial interventions experienced greater success with their software reuse programs. Implications for theory and practice are discussed.
Firms have been investing over $5 billion a year in recent years on new information technology and software in their manufacturing plants. In this study, we develop a conceptual model based on the theory of dynamic capabilities to study how manufacturing plants realize improvements in plant performance by leveraging plant information systems to enable implementation of advanced manufacturing capabilities. We develop hypotheses about relationships between information systems, their impact on manufacturing practices, and the overall impact on plant performance. Analysis of survey data from 1,077 U.S. manufacturing plants provides empirical support for the dynamic capabilities model and suggests that manufacturing capabilities mediate the impact of information systems on plant performance. Our results underscore the importance of manufacturing and organizational capabilities in studying the impact of IT on manufacturing plant productivity, and provide a sharper theoretical lens to evaluate their impact.
In a world where information technology is both important and imperfect, organizations and individuals are faced with the ongoing challenge of determining how to use complex, fragile systems in dynamic contexts to achieve reliable outcomes. While reliability is a central concern of information systems practitioners at many levels, there has been limited consideration in information systems scholarship of how firms and individuals create, manage, and use technology to attain reliability. We propose that examining how individuals and organizations use information systems to reliably perform work will increase both the richness and relevance of IS research. Drawing from studies of individual and organizational cognition, we examine the concept of mindfulness as a theoretical foundation for explaining efforts to achieve individual and organizational reliability in the face of complex technologies and surprising environments. We then consider a variety of implications of mindfulness theories of reliability in the form of alternative interpretations of existing knowledge and new directions for inquiry in the areas of IS operations, design, and management.
An understanding of culture is important to the study of information technologies in that culture at various levels, including national, organizational, and group, can influence the successful implementation and use of information technology. Culture also plays a role in managerial processes that may directly, or indirectly, influence IT. Culture is a challenging variable to research, in part because of the multiple divergent definitions and measures of culture. Notwithstanding, a wide body of literature has emerged that sheds light on the relationship of IT and culture. This paper sets out to provide a review of this literature in order to lend insights into our understanding of the linkages between IT and culture. We begin by conceptualizing culture and laying the groundwork for a values-based approach to the examination of IT and culture. Using this approach, we then provide a comprehensive review of the organizational and cross-cultural IT literature that conceptually links these two traditionally separate streams of research. From our analysis, we develop six themes of IT-culture research emphasizing culture’s impact on IT, IT’s impact on culture, and IT culture. Building upon these themes, we then develop a theory of IT, values, and conflict. Based upon the theory, we develop propositions concerning three types of cultural conflict and the results of these conflicts. Ultimately, the theory suggests that the reconciliation of these conflicts results in a reorientation of values. We conclude with the particular research challenges posed in this line of inquiry.
The emerging work on understanding open source software has questioned what leads to effectiveness in OSS development teams in the absence of formal controls, and it has pointed to the importance of ideology. This paper develops a framework of the OSS community ideology (including specific norms, beliefs, and values) and a theoretical model to show how adherence to components of the ideology impacts effectiveness in OSS teams. The model is based on the idea that the tenets of the OSS ideology motivate behaviors that enhance cognitive trust and communication quality and encourage identification with the project team, which enhances affective trust. Trust and communication in turn impact OSS team effectiveness. The research considers two kinds of effectiveness in OSS teams: the attraction and retention of developer input and the generation of project outputs. Hypotheses regarding antecedents to each are developed. Hypotheses are tested using survey and objective data on OSS projects. Results support the main thesis that OSS team members’ adherence to the tenets of the OSS community ideology impacts OSS team effectiveness and reveal that different components impact effectiveness in different ways. Of particular interest is the finding that adherence to some ideological components was beneficial to the effectiveness of the team in terms of attracting and retaining input, but detrimental to the output of the team. Theoretical and practical implications are discussed.
Ad hoc query formulation is an important task in effectively utilizing organizational data resources. To facilitate this task, managers and casual end-users are commonly presented with database views expressly constructed for their use. Differences in the way in which things, states, and events are represented in such views can affect a user’s ability to understand the database, potentially leading to different levels of performance (i.e., accuracy, confidence, and prediction of the accuracy of their queries). An experiment was conducted over the Internet involving 342 subjects from 6 universities in North America and Europe to investigate these effects. When presented with an event-based view, subjects expressing low or very low comfort levels in reading entity-relationship diagrams expressed confidence that better predicted query accuracy although there were no significant differences in actual query accuracy or level of confidence expressed.
Best practice exemplars suggest that digital platforms play a critical role in managing supply chain activities and partnerships that generate performance gains for firms. However, there is limited academic investigation on how and why information technology can create performance gains for firms in a supply chain management (SCM) context. Grant’s (1996) theoretical notion of higher-order capabilities and a hierarchy of capabilities has been used in recent information systems research by Barua et al. (2004), Sambamurthy et al. (2003), and Mithas et al. (2004) to reframe the conversation from the direct performance impacts of IT resources and investments to how and why IT shapes higher-order process capabilities that create performance gains for firms. We draw on the emerging IT-enabled organizational capabilities perspective to suggest that firms that develop IT infrastructure integration for SCM and leverage it to create a higher-order supply chain integration capability generate significant and sustainable performance gains. A research model is developed to investigate the hierarchy of IT-related capabilities and their impact on firm performance. Data were collected from 110 supply chain and logistics managers in manufacturing and retail organizations. Our results suggest that integrated IT infrastructures enable firms to develop the higher-order capability of supply chain process integration. This capability enables firms to unbundle information flows from physical flows, and to share information with their supply chain partners to create information-based approaches for superior demand planning, for the staging and movement of physical products, and for streamlining voluminous and complex financial work processes. Furthermore, IT-enabled supply chain integration capability results in significant and sustained firm performance gains, especially in operational excellence and revenue growth. Managerial initiatives should be directed at developing an integrated IT infrastructure and leveraging it to create process capabilities for the integration of resource flows between a firm and its supply chain partners.
Determining prices is a key management task for a merchant. IT-enabled electronic markets facilitate price discovery by both buyers and sellers compared to traditional, physical markets. Recent research on electronic markets has revealed that IT has increased market transparency due to increased accessibility and availability of market information. However, what online sellers do in terms of strategic pricing decisions, in particular price adjustment behavior over time, has not been fully investigated. Due to the ease of making price changes, electronic sellers can execute a number of different pricing strategies, including setting the frequency and amount of price changes. We investigate the “opaque” side of electronic markets by exploring online sellers’ price adjustment patterns over time. More specifically, we identify four questions related to pricing decisions, which lead to hypotheses about how managers determine prices in electronic markets. The paper tests the hypotheses with data from the online computer commodity market. We found, through a simulation analysis, that this market exhibits synchronized price changes, not random changes that are frequently found in traditional markets. Interestingly, small price increases occur more frequently than decreases, while the frequency of price adjustment is significantly associated with a product’s price dispersion. A ranking analysis suggests that online sellers change their price strategies frequently, which makes it difficult for consumers to respond appropriately. The paper discusses the implications of our findings for management and for future research on market transparency and strategic pricing in electronic markets.
Prior research has examined age, gender, experience, and voluntariness as the main moderators of beliefs on technology acceptance. This paper extends this line of research beyond these demographic and situational variables. Motivated by research that suggests that behavioral models do not universally hold across cultures, the paper identifies espoused national cultural values as an important set of individual difference moderators in technology acceptance. Building on research in psychological anthropology and cultural psychology that assesses cultural traits by personality tests at the individual level of analysis, we argue that individuals espouse national cultural values to differing degrees. These espoused national cultural values of masculinity/femininity, individualism/collectivism, power distance, and uncertainty avoidance are incorporated into an extended model of technology acceptance as moderators. We conducted two studies to test our model. Results indicated that, as hypothesized, social norms are stronger determinants of intended behavior for individuals who espouse feminine and high uncertainty avoidance cultural values. Contrary to expectations, espoused masculinity/femininity values did not moderate the relationship between perceived usefulness and behavioral intention but, as expected, did moderate the relationship between perceived ease of use and behavioral intention.
This paper draws on research from a wide literature base to develop a model relating Web navigation systems, disorientation, engagement, user performance, and intentions. The model is tested in an experimental study examining the effects of one simple and two global navigation systems. Although well-accepted design guidelines were followed for the first global navigation system, it was not superior to the simple system. However, the second global navigation system resulted in lower disorientation than the simple system. Based on the study’s results, two design guidelines to govern the development of future Web-based systems are suggested.
Electronic marketplaces (EMPs) are widely assumed to increase price transparency and hence lower product prices. Results of empirical studies have been mixed, with several studies showing that product prices have not decreased and others showing that prices have increased in some cases. One explanation is that sellers prefer not to join EMPs with high price transparency, leading highly price transparent EMPs to fail. Therefore, in order to be successful, EMPs might be expected to avoid high price transparency. But that strategy creates a catch-22 for EMPs on the buy side: Why would buyers want to join EMPs in the absence of price transparency and the benefit of lower prices? We argue that successful EMPs must provide compensatory benefits for sellers in the case of high price transparency and for buyers in the case of low price transparency.To understand how EMPs could succeed, regardless of price transparency, we examined the relationships among EMP strategy, price transparency, and performance by analyzing all 19 EMPs that compete by selling a broad range of standard electronics components. We found that all EMPs pursuing a low cost strategy had high price transparency and performed poorly. All EMPs that performed well pursued strategies of differentiation, but, interestingly, not all successful EMPs avoided price transparency: Some EMPs succeeded despite enabling high price transparency. We therefore examined two differentiated EMPs in greater depth— one with high price transparency, the other with low price transparency—to show how they achieved strategic alignment of activities and resources and provided compensatory benefits for their customers.
This paper investigates the influence of enterprise systems implementation on operational performance. The work extends the literature on enterprise systems by focusing on changes in process dynamics as a source for ongoing firm-level performance improvement. A case discussion of Tristen Corporation, a firm that implemented ERP and subsequently experienced benefits through gains to its continuous improvement efforts, is examined in light of theorized impacts of such implementations on process dynamics. Analyses of longitudinal data suggest that performance along a key metric motivating the ERP initiative (i.e., order fulfillment lead-time) showed a significant improvement immediately after system deployment. The data further suggest that the system implementation gave rise to an ongoing trend of performance improvement, in contrast to a stable performance trend prior to go-live.
Academic researchers access commercial web sites to collect research data. This research practice is likely to increase. Is this appropriate? Is this legal? Such commercial web sites are maintained to achieve business objectives; research access uses site resources for other purposes. Web site administrators may, therefore, deem academic data collection inappropriate. Is there a process to make research access more open and acceptable to web site owners and administrators? These are significant issues. This article clarifies the problems and suggests possible approaches to handle the issues with sensitivity and openness.Research access to commercial web sites may be manual (using a standard web browser) or automated (using automated data collection agents). These approaches have different effects on web sites. Researchers using manual access tend to make a limited number of page requests because manual access is costly to perform. Researchers using automated access methods can request large numbers of pages at a low cost. Therefore, web site administrators tend to view manual access and automated access very differently.Because of the number of accesses and the nonbusiness purpose, automated research requests for data are sometimes blocked by site administration using a variety of means (both technological and legal). This paper details the pertinent legal issues including trespass, copyright violation, and breech of contract. It also explains the nature of express and implied consent by site administration for research access.Based on the issues presented, guidelines for researchers are proposed to reduce objections to research activities, to facilitate communication with web site administration, and to achieve express or implied consent. These include notification to web site administration of intended automated research activity, description of the research project posted as a web page, and clear identification of automated requests for web pages. In order to encourage good research practices with respect to automated data collection, suggestions are made with respect to disclosing methods used in research papers and for self regulation by academic associations.
IS research has considered the outsourcing decision from the perspective of transaction cost economics (TCE) and institutional theory. In this research, we consider how the appropriation of the logic of transaction cost economics is contingent on decision makers’ institutional context. The institutional contexts contrasted are professional versus political contexts. In a survey of 214 city governments in the United States, we substantiate the existence of these two institutional contexts, a distinction that has been noted to extend into the private sector as well.Subsequent analyses of the moderating effects of institutional context on the application of the TCE heuristic to the outsourcing decision revealed the following: The institutional context moderated the impacts of “human frailty” conditions—of opportunism and bounded rationality—and of transaction frequency on outsourcing decisions. In professional contexts, opportunism reduced outsourcing and frequency increased outsourcing; in political contexts, bounded rationality fostered outsourcing and frequency dissuaded outsourcing. However, no institutional moderation was noted for the situational conditions of asset specificity and uncertainty. Instead, situational conditions were found to increase the incidence of outsourcing across both contexts.Findings about the contingent effects of human frailty conditions augment our understanding of the outsourcing phenomenon by emphasizing that decision makers’ attentiveness to the logic of transaction costs during outsourcing is shaped by their institutional context. Findings with regard to situational conditions suggest a need for future research to consider the role of another contextual factor—resource munificence—in mitigating the effects of situational conditions on responses to transaction costs.
A frequent characterization of open source software is the somewhat outdated, mythical one of a collective of supremely talented software hackers freely volunteering their services to produce uniformly high-quality software. I contend that the open source software phenomenon has metamorphosed into a more mainstream and commercially viable form, which I label as OSS 2.0. I illustrate this transformation using a framework of process and product factors, and discuss how the bazaar metaphor, which up to now has been associated with the open source development process, has actually shifted to become a metaphor better suited to the OSS 2.0 product delivery and support process. Overall the OSS 2.0 phenomenon is significantly different from its free software antecedent. Its emergence accentuates the fundamental alteration of the basic ground rules in the software landscape, signifying the end of the proprietary-driven model that has prevailed for the past 20 years or so. Thus, a clear understanding of the characteristics of the emergent OSS 2.0 phenomenon is required to address key challenges for research and practice.
The aim of this research essay is to examine the structural nature of theory in Information Systems. Despite the importance of theory, questions relating to its form and structure are neglected in comparison with questions relating to epistemology. The essay addresses issues of causality, explanation, prediction, and generalization that underlie an understanding of theory. A taxonomy is proposed that classifies information systems theories with respect to the manner in which four central goals are addressed: analysis, explanation, prediction, and prescription. Five interrelated types of theory are distinguished: (1) theory for analyzing, (2) theory for explaining, (3) theory for predicting, (4) theory for explaining and predicting, and (5) theory for design and action. Examples illustrate the nature of each theory type. The applicability of the taxonomy is demonstrated by classifying a sample of journal articles. The paper contributes by showing that multiple views of theory exist and by exposing the assumptions underlying different viewpoints. In addition, it is suggested that the type of theory under development can influence the choice of an epistemological approach. Support is given for the legitimacy and value of each theory type. The building of integrated bodies of theory that encompass all theory types is advocated.
Unstructured data, most of it text-based and computer- mediated, makes up a rapidly growing majority of the knowledge store of most organizations. Entire classes of information systems—knowledge management systems and enterprise content management systems—have emerged to monitor, manage, and support decision making from this primarily textual data.IS research has treated text as a unitary variable. However, research from cognitive science strongly suggests that a deeper investigation of how text is comprehended would allow the development of more effective computer-based knowledge and communications systems. Our research extends IS research on the effects of information presentation on decision making by investigating the attributes of text rather than comparing text to other information presentation modes such as graphs or numbers. Our study also contributes to the sparse empirical IS research on problem formulation, the initial phase of decision making.Informed by research on information presentation, decision making, and narrative comprehension, we designed a series of experiments that demonstrate that the explicit inclusion of goal information for activities in narrative descriptions of problematic business processes increases overall comprehension, decision-making confidence, and short and long term recall. Based on our experimental findings we propose that augmenting text-based IS to elicit and saliently present explicit goal information would significantly enhance the decision support capability of these systems especially for rapid, ad hoc decisions about business process situations.
This study examines how processes of external influence shape information technology acceptance among potential users, how such influence effects vary across a user population, and whether these effects are persistent over time. Drawing on the elaboration-likelihood model (ELM), we compared two alternative influence processes, the central and peripheral routes, in motivating IT acceptance. These processes were respectively operationalized using the argument quality and source credibility constructs, and linked to perceived usefulness and attitude, the core perceptual drivers of IT acceptance. We further examined how these influence processes were moderated by users’ IT expertise and perceived job relevance and the temporal stability of such influence effects. Nine hypotheses thus developed were empirically validated using a field survey of document management system acceptance at an eastern European governmental agency. This study contributes to the IT acceptance literature by introducing ELM as a referent theory for acceptance research, by elaborating alternative modes of influence, and by specifying factors moderating their effects. For practitioners, this study introduces influence processes as policy tools that managers can employ to motivate IT acceptance within their organizations, benchmarks alternative influence strategies, and demonstrates the need for customizing influence strategies to the specific needs of a user population.
Recently, an option-based risk management (OBRiM) framework has been proposed to control risk and maximize value in information technology investment decisions. While the framework is prescriptive in nature, its core logic rests on a set of normative risk-option mappings for choosing which particular real options to embed in an investment in order to control specific risks. This study tests empirically whether these mappings are observed in practice. The research site is a large Irish financial services organization with well established IT risk management practices not tied to any real options framework. Our analysis of the risk management plans developed for a broad portfolio of 50 IT investments finds ample empirical support for OBRiM’s risk-option mappings. This shows that IT managers follow the logic of option-based risk management, although purely based on intuition. Unfortunately, reliance on this logic based on intuition alone could lead to suboptimal or counterproductive risk management practices. We therefore argue that managerial intuition ought to be supplemented with the use of formal real option models, which allow for better quantitative insights into which risk mitigations to pursue and combine in order to effectively address the risks most worth controlling.
Successful product and process design depends on management’s ability to integrate fragmented pockets of specialized knowledge. This integrative capability has important implications for large-scale information technology projects. This article examines the relationship between timely project completion and two dimensions of management’s integrative capability: access to external knowledge and internal knowledge integration. Measures of these two dimensions are used to predict on-time project completion, where completion is a function of the duration of IT-related project delays. In a longitudinal study of 74 enterprise application integration projects in the medical sector, integrative capability was measured from the point of view of the CIO and a facility IT manager. Accounting for several project controls, our Cox regression results indicate both integrative dimensions significantly mitigate the duration of IT-related project delays, thus promoting timely project completion. The analysis also reveals the importance of taking management structure into consideration when studying IT phenomena in networked organizations.
Although increasing evidence suggests that superior performance requires alignment between firms’ strategies and production processes, it is not known if such alignment is relevant for software development processes. This study breaks new ground by examining how firms align their software processes, products, and strategies in Internet application development. Drawing upon the literatures in strategy, operations management, and information systems, we identify four dimensions that influence alignment: the business unit strategy, the level of product customization, the level of process customization, and the volume of customers. To examine how these dimensions are synchronized, we conducted detailed case studies of Internet application development in nine varied firms including both start-ups and established “brick and mortar” companies. Our analyses reveal that the firms in our study do use differing processes for Internet application development, and that many of the firms match their software process choices to product characteristics, customer volume, and business unit strategies. We develop concept maps for the firms that are in alignment to illustrate how managers configure specific product and process dimensions. We also offer potential explanations for why some firms are misaligned, such as attempting to execute incompatible strategies, the lack of coordination between marketing and production strategies, the too rapid expansion of process scope, and inflexible barriers to rapid adaptation of process. Our study contributes detailed insights into how software processes are customized to complement different types of product requirements and strategies.
Theoretical and empirical research in technology acceptance, while acknowledging the importance of individual beliefs about the compatibility of a technology, has produced equivocal results. This study focuses on further conceptual development of this important belief in technology acceptance. Unlike much prior research that has focused on only a limited aspect of compatibility, we provide a more comprehensive conceptual definition that disaggregates the content of compatibility into four distinct and separable constructs: compatibility with preferred work style, compatibility with existing work practices, compatibility with prior experience, and compatibility with values. We suggest that the form of the multidimensional compatibility construct is best modeled as a multivariate structural model. Based on their conceptual definitions, we develop operational measures for the four compatibility variables. We assess the nomological validity of our conceptualization by situating it within the technology acceptance model. In contrast to prior research, which has regarded beliefs of compatibility as an independent antecedent of technology acceptance outcomes, we posit causal linkages not only among the four compatibility beliefs, but also between compatibility beliefs and usefulness, and ease of use. We test our theoretical model with a field sample of 278 users of a customer relationship management system in the context of a large bank. Scale validation indicates that the operational measures of compatibility developed in this study have acceptable psychometric properties, which support the existence of four distinct constructs. Results largely support the theorized relationships.
In the context of personalization technologies, such as Web-based product-brokering recommendation agents (RAs) in electronic commerce, existing technology acceptance theories need to be expanded to take into account not only the cognitive beliefs leading to adoption behavior, but also the affect elicited by the personalized nature of the technology. This study takes a trust-centered, cognitive and emotional balanced perspective to study RA adoption. Grounded on the theory of reasoned action, the IT adoption literature, and the trust literature, this study theoretically articulates and empirically examines the effects of perceived personalization and familiarity on cognitive trust and emotional trust in an RA, and the impact of cognitive trust and emotional trust on the intention to adopt the RA either as a decision aid or as a delegated agent.An experiment was conducted using two commercial RAs. PLS analysis results provide empirical support for the proposed theoretical perspective. Perceived personalization significantly increases customers’ intention to adopt by increasing cognitive trust and emotional trust. Emotional trust plays an important role beyond cognitive trust in determining customers’ intention to adopt. Emotional trust fully mediates the impact of cognitive trust on the intention to adopt the RA as a delegated agent, while it only partially mediates the impact of cognitive trust on the intention to adopt the RA as a decision aid. Familiarity increases the intention to adopt through cognitive trust and emotional trust.
Personalized information technology services have become a ubiquitous phenomenon. Companies worldwide are using the web to provide personalized offerings and unique experiences to their customers. While there is a lot of hype about delivering personalized services over the web, little is known about the effectiveness of web personalization and the link between the IT artifact (the personalization agent) and the effects it exerts on a user’s information processing and decision making. To address the impact of personalized content, this article theoretically develops and empirically tests a model of web personalization. The model is grounded on social cognition and consumer research theories adapted to the peculiar features of web personalization. The influence of a personalization agent is mediated by two variables: content relevance and self reference. Hypotheses generated from the model are empirically tested in a laboratory experiment and a field study. The findings indicate that content relevance, self reference, and goal specificity affect the attention, cognitive processes, and decisions of web users in various ways. Also, users are found to be receptive to personalized content and find it useful as a decision aid. Theoretical and practical implications of the findings are discussed.
As firms seek to improve coordination through the use of electronic interorganizational systems (IOS), open standards are becoming increasingly important. To better understand the process of standards diffusion, we investigate firms’ migration from proprietary or less-open IOS (i.e., electronic data interchange or EDI) to open-standard IOS (i.e., the Internet). Theoretical work in economics suggests that network effects are a determinant of network adoption, yet the extant literature falls short of empirical testing of the theory. We develop a conceptual model that features network effects, expected benefits, and adoption costs as prominent antecedents. We examine the model on a large dataset of 1,394 firms. The empirical results demonstrate the significant impacts of network effects on open-standard IOS adoption. We find that adoption costs are a significant barrier to open-standard IOS adoption, but EDI users and nonusers treat this very differently: EDI users are much more sensitive to the costs of switching to the new standard. This finding illustrates that experience with older standards may create switching costs and make it difficult to shift to open and potentially better standards, a phenomenon called “excess inertia” in technology change. Further testing the underlying factors that contribute to network effects and adoption costs, we find that trading community influence is a key driver of network effects, while managerial complexity, as opposed to financial costs, is a key determinant of adoption costs. Overall we believe that this study, based on a rigorous empirical analysis of a unique international dataset, provides valuable insights into a set of key factors that influence standards diffusion.
This paper examines the potential social costs of standardization, including possible vendor reactions to standards and their impacts on the adoption of new technology and long-term market structure. Specifically, we study how vendors might react to standards in the market for routers and switches, two of the most important pieces of networking hardware for the information systems infrastructure of modern firms. Using data from over 22,000 establishments surveyed by Harte Hanks Market Intelligence, we provide evidence that vendors are able to maintain high switching costs in the market for routers and switches despite the presence of open standards in the industry. Several vendor actions are discussed in this paper, including manipulating horizontal compatibility between comparable rival products and vertical compatibility between complementary products, maintaining a broader product line, creating product suites, and targeting specific market segments. Our results further suggest that the presence of switching costs can lead to inefficient adoption of new information technology and that vendors may be able to influence the speed of new information technology adoption.
This paper addresses the role of power and politics in setting standards. It examines the interaction of external contingencies, powerful agents, resources, meaning, and membership of relevant social and institutional groupings in generating successful political outcomes. To study these interactions, the paper adopts the circuits of power, a theoretical framework taken from the social sciences, and applies it to understanding the creation and development of the first standard in information security management. An informal group of UK security chiefs sparked off a process which led first to BS7799, the British standard, and later to ISO 17799, the international standard. The case study portrays how the institutionalization of this ad hoc development process results from the interactions of power among the stakeholders involved. The case study also shows how the different interests and objectives of the stakeholders were influenced by exogenous contingencies and institutional forces. The paper discusses theoretical and practical implications for the future development of such standards.
In order to create Internet standards, people and ideas move across many institutions. By drawing upon the new institutionalism and on organizational ecology, we develop an ecological approach to studying this movement. The approach examines the birth and death of standards bodies and the ideas they cultivate. We apply the approach to the history of Web services choreography standards, in which over 500 participants traversed nine institutions during a 12-year period. We explain critical aspects of this history by analyzing patterns of movement of standardization ideas. We show that standard-making institutions refuse to legitimate standards by utilizing bylaws which reflect the values of the institution; these values reflect the design legacy of the Internet. We formulate conjectures about the dynamics of the birth and death of working groups inside larger institutions that form a population ecology. We discuss plausible explanations for why specific Internet standard-making efforts do not resolve quickly. The theoretical implication of the study is that an ecological approach will apply well to inventions that have been incubated, such as the Internet. The pragmatic implication is that changes to institutional Internet governance, particularly to the bylaws of standards bodies, can have drastic and unintended effects that will reshape the standard-making ecology.
This paper is motivated by the following question: What drives the diffusion of a communication standard and what results can we expect? Past literature provides many instructive but mostly unrelated answers. Frequent findings are startup problems, penguin effects, and tendencies toward monopoly. But substantial problems in applying the models to concrete standardization problems reveal that the dynamics are probably more complex. Not all networks are ultimately conquered by a single standard once it has attracted a certain number of users. And not all diffusion results are either complete or no standardization.We address the question of the conditions of particular diffusion behaviors by developing a formal standardization model that captures all fragmented phenomena in a unified approach. Drawing from findings of other research, we incorporate the structure of the underlying user network as an important determinant for diffusion behaviors. The approach allows us to disclose varying conditions that generate frequently observed standardization behaviors as special parameter constellations of the model. Using equilibrium analysis and computer simulations we identify a standardization gap that reveals the magnitude of available standardization gains for individuals and the network as a whole. The analysis shows that network topology and density have a strong impact on standard diffusion and that the renowned tendency toward monopoly is far less common. We also report how the model can be used to solve corporate standardization problems.
This paper addresses the general question proposed by the call of this special issue: “What historical or contingent events and factors influence the creation of ICT standards, and in particular, their success or failure?” Based on a case study conducted over a period of three years in a Norwegian hospital on the standardization process of an electronic patient record (EPR), the paper contributes to the current discussion on the conceptualization of standard-making in the field of Information Systems. By drawing upon the concepts of logic of ordering adopted from actor–network theory and upon reflexivity and the unexpected side effects adopted from reflexive modernization, the paper makes three key contributions: (1) it demonstrates the socio-technical complexity of IS standards and standardization efforts; (2) it shows how complexity generates reflexive processes that undermine standardization aims; and (3) it suggests a theoretical interpretation of standardization complexity by using ideas from complexity theory and the theory of reflexive modernization. These research questions are addressed by offering an historical and contingent analysis of the complexity dynamics emerging from the case.
Vertical information systems (VIS) standards are technical specifications designed to promote coordination among the organizations within (or across) vertical industry sectors. Examples include the bar code, electronic data interchange (EDI) standards, and RosettaNet business process standards in the electronics industry. This contribution examines VIS standardization through the lens of collective action theory, applied in the literature to information technology product standardization, but not yet to VIS standardization, which is led by heterogeneous groups of user organizations rather than by IT vendors. Through an intensive case analysis of VIS standardization in the U.S. residential mortgage industry, VIS standardization success is shown to be as problematic as IT product standardization success, but for different reasons.VIS standardization involves two linked collective action dilemmas—standards development and standards diffusion— with different characteristics, such that a solution to the first may fail to resolve the second. Whereas prior theoretical and empirical research shows that IT product standardization efforts tend to splinter into rival factions that compete through standards wars in the marketplace, successful VIS standards consortia must encompass heterogeneous groups of user organizations and IT vendors without fragmenting. Some tactics successfully used to solve the collective action dilemma of VIS standardization (e.g., governance mechanisms and policies about intellectual property protection) are also used by IT product standardization efforts, but some are different, and successful VIS standardization requires a package of solutions tailored to fit and jointly resolve the specific dilemmas of particular VIS standards initiatives.
We develop and test a theoretical model to investigate the assimilation of enterprise systems in the post-implementation stage within organizations. Specifically, this model explains how top management mediates the impact of external institutional pressures on the degree of usage of enterprise resource planning (ERP) systems. The hypotheses were tested using survey data from companies that have already implemented ERP systems. Results from partial least squares analyses suggest that mimetic pressures positively affect top management beliefs, which then positively affects top management participation in the ERP assimilation process. In turn, top management participation is confirmed to positively affect the degree of ERP usage. Results also suggest that coercive pressures positively affect top management participation without the mediation of top management beliefs. Surprisingly, we do not find support for our hypothesis that top management participation mediates the effect of normative pressures on ERP usage, but instead we find that normative pressures directly affect ERP usage. Our findings highlight the important role of top management in mediating the effect of institutional pressures on IT assimilation. We confirm that institutional pressures, which are known to be important for IT adoption and implementation, also contribute to post-implementation assimilation when the integration processes are prolonged and outcomes are dynamic and uncertain.This paper is a recipient of one of the 2011 Emerald Management Reviews Citations of Excellence Awards.
Online search has become a significant activity in the daily lives of individuals throughout much of the world. The almost instantaneous availability of billions of web pages has caused a revolution in the way people seek information. Despite the increasing importance of online search behavior in decision making and problem solving, very little is known about why people stop searching for information online. In this paper, we review the literature concerning online search and cognitive stopping rules, and then describe specific types of information search tasks. Based on this theoretical development, we generated hypotheses and conducted an experiment with 115 participants each performing three search tasks on the web. Our findings show that people utilize a number of stopping rules to terminate search, and that the stopping rule used depends on the type of task performed. Implications for online information search theory and practice are discussed.
This paper reports the results of an exploratory field experiment in Singapore that assessed the values of two types of privacy assurance: privacy statements and privacy seals. We collaborated with a local firm to host the experiment on its website with its real domain name, and the subjects were not informed of the experiment. Hence, the study provided a field observation of the subjects’ behavioral responses toward privacy assurances. We found that (1) the existence of a privacy statement induced more subjects to disclose their personal information but that of a privacy seal did not; (2) monetary incentive had a positive influence on disclosure; and (3) information request had a negative influence on disclosure. These results were robust in other specifications that used alternative measures for some of our model variables. We discuss this study in relation to the extant privacy literature, most of which employs surveys and laboratory experiments for data collection, and draw related managerial implications.
Recommendation agents (RAs) are software agents that elicit the interests or preferences of individual consumers for products, either explicitly or implicitly, and make recommendations accordingly. RAs have the potential to support and improve the quality of the decisions consumers make when searching for and selecting products online. They can reduce the information overload facing consumers, as well as the complexity of online searches. Prior research on RAs has focused mostly on developing and evaluating different underlying algorithms that generate recommendations. This paper instead identifies other important aspects of RAs, namely RA use, RA characteristics, provider credibility, and factors related to product, user, and user–RA interaction, which influence users’ decision-making processes and outcomes, as well as their evaluation of RAs. It goes beyond generalized models, such as TAM, and identifies the RA-specific features, such as RA input, process, and output design characteristics, that affect users’ evaluations, including their assessments of the usefulness and ease-of-use of RA applications. Based on a review of existing literature on e-commerce RAs, this paper develops a conceptual model with 28 propositions derived from five theoretical perspectives. The propositions help answer the two research questions: (1) How do RA use, RA characteristics, and other factors influence consumer decision making processes and outcomes? (2) How do RA use, RA characteristics, and other factors influence users’ evaluations of RAs? By identifying the critical gaps between what we know and what we need to know, this paper identifies potential areas of future research for scholars. It also provides advice to information systems practitioners concerning the effective design and development of RAs.
This exploratory study analyzes the relationship between organizational culture and the deployment of systems development methodologies. Organizational culture is interpreted in terms of the competing values model and deployment as perceptions of the support, use, and impact of systems development methodologies. The results show that the deployment of methodologies by IS developers is primarily associated with a hierarchical culture that is oriented toward security, order, and routinization. IT managers’ critical attitudes of the deployment of methodologies in organizations with a strong rational culture (focusing on productivity, efficiency, and goal achievement) is also worth noting. Based on its empirical findings, the paper proposes a theoretical model to explain the impact of organizational culture on the deployment of systems development methodologies.
This study examines the antecedents of turnover intention among information technology road warriors. Road warriors are IT professionals who spend most of their workweek away from home at a client site. Building on Moore’s (2000) work on turnover intention, this article develops and tests a model that is context-specific to the road warrior situation. The model highlights the effects of work–family conflict and job autonomy, factors especially applicable to the road warrior’s circumstances. Data were gathered from a company in the computer and software services industry. This study provides empirical evidence for the effects of work–family conflict, perceived work overload, fairness of rewards, and job autonomy on organizational commitment and work exhaustion for road warriors. The results suggest that work–family conflict is a key source of stress among IT road warriors because they have to juggle family and job duties as they work at distant client sites during the week. These findings suggest that the context of the IT worker matters to turnover intention, and that models that are adaptive to the work context will more effectively predict and explain turnover intention.
Despite a decade since the inception of B2C e-commerce, the uncertainty of the online environment still makes many consumers reluctant to engage in online exchange relationships. Even if uncertainty has been widely touted as the primary barrier to online transactions, the literature has viewed uncertainty as a “background” mediator with insufficient conceptualization and measurement. To better understand the nature of uncertainty and mitigate its potentially harmful effects on B2C e-commerce adoption (especially for important purchases), this study draws upon and extends the principal-agent perspective to identify and propose a set of four antecedents of perceived uncertainty in online buyer–seller relationships—perceived information asymmetry, fears of seller opportunism, information privacy concerns, and information security concerns—which are drawn from the agency problems of adverse selection (hidden information) and moral hazard (hidden action).To mitigate uncertainty in online exchange relationships, this study builds upon the principal–agent perspective to propose a set of four uncertainty mitigating factors—trust, website informativeness, product diagnosticity, and social presence— that facilitate online exchange relationships by overcoming the agency problems of hidden information and hidden action through the logic of signals and incentives. The proposed structural model is empirically tested with longitudinal data from 521 consumers for two products (prescription drugs and books) that differ on their level of purchase involvement. The results support our model, delineating the process by which buyers engage in online exchange relationships by mitigating uncertainty. Interestingly, the proposed model is validated for two distinct targets, a specific website and a class of websites. Implications for understanding and facilitating online exchange relationships for different types of purchases, mitigating uncertainty perceptions, and extending the principal– agent perspective are discussed.
This paper focuses on the process of implementing strategic information systems (SIS) by studying the radical changes it may bring to an organization’s deep structure. It argues that a full understanding of the process of implementation of such systems should include not only technical aspects but also the social dynamics of an organization; specifically core values, distribution of power and mechanisms of control. A theoretical framework is formulated based on punctuated equilibrium and previous SIS literature, and is applied to an exploratory case study conducted in a Latin American public organization. The case study depicts how the initiative to implement SIS was the result of external and internal disturbances. The case analysis highlights relationships between an organization’s deep structure and SIS implementation. The paper concludes by discussing the theoretical and practical implications of the study. These include (1) the role of the formal organizational structure in influencing the outcome of SIS implementations, (2) the impact of exogenous contingencies such as elections and external funding that may create a sense of crisis and (3) the influence of newcomers who may be brought in to solve the crisis.
In today’s organizations, employees have an ever-increasing variety of communication media to use in the performance of work activities. In this study, we seek to expand our understanding of media usage in organizations where there is a multiplicity of communication media available to employees. We use communication media repertoires as the lens through which we explore how media is used in the support of communication-based work performed by individuals in complex organizational settings. Data were collected in sales divisions at two large corporations in the information technology industry. We compared multiple media use within and between the two sales divisions, and identified similarities and differences in repertoires. Our findings suggest that use of repertoires is influenced by institutional conditions (e.g., incentives, trust, and physical proximity) and situational conditions (e.g., urgency, task, etc.), and by routine use of the media over time. Based on the findings, we propose a framework for investigating the use of multiple media in organizations through examination of communication media repertoires. Implications of these findings for research and practice are discussed.
The development of appropriate integrated and scalable information systems in the health sector in developing countries has been difficult to achieve, and is likely to remain elusive in the face of continued fragmented funding of health programs, particularly related to the HIV/AIDS epidemic. In this article, we propose a strategy for developing information infrastructures in general and in particular for the health care sector in developing countries. We use complexity science to explain the challenges that need to be addressed, in particular the need for standards that can adapt to a changing health care environment, and propose the concept of flexible standards as a key element in a sustainable infrastructure development strategy. Drawing on case material from a number of developing countries, a case is built around the use of flexible standards as attractors, arguing that if they are well defined and simple, they will be able to adapt to the frequent changes that are experienced in the complex health environment. A number of paradoxes are highlighted as useful strategies, integrated independence being one that encourages experimentation and heterogeneity to develop and share innovative solutions while still conforming to simple standards. The article provides theoretical concepts to support standardization processes in complex systems, and to suggest an approach to implement health standards in developing country settings that is sensitive to the local context, allows change to occur through small steps, and provides a mechanism for scaling information systems.
Information systems design and development processes by their very nature involve a multiplicity of knowledge systems, including the technology itself, the methodologies for system development, and knowledge relating to the application domain. When an information system is used to advance socio-economic development in less developed countries (LDCs), there are additional sources contributing to this multiplicity. In the case of land management applications, it is important to consider the knowledge that communities have of the land they inhabit. This paper stresses the importance of constructing knowledge alliances between these multiple knowledge systems in order to support more effective IS development and implementation. The term knowledge alliance refers not merely to the material characteristics of the knowledge inscribed in technology, but also to the indigenous knowledge of the various communities involved. This includes the social setting that has shaped the practices which are responsible for the communities’ production, articulation, and use of knowledge. Two key theoretical concepts, namely boundary objects and participation, are drawn upon both to understand the multiplicity of knowledge systems and to suggest possible approaches to the creation of effective knowledge alliances. The empirical setting for this analysis is a study of the use of geographical information systems for land management in India. This research is not of merely theoretical significance, but also carries important practical implications for scientists and administrators involved in the development of IS, particularly in LDCs.
Most information systems research takes for granted the assumption that IS practice and associated organizational change can be effectively understood as a process of technical reasoning and acting governed by a mix of concerns about software construction, administrative control, and economic gain. Its mission has been to empower managers, IS engineers, and information and communication technology users with knowledge and techniques for effective decision making. However, empirical research frequently encounters human activity that is at odds with the assumed pattern of rational behavior. Recent work tries to explain behavior in IS and organizational change in terms of social processes rather than as a consideration of rational techniques of professional practice. In this paper we address this ambivalence of the IS field with regard to technical/rational knowledge and practice. We draw from the theoretical work of Michel Foucault on power/knowledge and the aesthetics of existence to argue that the rational techniques of IS practice and the power dynamics of an organization and its social context are closely intertwined, requiring each other to be sustained. Furthermore, we develop a context-specific notion of rationality in IS innovation, through which interested parties judge the value of an innovation for their lives and consequently support or subvert its course. We demonstrate these ideas with a case study of a social security organization in Greece.
This study compares two conceptual (resource-centered and contingency-based) and two analytical (linear and nonlinear) approaches that can be used to assess the strategic value of information technology. Two hypotheses related to these approaches are developed and tested based on matched survey data collected from the CEOs and CIOs of 110 firms. The results indicate that the resource-centered and contingency-based approaches provide complementary understanding of the strategic value of IT. On the one hand, the contingency-based approach is better at explaining the impact of cost-related IT applications on firm performance. Alignment between business strategy and information systems strategy on cost reduction was found to have a significant negative association with firm expense. On the other hand, the resource-centered perspective has a stronger predictive ability of IT impact on firm revenue and profitability. Our results indicate that investments in growth-oriented applications were directly and positively related to firm revenue. An ANOVA test indicates that the nonlinear approaches provide additional insights that help to better understand the relationship between alignment and performance. The response surface method (RSM) shows that high-end strategic alignment (i.e., fit occurring when business strategy and IT strategy are both high) leads to superior performance compared to low-end strategic alignment (i.e., fit occurring when business strategy and IT strategy are both low). We discuss the implications of this study for research and practice and conclude with suggestions for future research directions.
This article is based on the introduction of a telemedicine system in the jungles of northeastern Peru. The system was designed by a European consortium led by a Spanish polytechnic in cooperation with two universities in Lima and the Peruvian Ministry of Health. The purpose of the system was to improve health conditions by extending science-based medicine into a region with well-established traditional healing practices. The central analytical focus of this article is on the interplay between the public health care system, which used the telemedicine system, and local health care practices. The manner in which scientific medicine was delivered through information technology and public health care services is analyzed in terms of the health personnel’s activity, the local population’s conceptions of health, and the trajectories followed by patients seeking recovery. The author participated in the design of the second evaluation of the telemedicine system and acted as a participant observer in the regional hospital and peripheral clinics. In addition to interviewing health care staff from the study area, the author also met with traditional healers, and patients in the districts whether or not they were involved in the telemedicine project. New institutional theory provided the analytical framework for the interpretation of the observed behavior of the public health care staff, traditional healers, and potential patients. Empirically, this study describes the informal aspects of the functioning of the telemedicine system, and its partial mismatch with the definitions of health and illness employed by local communities and healers. An argument is made that people’s construction of their health, which is embedded in their normal patterns of action, should be identified, and then considered in the design, implementation, and evaluation of future telemedicine projects. This article problematizes an approach to telemedicine-based health development that is weakly accountable to local social contexts and their diversity.
Research has investigated the main effect of training on information systems implementation success. However, empirical support for this model is inconsistent. We propose a contingent model in which the effect of training on IS implementation success is a function of technical complexity and task interdependence. A meta-analysis of the literature finds strong support for the model, explaining the inconsistent findings reported in the literature. Implications for theory and practice are discussed.
Using a systems perspective, a conceptual model is developed that encompasses a broad class of systems whose fundamental purpose is the support of managerial actions and decision making. The term management support systems (MSS) is used to label this broad class. This model is based on an extensive review of the relevant literature and available research. The result provides an integrated systems model of the phenomena involved and points to gaps in the research that arise largely from the attempts to examine various classes of MSS as separate entities. The research presented here is based on the premise that there are fundamental core consistencies or similarities among various types of systems that have evolved in the past several decades to support decision making. It presents a conceptual, theoretical model drawn from findings about various types of support systems described in the literature such as decision support systems (DSS), executive information systems (EIS), knowledge management systems (KMS), and business intelligence (BI). Pragmatic insights are provided by the conceptual model and recommendations for future research are discussed.
Although the management of information assets—specifically, of text documents that make up 80 percent of these assets— an provide organizations with a competitive advantage, the ability of information retrieval (IR) systems to deliver relevant information to users is severely hampered by the difficulty of disambiguating natural language. The word ambiguity problem is addressed with moderate success in restricted settings, but continues to be the main challenge for general settings, characterized by large, heterogeneous document collections.In this paper, we provide preliminary evidence for the usefulness of statistical natural language processing (NLP) techniques, and specifically of collocation indexing, for IR in general settings. We investigate the effect of three key parameters on collocation indexing performance: directionality, distance, and weighting. We build on previous work in IR to (1) advance our knowledge of key design elements for collocation indexing, (2) demonstrate gains in retrieval precision from the use of statistical NLP for general-settings IR, and, finally, (3) provide practitioners with a useful cost-benefit analysis of the methods under investigation.
As organizations operate across greater distances, scholars are increasingly interested in the work of geographically dispersed teams and the technologies that they use to communicate and coordinate their work. However, research has generally not specified the dimensions (spatial, temporal, or configurational) and degrees of team dispersion, nor has it articulated the theoretical connections between those dimensions and important team outcomes. This research essay expands upon previous field and lab studies of dispersed teamwork by presenting a new conceptualization of dispersion as a continuous, multidimensional construct, in which each dimension is theoretically linked with different outcomes. We illustrate this new conceptualization with a series of examples from real dispersed teams and present implications for research regarding technology use.
In this study, we propose that perceived website complexity (PWC) is central to understanding how sophisticated features of a website (such as animation, audio, video, and rollover effects) affect a visitor’s experience at the site. Although previous research suggests that several elements of perceived complexity (e.g., amount of text, animation, graphics, range and consistency of web pages configuring a website, ease of navigating through it, and clarity of hyperlinks) affect important user outcomes, conflicting results yielded by previous research have created an important debate: does complexity enhance or inhibit user experience at a website. In this study, we draw on the task complexity literature to develop a broad and holistic model that examines the antecedents and consequences of PWC. Our results provide two important insights into the relationship between PWC and user outcomes. First, the positive relationship between objective complexity and PWC was moderated by user familiarity. Second, online task goals (goal-directed search and experiential browsing) moderated the relationship between PWC and user satisfaction. Specifically, the relationship between PWC and user satisfaction was negative for goal-directed users and inverted-U for experiential users. The implications of this finding for the practice of website design are discussed.
This study assesses and compares four product presentation formats currently used online: static pictures, videos without narration, videos with narration, and virtual product experience (VPE), where consumers are able to virtually feel, touch, and try products. The effects of the four presentation formats on consumers’ product understanding as well as the moderating role of the complexity of product understanding tasks were examined in a laboratory experiment.Two constructs used to measure product understanding performance are actual product knowledge and perceived website diagnosticity (i.e., the extent to which consumers believe a website is helpful for them to understand products). The experimental results show that (1) both videos and VPE lead to higher perceived website diagnosticity than static pictures; (2) under a moderate task complexity condition, VPE and videos lead to the same level of actual product knowledge, but all are more effective than static pictures; (3) under a high task complexity condition, all four presentation formats are equally effective in terms of actual product knowledge. Moreover, the results also indicate that it is perceived website diagnosticity, not actual product knowledge, that affects the perceived usefulness of websites, which further influences consumers’ intentions to revisit the websites.
Information systems professionals increasingly face changes in their work environment. Some of these changes are incremental, but many require fundamental shifts in mindset (referred to as a mindshift). Within the domain of software development, previous research has determined that veteran developers experience difficulty making the transition to new forms of development. Although prior research has brought awareness to the problems caused by a mindshift and has provided some insight, it has not answered the question of why software developers have difficulty making the transition. This study begins to answer that question by positing and examining the mindshift learning theory (MLT). The MLT suggests that the degree of perceived novelty of the fundamental concepts that characterize the new mindset will impact learning. Specifically, concepts may be perceived as novel (i.e., not familiar to the learner), changed (i.e., similar to a known concept, but a different meaning in the new context), or carryover (i.e., known concept with a similar meaning in the new context). As an exemplar mindshift learning situation, this study explores the phenomenon in the context of software developers transitioning from traditional to object-oriented (OO) software development. Findings indicate that software developers had higher knowledge scores on the OO concepts they perceived as novel or carryover compared to those they perceived as changed. Thus, developers experienced detrimental interference from their existing traditional software development knowledge structure when trying to learn OO software development. The findings have implications for organizations and individuals as an understanding of mindshifts could mean an easier transition through decreased frustration and a more effective learning process.
This study combines a narrative review with meta-analytic techniques to yield important insights about the existing research on turnover of information technology professionals. Our narrative review of 33 studies shows that the 43 antecedents to turnover intentions of IT professionals could be mapped onto March and Simon’s (1958) distal-proximal turnover framework. Our meta-analytic structural equation modeling shows that proximal constructs of job satisfaction (reflecting the lack of desire to move) and perceived job alternatives (reflecting ease of movement) partially mediate the relationships between the more distal individual attributes, job-related and perceived organizational factors, and IT turnover intentions. Building on the findings from our review, we propose a new theoretical model of IT turnover that presents propositions for future research to address existing gaps in the IT literature.
Internet auctions demonstrate that advances in information technologies can create more efficient venues of exchange between large numbers of traders. However, the growth of Internet auctions has been accompanied by a corresponding growth in Internet auction fraud. Much extant research on Internet auction fraud in the information systems literature is conducted at the individual level of analysis, thereby limiting its focus to the choices of individual traders or trading dyads. The criminology literature, in contrast, recognizes that social and community factors are equally important influences on the perpetration and prevention of crime. We employ social disorganization theory as a lens to explain how online auction communities address auction fraud and how those communities interact with formal authorities. We show how communities may defy, coexist, or cooperate with the formal authority of auction houses. These observations are supported by a qualitative analysis of three cases of online anticrime communities operating in different auction product categories. Our analysis extends aspects of social disorganization theory to online communities. We conclude that community-based clan control may operate in concert with authority-based formal control to manage the problem of Internet auction fraud more effectively.
While researchers go to great lengths to justify and prove theoretical links between constructs, the relationship between measurement items and constructs is often ignored. By default, the relationship between construct and item is assumed to be reflective, meaning that the measurement items are a reflection of the construct. Many times, though, the nature of the construct is not reflective, but rather formative. Formative constructs occur when the items describe and define the construct rather than vice versa.In this research, we examine whether formative constructs are indeed being mistaken for reflective constructs by information systems researchers. By examining complete volumes of MIS Quarterly and Information Systems Research over the last 3 years, we discovered that a significant number of articles have indeed misspecified formative constructs. For scientific results to be valid, we argue that researchers must properly specify formative constructs. This paper discusses the implications of different patterns of common misspecifications of formative constructs on both Type I and Type II errors. To avoid these errors, the paper provides a roadmap to researchers to properly specify formative constructs. We also discuss how to address formative constructs within a research model after they are specified.
This study directly tests the effect of personality and cognitive style on three measures of Internet use. The results support the use of personality—but not cognitive style—as an antecedent variable. After controlling for computer anxiety, self-efficacy, and gender, including the “Big Five” personality factors in the analysis significantly adds to the predictive capabilities of the dependent variables. Including cognitive style does not. The results are discussed in terms of the role of personality and cognitive style in models of technology adoption and use.
Past research in the area of information systems acceptance has primarily focused on initial adoption under the implicit assumption that IS usage is mainly determined by intention. While plausible in the case of initial IS adoption, this assumption may not be as readily applicable to continued IS usage behavior since it ignores that frequently performed behaviors tend to become habitual and thus automatic over time. This paper is a step forward in defining and incorporating the “habit” construct into IS research. Specifically, the purpose of this study is to explore the role of habit and its antecedents in the context of continued IS usage.Building on previous work in other disciplines, we define habit in the context of IS usage as the extent to which people tend to perform behaviors (use IS) automatically because of learning. Using recent work on the continued usage of IS (IS continuance), we have developed a model suggesting that continued IS usage is not only a consequence of intention, but also of habit. In particular, in our research model, we propose IS habit to moderate the influence of intention such that its importance in determining behavior decreases as the behavior in question takes on a more habitual nature. Integrating past research on habit and IS continuance further, we suggest how antecedents of behavior/behavioral intention as identified by IS continuance research relate to drivers of habitualization.We empirically tested the model in the context of voluntary continued WWW usage. Our results support the argument that habit acts as a moderating variable of the relationship between intentions and IS continuance behavior, which may put a boundary condition on the explanatory power of intentions in the context of continued IS usage. The data also support that satisfaction, frequency of past behavior, and comprehensiveness of usage are key to habit formation and thus relevant in the context of IS continuance behavior. Implications of these findings are discussed and managerial guidelines presented.
Aligning social structures and technology capabilities is a significant challenge to information technology-related organizational change. It is particularly challenging in institutionalized settings such as hospitals. We report an interpretive field study of computerized physician order entry (CPOE) at an acute-care hospital, in which we investigated how institutionally triggered and technology-triggered change interacted in complementary processes to engender alignment. Social structure changes included increased interdependency among clinical departments, multidisciplinary cooperation across clinical disciplines, and standardization in clinical decision-making. Organization members also enacted institutionalized interaction patterns with physicians by deferring to their preferences for CPOE use. The cumulative influence of change triggers nonetheless facilitated the hospital’s realization of clinical goals. We drew on Barley’s (1990) role- and network-based model for technology and structure alignment. Nonetheless, we extended this micro-level analytic approach to account for the influence of change in the macro-institutional environment. Our analysis clarified the extent of structure change attributable to the CPOE technology and highlighted institutional forces that promoted yet inhibited change. The case also highlighted the importance of role networks on the trajectory and outcomes of organizational change processes.
The objective of this paper is to contribute to a deeper understanding of system usage in organizations by examining its multilevel nature. Past research on system usage has suffered from a levels bias, with researchers studying system usage at single levels of analysis only (e.g., the individual, group, or organizational level). Although single-level research can be useful, we suggest that studying organizations one level at a time will ultimately lead to an unnatural, incomplete, and very disjointed view of how information systems are used in practice. To redress this situation, we draw on recent advances in multilevel theory to present system usage as a multilevel construct and provide an illustration for what it takes for researchers to study it as such. The multilevel perspective advanced in this article offers rich opportunities for theoretical and empirical insights and suggests a new foundation for in-depth research on the nature of system usage, its emergence and change, and its antecedents and consequences.
Prior research on technological frames indicates that many of the difficulties associated with systems implementation stem from differences in the meanings users, managers, and system developers attribute to automation projects. Although the concept of technological frames has been used to explore the bases for intergroup conflict during implementation, it is also a useful device for probing more deeply into the effects complex systems have on users’ perceptions of their work and the role-altering effects of new technologies. Drawing upon personal construct theory and job characteristics theory, we adapted the repertory grid technique to explore the technology-in-use frames of a group of occupationally certified fingerprint technicians (FPTs). Our investigation reveals the important role the FPTs’ occupationally defined values and norms played in structuring their existing work practices and the tensions produced by organizationally mandated efforts to restructure the logic of their expertise-based hierarchies. These insights illuminate the effects work redesign had on the FPTs’ task environment, the process logic that guided specific work practices, and the roles defined by their expertise-based hierarchies, providing a basis for understanding the FPTs’ unanticipated reactions to the automation of their work.
As the role of virtual teams in organizations becomes increasingly important, it is crucial that companies identify and leverage team members’ knowledge. Yet, little is known of how virtual team members come to recognize one another’s knowledge, trust one another’s expertise, and coordinate their knowledge effectively. In this study, we develop a model of how three behavioral dimensions associated with transactive memory systems (TMS) in virtual teams—expertise location, task–knowledge coordination, and cognition-based trust—and their impacts on team performance change over time. Drawing on the data from a study that involves 38 virtual teams of MBA students performing a complex web-based business simulation game over an 8-week period, we found that in the early stage of the project, the frequency and volume of task-oriented communications among team members played an important role in forming expertise location and cognition-based trust. Once TMS were established, however, task-oriented communication became less important. Instead, toward the end of the project, task–knowledge coordination emerges as a key construct that influences team performance, mediating the impact of all other constructs. Our study demonstrates that TMS can be formed even in virtual team environments where interactions take place solely through electronic media, although they take a relatively long time to develop. Furthermore, our findings show that, once developed, TMS become essential to performing tasks effectively in virtual teams.
In this paper, we develop a framework for understanding contribution behaviors, which we define as voluntary acts of helping others by providing information. Our focus is on why and how people make contributions in geographically distributed organizations where contributions occur primarily through information technologies. We develop a model of contribution behaviors that delineates three mediating mechanisms: (1) awareness; (2) searching and matching; and (3) formulation and delivery. We specify the cognitive and motivational elements involved in these mechanisms and the role of information technology in facilitating contributions. We discuss the implications of our framework for developing theory and for designing technology to support contribution behaviors.This paper is a recipient of one of the 2009 Emerald Management Reviews Citations of Excellence Awards.
In the decision support systems literature, most studies have concentrated on the direct effects of DSS use and design on decision outcomes and user performance in the workplace. Fewer DSS studies have integrated decision process variables, such as user beliefs and attitudes, in their models. In this paper, we examine the mediating role of decision process variables in the use of an online customer DSS. We do so through an experimental study of an alternative-based and an attribute-based DSS for product customization by online customers. Using cognitive fit and flow theories, we develop a theoretical model with four mediating decision process variables (perceived usefulness, perceived ease of use, perceived enjoyment, and perceived control) and two of their antecedents: interface design (attribute-based versus alternative-based) and task complexity (choice set size). Our results show that the impact of DSS interface design on behavioral intentions is fully mediated by perceived usefulness and perceived enjoyment, although not by perceived control. Specifically, we verify that users of an attribute-based DSS express higher perceived usefulness and perceived enjoyment than users of an alternative-based one. In addition, we find that task complexity has an interesting relationship with usefulness and enjoyment, both of which follow an inverted U-shaped curve as choice set size increases. Finally, we find that for users of the alternative-based DSS, perceived ease of use and perceived control decrease as task complexity increases. However, the attribute-based DSS alleviates that decline for both variables. Among other contributions, our results indicate the importance of including decision process variables when studying DSS as well as the complex effect of task complexity on those variables. Our study also provides some important guidelines for online companies that provide customer DSS on their websites, especially the danger of providing too many product choice options that can overwhelm customers and harm their shopping experience.
Digital inequality is one of the most critical issues in the knowledge economy. The private and public sectors have devoted tremendous resources to address such inequality, yet the results are inconclusive. Theoretically grounded empirical research is needed both to expand our understanding of digital inequality and to inform effective policy making and intervention. The context of our investigation is a city government project, known as the LaGrange Internet TV initiative, which allowed all city residents to access the Internet via their cable televisions at no additional cost. We examine the residents’ post-implementation continued use intentions through a decomposed theory of planned behavior perspective, which is elaborated to include personal network exposure. Differences in the behavioral models between socio-economically advantaged and disadvantaged users who have direct usage experience are theorized and empirically tested. The results reveal distinct behavioral models and isolate the key factors that differentially impact the two groups. The advantaged group has a higher tendency to respond to personal network exposure. Enjoyment and confidence in using information and communication technologies, availability, and perceived behavioral control are more powerful in shaping continued ICT use intention for the disadvantaged. Implications for research and practice are discussed.
The Internet has the potential to fundamentally change the structure of marketing channels, but only if consumers choose to adopt electronic channels. Thus, this paper aims to develop a more nuanced understanding of consumer channel choices. Specifically, it contends that it is important to examine consumers’ intent to adopt electronic channels, not as a monolithic decision, but as a choice they make at each of four stages in the purchase process: requirements determination, vendor selection, purchase, and after-sales service. Innovation diffusion theory suggests that consumers make adoption decisions based on their perceptions of the relative advantage of the innovation. The relative advantage of electronic channels is conceptualized as a multidimensional construct involving a cumulative assessment of the perceived relative merits of channels on three dimensions: convenience, trust, and efficacy of information acquisition. Combining the multidimensional nature of relative advantage with the multistage purchase process, the central assertion, and intended contribution, of this paper is to show that the relative advantage of electronic channels, and the influence of each dimension of relative advantage on the adoption of electronic channels, will vary across the different stages of the purchase process.Survey data were collected from faculty and staff at a large university about their intention to use the web for auto insurance transactions. The results provide support for the multidimensional nature of relative advantage, although the emergent factors do not align neatly with the hypothesized dimensions (convenience, trust, and efficacy of information acquisition) or stages. Results of the study support three conclusions. First, the dimensions along which consumers assess relative advantage blend hypothesized dimensions such as trust and convenience with stages of the purchase process. Second, consumers consider the relative advantage of channels at two distinct stages of the purchase process: gathering information and executing the transaction. Third, different dimensions of relative advantage are critical in predicting consumer channel choice at each stage.
This study identifies governance patterns for information technology investment decision processes and explores the impact of organizations’ investment characteristics, external environment, and internal context on the shaping of those patterns. By identifying the lead actors of the initiation, development, and approval stages in IT governance, the patterns of 57 IT investment decisions at 6 hospitals are analyzed. The results reveal seven IT governance archetypes: (1) top management monarchy, (2) top management–IT duopoly, (3) IT monarchy, (4) administration monarchy, (5) administration– IT duopoly, (6) professional monarchy, and (7) professional– IT duopoly. Each archetype is analyzed by taking into account four specific factors: IT investment level, external influence, organizational centralization, and IT function power. This study makes several contributions to IT governance theory and practice. First, IT governance is reframed to include pre-decision stages, highlighting the importance of participants other than the final decision maker. Second, the variation of IT governance archetypes suggests that even when top management approval is required, the IT department may not play a key role in the IT investment decision process. Third, governance of the pre-decision initiation and development stages is found to be jointly affected by several contextual factors, suggesting that the allocation of final decision rights is only a part of IT governance. While decision rights may be allocated by the organization a priori, the actual patterns of IT governance are contingent on contextual factors. It is important to understand how IT governance archetypes are shaped because they may affect desired outcomes of IT investments.
This paper takes a first step in aiding researchers to improve the relevance of their research to practice. By proposing that Information Systems researchers conduct applicability checks with practitioners on the research objects (for example, theories, models, frameworks, processes, technical artifacts, or other theoretically based IS artifacts) they either produce or use in theory-focused research, our paper presents an actionable, systematic approach to evaluating, establishing, and further improving research relevance. Furthermore, because it is an approach that can be conducted as an additional step either at the beginning or the end of the traditional research life cycle, it leaves untouched the rigorous methods used to conduct the study, that is, it does not compromise traditional research models. The approach we propose is based on the analyses of three dimensions of relevance that are critical to practitioners’ attempts to internalize IS research findings (importance, accessibility, and suitability), and a comprehensive set of solutions that can be used to address them. Our analysis reveals that the most critical dimension for practice is the importance of the research to the needs of practice. The solution we propose to address that need is to conduct an applicability check on the research objects of interest. The applicability check forms an integral part of the research process, either prior to or following engagement in a typical research process. We present principles and criteria for the conduct and evaluation of an applicability check, which is primarily based on the focus group method, and secondarily on a modified nominal group technique.
The work of the contemporary British sociologist Anthony Giddens, and in particular his structuration theory, has been widely cited by Information Systems researchers. This paper presents a critical review of the work of Giddens and its application in the Information Systems field. Following a brief overview of Giddens’s work as a whole, some key aspects of structuration theory are described, and their implications for Information Systems research discussed. We then identify 331 Information Systems articles published between 1983 and 2004 that have drawn on Giddens’s work and analyze their use of structuration theory. Based on this analysis a number of features of structurational research in the Information Systems field and its relationship to Giddens’s ideas are discussed. These findings offer insight on Information Systems researchers’ use of social theory in general and suggest that there may be significant opportunities for the Information Systems field in pursuing structurational research that engages sympathetically, yet critically, with Giddens’s work.
End user satisfaction (EUS) is critical to successful information systems implementation. Many EUS studies in the past have attempted to identify the antecedents of EUS, yet most of the relationships found have been criticized for their lack of a strong theoretical underpinning. Today it is generally understood that IS failure is due to psychological and organizational issues rather than technological issues, hence individual differences must be addressed. This study proposes a new model with an objective to extend our understanding of the antecedents of EUS by incorporating three well-founded theories of motivation, namely expectation theory, needs theory, and equity theory. The uniqueness of the model not only recognizes the three different needs (i.e., work performance, relatedness, and self-development) that users may have with IS use, but also the corresponding inputs required from each individual to achieve those needs fulfillments, which have been ignored in most previous studies. This input/needs fulfillment ratio, referred to as equitable needs fulfillment, is likely to vary from one individual to another and satisfaction will only result in a user if the needs being fulfilled are perceived as “worthy” to obtain.The partial least squares (PLS) method of structural equation modeling was used to analyze 922 survey returns collected form the hotel and airline sectors. The results of the study show that IS end users do have different needs. Equitable work performance fulfillment and equitable relatedness fulfillment play a significant role in affecting the satisfaction of end users. The results also indicate that the impact of perceived IS performance expectations on EUS is not as significant as most previous studies have suggested. The conclusion is that merely focusing on the technical soundness of the IS and the way in which it benefits employees may not be sufficient. Rather, the input requirements of users for achieving the corresponding needs fulfillments also need to be examined.
In this paper, we present an economic learning model that helps to formalize the complex relationships among an offshoring firm’s knowledge levels, production costs, and coordination costs. Specifically, we model a domestic firm’s use of a selective offshore strategy (i.e., offshoring only a portion of its information technology activities) to exploit, through IT investments or contractual provisions, the foreign vendor’s large, scale-driven repository of production knowledge. We illustrate the conditions under which knowledge transfers during offshoring may reduce a domestic firm’s in-house production costs, leading to total cost savings in both the short term and the long term. Alternatively, when knowledge transfers are not sufficiently large, some short-lived offshoring projects may generate substantial cost savings to the domestic firm; however, long-lived offshoring projects may cause a disruption in the knowledge supply chain, resulting in substantial losses in the later stages of the project. A firm that fails to realize the costs associated with such a disruption soon enough in the project life may find itself locked into a disadvantageous offshoring agreement without any recourse. However, a domestic firm may be able to overcome a disruption in its knowledge supply chain by exploiting the learning-by-doing production knowledge generated by the foreign vendor’s economies of scale. The managerial implications derived from our learning model may help guide firms as they consider the impacts of offshore contracts and knowledge management investments on firm knowledge, production costs, and coordination costs.
In this paper we develop a learning-mediated model of offshore software project productivity and quality to examine whether widely adopted structured software processes are effective in mitigating the negative effects of work dispersion in offshore software development. We explicate how the key process areas of the capability maturity model (CMM) can be utilized as a platform to launch learning routines in offshore software development and thereby explain why some offshore software development process improvement initiatives are more effective than others. We validate our learning-mediated model of offshore software project performance by utilizing data collected from 42 offshore software projects of a large firm that operates at the CMM level-5 process maturity. Our results indicate that investments in structured processes mitigate the negative effects of work dispersion in offshore software development. We also find that the effect of software process improvement initiatives is mediated through investments in process-based learning activities. These results imply that investments in structured processes and the corresponding process-based learning activities can be an economically viable way to counter the challenges of work dispersion and improve offshore project performance. We discuss the implication of these results for the adoption of normative process models by offshore software firms.
This paper investigates two-stage offshoring as experienced by the Irish sites of two large global companies, headquartered in the United States, with significant software development operations. As part of these companies, the Irish sites act as a bridge in their offshoring arrangements: While the U.S. sites offshore work to Ireland, the Irish sites offshore work further to India and, hence, have experience of being both customer and vendor in two-stage offshore sourcing relationships. Using a framework derived from relational exchange theory (RET), we conducted multiple case study research to investigate and develop an initial theoretical model of the implementation of this two-stage offshoring bridge model. Our study shows that while both companies act as bridges in two-stage offshoring arrangements, their approaches differ in relation to (1) team integration, (2) organizational level implementation, and (3) site hierarchy. Although, there are opportunities afforded by the bridge model at present, the extent to which these opportunities will be viable into the future is open to question. As revealed in our study, temporal location seems to favor a bridge location such as Ireland, certainly with United States–Asian partners. However, location alone will not be enough to maintain position in future two-stage offshoring arrangements. Furthermore, our research supports the view that offshoring tends to progress through a staged sequence of progressively lower cost destinations. Such a development suggests that two-stage offshoring, as described in this paper, will eventually become what we would term multistage offshoring.
Gaining economic benefits from substantially lower labor costs has been reported as a major reason for offshoring labor-intensive information systems services to low-wage countries. However, if wage differences are so high, why is there such a high level of variation in the economic success between offshored IS projects? This study argues that offshore outsourcing involves a number of extra costs for the client organization that account for the economic failure of offshore projects. The objective is to disaggregate these extra costs into their constituent parts and to explain why they differ between offshored software projects. The focus is on software development and maintenance projects that are offshored to Indian vendors. A theoretical framework is developed a priori based on transaction cost economics (TCE) and the knowledge-based view of the firm, complemented by factors that acknowledge the specific offshore context. The framework is empirically explored using a multiple case study design including six offshored software projects in a large German financial service institution. The results of our analysis indicate that the client incurs post- contractual extra costs for four types of activities: (1) requirements specification and design, (2) knowledge transfer, (3) control, and (4) coordination. In projects that require a high level of client-specific knowledge about idiosyncratic business processes and software systems, these extra costs were found to be substantially higher than in projects where more general knowledge was needed. Notably, these costs most often arose independently from the threat of opportunistic behavior, challenging the predominant TCE logic of market failure. Rather, the client extra costs were particularly high in client-specific projects because the effort for managing the consequences of the knowledge asymmetries between client and vendor was particularly high in these projects. Prior experiences of the vendor with related client projects were found to reduce the level of extra costs but could not fully offset the increase in extra costs in highly client-specific projects. Moreover, cultural and geographic distance between client and vendor as well as personnel turnover were found to increase client extra costs. Slight evidence was found, however, that the cost-increasing impact of these factors was also leveraged in projects with a high level of required client-specific knowledge (moderator effect).
This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy—opensourcing, as we term it here—whereby commercial companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had “liberated” what had hitherto been proprietary software and sought to grow a global open source community around their product. We followed this with a large-scale survey involving additional exemplars of the phenomenon. The study identifies a number of symmetrical and complementary customer and community obligations that are associated with opensourcing success. We also identify a number of tension points on which customer and community perceptions tend to vary. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness: The customer and community need to establish a trusted partnership of shared responsibility in building an overall opensourcing ecosystem. The study reveals an ongoing shift from OSS as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises. It also reveals that opensourcing provides ample opportunity for companies to headhunt top developers, hence moving from outsourcing to a largely unknown OSS workforce toward recruitment of developers from a global open source community whose talents have become known as a result of the opensourcing experience.
In a world that is flat, where all clients and providers can easily transact with one another, offshoring represents the proposition that information technology providers from low-wage nations can now underbid providers from high-wage nations and win contracts. We examined a particularly flat “world”—an online programming marketplace—and found that this profound tilt to low-wage nations is overstated. We analyzed the entire history of transactions at one of the major online programming marketplaces, a marketplace for outsourcing small IT projects. The data spanned 38 months and included over 263,000 bids by over 31,000 providers from 70 countries on over 20,000 small IT projects requested by over 7,900 clients from 59 countries.Contrary to the world-is-flat proposition, the data in this particular site show some client preference for domestic providers. However, the largest group of clients, the American clients, are a marked exception to clients in the rest of the world: they give relatively less preference to domestic providers. In a sense, the American clients have a higher preference for offshore providers. Among non-American clients the preference for domestic providers is mitigated when both client and provider are from an English-speaking nation. Relative bid price, often very low already, also determines the winning bid, as does the ratio of purchasing power parity (PPP) between the country of the client and the country of the provider. Nonetheless, the strongest determinant of the winning bid is client loyalty: the client gives very strong preference to a provider with whom there has been a previous relationship, regardless of whether the provider is offshore or domestic.
Increasingly, firms source more complex and strategic as well as harder to codify information technology projects to low-cost offshore locations. Completing such projects successfully requires close collaboration among all participants. Yet, achieving such collaboration is extremely difficult because of the complexity of the context: multiple and overlapping boundaries associated with diverse organizational and national contexts separate the participants. These boundaries also lead to a pronounced imbalance of resources among onshore and offshore contributors giving rise to status differences and inhibiting collaboration. This research adopts a practice perspective to investigate how differences in country and organizational contexts give rise to boundaries and associated status differences in offshore application development projects and how these boundaries and status differences can be renegotiated in practice to establish effective collaboration. To illustrate and refine the theory, a qualitative case study of a large financial services firm, which sourced a variety of high-end IT work to its wholly owned subsidiaries (“captive centers”) and to third party vendors in multiple global locations (in particular, to India and Russia), is presented. Using a grounded theory approach, the paper finds that differences in country contexts gave rise to a number of boundaries that inhibited collaboration effectiveness, while differences in organizational contexts were largely mediated through organizational practices that treated vendor centers and captive units similarly. It also shows that some key onshore managers were able to alleviate status differences and facilitate effective collaboration across diverse country contexts by drawing on their position and resources. Implications are drawn for the theory and practice of global software development and multiparty collaboration.
Studies have shown the knowledge transfer problems that arise when communication and storage technologies are employed to accomplish work across time and space. Much less is known about knowledge transfer problems associated with transformational technologies, which afford the creation, modification, and manipulation of digital artifacts. Yet, these technologies play a critical role in offshoring by allowing the distribution of work at the task level, what we call task-based offshoring. For example, computer-aided engineering applications transform input like physical dimensions, location coordinates, and material properties into computational models that can be shared electronically among engineers around the world as they work together on analysis tasks. Digital artifacts created via transformational technologies often embody implicit knowledge that must be correctly interpreted to successfully act upon the artifacts. To explore what problems might arise in interpreting this implicit knowledge across time and space, and how individuals might remedy these problems, we studied a firm that sent engineering tasks from home sites in Mexico and the United States to an offshore site in India. Despite having proper formal education and ample tool skills, the Indian engineers had difficulty interpreting the implicit knowledge embodied in artifacts sent to them from Mexico and the United States. To resolve and prevent the problems that subsequently arose, individuals from the home sites developed five new work practices to transfer occupational knowledge to the offshore site. The five practices were defining requirements, monitoring progress, fixing returns, routing tasks strategically, and filtering quality. The extent to which sending engineers in our study were free from having to enact these new work practices because on-site coordinators acted on their behalf predicted their perceptions of the effectiveness of the offshoring arrangement, but Indian engineers preferred learning from sending engineers, not on-site coordinators. Our study contributes to theories of knowledge transfer and has practical implications for managing task-based offshoring arrangements.
Achieving shared, common, or mutual understandings among geographically dispersed workers is a central concern in the distributed work literature. Nonetheless, little is known yet about the socio-cognitive acts and communication processes involved with synchronizing and cocreating understandings in such settings. Building on a case study of a geographically distributed information systems development project at one of India’s largest offshore vendors, we postulate that knowledge and experience asymmetries, and requirements and task characteristics (such as complexity, instability, ambiguity, and novelty) prompt onsite and offshore team members to engage in acts of sensegiving, sensedemanding, and sensebreaking. This allows them to make sense of their tasks and their environment, and it increases the likelihood that congruent and actionable understandings emerge. Furthermore, it assists them in cocreating novel understandings, especially when acts of sensegiving and sensedemanding are complemented with instances of sensebreaking. Our results contribute to the literature by explaining how distributed team members mitigate problems of understanding, transfer preexisting understandings, and cocreate novel understandings. Acts of sensegiving, sensedemanding, and sensebreaking allow distributed team members to jointly explore and generate value, thereby amplifying the performance of distributed workers.
This study examines the role of business familiarity in determining how software development outsourcing projects are managed and priced to address risks. Increased business familiarity suggests both more prior knowledge, and hence reduced adverse selection risk, and increased implied trust about future behavior, and hence implied reduced moral hazard risk. Preferring high business familiarity partners may also alleviate concerns about incomplete contracts. By reducing these risks, higher business familiarity is hypothesized to be associated with higher priced projects, reduced penalties, and an increased tendency to contract on a time and materials rather than a fixed price basis. These hypotheses were examined with objective contractual legal data from contracts made by a leading international bank. Integrating trust theory into agency theory and into incomplete contract theory and examining unique contract data, the contribution of the study is to show that the premium on business familiarity and the trust it implies is not in directly affecting price, but, rather, in changing how the relationship is managed toward a tendency to sign time and materials contracts. Implications about integrating trust into agency theory and incomplete contract theory, as well as implications regarding trust premiums and software development outsourcing, are discussed.
The part-of construct is a fundamental element of many conceptual modeling grammars that is used to associate one thing (a component) with another thing (a composite). Substantive theoretical issues surrounding the part-of construct remain to be resolved, however. For instance, contrary to widespread claims, some researchers now argue the relationship between components and composites is not always transitive. Moreover, how the part-of construct should be represented in a conceptual schema diagram remains a contentious issue. Some analysts argue composites should be represented as a relationship or association. Others argue they should be represented as an entity. In this paper we use an ontological theory to support our arguments that composites should be represented as entities and not relationships or associations. We also describe an experiment that we undertook to test whether representing composites as relationships or entities enables users to understand a domain better. Our results support our arguments that using entities to represent composites enables users to better understand a domain.
Although Internet users are expected to respond in various ways to privacy threats from online companies, little attention has been paid so far to the complex nature of how users respond to these threats. This paper has two specific goals in its effort to fill this gap in the literature. The first, so that these outcomes can be systematically investigated, is to develop a taxonomy of information privacy-protective responses (IPPR). This taxonomy consists of six types of behavioral responses—refusal, misrepresentation, removal, negative word-of-mouth, complaining directly to online companies, and complaining indirectly to third-party organizations—that are classified into three categories: information provision, private action, and public action. Our second goal is to develop a nomological model with several salient antecedents—concerns for information privacy, perceived justice, and societal benefits from complaining—of IPPR, and to show how the antecedents differentially affect the six types of IPPR. The nomological model is tested with data collected from 523 Internet users. The results indicate that some discernible patterns emerge in the relationships between the antecedents and the three groups of IPPR. These patterns enable researchers to better understand why a certain type of IPPR is similar to or distinct from other types of IPPR. Such an understanding could enable researchers to analyze a variety of behavioral responses to information privacy threats in a fairly systematic manner. Overall, this paper contributes to researchers’ theory-building efforts in the area of information privacy by breaking new ground for the study of individuals’ responses to information privacy threats.
Culture plays an increasingly important role in information systems initiatives, and it receives considerable attention from researchers who have studied a variety of aspects of its role in IS initiatives. Notwithstanding the contributions of research to date, our knowledge of how culture influences— and is influenced by—the development and use processes and an information system itself remains fragmented. Knowledge fragmentation is amplified by the fact that conceptualizations of culture differ among researchers. Indeed, most researchers agree that culture consists of patterns of meaning underlying a variety of manifestations. Researchers diverge, however, on the degree of consensus on these interpretations that they assume to be reached within a collective. In order to integrate these divergent conceptualizations of culture, we adopt the view that no single perspective is sufficient to capture the complexity of interplay between culture, the processes of developing and using an IS, and the IS itself. We have, therefore, adopted a conceptualization that views culture from three perspectives—integration, differentiation, and fragmentation—that come into play simultaneously and jointly. Using this conceptualization, the paper synthesizes what is known about the role of culture in IS initiatives, and proposes a model of the relationships between culture, the development and use processes, and an information system.
What is the intellectual core of the information systems discipline? This study uses latent semantic analysis to examine a large body of published IS research in order to address this question. Specifically, the abstracts of all research papers over the time period from 1985 through 2006 published in three top IS research journals—MIS Quarterly, Information Systems Research, and Journal of Management Information Systems—were analyzed. This analysis identified five core research areas: (1) information technology and organizations; (2) IS development; (3) IT and individuals; (4) IT and markets; and (5) IT and groups. Over the time frame of our analysis, these core topics have remained quite stable. However, the specific research themes within each core area have evolved significantly, reflecting research that has focused less on technology development and more on the social context in which information technologies are designed and used. As such, this analysis demonstrates that the information systems academic discipline has maintained a relatively stable research identity that focuses on how IT systems are developed and how individuals, groups, organizations, and markets interact with IT.
The information technology professional is regularly expected to work with colleagues in both IT and other areas of the organization. During these interactions, the IT employee is expected to conform to occupational or organizational norms regarding the display of emotion. How do these display norms affect the IT professional? This study examines an IT professional’s emotional dissonance, the conflict between norms of emotional display and an employee’s felt emotion. Emotional dissonance is studied as a factor of IT professionals’ work exhaustion, job satisfaction, and turnover intention, modeled as an extension to the work of Moore (2000a). The results indicate emotional dissonance predicts work exhaustion better than do perceived workload, role conflict, or role ambiguity, constructs which have long been associated with work exhaustion. Job satisfaction is influenced directly by role ambiguity and work exhaustion. In turn, job satisfaction influences employee turnover intention. We discuss implications of these findings for both IT management and future research.
This paper expands, refines, and explicates media synchronicity theory, originally proposed in a conference proceeding in 1999 (Dennis and Valacich 1999). Media synchronicity theory (MST) focuses on the ability of media to support synchronicity, a shared pattern of coordinated behavior among individuals as they work together. We expand on the original propositions of MST to argue that communication is composed of two primary processes: conveyance and convergence. The familiarity of individuals with the tasks they are performing and with their coworkers will also affect the relative amounts of these two processes. Media synchronicity theory proposes that for conveyance processes, use of media supporting lower synchronicity should result in better communication performance. For convergence processes, use of media supporting higher synchronicity should result in better communication performance. We identify five capabilities of media (symbol sets, parallelism, transmission velocity, rehearsability, and reprocessability) that influence the development of synchronicity and thus the successful performance of conveyance and convergence communication processes. The successful completion of most tasks involving more than one individual requires both conveyance and convergence processes, thus communication performance will be improved when individuals use a variety of media to perform a task, rather than just one medium.
Businesses can choose who they want to be online. Product and company attributes that are directly perceivable in the real world can be manipulated to make a favorable impression on online buyers. This study examines whether creating a more professional online e-image can signal consumers about unobservable product or company quality, and whether this signal influences their willingness to transact with the company, and ultimately the prices they are willing to pay for the company’s goods and services. An empirical study is presented that examines two online auction businesses utilizing different company names and auction listing styles to sell items in parallel over the course of one year. The findings suggest that increasing the quality of an auction business’s e-image does increase consumers’ willingness to transact with the business, and increases prices received at auction. The study also demonstrates the ability to use eBay as an experimental laboratory for testing a variety of hypotheses about purchasing behavior online.
Employees’ underutilization of new information systems undermines organizations’ efforts to gain benefits from such systems. The two main predictors of individual-level system use in prior research—behavioral intention and facilitating conditions—have limitations that we discuss. We introduce behavioral expectation as a predictor that addresses some of the key limitations and provides a better understanding of system use. System use is examined in terms of three key conceptualizations: duration, frequency, and intensity. We develop a model that employs behavioral intention, facilitating conditions, and behavioral expectation as predictors of the three conceptualizations of system use. We argue that each of these three determinants play different roles in predicting each of the three conceptualizations of system use. We test the proposed model in the context of a longitudinal field study of 321 users of a new information system. The model explains 65 percent, 60 percent, and 60 percent of the variance in duration, frequency, and intensity of system use respectively. We offer theoretical and practical implications for our findings.
Content analysis of computer-mediated communication (CMC) is important for evaluating the effectiveness of electronic communication in various organizational settings. CMC text analysis relies on systems capable of providing suitable navigation and knowledge discovery functionalities. However, existing CMC systems focus on structural features, with little support for features derived from message text. This deficiency is attributable to the informational richness and representational complexities associated with CMC text. In order to address this shortcoming, we propose a design framework for CMC text analysis systems. Grounded in systemic functional linguistic theory, the proposed framework advocates the development of systems capable of representing the rich array of information types inherent in CMC text. It also provides guidelines regarding the choice of features, feature selection, and visualization techniques that CMC text analysis systems should employ. The CyberGate system was developed as an instantiation of the design framework. CyberGate incorporates a rich feature set and complementary feature selection and visualization methods, including the writeprints and ink blots techniques. An application example was used to illustrate the system’s ability to discern important patterns in CMC text. Furthermore, results from numerous experiments conducted in comparison with benchmark methods confirmed the viability of CyberGate’s features and techniques. The results revealed that the CyberGate system and its underlying design framework can dramatically improve CMC text analysis capabilities over those provided by existing systems.
Organizing phenomena into classes is a pervasive human activity. The ability to classify phenomena encountered in daily life in useful ways is essential to human survival and adaptation. Not surprisingly, then, classification-oriented activities are widespread in the information systems field. Classes or entity types play a central role in conceptual modeling for information systems requirements analysis, as well as in the design of databases and object-oriented software. Furthermore, classification is the primary task in applications such as data mining and the development of domain ontologies to support information sharing in semantic web applications. However, despite the pervasiveness of classification, little research has proposed well-grounded guidelines for identifying, evaluating, and choosing classes when modeling a domain or designing information systems artifacts. In this paper, we adopt the cognitive notions of inference and economy to derive a set of principles to guide effective and efficient classification. We present a model for characterizing what may be considered useful classes in a given context based on the inferences that can be drawn from membership in a class. This foundation is then used to suggest practical design rules for evaluating and refining potential classes. We illustrate the use of the rules by showing that applying them to a previously published example yields meaningful changes. We then present an evaluation by a panel of experts who compared the published and revised models. The evaluation shows that following the rules leads to semantically clearer models that are preferred by experts. The paper concludes by outlining possible future research directions.
Managers frequently face ill-structured or “wicked” problems. Such problems are characterized by a large degree of uncertainty with respect to how the problem should be approached and how to establish and evaluate the set of alternative solutions. A design theory nexus is a set of constructs and methods that enable the construction of models that connect numerous design theories with alternative solutions. It thereby offers a unique problem-solving approach that is particularly useful for addressing ill-structured or wicked problems. For each alternative solution in a design theory nexus one or more unique criteria are established to formulate a specific design theory. We develop a general method for constructing a design theory nexus and illustrate its utility using two field studies. One develops and applies an organizational change nexus. The other develops and applies a user involvement nexus. Each is a specific instantiation of the general design theory nexus constructs. Usingthese illustrations, we provide examples of how to evaluate
A major problem for firms making information technology investment decisions is predicting and understanding the effects of future technological developments on the value of present technologies. Failure to adequately address this problem can result in wasted organization resources in acquiring, developing, managing, and training employees in the use of technologies that are short-lived and fail to produce adequate return on investment. The sheer number of available technologies and the complex set of relationships among them make IT landscape analysis extremely challenging. Most IT-consuming firms rely on third parties and suppliers for strategic recommendations on IT investments, which can lead to biased and generic advice. We address this problem by defining a new set of constructs and methodologies upon which we develop an IT ecosystem model. The objective of these artifacts is to provide a formal problem representation structure for the analysis of information technology development trends and to reduce the complexity of the IT landscape for practitioners making IT investment decisions. We adopt a process theory perspective and use a combination of visual mapping and quantification strategies to develop our artifacts and a state diagram-based technique to represent evolutionary transitions over time. We illustrate our approach using two exemplars: digital music technologies and wireless networking technologies. We evaluate the utility of our approach by conducting in-depth interviews with IT industry experts and demonstrate the contribution of our approach relative to existing techniques for technology forecasting.
The space of design alternatives for a business process is typically very large, with technology, location, and other factors combining to generate seemingly endless possibilities. This paper introduces a set of artifacts that support process designers in their efforts to manage this critical business problem: (1) a grammar-based method to generate and manage business process design alternatives and (2) a software prototype that provides support for the use of this method. The method and prototype software are demonstrated with a grammar for a sales process. The method improves on existing approaches by offering the generative power of grammar-based methods while addressing the principal challenge to using such approaches in the design of business processes: the limitations on automated evaluation of alternatives and thus the need to provide designers with tools to manage the potentially overwhelming array of choices generated by design grammars.
In this paper, we propose a partial solution to the problem of the relevance of information systems research by adjusting doctoral programs to the specific needs and talents of doctoral students that have significant prior professional life experience. The purpose of this paper is first to recognize that the “professionally qualified doctoral student” (PQDS) has a different type of knowledge that may give her/him some advantages over other students, including greater symbolic capital. We examine the epistemic evidence for the claim that part of their practical experience constitutes a specific type of “applicative” knowledge that should be considered as different from but of equal value to theory, which has been the mainstay of academic education. Three independent lines of academic research contribute such evidence: the communities of practice literature, philosophical perspectives on applicative knowledge, and cognitive sciences. We argue that PQDSs may benefit from doctoral programs with specific features designed to leverage their practical knowledge. In turn, they may be able to “boundary span” and publish research results in forms that are appreciated by their professional communities. Finally we discuss some practical institutional issues that could be addressed if we are to sustain this profile of researchers.
Information technology enabled exchanges in electronic markets have significant implications for buyer–supplier relationships. Building on studies that emphasize the role of intangible assets in interorganizational relationships, this study argues that buyers are less likely to use reverse auctions for supplier relationships involving a high degree of non-contractibility. The argument complements traditional transaction cost economics arguments that focus on the impact of asset specificity and product specialization. We identify six dimensions of non-contractibility—quality, supplier technological investments, information exchange, responsiveness, trust, and flexibility—which encompass task-based and interaction-based non-contractibility. The study finds that, together with product specialization, these non-contractible elements of interorganizational relationships have greater explanatory power for reverse auction use than asset specificity. This result highlights the importance of supplier investments in non-contractible elements of exchange relationships in an increasingly dynamic service- and knowledge-based economy.
Nearly all prior studies on the technology acceptance model (TAM) have used Likert scales to measure the model’s constructs, but the use of only this type of scale has two shortcomings. One is that such use prevents us from exposing the model’s constructs to a robust test of their measure and relationships to each other, termed their nomological validity. The other is that such use leaves us unsure about whether or not we have selected an efficient way, in terms of survey completion time, to assess these constructs. Past researchers have used short form scales to address the issue of efficiency, but there are problems that may result from such efforts. In this study, we address both shortcomings by exploring the use of a semantic differential scale, which we refer to as a fast form, to assess the constructs of TAM. In this regard, we do three things. First, we describe how fast form as a scale may be developed. Second, we conduct a psychometric evaluation of the constructs that are measured by the fast form and examine their relationships. Third, we assess the efficiency of the fast form by comparing the time required to complete a survey with it to that which is required to complete a survey with Likert scales. Our results confirm that the constructs that are measured by the fast form are psychometrically equivalent to those that are measured by the Likert scales. The relationship among these constructs was unchanged, providing strong evidence for nomological validity. The fast form also yielded a 40 percent reduction in the survey completion time, proving its superior efficiency. We conclude with a description of the implications of these results for research and practice.
Negotiation is increasingly being conducted over computer media, such as e-mail and instant messaging, because of the potential for time savings and monetary benefits. However, these media can affect negotiators’ behaviors as they engage in what is called concession making, which is a process by which they make offers that yield benefits to their opponents. In this paper, we focus on how and why conducting negotiations via computer media can affect this process, especially when negotiators have unequal power. Our research model is based on theories from the information systems, negotiation, and social psychology literatures. Via a laboratory experiment, we find that concessions made by the first individual to make an offer (the first mover) were not typically reciprocated by his/her negotiating opponent (the second mover). Thus, in the context of computer-mediated negotiation, it appears that second movers are, among other things, more likely to violate the well-established norm of reciprocity. This can result in significant disadvantages for the first mover, independent of power differences between negotiators. In addition, we find that, contrary to face-to-face negotiations, increased power of one negotiator resulted in his/her having less influence in terms of getting larger concessions from the other negotiator. In general, these findings support the notion that computer-mediated negotiation can be significantly different than face-to-face negotiation.
Online users often need to make adoption decisions without accurate information about the product values. An informational cascade occurs when it is optimal for an online user, having observed others’ actions, to follow the adoption decision of the preceding individual without regard to his own information. Informational cascades are often rational for individual decision making; however, it may lead to adoption of inferior products. With easy availability of information about other users’ choices, the Internet offers an ideal environment for informational cascades. In this paper, we empirically examine informational cascades in the context of online software adoption. We find user behavior in adopting software products is consistent with the predictions of the informational cascades literature. Our results demonstrate that online users’ choices of software products exhibit distinct jumps and drops with changes in download ranking, as predicted by informational cascades theory. Furthermore, we find that user reviews have no impact on user adoption of the most popular product, while having an increasingly positive impact on the adoption of lower ranking products. The phenomenon persists after controlling for alternative explanations such as network effects, word-of-mouth effects, and product diffusion. Our results validate informational cascades as an important driver for decision making on the Internet. The finding also offers an explanation for the mixed results reported in prior studies with regard to the influence of online user reviews on product sales. We show that the mixed results could be due to the moderating effect of informational cascades.
This study extends the view that formal contracts and relational governance function as complements rather than as substitutes. We investigate how specific characteristics of service level agreements (SLAs) impact relational governance in information technology outsourcing relationships. Eleven contractual elements (categorized into three SLA characteristics: foundation, change, and governance characteristics) are hypothesized to act as complements of three relational governance attributes: relational norms, harmonious conflict resolution, and mutual dependence. Data for the study were collected through a survey of South Korean IT executives. Results of the study support the fundamental proposition of complementarity between formal contracts and relational governance, and indicate that well-structured SLAs have significant positive influence on the various aspects of relational governance in IT outsourcing relationships. However, the study also reveals that change characteristics of SLAs may act as a substitute for relational governance as these characteristics were found to dampen the level of trust and commitment through moderation effects. Overall, the findings support the proposition that well-developed SLAs not only provide a way to measure the service provider’s performance, but also enable effective management of outsourcing engagements through the development of partnership-style relationships with high levels of trust and commitment.
Multigroup or between-group analyses are common in the information systems literature. The ability to detect the presence or absence of between-group differences and accurately estimate the strength of moderating effects is important in studies that attempt to show contingent effects. In the past, IS scholars have used a variety of approaches to examine these effects, with the partial least squares (PLS) pooled significance test for multigroup becoming the most common (e.g., Ahuja and Thatcher 2005; Enns et al. 2003; Zhu et al. 2006). In other areas of social sciences (Epitropaki and Martin 2005) and management (Mayer and Gavin 2005; Song et al. 2005) research, however, there is greater emphasis on the use of covariance-based structural equation modeling multigroup analysis. This paper compares these two methods through Monte Carlo simulation. Our findings demonstrate the conditions under which covariance-based multigroup analysis is more appropriate as well as those under which there either is no difference or the component-based approach is preferable. In particular, we find that when data are normally distributed, with a small sample size and correlated exogenous variables, the component-based approach is more likely to detect differences between-group than is the covariance-based approach. Both approaches will consistently detect differences under conditions of normality with large sample sizes. With non-normally distributed data, neither technique could consistently detect differences across the groups in two of the paths, suggesting that both techniques struggle with the prediction of a highly skewed and kurtotic dependent variable. Both techniques detected the differences in the other paths consistently under conditions of non-normality, with the component-based approach preferable at moderate effect sizes, particularly for smaller samples.
Sustained website traffic through consumers’ patronage at the post-adoption stages is known as a key to the survival of an online service provider. Although a firm’s survival depends much on repeated use, whether or not a firm survives is also influenced by a variety of other behavioral outcomes that include, but are not limited to, word-of-mouth, willingness to pay, and inattentiveness to alternatives. Whereas post-adoption research has recently paid attention to repeated use, the information systems field still lacks a systematic investiation into other behavioral outcomes that transcend mere usage. In an attempt to extend the horizons of post-adoption research, we develop and test a model that explains post-adoption behaviors in the context of online services. First, drawing on a dual model of relationship maintenance in consumer behavior research, we propose a conceptual framework to study and explain online consumer behavior. In particular, our model predicts that two contrasting mechanisms, that is, dedication and constraint, are the main drivers of post-adoption phenomena (i.e., consumers’ post-adoption reactions to online services—beliefs, attitudes, intentions, and behaviors). We empirically test the proposed dual model through the use of data collected from 510 users of online portals. The results of structural equation modeling analysis indicate that, as expected, the dedication- and constraint-based mechanisms simultaneously, yet differentially, determine online consumer behavior. In general, our findings suggest that it is essential in examining the complex nature of post-adoption phenomena to take into account the interplay of the dedication- and constraint-based mechanisms.
In this paper, the authors show that PLS path modeling can be used to assess a hierarchical construct model. They provide guidelines outlining four key steps to construct a hierarchical construct model using PLS path modeling. This approach is illustrated empirically using a reflective, fourth-order latent variable model of online experiential value in the context of online book and CD retailing. Moreover, the guidelines for the use of PLS path modeling to estimate parameters in a hierarchical construct model are extended beyond the scope of the empirical illustration. The findings of the empirical illustration are used to discuss the use of covariance-based SEM versus PLS path modeling. The authors conclude with the limitations of their study and suggestions for future research.
Extreme programming is currently gaining popularity as an alternate software development methodology. Pair programming, a core practice of this methodology, involves two programmers working collaboratively to develop software. This study examined the efficacy of pair programming by comparing the performance effectiveness and affective responses of collaborating pairs with those of individual programmers treated as nominal pairs. In a controlled laboratory experiment involving student subjects, proxies for entry level programmers working on entry level tasks, two factors were manipulated: programming setting (collaborative pair versus individuals) and programming task complexity (high versus low). Participants who worked in the individual condition were randomly combined into nominal pairs. The performance and affective responses of the collaborating pairs were then compared with those of the best performers and the second best performers of each nominal pair. Results indicated that programming pairs performed at the level above the second best performers and at the level of the best performers in each nominal pair. This relationship was found to be consistent across both levels of task complexity. Consequently, there was no evidence of an “assembly bonus effect,” where the performance of a collaborating pair exceeds the performance of its best member working alone. While this finding may appear counterintuitive due to the general perception of two heads being better than one, it is consistent with the findings in small group research. When affective responses were considered, programming pairs reported higher levels of satisfaction than those of the best and second-best performing members in nominal pairs. They also showed higher levels of confidence in their performance compared to those of the second-best members. But the confidence levels of pairs were no different from those of the best performing members in nominal pairs. Theoretical and practical implications of these findings are presented.
This paper describes the development of the technology threat avoidance theory (TTAT), which explains individual IT users’ behavior of avoiding the threat of malicious information technologies. We articulate that avoidance and adoption are two qualitatively different phenomena and contend that technology acceptance theories provide a valuable, but incomplete, understanding of users’ IT threat avoidance behavior. Drawing from cybernetic theory and coping theory, TTAT delineates the avoidance behavior as a dynamic positive feedback loop in which users go through two cognitive processes, threat appraisal and coping appraisal, to decide how to cope with IT threats. In the threat appraisal, users will perceive an IT threat if they believe that they are susceptible to malicious IT and that the negative consequences are severe. The threat perception leads to coping appraisal, in which users assess the degree to which the IT threat can be avoided by taking safeguarding measures based on perceived effectiveness and costs of the safeguarding measure and self-efficacy of taking the safeguarding measure. TTAT posits that users are motivated to avoid malicious IT when they perceive a threat and believe that the threat is avoidable by taking safeguarding measures; if users believe that the threat cannot be fully avoided by taking safeguarding measures, they would engage in emotion-focused coping. Integrating process theory and variance theory, TTAT enhances our understanding of human behavior under IT threats and makes an important contribution to IT security research and practice.
This paper argues that Web 2.0 tools, specifically wikis, have begun to influence business and knowledge sharing practices in many organizations. Information Systems researchers have spent considerable time exploring the impact and implications of these tools in organizations, but those same researchers have not spent sufficient time considering whether and how these new technologies may provide opportunities for us to reform our core practices of research, review, and teaching. To this end, this paper calls for the IS discipline to engage in two actions related to wikis and other Web 2.0 tools. First, the IS discipline ought to engage in critical reflection about how wikis and other Web 2.0 tools could allow us to conduct our core processes differently. Our existing practices were formulated during an era of paper-based exchange; wikis and other Web 2.0 tools may enable processes that could be substantively better. Nevertheless, users can appropriate information technology tools in unexpected ways, and even when tools are appropriated as expected there can be unintended negative consequences. Any potential changes to our core processes should, therefore, be considered critically and carefully, leading to our second recommended action. We advocate and describe a series of controlled experiments that will help assess the impact of these technologies on our core processes and the associated changes that would be necessary to use them. We argue that these experiments can provide needed information regarding Web 2.0 tools and related practice changes that could help the discipline better assess whether or not new practices would be superior to existing ones and under which circumstances.
Within the emerging context of the digitization of health care, electronic health records (EHRs) constitute a significant technological advance in the way medical information is stored, communicated, and processed by the multiple parties involved in health care delivery. However, in spite of the anticipated value potential of this technology, there is widespread concern that consumer privacy issues may impede its diffusion. In this study, we pose the question: Can individuals be persuaded to change their attitudes and opt-in behavioral intentions toward EHRs, and allow their medical information to be digitized even in the presence of significant privacy concerns? To investigate this question, we integrate an individual’s concern for information privacy (CFIP) with the elaboration likelihood model (ELM) to examine attitude change and likelihood of opting-in to an EHR system. We theorize that issue involvement and argument framing interact to influence attitude change, and that concern for information privacy further moderates the effects of these variables. We also propose that likelihood of adoption is driven by concern for information privacy and attitude. We test our predictions using an experiment with 366 subjects where we manipulate the framing of the arguments supporting EHRs. We find that an individual’s CFIP interacts with argument framing and issue involvement to affect attitudes toward the use of EHRs. In addition, results suggest that attitude toward EHR use and CFIP directly influence opt-in behavioral intentions. An important finding for both theory and practice is that even when people have high concerns for privacy, their attitudes can be positively altered with appropriate message framing. These results as well as other theoretical and practical implications are discussed.
The technology acceptance model (TAM) asserts that ease of use and usefulness are two primary determinants of behavioral intention and usage. A parallel research stream emphasizes voluntariness, a key social influence and contextual variable, as a critical factor in information technology (IT) adoption, but pays little attention to its role in TAM. This paper addresses this particular absence by investigating the impact of environment-based voluntariness on the relationships among the four primary TAM constructs. A meta-analysis of 71 empirical studies provides strong support for the hypotheses that environment-based voluntariness moderates the effects of ease of use and usefulness on behavioral intention, but not the effect of ease of use on usefulness. Moreover, inconsistent with our expectations, environment-based voluntariness does not moderate the effects of ease of use and usefulness on usage. By further analyzing the data set, we suggest this may be because of the relatively small sample size, the presence of other factors, or the inappropriate measurement of usage in previous studies. The current study contributes not only to the distinction between user-based and environment-based voluntariness but also to a more complete understanding of user acceptance of IT across system-use environments.
This paper extends the effort–accuracy framework of cognition by taking into account the perceived strategy restrictiveness of decision aids, and tests the extended framework in a context in which online decision aids are used to elicit consumers’ preferences, automate the processing of the preferences, and provide product advice for consumers. Three types of decision aids with different decision strategy support capabilities (an additive-compensatory based aid, an elimination-based aid, and a hybrid aid supporting both strategies) are compared in terms of users’ perceptions of strategy restrictiveness, advice quality, and cognitive effort. These comparisons are grounded on the properties of normativeness and complementarity of decision strategies employed by the aids. A normative strategy takes into account both the users’ attribute preferences and the relative importance of such preferences, and allows for trade-offs among preferences (e.g., additive–compensatory). Strategy complementarity indicates support for decision rules based on multiple strategies (e.g., both additive–compensatory and elimination strategies).The experimental results support the validity of the extended effort–accuracy–restrictiveness framework and the effects of strategy normativeness, but not the effects of strategy complementarity. In addition to the perceptions of cognitive effort and advice quality, perceived strategy restrictiveness exerts a significant influence on consumers’ intentions to use online decision aids. The additive–compensatory aid is perceived to be less restrictive, of higher quality, and less effortful than the elimination aid, whereas the hybrid aid is not perceived to be any different from the additive–compensatory aid.
The creative industries have frequently expressed concern that they can’t compete with freely available copies of their content. Competing with free is particularly concerning for movie studios, whose content may be more prone to single-use consumption than other industries such as music. This issue has gained renewed importance recently with the advent of new digital video recording and distribution technologies, and the widespread availability of Internet piracy.We examine competition between “free” and paid video content in two important contexts: the impact of legitimate free distribution in one channel on sales through paid channels, and the impact of illegitimate free distribution in pirated channels on sales through paid channels. We do this by studying the impact of movie broadcasts on DVD demand and the impact of piracy availability at the time of broadcast on DVD demand. Our data include all movies shown on over-the-air and cable television during an eight-month period in 2005–2006.With respect to the impact of movie broadcasts on piracy and sales, we find that movie broadcasts on over-the-air networks result in a significant increase in both DVD sales at Amazon. com and illegal downloads for those movies that are available on BitTorrent at the time of broadcast. With respect to the impact of piracy on sales, we use the television broadcast as an exogenous demand shock and find that the availability of pirated content at the time of broadcast has no effect on post-broadcast DVD sales gains. Together our results suggest that creative artists can use product differentiation and market segmentation strategies to compete with freely available copies of their content. Specifically, the post-broadcast increase in DVD sales suggests that giving away content in one channel can stimulate sales in a paid channel if the free content is sufficiently differentiated from its paid counterpart. Likewise, our finding that the presence of pirated content does not cannibalize sales for the movies in our sample suggests that if free and paid products appeal to separate customer segments, the presence of free products need not harm paid sales.
Evolutionary psychology holds great promise as one of the possible pillars on which information systems theorizing can take place. Arguably, evolutionary psychology can provide the key to many counterintuitive predictions of behavior toward technology, because many of the evolved instincts that influence our behavior are below our level of conscious awareness; often those instincts lead to behavioral responses that are not self-evident. This paper provides a discussion of information systems theorizing based on evolutionary psychology, centered on key human evolution and evolutionary genetics concepts and notions. It is argued here that there is often a need to integrate evolutionary and non-evolutionary theories, and four important preconditions for the successful integration of evolutionary and non-evolutionary theories are discussed. An example of integration of evolutionary and non-evolutionary theories is provided. The example focuses on one evolutionary information systems theory—media naturalness theory—previously developed as an alternative to media richness theory, and one non-evolutionary information systems theory, channel expansion theory.
In the past few years, we have witnessed the increasing ubiquity of user-generated content on seller reputation and product condition in Internet-based used-good markets. Recent theoretical models of trading and sorting in used-good markets provide testable predictions to use to examine the presence of adverse selection and trade patterns in such dynamic markets. A key aspect of such empirical analyses is to distinguish between product-level uncertainty and seller-level uncertainty, an aspect the extant literature has largely ignored. Based on a unique, 5-month panel data set of user-generated content on used good quality and seller reputation feedback collected from Amazon, this paper examines trade patterns in online used-good markets across four product categories (PDAs, digital cameras, audio players, and laptops). Drawing on two different empirical tests and using content analysis to mine the textual feedback of seller reputations, the paper provides evidence that adverse selection continues to exist in online markets. First, it is shown that after controlling for price and other product, and for seller-related factors, higher quality goods take a longer time to sell compared to lower quality goods. Second, this result also holds when the relationship between sellers’ reputation scores and time to sell is examined. Third, it is shown that price declines are larger for more unreliable products, and that products with higher levels of intrinsic unreliability exhibit a more negative relationship between price decline and volume of used good trade. Together, our findings suggest that despite the presence of signaling mechanisms such as reputation feedback and product condition disclosures, the information asymmetry problem between buyers and sellers persists in online markets due to both product-based and seller-based information uncertainty. No consistent evidence of substitution or complementarity effects between product-based and seller-level uncertainty are found. Implications for research and practice are discussed.
This paper provides an introspective assessment of the current state of management information systems as a research discipline using the “lens” of the informing sciences. Based on this assessment, we observe that the degree to which MIS research is informing its key external clients— practitioners, students, and researchers in other disciplines— has declined over the years. This problem is particularly acute with respect to informing practitioners. Unfortunately, practitioner support may be critical in making up for lost resources caused by declining student enrollments. Despite this dire prognostication, we believe that it is possible to reverse this trend. Drawing upon cognitive science and diffusion of innovations research, we analyze the source of the problem and then present five recommendations aimed at leading MIS journals, scholars, and professional societies for improving the ability of MIS research to engage and inform its external clients.
Prior research has extensively studied individual adoption and use of information systems, primarily using beliefs as predictors of behavioral intention to use a system that in turn predicts system use. We propose a model of acceptance with peer support (MAPS) that integrates prior individual-level research with social networks constructs. We argue that an individual’s embeddedness in the social network of the organizational unit implementing a new information system can enhance our understanding of technology use. An individual’s coworkers can be important sources of help in overcoming knowledge barriers constraining use of a complex system, and such interactions with others can determine an employee’s ability to influence eventual system configuration and features. We incorporate network density (reflecting “get-help” ties for an employee) and network centrality (reflecting “give-help” ties for an employee), drawn from prior social network research, as key predictors of system use. Further, we conceptualize valued network density and valued network centrality, both of which take into account ties to those with relevant system-related information, knowledge, and resources, and employ them as additional predictors. We suggest that these constructs together are coping and influencing pathways by which they have an effect on system use. We conducted a 3-month long study of 87 employees in one business unit in an organization. The results confirmed our theory that social network constructs can significantly enhance our understanding of system use over and above predictors from prior individual-level adoption research.
Qualitative research is just as able as quantitative research to follow certain fundamental principles of logic in general and scientific reasoning in particular. Two such principles are the logic of modus ponens and the logic of modus tollens. In this essay, we frame different research approaches— positivist research, interpretive research, action research, and design research—in the forms of modus ponens and modus tollens. Three issues emerge from this framing and call into question how research is now conducted in the discipline of information systems. They are the issue of a common scientific basis, the issue of the fallacy of affirming the consequent, and the issue of summative validity. Both rigor and relevance in information systems research may be better achieved by attending to the three issues.
This paper presents a meta-analysis-based technique to estimate the effect of common method variance on the validity of individual theories. The technique explains between-study variance in observed correlations as a function of the susceptibility to common method variance of the methods employed in individual studies. The technique extends to mono-method studies the concept of method variability underpinning the classic multitrait–multimethod technique. The application of the technique is demonstrated by analyzing the effect of common method variance on the observed correlations between perceived usefulness and usage in the technology acceptance model literature. Implications of the technique and the findings for future research are discussed.
Researchers have long known that research methods influence construct measurements and that this influence, or method bias, can lead to false conclusions. Despite much work in the methodological literature on specific aspects of method bias, such as common method bias and self-report bias, the meaning of method bias remains unclear, and there is no comprehensive approach for dealing with it. This paper offers a clear definition of method bias, proposes a more comprehensive approach for dealing with it, and describes a demonstration exercise applying the approach in an empirical study of how individual system use and task performance relate. The demonstration suggests that the approach is feasible and illustrates how it can help researchers test theories and identify new research opportunities.
One of the most welcome recent developments in Information Systems scholarship has been the growing interest in individuals’ continuing use of information technology well after initial adoption, known in the literature as IT usage, IT continuance, and post-adoptive IT usage. In this essay, we explore the theoretical underpinnings of IS research on continuing IT use. Although the IS literature on continuing IT use emphasizes the role of habitual behavior that does not require conscious behavioral intention, it does so in a way that largely remains faithful to the theoretical tradition of planned behavior and reasoned action. However, a close reading of reference literatures on automatic behavior (behavior that is not consciously controlled) and the influences of emotion on behavior suggests that planned behavior and reasoned action may not provide the best theoretical foundation for the study of continuing IT use. As a result, we call for empirical research that directly compares and contrasts the consensus theory of continuing IT use with rival theories that place much greater emphasis on unplanned and unreasoned action.
Building consumer trust is important for new or unknown Internet businesses seeking to extend their customer reach globally. This study explores the question: Should website designers take into account the cultural characteristics of prospective customers to increase trust, given that different trust-building web strategies have different cost implications? In this study, we focused on two theoretically grounded practical web strategies of customer endorsement, which evokes unit grouping, and portal affiliation, which evokes reputation categorization, and compared them across two research sites: Australia (individualistic culture) and Hong Kong (collectivistic culture). The results of the laboratory experiment we conducted, on the website of an online bookstore, revealed that the impact of peer customer endorsements on trust perceptions was stronger for subjects in Hong Kong than Australia and that portal (Yahoo) affiliation was effective only in the Australian site. A follow-up study was conducted as a conceptual replication, and provided additional insights on the effects of customer endorsement versus firm affiliation on trust-building. Together, these findings highlight the need to consider cultural differences when identifying the mix of web strategies to employ in Internet store websites.
Effective visual design of e-commerce websites enhances website aesthetics and emotional appeal for the user. To gain insight into how Internet users perceive human images as one element of website design, a controlled experiment was conducted using a questionnaire, interviews, and eye-tracking methodology. Three conditions of human images were created including human images with facial features, human images without facial features, and a control condition with no human images. It was expected that human images with facial features would induce a user to perceive the website as more appealing, having warmth or social presence, and as more trustworthy. In turn, higher levels of image appeal and perceived social presence were predicted to result in trust. All expected relationships in the model were supported except no direct relationship was found between the human image conditions and trust. Additional analyses revealed subtle differences in the perception of human images across cultures (Canada, Germany, and Japan). While the general impact of human images seems universal across country groups, based on interview data four concepts emerged—aesthetics, symbolism, affective property, and functional property—with participants from each culture focusing on different concepts as applied to website design. Implications for research and practice are discussed.
The integrative framework of technology use (IFTU) posits that to fully explain post-adoption phenomena, four mechanisms—namely, reason-oriented action, sequential updating, feedback, and habit—should be taken into account simultaneously in a unified model. Although IFTU sheds light on the four mechanisms underlying technology use, it lacks a coherent theoretical explanation for the underlying force that leads to the four mechanisms. To offer a more generalized and richer description of the four mechanisms, this study extends IFTU by drawing on the process model of memory in cognitive psychology. In addition, based on the extended IFTU paradigm, a three-wave panel model is developed that incorporates not only proximal effects but also distal effects of the four mechanisms on post-adoption phenomena. Three different sets of data (n = 195, 160, and 342, respectively) are used to test the proposed model. The results of data analysis show that, as expected, the four mechanisms have proximal effects on subsequent evaluations and behavior. Furthermore, consistent with the memory perspective, the sequential updating and habit mechanisms are found to have distal effects on post-adoption phenomena even after controlling for their proximal effects. Overall, the findings of this study indicate that the memory perspective offers not only a seamless explanation of the four mechanisms underlying technology use but also yields deeper insights that can be validated only through a three-or-more-wave panel study. This research contributes to the literature by demonstrating that the extended IFTU paradigm has the potential to serve as a coherent theoretical framework on post-adoption phenomena in which prior experiences are internalized into memories, which in turn regulate later experiences.
Information systems offshoring has emerged as a significant force in the global political economy, an important source of firm-specific competitive advantage, and a focal point for debates over the benefits and costs of globalization. As worldwide competition exerts increasing pressure on the IS function of firms to become geographically unbundled, and IS services are dispersed among increasingly distant and unfamiliar locations, the issue of risk emerges as a significant factor in decisions about where to locate offshore facilities. Drawing from prior research in IS outsourcing/offshoring and theoretical perspectives from international strategy and multinational management, we examine the determinants of risk firms bear in their offshoring decisions. In particular, the current paper explores firm-level and environment-level “push” factors that drive firms to accept increasingly greater degrees of host country risk. We predict that firm-level risk outcomes for locating IS offshore facilities will be influenced by prior firm-specific experience, the relative gap between home and host country risk levels, and the overall movement by IS offshore services providers toward increasingly riskier locations. We test these hypotheses on a proprietary data set of more than 850 information technology and software offshoring projects in 55 host countries worldwide during the period 2000 through 2005. We find that firm-specific experience and the core “risk gap” between home and host country are predictive of companies pursuing progressively riskier locations, but that their effects dissipate as environment-wide experience is incorporated into our model. Our analysis suggests that broader dynamics in the competitive environment are powerful contributors to the overall observation that IS offshoring is moving to increasingly high-risk locations. This trend has implications for the management, security, and global integration of information systems. Our study contributes to the literature on risk and IS offshoring in providing the first worldwide empirical examination of the determinants of actual firm IS offshoring behavior with respect to offshoring location risk.
Agency theory has served as a key basis for identifying drivers of offshore information system project success. Consequently, the role of relational factors in driving project success has been overlooked in this literature. In this paper, we address this gap by integrating the social embeddedness perspective and the culture literature to theorize how and why relational factors affect the success of offshore IS projects that are strategic in nature. We identify organizational and interpersonal cultural differences as critical success factors in this context. Using data from a longitudinal field study of 155 offshore IS projects managed by 22 project leaders, we found evidence of a relationship between hypothesized relational factors and two measures of offshore IS project success—namely, project cost overruns and client satisfaction—over and above the effects of project characteristics and agency factors. Specifically, we found that information exchange, joint problem solving, and trust reduce project cost overruns and improve client satisfaction. We also found a relationship between cultural differences at the organizational and team level, and offshore IS project success. The model explained 40 percent and 41 percent of the variance in project cost overruns and client satisfaction, respectively, for projects with a client representative. For projects with no client representative, the model explained 35 percent and 37 percent of the variance in project cost overruns and client satisfaction, respectively. Collectively, the results have important theoretical and practical implications for how client– vendor relationships should be managed when partnering with offshore firms and designing offshore IS project teams.
User resistance to information systems implementation has been identified as a salient reason for the failure of new systems and hence needs to be understood and managed. While previous research has explored the reasons for user resistance, there are gaps in our understanding of how users evaluate change related to a new information system and decide to resist it. In particular, missing in the explanation of user decision making is the concept of status quo bias, that is, that user resistance can be due to the bias or preference to stay with the current situation. Motivated thus, this study develops a model to explain user resistance prior to a new IS implementation by integrating the technology acceptance and resistance literatures with the status quo bias perspective. The results of testing the model in the context of a new enterprise system implementation indicate the central role of switching costs in increasing user resistance. Further, switching costs also mediate the relationship between other antecedents (colleague opinion and self-efficacy for change) and user resistance. Additionally, perceived value and organizational support for change are found to reduce user resistance. This research advances the theoretical understanding of user acceptance and resistance prior to a new IS implementation and offers organizations suggestions for managing such resistance.
A number of models and theories in information systems research include concepts of a match between two variables or states. The development of measures for this concept can present problems, because decisions must be made about the nature of the comparison. Should indirect measures of the match be employed, then methodological issues arise about how to best handle the measure when testing the model. Difference scores are commonly used to measure a match between variables or states in IS research, but these have implicit assumptions about the theory and data characteristics that are often false. Not unexpectedly, false assumptions can lead to erroneous conclusions about the relationships among the variables that are used to determine a match in a research model. The implicit assumptions restrict the form of the relationships and limit the IS researcher’s ability to understand the possible interplay among theoretical concepts. We suggest some guidelines for the formation and testing of models that measure the match. In addition, we recommend polynomial regression analysis as one means of analyzing the more complex relationships in IS studies. We then use an IS service quality example to illustrate the issues involved in the use of matching variables and make suggestions with regard to using or avoiding difference scores.
This study investigates selective reporting behaviors that are pursued by project managers when communicating the status of their information system initiatives to their executives. To understand the types, motivations, impacts, and antecedents of such behaviors, a message-exchange perspective is adopted and the prior literature on IS project status reporting is reviewed. This study incorporates an empirical investigation that examined the influence of five dyadic factors on selective reporting using a survey of 561 project managers. The findings of the study reveal a positive effect of reporting quality on project performance and indicate that a specific type of selective reporting behavior (optimistic biasing) has a degrading effect on reporting quality. Moreover, the findings show that all five antecedents have a significant influence on the propensity of project managers to report selectively. Specifically, the project executive’s power, the project manager’s trust in the executive, and the executive’s quality of communication impact selective reporting directly; the executive’s familiarity with the IS development process and the executive’s organizational affiliation vis-à-vis that of the project manager have an indirect influence (it is mediated through other factors). The effects of each of these factors on the two types of selective reporting (optimistic and pessimistic biasing) are examined, and the implications of these findings for both researchers and managers are discussed in this article.
In striving to learn about an information technology innovation, organizations draw on knowledge resources available in the community of diverse interests that convenes around that innovation. But even as such organizations learn about the innovation, so too does the larger community. Community learning takes place as its members reflect upon their learning and contribute their experiences, observations, and insights to the community’s on-going discourse on the innovation. Community learning and organizational learning thus build upon one another in a reciprocal cycle over time, as the stock of interpretations, adoption rationales, implementation strategies, and utilization patterns is expanded and refined. We advance an overall model of this learning cycle, drawing on two community-level theories (management fashion and organizing vision), both of which complement the dominant emphases of the literature on IT innovation and learning. Relative to this cycle, we then empirically examine, in particular, the dependence of community learning on organizational learning. Sampling the public discourse on enterprise resource planning (ERP) over a 14-year period, we explore how different kinds of organizational actors can play different roles, at different times, in contributing different types of knowledge to an innovation’s public discourse. The evidence suggests that research analysts and technology vendors took leadership early on in articulating the “know-what” (interpretation) and “know-why” (rationales) for ERP, while later on adopters came to dominate the discourse as its focus shifted to the “know-how” (strategies and capabilities). We conclude by identifying opportunities for further inquiry on and strategic management of community learning and its interactions with organizational learning.
Building on neo-institutional theory and theories of innovation and diffusion, recent work in the field of management has suggested that management research and practice is characterized by fashions. A management fashion is a relatively transitory belief that a certain management technique leads rational management progress. Using bibliographic research, we apply Abrahamson’s management fashion theory to information systems research and practice. Our findings reveal that information systems research and practice, like management research and practice, is indeed characterized by fashions. These “IS fashion waves” are relatively transitory and represent a burst of interest in particular topics by IS researchers and practitioners. However, while our findings show that IS research closely parallels practice, we suggest that a more proactive engagement of IS academics is needed in the IS fashion-setting process.
This paper focuses on strategic information flows between buyers and suppliers within logistics supply chain relationships and on subsequent relationship-specific performance outcomes. Our analysis of dyadic data collected from 91 buyer–supplier logistics relationships finds that buyer and supplier strategic information flows positively impact the relationship-specific performance of both sharing and receiving parties. Specifically, each party gains financially from improved management of assets, reduced costs of operations, and enhanced productivity. Moreover, each benefits operationally from improved planning, control, and flexibility of resources. Buyer dependence on the supplier increases buyer strategic information flows to the supplier. Additionally, buyer IT customization and both buyer and sup plier trusting beliefs in the receiving party positively impact strategic information sharing with partners. This study suggests that partnerships for supply chain services engage in cooperative initiatives to generate relational rents and are an alternative to conventional “arms length” transactional exchanges. These partnerships need to be motivated to go beyond the sharing of order-related information (which must occur in transactional exchanges) and to share strategic information (which has the potential for both additional rent generation and risks of misappropriation).
Empirical results both from information technology acceptance research as well as from other fields suggest that attitude and subjective norms may have a nonlinear relationship. Based on the economic theory of complementarities, the present paper hypothesizes a substitution relationship or negative synergy between attitude and subjective norms in organizational IT use contexts. Employing two methods for modeling and measuring nonlinear effects of latent constructs, as well as two approaches for visualizing and interpreting interaction and quadratic terms, structural equation modeling analysis of data collected from 258 users of a variety of IT applications in 14 organizations provides support for the hypothesis that attitude and subjective norms were substitutes in predicting intention to use.
Online auctions enable market-level interactions or interdependency of outcomes, which were not observed in physical auctions. One such set of interactions takes place when multiple auctions are conducted to sell identical items by an identical seller in an overlapping manner. This research focuses on overlapping auctions, their interactions, and the related impact on bidder behavior. We introduce the notion of auction “overlap” and examine the impact of market-level factors such as the price information revealed from prior auctions, degree of overlap, the auction format, and the overall market supply on a given auction’s price. Despite a competitive setting, we find that, ceteris paribus, English auctions, on average, extract roughly 8.6 percent more revenue per unit than multiunit uniform-price Dutch auctions. We discover that the overlapping auctions attract institutional bidders, who bid in a participatory manner across multiple auctions, and that such bidders exert a downward pressure on auction prices. We find that overlap of an auction with other competing auctions has a significant negative influence on prices, and information about following auctions has a stronger negative influence than information about prior closing auctions. By estimating the expected price difference, we provide practitioners, who have private knowledge of their internal holding costs, a benchmark that can be used in deciding between using overlapping single-unit English auctions and multiunit Dutch auctions.
Protecting the privacy of personal information continues to pose significant challenges for organizations. Because consumers are vulnerable in their dealings with businesses due to a lack of information about and an inability to control the subsequent use of their personal information, we argue that organizations have a moral responsibility to these individuals to avoid causing harm and to take reasonable precautions toward that end. We further argue that firms can enhance their privacy programs by moving beyond merely complying with laws and other regulations and creating a culture of integrity that combines a concern for the law with an emphasis on managerial responsibility for the firm’s organizational privacy behaviors. We use two high-profile data breaches experienced by two U.S. companies, ChoicePoint and TJX, to illustrate our arguments for enhancing organizational level privacy programs based on ethical reasoning. In doing so, this paper contributes to the dearth of prior organizational-level privacy research, which has largely overlooked ethical issues or the personal harms often caused by privacy violations. We conclude with recommendations for ways organizations can improve their privacy programs by incorporating moral responsibility.
Within the Information Systems literature, there has been an emerging interest in the use of formative measurement in structural equation modeling (SEM). This interest is exemplified by descriptions of the nature of formative measurement (e.g., Chin 1998a), and more recently the proper specification of formatively measured constructs (Petter et al. 2007) as well as application of such constructs (e.g., Barki et al. 2007). Formative measurement is a useful alternative to reflective measurement. However, there has been little guidance on interpreting the results when formative measures are employed. Our goal is to provide guidance relevant to the interpretation of formative measurement results through the examination of the following six issues: multicollinearity; the number of indicators specified for a formatively measured construct; the possible co-occurrence of negative and positive indicator weights; the absolute versus relative contributions made by a formative indicator; nomological network effects; and the possible effects of using partial least squares (PLS) versus covariance-based SEM techniques. We provide prescriptions for researchers to consider when interpreting the results of formative measures as well as an example to illustrate these prescriptions.
Human life is dependent upon the natural environment, which, most would agree, is rapidly degrading. Business enterprises are a dominant form of social organization and contribute to the worsening, and enhancement, of the natural environment. Scholars in the administrative sciences examine questions spanning organizations and the natural environment but have largely omitted the information systems perspective. We develop a research agenda on information systems innovation for environmental sustainability that demonstrates the critical role that IS can play in shaping beliefs about the environment, in enabling and transforming sustainable processes and practices in organizations, and in improving environmental and economic performance. The belief–action–outcome (BAO) framework and associated research agenda provide the basis for a new discourse on IS for environmental sustainability.
Organizations today outsource diverse business processes to achieve a wide variety of business objectives ranging from reduction of costs to innovation and business transformation. We build on the information processing view of the firm to theorize that performance heterogeneity across business process outsourcing (BPO) exchanges is a function of the design of information capabilities (IC) that fit the unique information requirements (IR) of the exchange. Further, we compare performance effects of the fit between IR and IC across dominant categories of BPO relationships to provide insights into the relative benefits of enacting such fit between the constructs. Empirical tests of our hypotheses using survey data on 127 active BPO relationships find a significant increase (decrease) in satisfaction as a result of the fit (misfit) between IR and IC of the relationship. The results have implications for how BPO relationships must be designed and managed to realize significant performance gains. The study also extends the IPV to identify IC that provide the incentives and means to process information in an interfirm relationship.
This paper informs the literature on the business value of information technology by conceptualizing a path from IT assets—that is, commodity-like or off-the-shelf information technologies—to sustainable competitive advantage. This path suggests that IT assets can play a strategic role when they are combined with organizational resources to create IT-enabled resources. To the extent that relationships between IT assets and organizational resources are synergistic, the ensuing IT-enabled resources are capable of positively affecting firms’ sustainable competitive advantage via their improved strategic potential. This is an important contribution since IT-related organizational benefits have been hard to demonstrate despite attempts to study them through a variety of methods and theoretical lenses. This paper synthesizes systems theory and the resource-based view of the firm to build a unified conceptual model linking IT assets with firm-level benefits. Several propositions are derived from the model and their implications for IS research and practice are discussed.
Customer reviews are increasingly available online for a wide range of products and services. They supplement other information provided by electronic storefronts such as product descriptions, reviews from experts, and personalized advice generated by automated recommendation systems. While researchers have demonstrated the benefits of the presence of customer reviews to an online retailer, a largely uninvestigated issue is what makes customer reviews helpful to a consumer in the process of making a purchase decision. Drawing on the paradigm of search and experience goods from information economics, we develop and test a model of customer review helpfulness. An analysis of 1,587 reviews from Amazon.com across six products indicated that review extremity, review depth, and product type affect the perceived helpfulness of the review. Product type moderates the effect of review extremity on the helpfulness of the review. For experience goods, reviews with extreme ratings are less helpful than reviews with moderate ratings. For both product types, review depth has a positive effect on the helpfulness of the review, but the product type moderates the effect of review depth on the helpfulness of the review. Review depth has a greater positive effect on the helpfulness of the review for search goods than for experience goods. We discuss the implications of our findings for both theory and practice.
While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifically, as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece, we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.
Little research has examined the impacts of enterprise resource planning (ERP) systems implementation on job satisfaction. Based on a 12-month study of 2,794 employees in a telecommunications firm, we found that ERP system implementation moderated the relationships between three job characteristics (skill variety, autonomy, and feedback) and job satisfaction. Our findings highlight the key role that ERP system implementation can have in altering well-established relationships in the context of technology-enabled organizational change situations. This work also extends research on technology diffusion by moving beyond a focus on technology-centric outcomes, such as system use, to understanding broader job outcomes.
What happens to organizations that chase the hottest information technologies? This study examines some of the important organizational impacts of the fashion phenomenon in IT. An IT fashion is a transitory collective belief that an information technology is new, efficient, and at the forefront of practice. Using data collected from published discourse and annual IT budgets of 109 large companies for a decade, I have found that firms whose names were associated with IT fashions in the press did not have higher performance, but they had better reputation and higher executive compensation in the near term. Companies investing in IT in fashion also had higher reputation and executive pay, but they had lower performance in the short term and then improved performance in the long term. These results support a fashion explanation for the middle phase diffusion of IT innovations, illustrating that following fashion can legitimize organizations and their leaders regardless of performance improvement. The findings also extend institutional theory from its usual focus on taken-for-granted practices to fashion as a novel source of social approval. This study suggests that practitioners balance between performance pressure and social approval when they confront whatever is hottest in IT.
As business and technology environments change at an unprecedented rate, software development agility to respond to changing user requirements has become increasingly critical for software development performance. Agile software development approaches, which emphasize sense-and-respond, self-organization, cross-functional teams, and continuous adaptation, have been adopted by an increasing number of organizations to improve their software development agility. However, the agile development literature is largely anecdotal and prescriptive, lacking empirical evidence and theoretical foundation to support the principles and practices of agile development. Little research has empirically examined the software development agility construct in terms of its dimensions, determinants, and effects on software development performance. As a result, there is a lack of understanding about how organizations can effectively implement an agile development approach.Using an integrated research approach that combines quantitative and qualitative data analyses, this research opens the black box of agile development by empirically examining the relationships among two dimensions of software development agility (software team response extensiveness and software team response efficiency), two antecedents that can be controlled (team autonomy and team diversity), and three aspects of software development performance (on-time completion, on-budget completion, and software functionality). Our PLS results of survey responses of 399 software project managers suggest that the relationships among these variables are more complex than what has been perceived by the literature. The results suggest a tradeoff relationship between response extensiveness and response efficiency. These two agility dimensions impact software development performance differently: response efficiency positively affects all of on-time completion, on-budget completion, and software functionality, whereas response extensiveness positively affects only software functionality. The results also suggest that team autonomy has a positive effect on response efficiency and a negative effect on response extensiveness, and that team diversity has a positive effect on response extensiveness. We conducted 10 post hoc case studies to qualitatively cross-validate our PLS results and provide rich, additional insights regarding the complex, dynamic interplays between autonomy, diversity, agility, and performance. The qualitative analysis also provides explanations for both supported and unsupported hypotheses. We discuss these qualitative analysis results and conclude with the theoretical and practical implications of our research findings for agile development approaches.
This study explores how team leaders sense the need for technology adaptation intervention in distributed, computer-mediated (“virtual”) teams. Analysis and coding of critical incident data collected in interviews of practicing leaders produce a five-trigger model including (1) external constraint, (2) internal constraint, (3) information and communication technology (ICT) inadequacy, (4) ICT knowledge, skills, and abilities inadequacy, and (5) trust and relationship inadequacies. The resulting five-trigger model provides several key contributions including (1) a diagnostic tool for examining real, multi-trigger team technology adaptation contexts, enabling better leader training and evaluation as well as improved research on team technology adaptation and interventions and (2) a better understanding of the relationship between the technology structure strength indicators in adaptive structuration theory and the need for team technology adaptation intervention.
Whether and how firms can employ relative rankings in search engine results pages (SERPs) to differentiate their brands from competitors in cyberspace remains a critical, puzzling issue in e-commerce research. By synthesizing relevant literature from cognitive psychology, marketing, and e-commerce, this study identifies key contextual factors that are conducive for creating brand positioning online via SERPs. In two experiments, the authors establish that when Internet users’ implicit beliefs (i.e., schema) about the meaning of the display order of search engine results are activated or heightened through feature priming, they will have better recall of an unknown brand that is displayed before the well-known brands in SERPs. Further, those with low Internet search skills tend to evaluate the unknown brand more favorably along the particular brand attribute that activates the search engine ranking schema. This research has both theoretical and practical implications for understanding the effectiveness of search engine optimization techniques.
This paper develops a long-term, multi-project model of factors affecting organizational benefits from enterprise systems (ES), then reports a preliminary test of the model. In the shorter-term half of the model, it is hypothesized that once a system has gone live, two factors, namely functional fit and overcoming organizational inertia, drive organizational benefits flowing from each major ES improvement project. The importance of these factors may vary from project to project. In the long-term half of the model, it is hypothesized that four additional factors, namely integration, process optimization, improved access to information, and on-going major ES business improvement projects, drive organizational benefits from ES over the long term. Preliminary tests of the model were conducted using data from 126 customer presentations from SAP’s 2003 and 2005 Sapphire U.S. conferences. All six factors were found to be important in explaining variance in organizational benefits from enterprise systems from the perspective of senior management.
The information systems field emerged as a new discipline of artificial science as a result of intellectual efforts to understand the nature and consequences of computer and communication technology in modern organizations. As the rapid development of digital technology continues to make computers and computing a part of everyday experiences, we are once again in need of a new discipline of the artificial. In this essay, I argue that the IS community must expand its intellectual boundaries by embracing experiential computing as an emerging field of inquiry in order to fill this growing intellectual void. Experiential computing involves digitally mediated embodied experiences in everyday activities through everyday artifacts that have embedded computing capabilities. Experiential computing is enabled by the mediation of four dimensions of human experiences (time, space, actors, and artifacts) through digital technology. Drawing on a research framework that encompasses both behavioral and design sciences, six research opportunities that the IS research community can explore are suggested. Ultimately, I propose that the IS field return to its roots, the science of the artificial, by decisively expanding the scope of its inquiry and establishing a new domain of research on computing in everyday life experiences.
The use of formative measurement in the field of Information Systems has increased, arguably due to statistical tools (e.g., PLS) that can test such models. However, in the literature, there exist two contradictory views on the potential deficiency of formative measurement. While opponents who are critical of formative measurement argue that there are native weaknesses of the formative approach in model estimation, propo nents who are in favor of using formative measurement counter that opponents’ research methods in measurement model specification are flawed. The goal of this work is to empirically test these opposing views on whether the alleged estimation instability of formative measurement is due to measurement model misspecification or simply the shortcoming of formative measurement. To assess the integrity of arguments of both parties, we adopt a research design in which four different cases are tested in terms of interpretational confounding and external consistency. We find that regardless of whether there is a specification issue, formative measures can lead to misleading outcomes. Based on the results, we offer guidelines that researchers may adopt in planning and executing data analysis with structural equation modeling. Given that the use of formative measurement is at a critical juncture in the IS field, we believe that the guidelines in this research note are important to promote appropriate use of the approach rather than relegate it to a bandwagon effect.
“Information” is poorly defined in the Information Systems research literature, and is almost always unspecified, a reflexive, all-purpose but indiscriminant solution to an unbounded variety of problems. We present a taxonomy of four views—token, syntax, representation, and adaptation—to enable scholars and practitioners to specify their concept of information. This taxonomy is normative, but we also provide a background review of the etymology and chronology of information, and we sample uses of the term in current IS research. IS research will improve as the term information, via the taxonomy we contribute, is employed more explicitly and consistently.
Information systems strategy is of central importance to IS practice and research. Our extensive review of the literature suggests that the concept of IS strategy is a term that is used readily; however, it is also a term that is not fully understood. In this study, we follow a perspective paradigm based on the strategic management literature to define IS strategy as an organizational perspective on the investment in, deployment, use, and management of IS. Through a systematic literature search, we identify the following three conceptions of IS strategy employed implicitly in 48 articles published in leading IS journals that focus on the construct of IS strategy: (1) IS strategy as the use of IS to support business strategy; (2) IS strategy as the master plan of the IS function; and (3) IS strategy as the shared view of the IS role within the organization. We find the third conception best fits our definition of IS strategy. As such, we consequently propose to operationalize IS strategy as the degree to which the organization has a shared perspective to seek innovation through IS. Specifically, our proposed IS strategic typology suggests an organization’s IS strategy falls into one of the two defined categories (i.e., IS innovator or IS conservative) or is simply undefined. We also develop measures for this new typology. We argue that the proposed instrument, which was cross-validated across both chief information officers and senior business executives, has the potential to serve as a diagnostic tool through which the organization can directly assess its IS strategy. We contend that our reconceptualization and operationalization of IS strategy provides theoretical and practical implications that advance the current level of understanding of IS strategy from extant studies within three predominant literature streams: strategic IS planning, IS/business strategic alignment, and competitive use of IS.
Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors—and specifically those of the brain—in decisions to trust, the biological influences should naturally include those related to gender. As trust considerations in economic decision making have become increasingly complex with the expansion of Internet use, understanding the related biological/brain functions and the involvement of gender provides a range of valuable insights.To show empirically that online trust is associated with activity changes in certain brain areas, we used functional magnetic resonance imaging (fMRI). In a laboratory experiment, we captured the brain activity of 10 female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. We found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, we found that women activated more brain areas than did men. These results confirm the empathizing– systemizing theory, which predicts gender differences in neural information processing modes.In demonstrating that perceived trustworthiness of Internet offers is affected by neurobiology, our study has major implications for both IS research and management. We confirm the value of a category of research heretofore neglected in IS research and practice, and argue that future IS research investigating human behavior should consider the role of biological factors. In practice, biological factors are a significant consideration for management, marketing, and engineering attempts to influence behavior.
Individual-level information systems adoption research has recently seen the introduction of expectation–disconfirmation theory (EDT) to explain how and why user reactions change over time. This prior research has produced valuable insights into the phenomenon of technology adoption beyond traditional models, such as the technology acceptance model. First, we identify gaps in EDT research that present potential opportunities for advances—specifically, we discuss methodological and analytical limitations in EDT research in information systems and present polynomial modeling and response surface methodology as solutions. Second, we draw from research on cognitive dissonance, realistic job preview, and prospect theory to present a polynomial model of expectation–disconfirmation in information systems. Finally, we test our model using data gathered over a period of 6 months among 1,143 employees being introduced to a new technology. The results confirmed our hypotheses that disconfirmation in general was bad, as evidenced by low behavioral intention to continue using a system for both positive and negative disconfirmation, thus supporting the need for a polynomial model to understand expectation disconfirmation in information systems.
Determining whom to trust and whom to distrust is a major decision in impersonal IT-enabled exchanges. Despite the potential role of both trust and distrust in impersonal exchanges, the information systems literature has primarily focused on trust, alas paying relatively little attention to distrust. Given the importance of studying both trust and distrust, this study aims to shed light on the nature, dimensionality, distinction, and relationship, and relative effects of trust and distrust on economic outcomes in the context of impersonal IT-enabled exchanges between buyers and sellers in online marketplaces.This study uses functional neuroimaging (fMRI) tools to complement psychometric measures of trust and distrust by observing the location, timing, and level of brain activity that underlies trust and distrust and their underlying dimensions. The neural correlates of trust and distrust are identified when subjects interact with four experimentally manipulated seller profiles that differ on their level of trust and distrust. The results show that trust and distrust activate different brain areas and have different effects, helping explain why trust and distrust are distinct constructs associated with different neurological processes. Implications for the nature, distinction and relationship, dimensionality, and effects of trust and distrust are discussed.
Although firms are expending substantial resources to develop technology and processes that can help safeguard the security of their computing assets, increased attention is being focused on the role people play in maintaining a safe computing environment. Unlike employees in a work setting, home users are not subject to training, nor are they protected by a technical staff dedicated to keeping security software and hardware current. Thus, with over one billion people with access to the Internet, individual home computer users represent a significant point of weakness in achieving the security of the cyber infrastructure. We study the phenomenon of conscientious cybercitizens, defined as individuals who are motivated to take the necessary precautions under their direct control to secure their own computer and the Internet in a home setting. Using a multidisciplinary, phased approach, we develop a conceptual model of the conscientious cybercitizen. We present results from two studies—a survey and an experiment—conducted to understand the drivers of intentions to perform security-related behavior, and the interventions that can positively influence these drivers. In the first study, we use protection motivation theory as the underlying conceptual foundation and extend the theory by drawing upon the public goods literature and the concept of psychological ownership. Results from a survey of 594 home computer users from a wide range of demographic and socio-economic backgrounds suggest that a home computer user’s intention to perform security-related behavior is influenced by a combination of cognitive, social, and psychological components. In the second study, we draw upon the concepts of goal framing and self-view to examine how the proximal drivers of intentions to perform security-related behavior identified in the first study can be influenced by appropriate messaging. An experiment with 101 subjects is used to test the research hypotheses. Overall, the two studies shed important new light on creating more conscientious cybercitizens. Theoretical and practical implications of the findings are discussed.
Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for e-commerce and online security.
Many organizations recognize that their employees, who are often considered the weakest link in information security, can also be great assets in the effort to reduce risk related to information security. Since employees who comply with the information security rules and regulations of the organization are the key to strengthening information security, understanding compliance behavior is crucial for organizations that want to leverage their human capital.This research identifies the antecedents of employee compliance with the information security policy (ISP) of an organization. Specifically, we investigate the rationality-based factors that drive an employee to comply with requirements of the ISP with regard to protecting the organization’s information and technology resources. Drawing on the theory of planned behavior, we posit that, along with normative belief and self-efficacy, an employee’s attitude toward compliance determines intention to comply with the ISP. As a key contribution, we posit that an employee’s attitude is influenced by benefit of compliance, cost of compliance, and cost of noncompliance, which are beliefs about the overall assessment of consequences of compliance or noncompliance. We then postulate that these beliefs are shaped by the employee’s outcome beliefs concerning the events that follow compliance or noncompliance: benefit of compliance is shaped by intrinsic benefit, safety of resources, and rewards, while cost of compliance is shaped by work impediment; and cost of noncompliance is shaped by intrinsic cost, vulnerability of resources, and sanctions. We also investigate the impact of information security awareness (ISA) on outcome beliefs and an employee’s attitude toward compliance with the ISP.Our results show that an employee’s intention to comply with the ISP is significantly influenced by attitude, normative beliefs, and self-efficacy to comply. Outcome beliefs significantly affect beliefs about overall assessment of consequences, and they, in turn, significantly affect an employee’s attitude. Furthermore, ISA positively affects both attitude and outcome beliefs. As the importance of employees’ following their organizations’ information security rules and regulations increases, our study sheds light on the role of ISA and compliance-related beliefs in an organization’s efforts to encourage compliance.
Information technology executives strive to align the actions of end users with the desired security posture of management and of the firm through persuasive communication. In many cases, some element of fear is incorporated within these communications. However, within the context of computer security and information assurance, it is not yet clear how these fear-inducing arguments, known as fear appeals, will ultimately impact the actions of end users. The purpose of this study is to investigate the influence of fear appeals on the compliance of end users with recommendations to enact specific individual computer security actions toward the mitigation of threats. An examination was performed that culminated in the development and testing of a conceptual model representing an infusion of technology adoption and fear appeal theories.Results of the study suggest that fear appeals do impact end user behavioral intentions to comply with recommended individual acts of security, but the impact is not uniform across all end users. It is determined in part by perceptions of self-efficacy, response efficacy, threat severity, and social influence. The findings of this research contribute to information systems security research, human–computer interaction, and organizational communication by revealing a new paradigm in which IT users form perceptions of the technology, not on the basis of performance gains, but on the basis of utility for threat mitigation.
Information security is a fundamental concern for corporations operating in today’s digital economy. The number of firms disclosing items concerning their information security on reports filed with the U.S. Securities and Exchange Commission (SEC) has increased in recent years. A question then arises as to whether or not there is value to the voluntary disclosures concerning information security. Thus, the primary objective of this paper is to assess empirically the market value of voluntary disclosures of items pertaining to information security. Based on a sample of 1,641 disclosing and 19,266 non-disclosing firm-years in a cross-sectional pooled model, our primary findings provide strong evidence that voluntarily disclosing items concerning information security is associated positively with the market value of a firm. These findings are based on the use of a market-value relevance model, as well as a bid-ask spread analysis. The study’s findings are robust to alternative statistical analyses. The findings also provide support for the signaling argument, which states that managers disclose information in a manner consistent with increased firm value. Finally, the study findings provide some insight into the strategic choice that firms make regarding voluntary disclosures about information security.
This paper examines user participation in information systems security risk management and its influence in the context of regulatory compliance via a multi-method study at the organizational level. First, eleven informants across five organizations were interviewed to gain an understanding of the types of activities and security controls in which users participated as part of Sarbanes-Oxley compliance, along with associated outcomes. A research model was developed based on the findings of the qualitative study and extant user participation theories in the systems development literature. Analysis of the data collected in a questionnaire survey of 228 members of ISACA, a professional association specialized in information technology governance, audit, and security, supported the research model. The findings of the two studies converged and indicated that user participation contributed to improved security control performance through greater awareness, greater alignment between IS security risk management and the business environment, and improved control development. While the IS security literature often portrays users as the weak link in security, the current study suggests that users may be an important resource to IS security by providing needed business knowledge that contributes to more effective security measures. User participation is also a means to engage users in protecting sensitive information in their business processes.
In this paper, a competitive software market that includes horizontal and quality differentiation, as well as a negative network effect driven by the presence of malicious agents, is modeled. Software products with larger installed bases, and therefore more potential computers to attack, present more appealing targets for malicious agents. One finding is that software firms may profit from increased malicious activity. Software products in a more competitive market are less likely to invest in security, while monopolistic or niche products are likely to be more secure from malicious attack. The results provide insights for IS managers considering enterprise software adoption.
Organizations need to protect information assets against cyber crime, denial-of-service attacks, web hackers, data breaches, identity and credit card theft, and fraud. Criminals often try to achieve financial, political, or personal gain through these attacks, so the threats that their actions prompt are insidious motivators for organizations to adopt information systems security (ISS) approaches. Extant ISS research has traditionally examined ISS in e-commerce business organizations. The present study investigates ISS within government, analyzing power relationships during an ISS standards adoption and accreditation process, where a head of state mandates that all government agencies are to comply with a national de jure ISS standard. Using a canonical action research method, designated managers of ISS services across small, medium, and large agencies were monitored and assessed for progress to accreditation through surveys, interviews, participant observation at round table forums, and focus groups. By 2008, accreditation status across the 89 agencies participating in this study was approximately 33 percent fully accredited, with 67 percent partially compliant. The research uses Clegg’s (1989) circuits of power framework to interpret power, resistance, norms, and cultural relationships in the process of compliance. The paper highlights that a strategy based on organization subunit size is helpful in motivating and assisting organizations to move toward accreditation. Mandated standard accreditation was inhibited by insufficient resource allocation, lack of senior management input, and commitment. Factors contributing to this resistance were group norms and cultural biases.
Employees’ failure to comply with information systems security policies is a major concern for information technology security managers. In efforts to understand this problem, IS security researchers have traditionally viewed violations of IS security policies through the lens of deterrence theory. In this article, we show that neutralization theory, a theory prominent in Criminology but not yet applied in the context of IS, provides a compelling explanation for ISsecurity policy violations and offers new insight into how employees rationalize this behavior. In doing so, we propose a theoretical model in which the effects of neutralization techniques are tested alongside those of sanctions described by deterrence theory. Our empirical results highlight neutralization as an important factor to take into account with regard to developing and implementing organizational security policies and practices.
Much ado has been made regarding user acceptance of new information technologies. However, research has been primarily based on cognitive models and little attention has been given to emotions. This paper argues that emotions are important drivers of behaviors and examines how emotions experienced early in the implementation of new IT applications relate to IT use. We develop a framework that classifies emotions into four distinct types: challenge, achievement, loss, and deterrence emotions. The direct and indirect rela tionships between four emotions (excitement, happiness, anger, and anxiety) and IT use were studied through a survey of 249 bank account managers. Our results indicate that excitement was positively related to IT use through task adaptation. Happiness was directly positively related to IT use and, surprisingly, was negatively associated with task adaptation, which is a facilitator of IT use. Anger was not related to IT use directly, but it was positively related to seeking social support, which in turn was positively related to IT use. Finally, anxiety was negatively related to IT use, both directly and indirectly through psychological distancing. Anxiety was also indirectly positively related to IT use through seeking social support, which countered the original negative effect of anxiety. Post hoc ANOVAs were conducted to compare IT usage of different groups of users experiencing similar emotions but relying on different adaptation behaviors. The paper shows that emotions felt by users early in the implementation of a new IT have important effects on IT use. As such, the paper provides a complementary perspective to understanding acceptance and antecedents of IT use. By showing the importance and complexity of the relationships between emotions and IT use, the paper calls for more research on the topic
Ethics is important in the Information Systems field as illustrated by the direct effect of the Sarbanes-Oxley Act on the work of IS professionals. There is a substantial literature on ethical issues surrounding computing and information technology in the contemporary world, but much of this work is not published nor widely cited in the mainstream IS literature. The purpose of this paper is to offer one contribution to an increased emphasis on ethics in the IS field. The distinctive contribution is a focus on Habermas’s discourse ethics. After outlining some traditional theories of ethics and morality, the literature on IS and ethics is reviewed, and then the paper details the development of discourse ethics. Discourse ethics is different from other approaches to ethics as it is grounded in actual debates between those affected by decisions and proposals. Recognizing that the theory could be considered rather abstract, the paper discusses the need to pragmatize discourse ethics for the IS field through, for example, the use of existing techniques such as soft systems methodology. In addition, the practical potential of the theory is illustrated through a discussion of its application to specific IS topic areas including Web 2.0, open source software, the digital divide, and the UK biometric identity card scheme. The final section summarizes ways in which the paper could be used in IS research, teaching, and practice.
Packaged software applications such as enterprise systems are designed to support generic rather than specific requirements, and hence are likely to be an imperfect fit in any particular instance. Using critical realism as our philosophical perspective, we conducted a three-year qualitative study of misfits that arose from an enterprise system (ES) implementation. A detailed analysis of the observed misfits resulted in a richer understanding of the concept of fit and of the ES artifact itself. Specifically, we found six misfit domains (functionality, data, usability, role, control and organizational culture) and within each, two types of misfit (deficiencies and impositions). These misfit types correspond to two newly defined types of fit: fit as coverage and fit as enablement. Our analysis of fit also revealed a new conceptualization of the ES artifact, with implications for IT artifacts in general.
Drawing on sociology of science foundations, we argue that, in order to survive and prosper, healthy applied disciplines must meet the dual demands of academic and practitioner audiences by demonstrating both focus and diversity in their research. First, we use this concomitant modality to explain why previous studies into the structure of the Information Systems discipline have reported contradictory results, with some finding a focused field while others conclude that the field is diverse. In support of our argument, we then present the results of a longitudinal, author co-citation analysis, looking across the full range of journals in which IS research is published. Our results suggest that the IS field has sustained a focus on research within three subfields over a 20-year period from 1986 to 2005: (1) a thematic miscellany of research on development, implementation, and use of systems in various application domains; (2) IS strategy and business outcomes; and (3) group work and decision support. At the same time, the field has demonstrated considerable diversity within and around these core subfields, with researchers responding flexibly to the rapidly changing field by investigating these areas with new paradigms and in new contexts, and by exploring new topics including inter-business and Internet applications, computer-supported collaborative work, virtual teams, and knowledge management. Finally, we demonstrate that, over the 20-year period from 1986 to 2005, the discipline has shifted from fragmented adhocracy to a polycentric state, which is particularly appropriate to an applied discipline such as IS that must address the dual demands of focus and diversity in a rapidly changing technological context.
Employee noncompliance with information systems security policies is a key concern for organizations. If users do not comply with IS security policies, security solutions lose their efficacy. Of the different IS security policy compliance approaches, training is the most commonly suggested in the literature. Yet, few of the existing studies about training to promote IS policy compliance utilize theory to explain what learning principles affect user compliance with IS security policies, or offer empirical evidence of their practical effectiveness. Consequently, there is a need for IS security training approaches that are theory-based and empirically evaluated. Accordingly, we propose a training program based on two theories: the universal constructive instructional theory and the elaboration likelihood model. We then validate the training program for IS security policy compliance training through an action research project. The action research intervention suggests that the theory-based training achieved positive results and was practical to deploy. Moreover, the intervention suggests that information security training should utilize contents and methods that activate and motivate the learners to systematic cognitive processing of information they receive during the training. In addition, the action research study made clear that a continuous communication process was also required to improve user IS security policy compliance. The findings of this study offer new insights for scholars and practitioners involved in IS security policy compliance.
Studies on groups within the MIS discipline have largely been based on the paradigm of methodological individualism. Commentaries on methodological individualism within the reference disciplines suggest that studies embracing this paradigm can lead to potentially misleading or incorrect conclusions. This study illustrates the appropriateness of the alternate non-reductionist approach to investigating group-related phenomenon, specifically in the context of technology adoption. Drawing on theories of group influence, prior research on conflict, technology characteristics, task– technology fit, group communication media, and recent theoretical work surrounding group technology adoption, the paper proposes and empirically tests a new non-reductionist model for conceptualizing technology adoption by groups. Further, the study also empirically compares this non-reductionist model with a (hypothetical) methodological individualist model of technology adoption by groups. Results strongly support most of the assertions of the non-reductionist model and highlight that this model provides a more robust explanation of technology adoption by groups than a methodological individualist view. Further, the study also highlights some conditions wherein the methodological individualist view fails to provide correct explanations. The implications of the study’s findings for future research are discussed.
The Internet was a major factor in the 2008 U.S. presidential campaign and has become an important tool for political communication and persuasion. Yet, information systems research is generally silent on the role of the Internet in politics. In this paper, we argue that IS is positioned to enhance understanding of the influence of the Internet on politics, and, more specifically, the process of election campaigning using Internet-based technologies such as Web 2.0. In this paper, we discuss how these technologies can change the nature of competition in politics and replace or complement traditional media. Our empirical study on how Web 2.0 technologies were used by the candidates leading up to the 2008 U.S. presidential primaries sheds light on how these technologies influenced candidate performance. Finally, we outline a research agenda highlighting where IS can contribute to the academic discourse on e-politics.
Consumer reviews may reflect not only perceived quality but also the difference between quality and price (perceived value). In markets where product prices change frequently, these price-influenced reviews may be biased as a signal of product quality when used by consumers possessing no knowledge of historical prices. In this paper, we develop an analytical model that examines the impact of price-influenced reviews on firm optimal pricing and consumer welfare. We quantify the price effects in consumer reviews for different formats of review systems using actual market prices and online consumer ratings data collected for the digital cameramarket. Our empirical results suggest that unidimensional ratings, commonly used in most review systems, can be substantially biased by price effects. In fact, unidimensional ratings are more closely correlated with ratings of product value than ratings of product quality. Our findings suggest the importance for firms to account for these price effects in their overall marketing strategy and suggest that review systems could better serve consumers by explicitly expanding review dimensions to separate perceived value and perceived quality.
In contemporary knowledge-based organizations, teams often play an essential role in leveraging knowledge resources. Organizations make significant investments in information technology to support knowledge management practices in teams. At the same time, recent studies show that the transactive memory system (TMS)—the specialized division of cognitive labor among team members that relates to the encoding, storage, and retrieval of knowledge—is an important factor that affects a team’s performance. Yet little is known of how IT support for knowledge management practices in organizations affects the development of TMS. Furthermore, the precise role of TMS on knowledge sharing and knowledge application, which in turn influences team performance, has not been fully explored. In order to close this gap in the literature, we conducted a field study that involved 139 on-going teams of 743 individuals from two major firms in South Korea. Our results show that IT support in organizations has a positive impact on the development of TMS in teams, and that both TMS and IT support have a positive impact on knowledge sharing and knowledge application. Furthermore, we found that knowledge sharing has a positive impact on knowledge application, which in turn has a direct impact on team performance. However, contrary to our expectation, knowledge sharing does not have a direct impact on team performance and its impact on team performance was fully mediated by knowledge application. Our research shows that organizations can improve team members’ meta-knowledge of who knows what through the careful investment in information technology. Finally, our results show that sharing knowledge alone is not enough. Organizations must ensure that shared knowledge is in fact applied in order to improve team performance.
This research concentrates on visual complexity and order as central factors in the design of webpages that enhance users’ positive emotional reactions and facilitate desirable psychological states and behaviors. Drawing on existing theories and empirical findings in the environmental psychology, human–computer interaction, and marketing research literatures, a research model is developed to explain the relationships among visual complexity and order design features of a webpage, induced emotional responses in users, and users’ approach behaviors toward the website as moderated by users’ metamotivational states. A laboratory experiment was conducted to test the model and its associated hypotheses. The results of the study suggested that a web user’s initial emotional responses (i.e., pleasantness and arousal), evoked by the visual complexity and order design features of a webpage when first encountered, will have carry-over effects on subsequent approach behavior toward the website. The results also revealed how webpage visual complexity and order influence users’ emotions and behaviors differently when users are in different metamotivational states. The salience and importance of webpage visual complexity and order for users’ feelings of pleasantness were largely dependent on users’ metamotivational states.
Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers’ intent.We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of ADR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.
Employees in many contemporary organizations work with flexible routines and flexible technologies. When those employees find that they are unable to achieve their goals in the current environment, how do they decide whether they should change the composition of their routines or the materiality of the technologies with which they work? The perspective advanced in this paper suggests that the answer to this question depends on how human and material agencies—the basic building blocks common to both routines and technologies—are imbricated. Imbrication of human and material agencies creates infrastructure in the form of routines and technologies that people use to carry out their work. Routine or technological infrastructure used at any given moment is the result of previous imbrications of human and material agencies. People draw on this infrastructure to construct a perception that a technology either constrains their ability to achieve their goals, or that the technology affords the possibility of achieving new goals. The case of a computer simulation technology for automotive design used to illustrate this framework suggests that perceptions of constraint lead people to change their technologies while perceptions of affordance lead people to change their routines. This imbrication metaphor is used to suggest how a human agency approach to technology can usefully incorporate notions of material agency into its explanations of organizational change.
Over the years, research on the implications of information technology on network governance structures has explored the “move to the market” and the “move to the middle” hypotheses. The middle is a space in which the logic and modalities of markets and hierarchies are intermingled. There is increasing evidence that most network relations reflect mixed-mode or hybrid logic. Despite the apparent advantages that make the middle so populous or “swollen” (Hennart 1993, p. 472), Kambil et al. (1999) highlight that it is riddled with uncertainty and high transaction costs. They label it “the conflicted middle” and propose that online marketplaces, specifically all-in-one markets, are capable of resolving this conflict. Unfortunately, however, Kambil et al. provide limited insight into both the nature of the conflict that plagues the middle and the ability of all-in-one markets to resolve it.To address these questions, this paper applies a role-theoretic perspective to the study of an e-marketplace that served the energy industry and evolved into an all-in-one market. Relying on an interpretive case study, this paper addresses the following research questions: (1) What is the nature of the conflict that characterizes the conflicted middle? (2) How do e-marketplaces, specifically all-in-one markets, help resolve this conflict? Our research highlights that brokers, trading partners, and agents who operate in the middle (where the contradictory logic of markets and hierarchies are mixed) experience goal, behavior, and identity conflict. All-in-one markets can help resolve these conflicts by supporting role integration at the group level and role segmentation at the individual level.
Conceptual modeling grammars are a fundamental means for specifying information systems requirements. However, the actual usage of these grammars is only poorly understood. In particular, little is known about how properties of these grammars inform usage beliefs such as usefulness and ease of use. In this paper, we use an ontological theory to describe conceptual modeling grammars in terms of their ontological deficiencies, and formulate two propositions in regard to how these ontological deficiencies influence primary usage beliefs. Using BPMN as an example modeling grammar, we surveyed 528 modeling practitioners to test the theorized relationships. Our results show that users of conceptual modeling grammars perceive ontological deficiencies to exist, and that these deficiency perceptions are negatively associated with usefulness and ease of use of these grammars. With our research, we provide empirical evidence in support of the predictions of the ontological theory of modeling grammar expressiveness, and we identify previously unexplored links between conceptual modeling grammars and grammar usage beliefs. This work implies for practice a much closer coupling of the act of (re-)designing modeling grammars with usage-related success metrics.
The quality and future of human existence are directly related to the condition of our natural environment, but we are damaging the environment. Scientific evidence has mounted a compelling case that human behavior is responsible for deterioration in the Earth’s natural environment, with the rate of deterioration predicted to increase in the future. Acknowledging this evidence, the governments of 192 countries have formally agreed to take action to resolve problems with the climate system, one of the most highly stressed parts of the natural environment. While the intention is clear, the question of how best to proceed is not.The research reported here undertook a three-phase approach of selecting, analyzing, and synthesizing relevant literature to develop a holistic, transdisciplinary, integrative framework for IT-enabled business transformation. The focus on business transformation is because business is recognized as being a critical contributor in realizing the challenges of environmental sustainability due to its potential capacity for innovation and change—locally, nationally, and globally. This article also serves as a resource base for researchers to begin to undertake significant information systems and multidisciplinary work toward the goal of environmental sustainability. Through selection and analysis of illustrative examples of current work from 12 academic disciplines across 6 core categories, the framework addresses the key issues of uncertainty:(1) What is meant by environmental sustainability?(2) What are its major challenges?(3) What is being done about these challenges?(4) What needs to be done?
How do information technology capabilities contribute to firm performance? This study develops a conceptual model linking IT-enabled information management capability with three important organizational capabilities (customer management capability, process management capability, and performance management capability). We argue that these three capabilities mediate the relationship between information management capability and firm performance. We use a rare archival data set from a conglomerate business group that had adopted a model of performance excellence for organizational transformation based on the Baldrige criteria. This data set contains actual scores from high quality assessments of firms and intraorganizational units of the conglomerate, and hence provides unobtrusive measures of the key constructs to validate our conceptual model.We find that information management capability plays an important role in developing other firm capabilities for customer management, process management, and performance management. In turn, these capabilities favorably influence customer, financial, human resources, and organizational effectiveness measures of firm performance. Among key managerial implications, senior leaders must focus on creating necessary conditions for developing IT infrastructure and information management capability because they play a foundational role in building other capabilities for improved firm performance. The Baldrige model also needs some changes to more explicitly acknowledge the role and importance of information management capability so that senior leaders know where to begin in their journey toward business excellence.
The enhanced abilities of online retailers to learn about their customers’ shopping behaviors have increased fears of dynamic pricing, a practice in which a seller sets prices based on the estimated buyer’s willingness-to-pay. However, among online retailers, a deviation from a one-price-for-all policy is the exception. When price discrimination is observed, it is often in the context of customer outrage about unfair pricing.One setting where pricing varies is the name-your-own-price (NYOP) mechanism. In contrast to a typical retail setting, in NYOP markets, it is the buyer who places an initial offer. This offer is accepted if it is above some threshold price set by the seller. If the initial offer is rejected, the buyer can update her offer in subsequent rounds. By design, the final purchase price is opaque to the public; the price paid depends on the individual buyer’s willingness-to-pay and offer strategy. Further, most forms of NYOP employ a fixed threshold price policy.In this paper, we compare a fixed threshold price setting with an adaptive threshold price setting. A seller who considers an adaptive threshold price has to weigh potentially greater profits against customer objections about the perceived fairness of such a policy. We first derive the optimal strategy for the seller. We analyze the effectiveness of an adaptive threshold price vis-à-vis a fixed threshold price on seller profit and customer satisfaction. Further, we evaluate the moderating effect of revealing the threshold price policy (adaptive versus fixed) to buyers. We test our model in a series of laboratory experiments and in a large field experiment at a prominent NYOP seller involving real purchases. Our results show that revealing the usage of an adaptive mechanism yields higher profits and more transactions than not revealing this information. In the field experiment, we find that applying a revealed adaptive threshold price can increase profits by over 20 percent without lowering customer satisfaction.
Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.
With the advent of e-commerce, the potential of new Internet technologies to mislead or deceive consumers has increased considerably. This paper extends prior classifications of deception and presents a typology of product-related deceptive information practices that illustrates the various ways in which online merchants can deceive consumers via e-commerce product websites. The typology can be readily used as educational material to promote consumer awareness of deception in e-commerce and as input to establish benchmarks for good business practices for online companies. In addition, the paper develops an integrative model and a set of theory-based propositions addressing why consumers are deceived by the various types of deceptive information practices and what factors contribute to consumer success (or failure) in detecting such deceptions. The model not only enhances our conceptual understanding of the phenomenon of product-based deception and its outcomes in e-commerce but also serves as a foundation for further theoretical and empirical investigations. Moreover, a better understanding of the factors contributing to or inhibiting deception detection can also help government agencies and consumer organizations design more effective solutions to fight online deception.
While criteria or principles for conducting positivist and interpretive research have been widely discussed in the IS research literature, criteria or principles for critical research are lacking. Therefore, the purpose of this paper is to propose a set of principles for the conduct of critical research in information systems. We examine the nature of the critical research perspective, clarify its significance, and review its major discourses, recognizing that its mission and methods cannot be captured by a fixed set of criteria once and for all, particularly as multiple approaches are still in the process of defining their identity. However, we suggest it is possible to formulate a set of principles capturing some of the commonalities of those approaches that have so far become most visible in the IS research literature. The usefulness of the principles is illustrated by analyzing three critical field studies in information systems. We hope that this paper will further reflection and debate on the important subject of grounding critical research methodology.
How many articles in highly rated journals do Information Systems research faculty publish to earn tenure? Which journals are highly rated outlets? Tenure candidates, promotion and tenure committees, and those who are asked to write external letters are frequently called upon to answer such questions. When Dennis et al. (2006) examined all IS Ph.D. graduates entering academic careers, few faculty had published enough articles in 20 “elite” journals in six years to meet tenure research expectations at research-intensive schools. Our study builds on the dialog started by Dennis et al. In our study, we counted the number of journal articles at the point of tenure for faculty who earned tenure within five to seven years after their Ph.D. graduation date. We also examined the effect of acknowledging different sets of journals as highly rated on the publication rates of faculty who earned tenure. Specifically, we examined the effects of expanding on Dennis et al. by including MIS Quarterly, Information Systems Research, Journal of Management Information Systems, Journal of the AIS, Information Systems Journal, European Journal of Information Systems, Journal of Information Technology, and Journal of Strategic Information Systems in the journal basket. We also looked at the effect of acknowledging highly rated non-IS business journals and highly rated computer science and engineering journals. Finally, we present journal publication benchmarks based on these findings for different types of research institutions.
Strategic information technology alignment remains a top priority for business and IT executives. Yet with a recent rise in environmental volatility, firms are asking how to be more agile in identifying and responding to market-based threats and opportunities. Whether alignment helps or hurts agility is an unresolved issue. This paper presents a variety of arguments from the literature that alternately predict a positive or negative relationship between alignment and agility. This relationship is then tested using a model in which agility mediates the link between alignment and firm performance under varying conditions of IT infrastructure flexibility and environmental volatility. Using data from a matched survey of IT and business executives in 241 firms, we uncover a positive and significant link between alignment and agility and between agility and firm performance. We also show that the effect of alignment on performance is fully mediated by agility, that environmental volatility positively moderates the link between agility and firm performance, and that agility has a greater impact on firm performance in more volatile markets. While IT infrastructure flexibility does not moderate the link between alignment and agility, except in a volatile environment, we reveal that IT infrastructure flexibility has a positive and significant main effect on agility. In fact, the effect of IT infrastructure flexibility on agility is as strong as the effect of alignment on agility. This research extends and integrates the literature on strategic IT alignment and organizational agility at a time when both alignment and agility are recognized as critical and concurrent organizational goals.
Although information systems researchers have long recognized the possibility for collective- level information technology use patterns and outcomes to emerge from individual-level IT use behaviors, few have explored the key properties and mechanisms involved in this bottom-up IT use process. This paper seeks to build a theoretical framework drawing on the concepts and the analytical tool of complex adaptive systems (CAS) theory. The paper presents a CAS model of IT use that encodes a bottom-up IT use process into three interrelated elements: agents that consist of the basic entities of actions in an IT use process, interactions that refer to the mutually adaptive behaviors of agents, and an environment that represents the social organizational contexts of IT use. Agent-based modeling is introduced as the analytical tool for computationally representing and examining the CAS model of IT use. The operationability of the CAS model and the analytical tool are demonstrated through a theory-building exercise translating an interpretive case study of IT use to a specific version of the CAS model. While Orlikowski (1996) raised questions regarding the impacts of employee learning, IT flexibility, and workplace rigidity on IT-based organization transformation, the CAS model indicates that these factors in individual-level actions do not have a direct causal linkage with organizational- level IT use patterns and outcomes. This theory-building exercise manifests the intriguing nature of the bottom-up IT use process: collective-level IT use patterns and outcomes are the logical and yet often unintended or unforeseeable consequences of individual-level behaviors. The CAS model of IT use offers opportunities for expanding the theoretical and methodological scope of the IT use literature.
Although the literature on alternatives to effect indicators is growing, there has been little attention given to evaluating causal and composite (formative) indicators. This paper provides an overview of this topic by contrasting ways of assessing the validity of effect and causal indicators in structural equation models (SEMs). It also draws a distinction between composite (formative) indicators and causal indicators and argues that validity is most relevant to the latter. Sound validity assessment of indicators is dependent on having an adequate overall model fit and on the relative stability of the parameter estimates for the latent variable and indicators as they appear in different models. If the overall fit and stability of estimates are adequate, then a researcher can assess validity using the unstandardized and standardized validity coefficients and the unique validity variance estimate. With multiple causal indicators or with effect indicators influenced by multiple latent variables, collinearity diagnostics are useful. These results are illustrated with a number of correctly and incorrectly specified hypothetical models.
Almost 30 years after the introduction of the CIO position, the ideal CIO reporting structure (whether the CIO should report to the CEO or the CFO) is yet to be identified. There is an intuitive assumption among some proponents of IT that the CIO should always report to the CEO to promote the importance of IT and the CIO’s clout in the firm, while some adversaries of IT call for a CIO–CFO reporting structure to keep a tab on IT spending. However, we challenge these two ad hoc prescriptions by arguing that neither CIO reporting structure is necessarily optimal, and that the CIO reporting structure should not be used to gauge the strategic role of IT in the firm. First, extending the strategy–structure paradigm, we propose that a firm’s strategic positioning (differentiation or cost leadership) should be a primary determinant of its CIO reporting structure. We hypothesize that differentiators are more likely to have their CIO report to the CEO in order to pursue IT initiatives that help the firm’s differentiation strategy. We also hypothesize that cost leaders are more likely to have their CIO report to the CFO to lead IT initiatives to facilitate the firm’s cost leadership strategy. Second, extending the alignment–fit view, we propose that firms that align their CIO reporting structure with their strategic positioning (specifically, differentiation with a CIO–CEO reporting structure and cost leadership with a CIO–CFO reporting structure) will have superior future performance.Longitudinal data from two periods (1990–1993 and 2006) support the proposed hypotheses, validating the relationship between a firm’s strategic positioning and its CIO reporting structure, and also the positive impact of their alignment on firm performance. These results challenge the ad hoc prescriptions about the CIO reporting structure, demonstrating that a CIO–CEO reporting structure is only superior for differentiators and a CIO–CFO reporting structure is superior only for cost leaders. The CIO reporting structure must, therefore, be designed to align with the firm’s strategic positioning, independent of whether IT plays a key strategic role in the firm.
Despite the fact that validating the measures of constructs is critical to building cumulative knowledge in MIS and the behavioral sciences, the process of scale development and validation continues to be a challenging activity. Undoubtedly, part of the problem is that many of the scale development procedures advocated in the literature are limited by the fact that they (1) fail to adequately discuss how to develop appropriate conceptual definitions of the focal construct, (2) often fail to properly specify the measurement model that relates the latent construct to its indicators, and (3) underutilize techniques that provide evidence that the set of items used to represent the focal construct actually measures what it purports to measure. Therefore, the purpose of the present paper is to integrate new and existing techniques into a comprehensive set of recommendations that can be used to give researchers in MIS and the behavioral sciences a framework for developing valid measures. First, we briefly elaborate upon some of the limitations of current scale development practices. Following this, we discuss each of the steps in the scale development process while paying particular attention to the differences that are required when one is attempting to develop scales for constructs with formative indicators as opposed to constructs with reflective indicators. Finally, we discuss several things that should be done after the initial development of a scale to examine its generalizability and to enhance its usefulness.
Despite renewed interest and many advances in methodology in recent years, information systems and organizational researchers face confusing and inconsistent guidance on how to choose amongst, implement, and interpret findings from the use of different measurement procedures. In this article, the related topics of measurement and construct validity are summarized and discussed, with particular focus on formative and reflective indicators and common method bias, and, where relevant, a number of allied issues are considered. The perspective taken is an eclectic and holistic one and attempts to address conceptual and philosophical essentials, raise salient questions, and pose plausible solutions to critical measurement dilemmas occurring in the managerial, behavioral, and social sciences.
Formatively measured constructs have been increasingly used in information systems research. With few exceptions, however, extant studies have been relying on the partial least squares (PLS) approach to specify and estimate structural models involving constructs measured with formative indicators. This paper highlights the benefits of employing covariance structure analysis (CSA) when investigating such models and illustrates its application with the LISREL program. The aim is to provide practicing IS researchers with an understanding of key issues and potential problems associated with formatively measured constructs within a covariance-based modeling framework and encourage them to consider using CSA in their future research endeavors.
Advances in information technology and e-commerce enable firms to make personalized offers to individual consumers based on information about the consumers. However, the collection and use of private information have caused serious concerns about privacy invasion by consumers, creating a personalization–privacy tradeoff. The key approach to address privacy concerns is via the protection of privacy through the implementation of fair information practices, a set of standards governing the collection and use of personal information. In this paper, we take a game-theoretic approach to explore the motivation of firms for privacy protection and its impact on competition and social welfare in the context of product and price personalization. We find that privacy protection can work as a competition-mitigating mechanism by generating asymmetry in the consumer segments to which firms offer personalization, enhancing the profit extraction abilities of the firms. In equilibrium, both symmetric and asymmetric choices of privacy protection by the firms can result, depending on the size of the personalization scope and the investment cost of protection. Further, as consumers become more concerned about their privacy, it is more likely that all firms adopt privacy protection. In the perspective of welfare, we show that autonomous choices of privacy protection by personalizing firms can improve social welfare at the expense of consumer welfare. We further find that regulation enforcing the implementation of fair information practices can be efficient from the social welfare perspective mainly by limiting the incentives of the firms to exploit the competition-mitigation effect.
The increasing dependence on information networks for business operations has focused managerial attention on managing risks posed by failure of these networks. In this paper, we develop models to assess the risk of failure on the availability of an information network due to attacks that exploit software vulnerabilities. Software vulnerabilities arise from software installed on the nodes of the network. When the same software stack is installed on multiple nodes on the network, software vulnerabilities are shared among them. These shared vulnerabilities can result in correlated failure of multiple nodes resulting in longer repair times and greater loss of availability of the network. Considering positive network effects (e.g., compatibility) alone without taking the risks of correlated failure and the resulting downtime into account would lead to overinvestment in homogeneous software deployment. Exploiting characteristics unique to information networks, we present a queuing model that allows us to quantify downtime loss faced by a rm as a function of (1) investment in security technologies to avert attacks, (2) software diversification to limit the risk of correlated failure under attacks, and (3) investment in IT resources to repair failures due to attacks. The novelty of this method is that we endogenize the failure distribution and the node correlation distribution, and show how the diversification strategy and other security measures/investments may impact these two distributions, which in turn determine the security loss faced by the firm. We analyze and discuss the effectiveness of diversification strategy under different operating conditions and in the presence of changing vulnerabilities. We also take into account the benefits and costs of a diversification strategy. Our analysis provides conditions under which diversification strategy is advantageous.
An electronic commerce marketing channel is fully mediated by information technology, stripping away much of a product’s physical informational cues, and creating information asymmetries (i.e., limited information). These asymmetries may impede consumers’ ability to effectively assess certain types of products, thus creating challenges for online sellers. Signaling theory provides a framework for understanding how extrinsic cues— signals—can be used by sellers to convey product quality information to consumers, reducing uncertainty and facilitating a purchase or exchange. This research proposes a model to investigate website quality as a potential signal of product quality and consider the moderating effects of product information asymmetries and signal credibility. Three experiments are reported that examine the efficacy of signaling theory as a basis for predicting online consumer behavior with an experience good. The results indicate that website quality influences consumers’ perceptions of product quality, which subsequently affects online purchase intentions. Additionally, website quality was found to have a greater influence on perceived product quality when consumers had higher information asymmetries. Likewise, signal credibility was found to strengthen the relationship between website quality and product quality perceptions for a high quality website. Implications for future research and website design are examined.
Firms often collaborate with other firms to set information technology standards in order to decrease each firm’s individual risk. But does this work? We propose that, in a capital market setting, establishing standards in a group does not decrease the total risk faced by an individual firm’s shareholders. However, the market risks its investors face decrease and idiosyncratic risks increase, changing the risk profiles of the group members. We collected data on standard-setting events from 1996 to 2005. In our dataset, a firm obtained a 4.07 percent, three-day cumulative risk-adjusted return on stock price when engaging in a standard-setting initiative, after controlling event year, firm size, and group size. More importantly, we found that an increase in the number of firms in the group decreased the risk-adjusted abnormal return and the market risk (as measured by beta) of each firm, but increased the idiosyncratic risk (as measured by the variance of firm returns). Our findings suggest that firms electing to participate in a large standardization group obtain a reduction in abnormal returns on stocks on the days of the standard-setting events. They also expect to reduce market risks but increase idiosyncratic risks after the standard-setting events, as compared to firms choosing to participate in a smaller group or attempting to standardize their products unilaterally. This study contributes to the literature on IT standards and standardization, and expands our understanding of the implications of standardization strategy on shareholder risks.
Limited attention has been directed toward examining post-adoption stages of the information system life cycle. In particular, the final stages of this life cycle have been largely ignored despite the fact that most systems eventually reach the end of their useful life. This oversight is somewhat surprising given that end-of-life decisions can have significant implications for user effectiveness, the value extracted from IS investments, and organizational performance. Given this apparent gap, a multi-method empirical study was undertaken to improve our understanding of organizational level information system discontinuance. Research commenced with the development of a broad theoretical framework consistent with the technology–organization– environment (TOE) paradigm. The resulting framework was then used to guide a series of semi-structured interviews with organizational decision makers in an effort to inductively identify salient influences on the formation of IS discontinuance intentions. A set of research hypotheses were formulated based on the understanding obtained during these interviews and subsequently tested via a random survey of senior IS decision makers at U.S. and Canadian organizations. Data obtained from the survey responses was analyzed using partial least squares (PLS). Results of this analysis suggest that system capability shortcomings, limited availability of system support, and low levels of technical integration were key determinants of increased intentions to replace an existing system. Notably, investments in existing systems did not appear to significantly undermine organizational replacement intentions despite support for this possibility from both theory and our semi-structured interviews
Game companies use five components—four core components and one complementary one—in a 5Cs model to ensure the control and development of virtual worlds. A multidisciplinary review of the literature reveals that game companies make use of copyright, codes, creativity, and community to do this. They use the contract as a complementary component to reinforce their control over the four basic components and to compensate for the lacunae they present. In order to examine the extent to which game companies use the contract in this way, an analysis is performed of all contractual documents from a sample of 20 virtual worlds, providing evidence of general trends and emphasizing any differences between the virtual worlds in terms of the business and gaming models sought by each game company. An explanation is provided of why these contracts do not constitute a sustainable model for the game companies, given the high level of legal insecurity they present. Some basic recommendations can be made in order to improve the sustainability of the 5Cs model by modifying these contracts in such a way that they are enforceable and by matching their content with appropriate business and gaming models. This could lead to further studies aimed at providing answers to some of the intriguing issues affecting scholars and practitioners.
Digital platforms for buying and selling agricultural commodities have generated significant interest in the trade literature as a way to link rural communities to the Internet. Yet, the extent to which these digital platforms actually translate into higher commodity prices for producers remains an open research question. We investigate this question by comparing transaction data on trading various grades of coffee from a recently implemented digital platform in India with similar transactions from a physical commodity auction held weekly, and farm-gate prices in the coffee producing regions of India. Although the digital platform prices closely track the physical commodity auction prices, producers obtain significantly higher prices when they sell the commodity through the digital platform rather than at the farm-gate through brokers who operate in their regions. However, coffee grades with higher price volatility and premium coffee grades that require face-to-face interactions to verify quality obtain lower prices on the digital platform. Our results also indicate that market participants who control the transaction obtain better prices. We discuss the implications of our findings for governments and platform providers.
In this research note, we examine the design, development, validation, and use of virtual worlds. Our purpose in doing so is to extend the design science paradigm by developing a set of design principles applicable to the context of virtual environments, particularly those using agent-based simulation as their underlying technology. Our central argument is that virtual worlds comprise a new class of information system, one that combines the structural aspects of traditional modeling and simulation systems in concert with emergent user dynamics of systems supporting emergent knowledge processes. Our approach involves two components. First, we review the characteristics of agent-based virtual worlds (ABVWs) to discern design requirements that may challenge current design theory. From this review, we derive a set of design principles based on deep versus emergent structures where deep structures reflect conventional modeling and simulation system architectures and emergent structures capture the unpredictable user–system dynamics inherent in emergent knowledge processes, which increasingly characterize virtual worlds. We illustrate how these design challenges are addressed with an exemplar of a complex mirror world, a large-scale ABVW we developed called Sentient World. Our contribution is the insight of partitioning ABVW architectures into deep and emergent structures that mirror modeling systems and emergent knowledge processes respectively, while developing extended design principles to facilitate their integration. We conclude with a discussion of the implications of our design principles for informing and guiding future research and practice.
Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers’ careers influence their judgments of these behaviors. Membership in the Association for Information Systems (AIS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers’ judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the AIS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.
Firms increasingly turn to online communities to create valuable information. These communities are empowered by new information technology-enabled collaborative tools, tools such as blogs, wikis, and social networks. Collaboration on these platforms is characterized by considerable membership turnover, which could have significant effects on collaborative outcomes. We hypothesize that membership retention relates in a curvilinear fashion to effective collaboration: positively up to a threshold and negatively thereafter. The longitudinal history of 2,065 featured articles on Wikipedia offers support for this hypotheses: Contributions from a mixture of new and experienced participants both increases the likelihood that an article will be promoted to featured article status and decreases the risk it will be demoted after having been promoted. These findings imply that, contrary to many of the assumptions in previous research, participant retention does not have a strictly positive effect on emerging collaborative environments. Further analysis of our data provides empirical evidence that knowledge creation and knowledge retention are actually distinct phases of community-based peer production, and that communities may on average experience more turnover than ideal during the knowledge retention phase.
This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.
Many organizational innovations can be explained by the movement of ideas and information from one social context to another, “from where they are known to where they are not” (Hargadon 2002, p. 41). A relatively new technology, social bookmarking, is increasingly being used in many organizations (McAfee 2006), and may enhance employee innovativeness by providing a new, socially mediated channel for discovering information. Users of such systems create publicly viewable lists of bookmarks (each being a hyperlink to an information resource) and often assign searchable keywords (“tags”) to these bookmarks. We explore two different perspectives on how accessing others’ bookmarks could enhance how innovative an individual is at work. First, we develop two hypotheses around the idea that quantity may be a proxy for diversity, following a well established literature that holds that the more information obtained and the larger the number of sources consulted, the higher the likelihood an individual will come across novel ideas. Next, we offer two hypotheses adapted from social network research that argue that the shape of the network of connections that is created when individuals access each others’ bookmarks can reflect information novelty, and that individuals whose networks bridge more structural holes and have greater effective reach are likely to be more innovative. An analysis of bookmarking system use in a global professional services firm provides strong support for the social diversity of information sources as a predictor of employee innovativeness, but no support that the number of bookmarks accessed matters. By extending the social networks literature to theorize the functionalities offered by social bookmarking systems, this research establishes structural holes theory as a valuable lens through which social technologies may be understood.
Virtual worlds have received considerable attention as platforms for entertainment, education, and commerce. But organizations are experiencing failures in their early attempts to lure customers, employees, or partners into these worlds. Among the more grievous problems is the inability to attract users back into a virtual environment. In this study, we propose and test a model to predict users’ intentions to return to a virtual world. Our model is based on the idea that users intend to return to a virtual world having conceived of it as a “place” in which they have had meaningful experiences. We rely on the interactionist theory of place attachment to explain the links among the constructs of our model. Our model is tested via a lab experiment. We find that users’ intentions to return to a virtual world is determined by a state of deep involvement (termed cognitive absorption) that users experience as they perform an activity and tend to lose track of time. In turn, cognitive absorption is determined by users’ awareness of whom they interact with and how they interact within a virtual world, what they interact about, and where, in a virtual sense, such interaction occurs. Our work contributes to theory in the following ways: it identifies state predictors of cognitive absorption, it conceives of virtual worlds in such a way as to account for users’ experiences through the notion of place, and it explains how the properties of a virtual world contribute to users’ awareness.
Emerging virtual worlds, such as the prominent Second Life, offer unprecedented opportunities for companies to collaborate with co-creating users. However, pioneering corporate co-creation systems fail to attract a satisfying level of participation and engagement. The experience users have with the co-creation system is the key to making virtual places a vibrant source of great connections, creativity, and co-creation. While prior research on co-creation serves as a foundation for this work, it does not provide adequate guidance on how to design co-creation systems in virtual worlds. To address this shortcoming, a 20-month action research project was conducted to study the user’s experience and to identify design principles for virtual co-creation systems. In two action research cycles, a virtual co-creation system called Ideation Quest was created, deployed, evaluated, and improved. The study reveals how to design co-creation systems and enriches research on co-creation to fit the virtual world context. Practitioners receive a helpful framework to leverage virtual worlds for co-creation.
With the rapid pace of technological development, individuals are frequently challenged to make sense of equivocal innovative technology while being given limited information. Virtual worlds are a prime example of such an equivocal innovative technology, and this affords researchers an opportunity to study sensemaking and the construction of perspectives about the organizational value of virtual worlds. This study reports on an analysis of the written assessments of 59 business professionals who spent an extended period of time in Second Life, a popular virtual world, and discursively made sense of the organizational value of virtual worlds. Through a Toulminian analysis of the claims, grounds, and warrants used in the texts they generated, we identify 12 common patterns of sensemaking and indicate that themes of confirmation, open-ended rhetoric, demographics, and control are evident in the different types of claims that were addressed. Further, we assert that the Toulminian approach we employ is a useful methodology for the study of sensemaking and one that is not bound to any particular theoretical perspective.
As broadband Internet access and virtual reality technology rapidly expand, virtual worlds and three-dimensional avatars will become more pervasive and widely adopted. In virtual worlds, people assume an identity as an avatar and interact with each other. The objective of this study is to theorize how users form attitudes and intentions regarding avatars in realistic, task-focused virtual world settings. To investigate these effects, this study proposes a conceptual framework based on dual-congruity perspectives (self-congruity and functional congruity). The results show that the more closely an avatar resembles its user, the more the user is likely to have positive attitudes (e.g., affection, connection, and passion) toward the avatar, and the better able to evaluate the quality and performance of apparel products. In the end, these positive attitudes toward an avatar and its usefulness positively affect users’ intentions to use the avatar. Based on this study, we propose that avatars representing users’ actual appearance may be helpful in experiencing and evaluating some business areas related to users’ lives in the real world (e.g., virtual apparel shopping, matchmaking, plastic surgery, fitness clubs, etc.); utilization of such avatars may be a new business opportunity likely to thrive in virtual worlds.
Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus–organism–response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants’ virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants’ virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.
This research uses theories of flow, telepresence, positive emotions, and brand equity to examine the effect of using two-dimensional versus three-dimensional virtual world environments on telepresence, enjoyment, brand equity, and behavioral intention. The findings suggest that the 3D virtual world environment produces both positive and negative effects on brand equity when compared to the 2D environment. The positive effect of the 3D virtual world environment on brand equity occurs through telepresence, a specific aspect of flow, as well as enjoyment. The negative effect on brand equity can be explained using distraction–conflict theory in which attentional conflicts faced by users of a highly interactive and rich medium resulted in distractions from attending to the brand. Brand equity, in turn, has a positive effect on behavioral intention. The results suggest that although the 3D virtual world environment has the potential to increase brand equity by offering an immersive and enjoyable virtual product experience, the rich environment can also be a distraction. Therefore, developers of virtual world branding sites need to take into account limitations in the information processing capacity and attention span of users when designing their sites in order to avoid cognitive overload, which can lead to users being distracted from branding information. This paper not only provides a theoretical foundation for explaining users’ experience with 2D versus 3D virtual world branding sites, but also provides insights to practitioners for designing 3D virtual world sites to enhance brand equity and intentions through user engagement.
To date, many important threads of information privacy research have developed, but these threads have not been woven together into a cohesive fabric. This paper provides an interdisciplinary review of privacy-related research in order to enable a more cohesive treatment. With a sample of 320 privacy articles and 128 books and book sections, we classify previous literature in two ways: (1) using an ethics-based nomenclature of normative, purely descriptive, and empirically descriptive, and (2) based on their level of analysis: individual, group, organizational, and societal.Based upon our analyses via these two classification approaches, we identify three major areas in which previous research contributions reside: the conceptualization of information privacy, the relationship between information privacy and other constructs, and the contextual nature of these relationships. As we consider these major areas, we draw three overarching conclusions. First, there are many theoretical developments in the body of normative and purely descriptive studies that have not been addressed in empirical research on privacy. Rigorous studies that either trace processes associated with, or test implied assertions from, these value-laden arguments could add great value. Second, some of the levels of analysis have received less attention in certain contexts than have others in the research to date. Future empirical studies—both positivist and interpretive—could profitably be targeted to these under-researched levels of analysis. Third, positivist empirical studies will add the greatest value if they focus on antecedents to privacy concerns and on actual outcomes. In that light, we recommend that researchers be alert to an overarching macro model that we term APCO (Antecedents -> Privacy Concerns -> Outcomes).
Information privacy refers to the desire of individuals to control or have some influence over data about themselves. Advances in information technology have raised concerns about information privacy and its impacts, and have motivated Information Systems researchers to explore information privacy issues, including technical solutions to address these concerns. In this paper, we inform researchers about the current state of information privacy research in IS through a critical analysis of the IS literature that considers information privacy as a key construct. The review of the literature reveals that information privacy is a multilevel concept, but rarely studied as such. We also find that information privacy research has been heavily reliant on student-based and USA-centric samples, which results in findings of limited generalizability. Information privacy research focuses on explaining and predicting theoretical contributions, with few studies in journal articles focusing on design and action contributions. We recommend that future research should consider different levels of analysis as well as multilevel effects of information privacy. We illustrate this with a multilevel framework for information privacy concerns. We call for research on information privacy to use a broader diversity of sampling populations, and for more design and action information privacy research to be published in journal articles that can result in IT artifacts for protection or control of information privacy.
How does users’ freedom of choice, or the lack thereof, affect interface preferences? The research reported in this article approaches this question from two theoretical perspectives. The first of these argues that an interface with a dominant market share benefits from the absence of competition because users acquire skills that are specific to that particular interface, which in turn reduces the probability that they will switch to a new competitor interface in the future. By contrast, the second perspective proposes that the advantage that a market leader has in being able to install a set of non-transferable skills in its user base is offset by a psychological force that causes humans to react against perceived constraints on their freedom of choice. We test a research model that incorporates the key predictions of these two theoretical perspectives in an experiment involving consequential interface choices. We find strong support for the second perspective, which builds upon the theory of psychological reactance.
Organizations often provide workers with knowledge management systems to help them obtain knowledge they need. A significant constraint on the effectiveness of such systems is that they assume workers know what knowledge they need (they know what they don’t know) when, in fact, they often do not know what knowledge they need (they don’t know what they don’t know). A way to overcome this problem is to use visual ontologies to help users learn relevant concepts and relationships in the knowledge domain, enabling them to search the knowledge base in a more educated manner. However, no guidelines exist for designing such ontologies. To fill this gap, we draw on theories of philosophical ontology and cognition to propose guidelines for designing visual ontologies for knowledge identification.We conducted three experiments to compare the effectiveness of guided ontologies, visual ontologies that followed our guidelines, to unguided ontologies, visual ontologies that violated our guidelines. We found that subjects performed considerably better with the guided ontologies, and that subjects could perceive the benefits of using guided ontologies, at least in some circumstances. On the basis of these results, we conclude that the way visual ontologies are presented makes a difference in knowledge identification and that theories of philosophical ontology and cognition can guide the construction of more effective visual representations. Furthermore, we propose that the principles we used to create the guided visual ontologies can be generalized for other cases where visual models are used to inform users about application domains.
Information technology is generally considered an enabler of a firm’s agility. A typical premise is that greater IT investment enables a firm to be more agile. However, it is not uncommon that IT can also hinder and sometimes even impede organizational agility. We propose and theorize this frequently observed but understudied IT–agility contradiction by which IT may enable or impede agility. We develop the premise that organizations need to develop superior firm-wide IT capability to successfully manage their IT resources to realize agility. We refine the conceptualization and measurement of IT capability as a latent construct reflected in its three dimensions: IT infrastructure capability, IT business spanning capability, and IT proactive stance. We also conceptualize two types of organizational agility: market capitalizing agility and operational adjustment agility. We then conduct a matched-pair field survey of business and information systems executives in 128 organizations to empirically examine the link between a firm’s IT capability and agility. Business executives responded to measurement scales of the two types of agility and organizational context variables, and IS executives responded to measurement scales of IT capabilities and IS context variables. The results show a significant positive relationship between IT capability and the two types of organizational agility. We also find a significant positive joint effect of IT capability and IT spending on operational adjustment agility but not on market capitalizing agility. The findings suggest a possible resolution to the contradictory effect of IT on agility: while more IT spending does not lead to greater agility, spending it in such a way as to enhance and foster IT capabilities does. Our study provides initial empirical evidence to better understand essential IT capabilities and their relationship with organizational agility. Our findings provide a number of useful implications for research and managerial practices.
Recent research has acknowledged the key role of information technology in helping build stronger and more enduring customer relationships. Personalized product recommendations (PPRs) adapted to individual customers’ preferences and tastes are one IT-enabled strategy that has been widely adopted by online retailers to enhance customers’ shopping experience. Although many online retailers have implemented PPRs on their electronic storefronts to improve customer retention, empirical evidence for the effects of PPRs on retention is sparse, and the limited anecdotal evidence is contradictory. We draw upon the household production function model in the consumer economics literature to develop a theoretical framework that explains the mechanisms through which PPRs influence customer store loyalty in electronic markets. We suggest that retailer learning that occurs as a result of customer knowledge obtained to enable personalization influences the efficiency of the online product brokering activity. Data collected from a two-phase lab experiment with 253 student subjects where the quality of PPRs was manipulated are used to empirically test the predictions of the theoretical model. Empirical analyses of the data indicate that retailer learning reflected in higher quality PPRs is associated with lower product screening cost, but higher product evaluation cost. We further find that higher quality PPRs are associated with greater value derived by consumers from the online product brokering activity in terms of higher decision making quality, which is positively associated with repurchase intention. The paper presents the implications, limitations, and contributions of this study along with areas for future research.
Virtually all of the extensive previous research investigating the effect of information systems proficiency on performance has been conducted at the individual level. Little research has investigated the relationship between IS proficiency and performance at the group level. In this paper, we argue that IS proficiency at the group level may be more than the simple sum or average of the IS proficiency of individual group members. Rather, effective group-level IS proficiency may also be a function of how a group’s IS proficiency is distributed across its members. Relying on concepts associated with social network analysis (SNA), we introduce the concept of centrality–IS proficiency alignment. We argue that groups will perform better if their more proficient members are highly central in the group’s communication and workflows network. Data from 468 employees in 32 workgroups show that centrality–IS proficiency alignment is significantly and positively related to performance across multiple systems examined individually and with the portfolio of systems examined as a whole. This approach effectively integrates the structural and resource perspectives of SNA, providing a roadmap so that others may follow a similar approach to address broader questions of group-level user–system interactions in the IS literature and more general questions of central resource alignment in the broader organizational literature.
What determines the success of open source projects? In this study, we investigate the impact of network social capital on open source project success. We define network social capital as the benefits open source developers secure from their membership in developer collaboration networks. We focus on one specific type of success as measured by the rate of knowledge creation in an open source project. Specific hypotheses are developed and tested using a longitudinal panel of 2,378 projects hosted at SourceForge. We find that network social capital is not equally accessible to or appropriated by all projects. Our main results are as follows. First, projects with greater internal cohesion (that is, cohesion among the project members) are more successful. Second, external cohesion (that is, cohesion among the external contacts of a project) has an inverse U-shaped relationship with the project’s success; moderate levels of external cohesion are best for a project’s success rather than very low or very high levels. Third, the technological diversity of the external network of a project also has the greatest benefit when it is neither too low nor too high. Fourth, the number of direct and indirect external contacts positively affects a project’s success such that the effect of the number of direct contacts is moderated by the number of indirect contacts. These results are robust to several control variables and alternate model specifications. Several theoretical and managerial implications are provided.
To compete in a highly dynamic marketplace, firms must frequently adapt and align their competitive strategies and information systems. The dominant literature on the strategic fit of a firm’s information systems focuses primarily on high-level measures of the strategic fit of a firm’s overall IS portfolio and the impact of fit on business performance. This paper addresses the need for a more fine-grained approach for assessing the specific areas of misfit between a firm’s competitive strategies and IS capabilities. We describe the design and evaluation of a multilevel strategic fit (MSF) measurement model that enables researchers and practitioners to measure the strategic fit of a firm’s information systems at both an overall and a detailed level. The steps in the model include identifying the relevant IS capabilities according to the type of system; measuring the current level of support for each capability using a capabilities instrument; identifying the ideal level of support for each capability using an adaptation of Conant et al.’s (1990) instrument to assess strategic archetype; and comparing the ideal and realized level of support for each capability. Evidence from a multiple case study analysis indicates that the fine-grained assessment of strategic fit can strengthen the validity, utility, and ease of corroboration of the strategic fit measurement outputs. The paper also demonstrates how an iterative design science research approach, with its emphasis on evaluating the utility of prototype artifacts, is well suited to developing field-tested and theoretically grounded measurement models and instruments that are accessible to practitioners. This focus on practical utility in turn provides researchers with results that can be more readily corroborated, thus improving the quality and usefulness of the research findings.
Technology addiction is a relatively new mental condition that has not yet been well integrated into mainstream MIS models. This study bridges this gap and incorporates technology addiction into technology use processes in the context of online auctions. It examines how user cognition and ultimately usage intentions toward an information technology are distorted by addiction to the technology. The findings from two empirical studies of 132 and 223 eBay users, using three different operationalizations of addiction, indicate that the level of online auction addiction distorts the way the IT artifact is perceived. Informing a range of cognition-modification processes, addiction to online auctions augments user perceptions of enjoyment, usefulness, and ease of use attributed to the technology, which in turn influence usage intentions. Overall, consistent with behavioral addiction models, the findings indicate that users’ levels of online auction addiction influence their reasoned IT usage decisions by altering users’ belief systems. The formation of maladaptive perceptions is driven by a combination of memory-, learning-, and bias-based cognition modification processes. Implications of the findings are discussed.
With the proliferation and ubiquity of information and communication technologies (ICTs), it is becoming imperative for individuals to constantly engage with these technologies in order to get work accomplished. Academic literature, popular press, and anecdotal evidence suggest that ICTs are responsible for increased stress levels in individuals (known as technostress). However, despite the influence of stress on health costs and productivity, it is not very clear which characteristics of ICTs create stress. We draw from IS and stress research to build and test a model of technostress. The person–environment fit model is used as a theoretical lens. The research model proposes that certain technology characteristics—like usability (usefulness, complexity, and reliability), intrusiveness (presenteeism, anonymity), and dynamism (pace of change)—are related to stressors (work overload, role ambiguity, invasion of privacy, work–home conflict, and job insecurity). Field data from 661 working professionals was obtained and analyzed. The results clearly suggest the prevalence of technostress and the hypotheses from the model are generally supported. Work overload and role ambiguity are found to be the two most dominant stressors, whereas intrusive technology characteristics are found to be the dominant predictors of stressors. The results open up new avenues for research by highlighting the incidence of technostress in organizations and possible interventions to alleviate it.
Little is known about how individuals come to relate to settings in virtual worlds (VWs), which are defined as digital environments in which individuals, groups, and even organizations interact in virtual (that is to say, nonphysical) spaces. This research develops a theory of virtual space and place (VSP), specifically relating this to the setting of Second Life (SL), a prominent social virtual world. We explore how three-dimensional space, as perceived by users, is able to provide them with an interactive experience with virtual objects, as well as with other VW denizens. To test our theory, we build interactive work tools in SL that are designed to reflect various degrees of motion range and to influence presence. The three information technology tools are evaluated by 150 business professionals who are either familiar or unfamiliar with SL. Implications for practice and directions for future research are discussed.
Do information technology investments improve firm profitability? If so, is this effect because such investments help improve sales, or is it because they help reduce overall operating expenses? How does the effect of IT on profitability compare with that of advertising and of research and development? These are important questions because investments in IT constitute a large part of firms’ discretionary expenditures, and managers need to understand the likely impacts and mechanisms to justify and realize value from their IT and related resource allocation processes. The empirical evidence in this paper, derived using archival data from 1998 to 2003 for more than 400 global firms, suggests that IT has a positive impact on profitability. Importantly, the effect of IT investments on sales and profitability is higher than that of other discretionary investments, such as advertising and R&D. A significant portion of the impact of IT on firm profitability is accounted for by IT-enabled revenue growth, but there is no evidence for the effect of IT on profitability through operating cost reduction. Taken together, these findings suggest that firms have had greater success in achieving higher profitability through IT-enabled revenue growth than through IT-enabled cost reduction. They also provide important implications for managers to make allocations among discretionary expenditures such as IT, advertising, and R&D. With regard to IT expenditures, the results imply that firms should accord higher priority to IT projects that have revenue growth potential over those that focus mainly on cost savings.This paper won the first-place award as European Research Paper of the Year for 2013.
Researchers in a number of disciplines, including Information Systems, have argued that much of past research may have incorrectly specified the relationship between latent variables and indicators as reflective when an understanding of a construct and its measures indicates that a formative specification would have been warranted. Coupled with the posited severe biasing effects of construct misspecification on structural parameters, these two assertions would lead to concluding that an important portion of our literature is largely invalid. While we do not delve into the issue of when one specification should be employed over another, our work here contends that construct misspecification, but with a particular exception, does not lead to severely biased estimates. We argue, and show through extensive simulations, that a lack of attention to the metric in which relationships are expressed is responsible for the current belief in the negative effects of misspecification.This article includes the responses by Cheryl Burke Jarvis, Scott B. MacKenzie, and Philip M. Podsakoff ("The Negative Consequences of Measurement Model Misspecification: A Response to Aguirre-Urreta and Marakas") and Stacie Peter, Arun Rai, and Detmar Straub ("The Critical Importance of Construct Measurement Specification: A Response to Aguirre-Urreta and Marakas).
Aguirre-Urreta and Marakas (A&M) suggest in their simulation “Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form,” that, like Jarvis et al. (2003), MacKenzie et al. (2005), and Petter et al. (2007) before them, bias does occur when formative constructs are misspecified as reflective. But A&M argue that the level of bias in prior simulation studies has been exaggerated. They parameterize their simulation models using standardized coefficients in contrast to Jarvis et al., MacKenzie et al., and Petter et al., who parameterize their simulation models using unstandardized coefficients. Thus, across these four simulation studies, biases in parameter estimates are likely to result in misspecified measurement models (i.e., using either unstandardized or standardized coefficients); yet, the biases are greater in magnitude when unstandardized coefficients are used to parameterize the misspecified model. We believe that regardless of the extent of the bias, it is critically important for researchers to achieve correspondence between the measurement specification and the conceptual meaning of the construct so as to not alter the theoretical meaning of the construct at the operational layer of the model. Such alignment between theory and measurement will safeguard against threats to construct and statistical conclusion validity.This article is a response to the article by Miguel Aguirre-Urreta and George Marakas, "Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form," and is included in the purchase of that article.
A firm can upgrade relevant skills of its programmers by ensuring their participation in carefully chosen open source projects. Highly skilled programmers are more valuable for the firm but participating in open source projects reduces the time they spend doing the firm’s projects. This tradeoff determines the optimal extent of programmer participation in open source for the firm. The extent of open source participation may also be influenced by the minimum compensation that must be paid to hire a programmer in the labor market. This is because providing better skills is a way of compensating the programmers by improving their future market value. Hence the firm may want to increase open source participation to keep direct wage payments in check. We develop an analytical model based on optimal control theory to characterize the employment contract that features the best mix of open source participation and wage payments. We also find that the firm benefits more from the presence of open source in a tight labor market (i.e., when programmers have good options besides the employment offered by the firm). On the other hand, programmers are compensated better in the presence of open source opportunities when they have few outside options. This benefit is more for less skilled programmers.
Information requirements determination (IRD) is concerned with developing accurate requirements for a proposed system, primarily by eliciting information from users and other organizational stakeholders. In this paper we build and test theory concerning a significant threat to the accuracy of information requirements, termed the misinformation effect. Misinformation is distorted, false, or other erroneous or misleading information that does not reflect the true state of the world or state of mind of the person communicating the information. The misinformation effect refers to the tendency of people to recall misleading or false information introduced to them following an event instead of original material learned or observed at the time the event occurred. During user–analyst communication in the IRD process, analysts may introduce misinformation in their discussions with users. We use the misinformation effect literature to hypothesize that in such circumstances users are likely to recall misinformation introduced by analysts rather than their true beliefs and knowledge of facts. Additionally, we use literature in social psychology to hypothesize that the misinformation effect will be stronger when misinformation is introduced using a social technique rather than a nonsocial technique. We conducted an experiment to test the misinformation effect in the requirements elicitation process. Results indicated that (1) introduction of misinformation reduces the accuracy of requirements provided by users, and (2) social techniques (interviews) are more vulnerable to the misinformation effect than nonsocial techniques (surveys). Our research contributes to the information systems literature by identifying an important reason that requirements provided by users may be inaccurate, and to IRD practice by identifying important dilemmas caused by the misinformation effect as well as potential solutions. We also contribute to the psychology literature by demonstrating the existence of the misinformation effect with users’ experiential factual knowledge and beliefs in a business context, and by aiding in understanding the underlying causes of the misinformation effect. We discuss implications of our findings and directions for future research to address challenges resulting from the misinformation effect.
Current reward structures in security vulnerability disclosure may be skewed toward benefitting nefarious usage of vulnerability information rather than responsible disclosure. Recently suggested market-based mechanisms offer incentives to responsible security researchers for discovering and reporting vulnerabilities. However, concerns exist that any benefits gained through increased incentives for responsible discovery may be lost through information leakage. Using perspectives drawn from the diffusion of innovations literature, we examine the effectiveness of market-based vulnerability disclosure mechanisms. Empirical examination of two years of security alert data finds that market-based disclosure restricts the diffusion of vulnerability exploitations, reduces the risk of exploitation, and decreases the volume of exploitation attempts.
Given that adoption of a new system often implies fully or partly replacing an incumbent system, resistance is often manifested as failure of a user to switch from an incumbent technology to a newly introduced one. Thus, a potential source of resistance to adopting a new system lies in the use of an incumbent system. Using the status quo bias and habit literatures as theoretical lenses, the study explains how use of an incumbent system negatively impacts new system perceptions and usage intentions. We argue that habitual use of an incumbent system, rationalization due to perceived transition costs, and psychological commitment due to perceived sunk costs all encourage development of inertia. Inertia in turn fully mediates the impact of these incumbent system constructs on constructs related to acceptance of the new system via psychological commitment based on cognitive consistency and by increasing the importance of normative pressures. Specifically, we hypothesize that inertia leads to decreased perceptions of the ease of use and relative advantage of a newly introduced system and has a negative impact on intentions to use the new system, above and beyond its impact through perceptions. Finally, we hypothesize that inertia moderates the relationship between subjective norm and intention, such that normative pressures to use a new system become more important in the presence of inertia. Empirical results largely support the hypothesized relationships showing the inhibiting effect of incumbent-system habit, transition and sunk costs, and inertia on acceptance of a new system. Our study thus extends theoretical understanding of the role of incumbent system constructs such as habit and inertia in technology acceptance, and lays the foundations for further study of the interplay between perceptions and cognition with respect to the incumbent system and those with respect to a new system.
In this paper, we argue that any effort to understand the state of the Information Systems field has to view IS research as a series of normative choices and value judgments about the ends of research. To assist a systematic questioning of the various ends of IS research, we propose a pragmatic framework that explores the choices IS researchers make around theories and methodologies, ethical methods of conduct, desirable outcomes, and the long-term impact of the research beyond a single site and topic area. We illustrate our framework by considering and questioning the explicit and implicit choices of topics, design and execution, and the representation of knowledge in experimental research—research often considered to be largely beyond value judgments and power relations. We conclude with the implications of our pragmatic framework by proposing practical questions for all IS researchers to consider in making choices about relevant topics, design and execution, and representation of findings in their research.
It has been conjectured that the peer-based recommendations associated with electronic commerce lead to a redistribution of demand from popular products or “blockbusters” to less popular or “niche” products, and that electronic markets will therefore be characterized by a “long tail” of demand and revenue. We test this conjecture using the revenue distributions of books in over 200 distinct categories on Amazon.com and detailed daily snapshots of co-purchase recommendation networks in which the products of these categories are situated. We measure how much a product is influenced by its position in this hyperlinked network of recommendations using a variant of Google’s PageRank measure of centrality. We then associate the average influence of the network on each category with the inequality in the distribution of its demand and revenue, quantifying this inequality using the Gini coefficient derived from the category’s Lorenz curve. We establish that categories whose products are influenced more by the recommendation network have significantly flatter demand and revenue distributions, even after controlling for variation in average category demand, category size, and price differentials. Our empirical findings indicate that doubling the average network influence on a category is associated with an average increase of about 50 percent in the relative revenue for the least popular 20 percent of products, and with an average reduction of about 15 percent in the relative revenue for the most popular 20 percent of products. We also show that this effect is enhanced by higher assortative mixing and lower clustering in the network, and is greater in categories whose products are more evenly influenced by recommendations. The direction of these results persists over time, across both demand and revenue distributions, and across both daily and weekly demand aggregations. Our work illustrates how the microscopic economic data revealed by online networks can be used to define and answer new kinds of research questions, offers a fresh perspective on the influence of networked IT artifacts on business outcomes, and provides novel empirical evidence about the impact of visible recommendations on the long tail of electronic commerce.
In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm’s participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm’s entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater “free-riding” benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.
It has been argued that platform technology owners cocreate business value with other firms in their platform ecosystems by encouraging complementary invention and exploiting indirect network effects. In this study, we examine whether participation in an ecosystem partnership improves the business performance of small independent software vendors (ISVs) in the enterprise software industry and how appropriability mechanisms influence the benefits of partnership. By analyzing the partnering activities and performance indicators of a sample of 1,210 small ISVs over the period 1996–2004, we find that joining a major platform owner’s platform ecosystem is associated with an increase in sales and a greater likelihood of issuing an initial public offering (IPO). In addition, we show that these impacts are greater when ISVs have greater intellectual property rights or stronger downstream capabilities. This research highlights the value of interoperability between software products, and stresses that value cocreation and appropriation are not mutually exclusive strategies in interfirm collaboration..
This study seeks to identify the means by which information technology helps cocreate relational value in the context of interfirm relationships in the logistics industry—a large and information-intensive industry. We identify a set of IT functionalities—single-location shipping, multilocation shipping, supply chain visibility, and financial settlement—that can be used to manage the flows of physical goods, information, and finances across locations in interfirm logistics processes. Progressively more advanced sets of IT functionalities, when implemented and used in the interfirm relationship to execute logistics processes, are proposed to form four distinct IT capability profiles of increased sophistication. Interfirm IT capability profiles of higher sophistication are proposed to help cocreate greater relational value by facilitating the flows of physical goods, information, and finances across locations in the interfirm logistics process. Besides their direct role in helping cocreate relational value, these interfirm IT capability profiles are proposed to further enhance relational value cocreation when complemented by interfirm communications for business development and IT development.Our empirical study was situated in one of the world’s largest logistics suppliers and over 2,000 of its interfirm relationships with buyers across industries. Integrated data from four archival sources on the IT functionalities implemented and used in interfirm logistics relationships, interfirm communications, relational value (share of wallet and loyalty), and multiple control variables were collected. The results show that the proposed interfirm IT capability profiles and interfirm communications have both a direct and an interaction effect on relational value. Implications for cocreating relational value in interfirm relationships with the aid of IT are discussed.
In this article, the association between the strength of information technology controls over management information systems and the subsequent forecasting ability of the information produced by those systems is investigated. The Sarbanes–Oxley Act of 2002 highlights the importance of information system controls by requiring management and auditors to report on the effectiveness of internal controls over the financial reporting component of the firm’s management information systems. We hypothesize and find evidence that management forecasts are less accurate for firms with information technology material weaknesses in their financial reporting system than the forecasts for firms that do not have information technology material weaknesses. In addition, we examine three dimensions of information technology material weaknesses: data processing integrity, system access and security, and system structure and usage. We find that the association with forecast accuracy appears to be strongest for IT control weaknesses most directly related to data processing integrity. Our results support the contention that information technology controls, as a part of the management information system, affect the quality of the information produced by the system. We discuss the complementary nature of our findings to the information and systems quality literature.
TThis paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences—namely, age, gender, and experience—are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.
Contemporary business organizations are increasingly turning their attention to jointly creating value with a variety of stakeholders, such as individual customers and other business organizations. However, a review of the literature reveals that very few studies have systematically examined value cocreation within business-to-business (B2B) contexts. Using a revelatory case study of the relationship between an ERP vendor with a global reputation and its partners, and informed by the resource-based view of the firm and related theoretical perspectives, we develop an understanding of value cocreation in B2B alliances associated with selling, extending, and implementing packaged software, specifically ERP systems. Our study reveals that there are different mechanisms underlying value cocreation within B2B alliances, and also points to several categories of contingency factors that influence these mechanisms. In addition to providing insights about the phenomenon of cocreation itself, the study contributes to the stream of packaged software literature, where the implications of value cocreation in alliances between packaged software vendors and their partners for the client organizations have not been sufficiently explored.
Absorptive capacity is a firm’s ability to identify, assimilate, transform, and apply valuable external knowledge. It is considered an imperative for business success. Modern information technologies perform a critical role in the development and maintenance of a firm’s absorptive capacity. We provide an assessment of absorptive capacity in the information systems literature. IS scholars have used the absorptive capacity construct in diverse and often contradictory ways. Confusion surrounds how absorptive capacity should be conceptualized, its appropriate level of analysis, and how it can be measured. Our aim in reviewing this construct is to reduce such confusion by improving our understanding of absorptive capacity and guiding its effective use in IS research. We trace the evolution of the absorptive capacity construct in the broader organizational literature and pay special attention to its conceptualization, assumptions, and relationship to organizational learning. Following this, we investigate how absorptive capacity has been conceptualized, measured, and used in IS research. We also examine how absorptive capacity fits into distinct IS themes and facilitates understanding of various IS phenomena. Based on our analysis, we provide a framework through which IS researchers can more fully leverage the rich aspects of absorptive capacity when investigating the role of information technology in organizations.
Examining action research publications in leading Information Systems journals as a particular genre of research communication, we develop the notion of style composition to understand how authors structure their arguments for a research contribution. We define style composition as the activity through which authors select, emphasize, and present elements of their research to establish premises, develop inferences, and present contributions in publications. Drawing on this general notion, we identify a set of styles that is characteristic of how IS action researchers compose their argument. Premise styles relate to the dual goals of action research through practical or theoretical positioning of the argument; inference styles combine insights from the problem-solving and the research cycles through inductive or deductive reasoning; and contribution styles focus on different types of contributions—experience report, field study, theoretical development, problem-solving method, and research method. Based on the considered sample, we analyze the styles adopted in selected publications and show that authors have favored certain styles while leaving others underexplored; further, we reveal important strengths and weaknesses in the composition of styles within the IS discipline. Based on these insights, we discuss how action research practices and writing can be improved, as well as how to further develop style compositions to support the publication of engaged scholarship research.
The information technology project control literature has documented that clan control is often essential in complex multistakeholder projects for project success. However, instituting clan control in such conditions is challenging as people come to a project with diverse skills and backgrounds. There is often insufficient time for clan control to develop naturally. This paper investigates the question, “How can clan control be enacted in complex IT projects?” Recognizing social capital as a resource, we conceptualize a clan as a group with strong social capital (i.e., where its members have developed their structural, cognitive, and relational ties to the point that they share common values and beliefs and are committed to a set of peer norms). We theorize that the enactment of clan control is a dual process of (1) building the clan by developing its social capital dimensions (structural, cognitive, and relational ties) or reappropriating social capital from elsewhere and (2) leveraging the clan by reinforcing project-facilitating shared values, beliefs, and norms, and inhibiting those that impede the achievement of project goals. We explore how clan control was enacted in a large IT project at a major logistics organization in which clan control was quickly instituted to avoid an impending project failure. Our research contributes to theory in three ways: (1) we reconcile the two differing views of clan control into a single framework, (2) we explain the role of controllers in enacting clan control, and (3) we clarify how formal control can be employed to develop clan control.
E-commerce is growing to represent an increasing share of overall sales revenue, and online sales are expected to continue growing for the foreseeable future. This growth translates into increased activity on the supporting infrastructure, leading to a corresponding need to scale the infrastructure. This is difficult in an era of shrinking budgets and increasing functional requirements. Increasingly, IT managers are turning to virtualized cloud providers, drawn by the pay-for-use business model. As cloud computing becomes more popular, it is important for data center managers to accomplish more with fewer dollars (i.e., to increase the utilization of existing resources). Advanced request distribution techniques can help ensure both high utilization and smart request distribution, where requests are sent to the service resources best able to handle them. While such request distribution techniques have been applied to the web and application layers of the traditional online application architecture, request distribution techniques for the data layer have focused primarily on online transaction processing scenarios. However, online applications often have a significant read-intensive workload, where read operations constitute a significant percentage of workloads (up to 95 percent or higher). In this paper, we propose a cost-based database request distribution (C-DBRD) strategy, a policy to distribute requests, across a cluster of commercial, off-the-shelf databases, and discuss its implementation. We first develop the intuition behind our approach, and describe a high-level architecture for database request distribution. We then develop a theoretical model for database load computation, which we use to design a method for database request distribution and build a software implementation. Finally, following a design science methodology, we evaluate our artifacts through experimental evaluation. Our experiments, in the lab and in production-scale systems, show significant improvement of database layer resource utilization, demonstrating up to a 45 percent improvement over existing request distribution techniques.
Firms invest in a variety of information technologies and seek to align their IT asset portfolios with two key performance outcomes: efficiency and innovation. Existing research makes the universalistic assumption that both outcomes will always be realized through firms’ IT asset portfolios. There has been limited research on the conditions under which firms’ IT asset portfolios should be oriented more toward efficiency or innovation. Here, we argue that the nature of the industry where a firm competes will have a significant moderating effect on the link between firms’ IT asset portfolios and efficiency or innovation outcomes. Using panel data that covers a wide range of industry environments, we find that at lower levels of dynamism, munificence, and complexity, IT asset portfolios are associated with a greater increase in efficiency. In contrast, in environments with higher levels of complexity, IT asset portfolios are associated with a greater increase in innovation (i.e., development of new products and processes, and exploration of growth opportunities). These results provide insights about how firms could realize strategic alignment by tailoring their IT asset portfolios toward an efficiency or innovation focus.
Long waits online undermine users’ evaluations of Web sites and their providers, triggering abandonment behaviors. Yet e-business researchers and practitioners have not perfected mechanisms to respond to online wait issues. A filler interface that runs during the wait for search results may influence online users’ perceived waiting time (PWT); however, no scientific investigation has attempted to design effective filler interfaces for managing online waits. By adopting resource allocation theory, cognitive absorption theory, and human computer interaction (HCI) theories (competition for attention, visual search, and motion effect), we design diverse filler interfaces and investigate their effects on antecedents of PWT. The proposed research model considers cognitive absorption factors such as temporal dissociation, focused immersion, and heightened enjoyment as antecedents of PWT, which in turn triggers three outcomes: affective appraisals, cognitive appraisals, and Web site use intention. A multistage, multimethod approach is used to test the research hypotheses. In the first stage, we compare a filler interface condition with a no-filler interface condition, and find the superiority of a filler interface with respect to inducing focused immersion and temporal dissociation. In the second stage, we conduct two controlled experiments to examine whether filler interfaces with various designs (varying the presence and relevance of image, text, and image motion) distinctly influence antecedents of PWT and confirm their distinctive effects on focused immersion, temporal dissociation, and heightened enjoyment. In addition, by conducting a structural equation modeling analysis, we find that our research model explains 51 percent, 51 percent, 44 percent, and 45 percent of the variance in PWT, affective appraisals, cognitive appraisals, and Web site use intention respectively. Theoretical and practical implications of these findings are provided.
Open source software (OSS) is a social and economic phenomenon that raises fundamental questions about the motivations of contributors to information systems development. Some developers are unpaid volunteers who seek to solve their own technical problems, while others create OSS as part of their employment contract. For the past 10 years, a substantial amount of academic work has theorized about and empirically examined developer motivations. We review this work and suggest considering motivation in terms of the values of the social practice in which developers participate. Based on the social philosophy of Alasdair MacIntyre, we construct a theoretical framework that expands our assumptions about individual motivation to include the idea of a long-term, value-informed quest beyond short-term rewards. This motivation–practice framework depicts how the social practice and its supporting institutions mediate between individual motivation and outcome. The framework contains three theoretical conjectures that seek to explain how collectively elaborated standards of excellence prompt developers to produce high-quality software, change institutions, and sustain OSS development. From the framework, we derive six concrete propositions and suggest a new research agenda on motivation in OSS.
Because changes in organizations and information technology environments are enduring, the alignment of the IT function with business objectives must not only be understood, but constantly renewed and adjusted. This is amply reflected in recent surveys of CIOs, which consistently suggest that the notion of alignment is a top challenge and management priority. Many CIOs face a double challenge when addressing the issue of alignment: they must first clarify top management’s expectations and assumptions about IT, which may be contradictory, and then understand their implications for how the IT department should be managed (i.e., translate the function’s strategic mission into an IT management model that adds value to the organization).The characterization of the IT function has constituted a central and growing subject of research in the information systems field. Although the extant literature has much to teach us, knowledge in this area is nevertheless fragmented and has not been properly integrated. In response to these limitations, this study proposes and tests a new theory of the contribution of the IT function. Specifically, our objective is to offer an explanation of the contribution of the IT function in organizations with a typology of ideal profiles.A field study was conducted in 24 large Canadian companies in order to validate a set of research propositions. Our results first suggest that there are five distinct “ideal” IT management profiles in organizations and each of these profiles tends to focus on specific sources of value. Next, we observed that IT functions that are close to the ideal of any given profile seem to be outperforming those with hybrid profiles. Finally, our findings provide a compelling explanation as to how ideal IT management profiles are adopted in organizations. The article concludes with a discussion of the theoretical and practical implications of the proposed theory.
This paper examines the objective career histories, mobility patterns, and career success of 500 individuals, drawn from the National Longitudinal Survey of Youth (NLSY79), who had worked in the information technology workforce. Sequence analysis of career histories shows that careers of the IT workforce are more diverse than the traditional view of a dual IT career path (technical versus managerial). This study reveals a new career typology comprising three broad, distinct paths: IT careers; professional labor market (PLM) careers; and secondary labor market (SLM) careers. Of the 500 individuals in the IT workforce, 173 individuals pursued IT careers while the remaining 327 individuals left IT for other high-status non-IT professional jobs in PLM or lower-status, non-IT jobs in SLM careers. Findings of this study contribute to refining the concept of “boundaryless” careers. By tracing the diverse trajectories of career mobility, we enrich our understanding of how individuals construct boundaryless careers that span not only organizational but also occupational boundaries. Career success did not differ in terms of average pay for individuals in IT and PLM careers. By contrast, individuals in SLM careers attained the lowest pay. We conclude this study with implications for future research and for the management of IT professionals’ careers.
The peer review process that has been in place for many years has recognized shortcomings. The Internet provides a means for changing this process. This paper offers a more transparent and inclusive design for peer review referred to as open knowledge creation. The design proposed utilizes Google knol and group services. The open knowledge creation design consists of four stages: creation, review/revision, evaluation/adoption, and publication. It is intended to offer existing or new journals an alternative to the traditional peer review of research.
Post-adoptive system use is often characterized by cycles of adaptation, in which people actively revise how they use information systems. This paper investigates how and why individual users revise their system use at the feature level. A new concept, adaptive system use (ASU), is conceptualized as a user’s revisions of which and how system features are used. This research identifies four specific ASU behaviors that collectively describe how people revise their use of system features. A model of ASU is developed based on Louis and Sutton’s (1991) research on how people switch to active thinking from automatic thinking. The model specifies three antecedents of ASU (novel situations, discrepancies, and deliberate initiatives) and two moderators (personal innovativeness in IT and facilitating conditions). An empirical study of 253 Microsoft Office users largely supported the research model. The findings suggest that triggers―including novel situations, discrepancies, and deliberate initiatives―are a significant impetus to ASU. This research also confirms moderating effects of personal innovativeness in IT. The findings also show the relationships among triggers: in addition to their direct impact on ASU, novel situations and deliberate initiatives exert their influence on ASU indirectly by giving rise to discrepancies in system use. Moreover, a cluster analysis identifies three heterogeneous triggering conditions and reveals that people engage in different ASU behaviors under different triggering conditions.
Online markets pose a difficulty for evaluating products, particularly experience goods, such as used cars, that cannot be easily described online. This exacerbates product uncertainty, the buyer’s difficulty in evaluating product characteristics, and predicting how a product will perform in the future. However, the IS literature has focused on seller uncertainty and ignored product uncertainty. To address this void, this study conceptualizes product uncertainty and examines its effects and antecedents in online markets for used cars (eBay Motors).Extending the information asymmetry literature from the seller to the product, we first theorize the nature and dimensions (description and performance) of product uncertainty. Second, we propose product uncertainty to be distinct from, yet shaped by, seller uncertainty. Third, we conjecture product uncertainty to negatively affect price premiums in online markets beyond seller uncertainty. Fourth, based on the information signaling literature, we describe how information signals (diagnostic product descriptions and third-party product assurances) reduce product uncertainty.The structural model is validated by a unique dataset comprised of secondary transaction data from used cars on eBay Motors matched with primary data from 331 buyers who bid on these used cars. The results distinguish between product and seller uncertainty, show that product uncertainty has a stronger effect on price premiums than seller uncertainty, and identify the most influential information signals that reduce product uncertainty.The study’s implications for the emerging role of product uncertainty in online markets are discussed.
In this paper, the interacting effect of formal contracts and relational governance on vendor profitability and quality in the software outsourcing industry are examined. We focus on a critical manifestation of relational governance—the presence of relational flexibility in the exchange relationship—and argue that the enacted observation of relational flexibility is driven by perceptions of exchange hazards. In a departure from extant literature, however, we propose that the benefits accruing from it are asymmetric and depend on how the exchange risks are apportioned by the formal contract. Formally, we hypothesize that relational flexibility provides greater benefits to an exchange partner that faces the greater proportion of risk in a project, induced through the contract. In addition, we hypothesize that these benefits manifest on the performance dimensions that are of importance to the risk-exposed partner. We test our hypotheses on 105 software projects completed by a software outsourcing vendor for multiple clients. The results show that relational flexibility positively affects profitability in only fixed price contracts, where the vendor faces greater risk, while positively affecting quality only in time and materials contracts, where the client is at greater risk. We thus provide evidence for the asymmetric benefits from relational governance, thereby arguing for a more contingent and limited view of the value of relational governance, based on risk-exposure, rather than the more expansive view prevalent in the literature contending that relational governance provides benefits for all parties to an exchange. We conclude with a discussion of the research and managerial implications of our findings.
Taking a control theory view of software process innovation, we tested prevalent beliefs regarding software process maturity and Information Systems employee attitudes and perceptions by surveying 736 IS professionals in 10 organizations at varying levels of the CMM (capability maturity model). Although anecdotal reports and the scant empirical studies to date suggest job attitudes and perceptions are more positive for employees in organizations at higher levels of software process maturity, we found evidence of a more complex picture. While our data supported expectations that role conflict and perceived work overload were lower for IS professionals in organizations at a level of maturity where software process behavioral controls are implemented, other results were not fully in line with prevalent beliefs. Most notably, IS workers reported significantly lower professional efficacy and lower job satisfaction in organizations at CMM Level 3, where behavioral controls are the dominant form of formal control, than in organizations at Level 1, which is relatively free of formal controls. Some anticipated positive attitudes and perceptions surfaced in organizations at the highest rungs of software process maturity (CMM Levels 4/5), where the established behavioral controls are supplemented by substantial outcome controls, as IS professionals reported lower role ambiguity and higher job satisfaction than did their counterparts in organizations at CMM Level 3.
In the Foreword to an MIS Quarterly Special Issue on PLS, the senior editors for the special issue noted that they rejected a number of papers because the authors attempted comparisons between results from PLS, multiple regression, and structural equation modeling (Marcoulides et al. 2009). They raised several issues they argued had to be taken into account to have legitimate comparison studies, supporting their position primarily by citing three authors: Dijkstra (1983), McDonald(1996), and Schneeweiss (1993). As researchers interested in conducting comparison studies, we read the Foreword carefully, but found it did not provide clear guidance on how to conduct “legitimate” comparisons. Nor did our reading of Dijksta, McDonald, and Schneeweiss raise any red flags about dangers in this kind of comparison research. We were concerned that instead of helping researchers to successfully engage in comparison research, the Foreword might end up discouraging that type of work, and might even be used incorrectly to reject legitimate comparison studies. This Issues and Opinions piece addresses the question of why one might conduct comparison studies, and gives an overview of the process of comparison research with a focus on what is required to make those comparisons legitimate. In addition, we explicitly address the issues raised by Marcoulides et al., to explore where they might (or might not) come into play when conducting or evaluating this type of study.
In this paper, we adopt the lens of absorptive capacity (ACAP), defined by two dimensions—the knowledge base (consisting of knowledge diversity, depth, and linkages) and routines (consisting of sensing and experimentation)—to explain how a software firm’s knowledge endowments influence its level of radical information technology innovation during a technological breakthrough. We distinguish three types of IT innovations—base, processes, and service innovation—that form an innovation ecology. We posit that (1) ACAP is a relational construct where the impact of the knowledge base is mediated by routines; (2) IT innovations are either externally adopted or internally generated; and (3) knowledge antecedents associated with different types of innovations differ. We hypothesize a three-step, mediated path (knowledge base à sensing à experimentation à innovation) for external innovation adoption, and a two-step path (knowledge diversity/depth à experimentation à innovation) for internal innovation creation to explain the software firm’s level of radical innovation across three IT innovation types. We validate the model through a cross-sector study that examined how 121 small software firms innovated with Internet computing. We confirm the mediated nature of ACAP for external base innovations, which are driven by all three knowledge-based factors as follows: (1) knowledge depth (direct positive effect); (2) knowledge diversity (mediated three-step path), (3) knowledge linkages (mediated three step path). Process innovations are externally driven by a three-step mediated path for knowledge linkages, as well as being directly affected by knowledge diversity, but negatively and directly impeded by knowledge depth. Service innovations are not driven by any mediated influence of ACAP, but driven directly by knowledge diversity. At the same time, both service and process innovations are strongly influenced by prior IT innovations: base and/or service. Several directions for future studies of radical IT innovation are proposed.
Canonical action research (CAR) aims to address real-world problems and improve organizational performance by combining scholarly observations with practical interventions. However, efforts to conduct CAR have revealed challenges that reflect a significant research–practice gap. We examine these challenges by revisiting the process, principles, and criteria of CAR developed earlier. The specific roles of two different types of theory in the cyclical action research process are considered. A project undertaken in two public relations firms illustrates how our methodological revision improves the rigor and quality of CAR. This article contributes both a significantly enhanced action research method, with detailed guidelines and suggestions that emphasize the roles of focal and instrumental theories, and an emerging theory of knowledge sharing that incorporates key elements of Chinese management and culture.
Recent work, in journals such as MIS Quarterly and Management Science, has highlighted the importance of evaluating the influence of common method bias (CMB) on the results of statistical analysis. In this research note, we assess the utility of the unmeasured latent method construct (ULMC) approach in partial least squares (PLS), introduced by Liang et al. (2007). Such an assessment of the ULMC approach is important, because it has been employed in 76 studies since it appeared in MIS Quarterly in early 2007. Using data generated via Monte Carlo simulations, we use PLS structural equation modeling (SEM) to demonstrate that the ULMC approach of Liang et al. is neither able to detect, nor control for, common method bias. Method estimates using this approach resulted in negligible estimates, regardless of whether there were some, large, or no method bias introduced in the simulated data. Our study contributes to the IS and research methods literature by illustrating that, and explaining why the ULMC approach does not accurately detect common method bias in PLS. Further, our results build on prior work done using covariance-based SEM questioning the usefulness of the ULMC technique for detecting CMB.
Allen and March provide a critique of one of our papers in which we argue composites should be represented as entities/objects in a conceptual model rather than relationships/associations (Shanks et al. 2008). They contend we have addressed a non-issue. Furthermore, they argue our theoretical rationale and empirical evidence have flaws. In this paper, we provide a response to their arguments. We show that the issue we address is substantive. We show, also, that our theoretical analysis and empirical results are robust. We find, instead, that Allen and March’s theoretical arguments and empirical evidence have flaws.
Empirical research is an important methodology for the study of conceptual modeling practices. The recently published article “Representing Part–Whole Relations in Conceptual Modeling: An Empirical Evaluation” (Shanks et al. 2008) uses the lens of ontology to study a relatively sophisticated aspect of conceptual modeling practice, the representation of aggregation and composition. It contends that some analysts argue that a composite should be represented as a relationship while others argue that a composite should be represented as an entity. We find no evidence of such a dispute in the data modeling literature. We observe that composites are objects. By definition, all object-types should be represented as entities. Therefore, using the relationship construct to represent composites should not be seen as a viable alternative. Additionally, we found significant conceptual and methodological issues within the study that call its conclusions into question. As a way to offer insight into the requisite methodological procedures for research in this area, we conducted two experiments that both explicate and address the issues raised. Our results call into question the utility of using ontology as a foundation for conceptual modeling practice. Furthermore, they suggest a contrary but at least equally plausible explanation for the results reported by Shanks et al. In conducting this work we hope to encourage dialogue that will be beneficial for future endeavors aimed at identifying, developing, and evaluating appropriate foundations for the discipline of conceptual modeling.
Critical realism is emerging as a viable philosophical paradigm for conducting social science research, and has been proposed as an alternative to the more prevalent paradigms of positivism and interpretivism. Few papers, however, have offered clear guidance for applying this philosophy to actual research methodologies. Under critical realism, a causal explanation for a given phenomenon is inferred by explicitly identifying the means by which structural entities and contextual conditions interact to generate a given set of events. Consistent with this view of causality, we propose a set of methodological principles for conducting and evaluating critical realism-based explanatory case study research within the information systems field. The principles are derived directly from the ontological and epistemological assumptions of critical realism. We demonstrate the utility of each of the principles through examples drawn from existing critical realist case studies. The article concludes by discussing the implications of critical realism based research for IS research and practice.
This research essay outlines a set of guidelines for conducting functional Magnetic Resonance Imaging (fMRI) studies in social science research in general and also, accordingly, in Information Systems research. Given the increased interest in using neuroimaging tools across the social sciences, this study aims at specifying the key steps needed to conduct an fMRI study while ensuring that enough detail is provided to evaluate the methods and results. The outline of an fMRI study consists of four key steps: (1) formulating the research question, (2) designing the fMRI protocol, (3) analyzing fMRI data, and (4) interpreting and reporting fMRI results. These steps are described with an illustrative example of a published fMRI study on trust and distrust in this journal (Dimoka 2010). The paper contributes to the methodological literature by (1) providing a set of guidelines for designing and conducting fMRI studies, (2) specifying methodological details that should be included in fMRI studies in academic venues, and (3) illustrating these practices with an exemplar fMRI study. Future directions for conducting high-quality fMRI studies in the social sciences are discussed.
Both theoretical and empirical evidence suggest that, in many markets with standards competition, network effects make the strong grow stronger and can “tip” the market toward a single, winner-take-all standard. We hypothesize, however, that low cost digital conversion technologies, which facilitate easy compatibility across competing standards, may reduce the strength of these network effects. We empirically test our hypotheses in the context of the digital flash memory card market.We first test for the presence of network effects in this market and find that network effects, as measured here, are associated with a significant positive price premium for leading flash memory card formats. We then find that the availability of digital converters reduces the price premium of the leading flash card formats and reduces the overall concentration in the flash memory market. Thus, our results suggest that, in the presence of low cost conversion technologies and digital content, the probability of market dominance can be lessened to the point where multiple, otherwise incompatible, standards are viable.Our conclusion that the presence of converters weakens network effects implies that producers of non-dominant digital goods standards benefit from the provision of conversion technology. Our analysis thus aids managers seeking to understand the impact of converters on market outcomes, and contributes to the existing literature on network effects by providing new insights into how conversion technologies can affect pricing strategies in these increasingly important digital settings.
This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.
Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants’ visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group’s activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.
There is a pervasive belief in the MIS research community that PLS has advantages over other techniques when analyzing small sample sizes or data with non-normal distributions. Based on these beliefs, major MIS journals have published studies using PLS with sample sizes that would be deemed unacceptably small if used with other statistical techniques. We used Monte Carlo simulation more extensively than previous research to evaluate PLS, multiple regression, and LISREL in terms of accuracy and statistical power under varying conditions of sample size, normality of the data, number of indicators per construct, reliability of the indicators, and complexity of the research model. We found that PLS performed as effectively as the other techniques in detecting actual paths, and not falsely detecting non-existent paths. However, because PLS (like regression) apparently does not compensate for measurement error, PLS and regression were consistently less accurate than LISREL. When used with small sample sizes, PLS, like the other techniques, suffers from increased standard deviations, decreased statistical power,and reduced accuracy. All three techniques were remarkably robust against moderate departures from normality, and equally so. In total, we found that the similarities in results across the three techniques were much stronger than the differences.
Tsang and Williams offer some good and provocative ideas in their critique of our earlier article on generalizing and generalizability. In this essay we will advance some new ideas by building on those collected in both Tsang and Williams and our original article (Lee and Baskerville 2003). Because IS is a pluralist scientific discipline, one in which both qualitative and quantitative (and both interpretive and positivist) research approaches are valued, “generalize” is unlikely to be a viable term or concept if only one IS research paradigm may lay claim to it and excludes others from using it. Both papers agree on this point, but approach the problem differently. Where we originally generalized generalizability by offering new language, Tsang and Williams conceptualize generalizability by framing it more closely to its older, more statistically oriented form. We agree about the importance of induction and about the classification or taxonomy of different types of induction. We build further in this essay, advancing the ethical questions raised by generalization: A formulation of judgment calls that need to be made when generalizing a theory to a new setting. We further demonstrate how the process of generalizing may actually proceed, based on the common ground between Tsang and Williams and our original article.This article is a response to "Generalization and Induction: Misconceptions, Clarifications, and a Classification of Induction," Eric W. K. Tsang and John N. Williams (forthcoming).
In “Generalizing Generalizability in Information Systems Research,” Lee and Baskerville (2003) try to clarify generalization and classify it into four types. Unfortunately, their account is problematic. We propose repairs. Central among these is our balance-of-evidence argument that we should adopt the view that Hume’s problem of induction has a solution, even if we do not know what it is. We build upon this by proposing an alternative classification of induction. There are five types of generalization: (1) theoretical, (2) within-population, (3) cross-population, (4) contextual, and (5) temporal, with theoretical generalization being across the empirical and theoretical levels and the rest within the empirical level. Our classification also includes two kinds of inductive reasoning that do not belong to the domain of generalization. We then discuss the implications of our classification for information systems research.
User resistance has long been acknowledged as a critical issue during information technology implementation. Resistance can be functional when it signals the existence of problems with the IT or with its effects; it can be dysfunctional when it leads to organizational disruption. Notwithstanding the nature of resistance, the implementers—business managers, functional managers, or IT professionals—have to address it. Although the literature recognizes the importance of user resistance, it has paid little attention to implementers’ responses—and their effect—when resistance occurs. Our study focuses on this phenomenon, and addresses two questions: What are implementers’ responses to user resistance? What are the effects of these responses on user resistance? To answer these questions, we conducted a case survey, which combines the richness of case studies with the benefits of analyzing large quantities of data. Our case database includes 89 cases with a total of 137 episodes of resistance. In response to our first research question, we propose a taxonomy that includes four categories of implementers’ responses to user resistance: inaction, acknowledgment, rectification, and dissuasion. To answer our second question, we adopted a set-theoretic analysis approach, which we enriched with content analysis of the cases. Based on these analyses, we offer a theoretical explanation of how implementers’ responses may affect the antecedents that earlier research found to be associated with user resistance behaviors.
The increasing popularity of Web 2.0 has led to exponential growth of user-generated content in both volume and significance. One important type of user-generated content is the blog. Blogs encompass useful information (e.g., insightful product reviews and information-rich consumer communities) that could potentially be a gold mine for business intelligence, bringing great opportunities for both academic research and business applications. However, performing business intelligence on blogs is quite challenging because of the vast amount of information and the lack of commonly adopted methodology for effectively collecting and analyzing such information. In this paper, we propose a framework for gathering business intelligence from blogs by automatically collecting and analyzing blog contents and bloggers’ interaction networks. Through a system developed using the framework, we conducted two case studies with one case focusing on a consumer product and the other on a company. Our case studies demonstrate how to use the framework and appropriate techniques to effectively collect, extract, and analyze blogs related to the topics of interest, reveal novel patterns in the blogger interactions and communities, and answer important business intelligence questions in the domains. The framework is sufficiently generic and can be applied to any topics of interest, organizations, and products. Future academic research and business applications related to the topics examined in the two cases can also be built using the findings of this study.
In unpredictable and unforgiving environments, organizations need to act with care and reliability, often referred to as collective mindfulness. We present a theory-generating, interpretative field study of a highly complex and successful building project by architect Frank O. Gehry. We argue that what has been labeled collective mindfulness is only possible through a dialectic process of collective minding, in which organizational actors simultaneously exhibit elements of being mindful and mindless. Our analysis reveals that collective minding emerges from struggling with contradictions in the five elements of mindfulness. We argue that when actors struggle with these dialectic tensions, the same information technology capabilities are enacted as multiple, contradictory technologies-in-practice. Implications for the further study of collective minding and the appropriation of IT capabilities are discussed.
Drawing from the social and relational perspectives, this study offers an innovative conceptualization and operational approach regarding the validation of self-reported customer demographic data, which has become an essential corporate asset for harnessing business intelligence. Specifically, based on social network and homophily paradigms in which individuals have a natural tendency to associate and interact frequently with others with similar characteristics, we constructed a relational inference model to determine the accuracy of self-administered consumer profiles. In addition, to further enhance the reliability of our model’s prediction capability, we employed the entropy mechanism that minimizes potential biases that may arise from a simple probabilistic approach. To empirically validate the accuracy of our inference framework, we obtained and analyzed over 20 million actual call transactions supplied by one of the largest global telecommunication service providers. The results suggest that our social network-based inference model consistently outperforms other competing mechanisms (e.g., weighted average and simple relational classifier) regardless of the criteria choice (e.g., number of call receivers, call duration, and call frequency), with an accuracy rate of approximately 93 percent. Finally, to confirm the generalizability of our findings, we conducted simulation experiments to validate the robustness of the results in response to variations in parameter values and increases in potential noise in the data. We discuss several implications related to business intelligence for both research and practice, and offer new directions for future studies.
Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies. In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.
Firms are increasingly sourcing internal information systems functions from external service providers. However, there is limited empirical evidence of the economic impact of this delivery option and, more specifically, of the productivity gains accruing to firms that have outsourced. Moreover, there is little evidence of the role and contributions of the individual mechanisms by which service providers create value for client firms. We are particularly interested in whether client firms benefit from the accumulated knowledge held by information technology (IT) service firms. In this paper, we examine the impact of IT outsourcing on the productivity of firms that choose this mode of services delivery focusing, on the role of IT-related knowledge. Since firms self-select into their optimal sourcing mode, we use a variety of econometric techniques including propensity score-based matching and switching regression to control for potential bias arising from endogenously determined sourcing modes. We demonstrate that IT outsourcing does lead to productivity gains for firms that select this mode of service delivery. Our results also suggest that IT-related knowledge held by IT services vendors enables these productivity gains, the magnitude of which is moderated by a firm’s IT intensity. Moreover, the value of outsourcing to a client firm increases with its propensity for outsourcing, which in turn depends on firm-specific attributes including efficiency level, financial leverage, and variability in business conditions. Our analyses also show that firms that outsource have been able to achieve additional productivity gains from contracting out compared with their counterfactuals.
Managed security service provider (MSSP) networks are a form of collaboration where several firms share resources such as diagnostics, prevention tools, and policies to provide security for their computer networks. While the decision to outsource the security operations of an organization may seem counterintuitive, there are potential benefits from joining an MSSP network that include pooling of risk and access to more security-enabling resources and expertise. We examine structural results explaining the reasons firms join an MSSP network, and characterize the growth of MSSP network size under different forms of ownership (monopoly versus consortium). We find that the need for an initial investment in MSSP networks (which is necessary to overcome the stalling effect) only affects the optimal network size for a consortium but has no impact on the optimal network size for a profit-maximizing monopolist. Our results provide an explanation why the majority of the MSSPs are for-profit entities and consortium-based MSSPs are less common. Such a market structure can be attributed to the potential for larger size by the for-profit MSSP owner combined with beneficial pricing structure and a lack of growth uncertainty for the early clients.
Managers make informed information technology investment decisions when they are able to quantify how IT contributes to firm performance. While financial accounting measures inform IT’s influence on retrospective firm performance, senior managers expect evidence of how IT influences prospective measures such as the firm’s market value. We examine the efficacy of IT’s influence on firm value combined with measures of financial performance for non-publicly traded (NPT) hospitals that lack conventional market-based measures. We gathered actual sale transactions for 146 NPT hospitals in the United States to derive the q ratio, a measure of market value. Our findings indicate that the influence of IT investment on the firm is more pronounced and statistically significant on firm value than exclusively on the accounting performance measures. Specifically, we find that the impact of IT investment is not significant on return on assets (ROA) and operating income for the same set of hospitals. This research note contributes to research and practice by demonstrating that the overall impact of IT is better understood when accounting measures are complemented with the firm’s market value. Such market valuation is also critical in merger and acquisition decisions, an activity that is likely to accelerate in the healthcare industry. Our findings provide hospitals, as well as other NPT firms, with insights into the impact of IT investment and a pragmatic approach to demonstrating IT’s contribution to firm value.
Electronic commerce has grown rapidly in recent years. However, surveys of online customers continue to indicate that many remain unsatisfied with their online purchase experiences. Clearly, more research is needed to better understand what affects customers’ evaluations of their online experiences. Through a large dataset gathered from two online websites, this study investigates the importance of product uncertainty and retailer visibility in customers’ online purchase decisions, as well as the mitigating effects of retailer characteristics. We find that high product uncertainty and low retailer visibility have a negative impact on customer satisfaction. However, a retailer’s service quality, website design, and pricing play important roles in mitigating the negative impact of high product uncertainty and low retailer visibility. Specifically, service quality can mitigate the negative impacts of low retailer visibility and high product uncertainty in online markets. Website design, on the other hand, helps to reduce the impact of product uncertainty when experience goods are involved.
In this paper, we present a method to make personalized recommendations when user preferences change over time. Most of the works in the recommender systems literature have been developed under the assumption that user preference has a static pattern. However, this is a strong assumption especially when the user is observed over a long period of time. With the help of a data set on employees’ blog reading behavior, we show that users’ product selection behaviors change over time. We propose a hidden Markov model to correctly interpret the users’ product selection behaviors and make personalized recommendations. The user preference is modeled as a hidden Markov sequence. A variable number of product selections of different types by each user in each time period requires a novel observation model. We propose a negative binomial mixture of multinomial to model such observations. This allows us to identify stable global preferences of users and to track individual users through these preferences. We evaluate our model using three real-world data sets with different characteristics. They include data on employee blog reading behavior inside a firm, users’ movie rating behavior at Netflix, and users’ music listening behavior collected through last.fm. We compare the recommendation performance of the proposed model with that of a number of collaborative filtering algorithms and a recently proposed temporal link prediction algorithm. We find that the proposed HMM-based collaborative filter performs as well as the best among the alternative algorithms when the data is sparse or static. However, it outperforms the existing algorithms when the data is less sparse and the user preference is changing. We further examine the performances of the algorithms using simulated data with different characteristics and highlight the scenarios where it is beneficial to use a dynamic model to generate product recommendation.
Retailers are increasingly exploiting sequential online auctions as an effective and low cost distribution channel for disposing large quantities of inventory. In such auction environments, bidders have the opportunity of participating in many auctions to learn and choose the bidding strategy that best fits their preferences. Previous studies have mostly focused on identifying bidding strategies in single, isolated online auctions. Using a large data set collected from sequential online auctions, we first characterize bidding strategies in this interesting online environment and then develop an empirical model to explain bidders’ adoption of different strategies. We also examine how bidders change their strategies over time. Our findings challenge the general belief that bidders employ their strategies regardless of experience or their specific demand. We find that bidders’ demand, participation experience, and auction design parameters affect their choice of bidding strategies. Bidders with unit demand are likely to choose early bidding strategies, while those with multiple unit demand adopt late bidding strategies. Auction design parameters that affect bidders’ perception of demand and supply trends affect bidders’ choice of bidding strategies. As bidders gain experience within a sequence of auctions, they start choosing late bidding strategies. Our findings help auctioneers to design auction sequences that maximize their objectives.
In the wake of the 2008 financial tsunami, existing methods and tools for managing financial risk have been criticized for weaknesses in monitoring and alleviating risks at the systemic level. A 2009 article in Nature suggested new approaches to modeling economic meltdowns are needed to prevent future financial crises. However, existing studies have not focused on analysis of systemic risk at the individual bank level in a banking network, which is essential for monitoring and mitigating contagious bank failures. To this end, we develop a network approach to risk management (NARM) for modeling and analyzing systemic risk in banking systems. NARM views banks as a network linked through financial relationships. It incorporates network and financial principles into a business intelligence (BI) algorithm to analyze systemic risk attributed to each individual bank via simulations based on real-world data from the Federal Deposit Insurance Corporation. Our research demonstrates the feasibility of modeling and analyzing systemic risk at the individual bank level in a banking network using a BI-based approach. In terms of business impact, NARM offers a new means for predicting contagious bank failures and determining capital injection priorities in the wake of financial crises. Our simulation study shows that under significant market shocks, the interbank payment relationship becomes more influential than the correlated bank portfolio relationship in determining an individual bank’s survival. These insights should help financial regulators devise more effective policies and mechanisms to prevent the collapse of a banking system. Further, NARM and the simulation procedure driven by real-world data proposed in this study have instructional value to similar research areas such as bank stress testing, where time series data and business networks may be studied.
An increasing number of organizations are now implementing customer relationship management (CRM) systems to support front-line employees’ service tasks. With the belief that CRM can enhance employees’ service quality, management often mandates employees to use the implemented CRM. However, challenges emerge if/when employees are dissatisfied with using the system. To understand the role of front-line employee users’ satisfaction with their mandated use of CRM in determining their service quality, we conducted a field study in one of the largest telecommunications service organizations in China and gathered time-lagged data from self-reported employee surveys, as well as from the firm’s archival data sources. Our results suggest that employees’ overall user satisfaction (UserSat) with their mandated use of CRM has a positive impact on employee service quality (ESQ) above and beyond the expected positive impacts that job dedication (JD) and embodied service knowledge (ESK) have on ESQ. Interestingly, the positive effect of UserSat on ESQ is comparable to the positive effects of JD and ESK, respectively, on ESQ. Importantly, UserSat and ESK have a substitutive effect on ESQ, suggesting that the impact of UserSat on ESQ is stronger/weaker for employees with lower/higher levels of ESK. Finally, ESQ predicts customer satisfaction with customer service employees (CSWCSE); ESQ also fully mediates the impacts of UserSat and ESK, and partially mediates the impact of JD, on CSWCSE. The results of this study emphasize the importance of user satisfaction in determining employees’ task outcomes when use of an information system is mandated.
Despite recent interest in studying information system habits, our understanding of how these habits develop and operate in an organizational context is still limited. Within organizations, IS habits may develop over long periods of time and are typically embedded within larger, frequently practiced, higher-level work routines or task sequences. When new systems are introduced for the purpose of at least partially replacing incumbent systems, existing IS habits embedded in these routines may inhibit adoption and use of the new systems. Therefore, understanding how work-related IS habits form, how they enable and inhibit behavior, and how they can be disrupted or encouraged requires that we examine them within the context of organizational and individual level work routines. The current study integrates psychology and organizational behavior literature on cognitive scripts and work routines to examine IS habits as they occur embedded within larger, more complex task sequences. The objective of the paper is to contribute to the IS habit literature by (1) situating IS habits within the context of their associated work routines and task sequences, and (2) providing a theoretical understanding of how incumbent system habits can be disrupted, and how development of new system habits can be encouraged, within this context. We draw from extant research in psychology, organizational behavior, and consumer behavior to offer propositions about context-focused organizational interventions to break, or otherwise discourage, the continued performance of incumbent system habits and to encourage the development of new system habits. Specifically, our theoretical development includes script disruption techniques, training-in-context, and performance goal suspension as organizational interventions that disrupt incumbent system habits. We further theorize how stabilizing contextual variables associated with modified work routines can facilitate the development of new system habits. The paper concludes by discussing the importance of combining intervention strategies to successfully disrupt incumbent system habits and encourage development of new system habits and thus facilitate adoption of new systems.
Affect is a critical factor in human decisions and behaviors within many social contexts. In the information and communication technology (ICT) context, a growing number of studies consider the affective dimension of human interaction with ICTs. However, few of these studies take systematic approaches, resulting in inconsistent conclusions and contradictory advice for researchers and practitioners. Many of these issues stem from ambiguous conceptualizations of various affective concepts and their relationships. Before researchers can address questions such as “what causes affective responses in an ICT context” and “what impacts do affective responses have on human interaction with ICTs,” a theoretical foundation for affective concepts and their relationships has to be established.This theory and review paper addresses three research questions: (1) What are pertinent affective concepts in the ICT context? (2) In what ways are these affective concepts similar to, or different from each other? (3) How do these affective concepts relate to or influence one another? Based on theoretical reasoning and empirical evidence, the affective response model (ARM) is developed. ARM is a theoretically bound conceptual framework that provides a systematic and holistic reference map for any ICT study that considers affect. It includes a taxonomy that classifies affective concepts along five dimensions: the residing, the temporal, the particular/general stimulus, the object/behavior stimulus, and the process/outcome dimensions. ARM also provides a nomological network to indicate the causal or co-occurring relationships among the various types of affective concepts in an ICT interaction episode. ARM has the power for explaining and predicting, as well as prescribing, potential future research directions.
Recent academic investigations of computer security policy violations have largely focused on non-malicious noncompliance due to poor training, low employee motivation, weak affective commitment, or individual oversight. Established theoretical foundations applied to this domain have related to protection motivation, deterrence, planned behavior, self-efficacy, individual adoption factors, organizational commitment, and other individual cognitive factors. But another class of violation demands greater research emphasis: the intentional commission of computer security policy violation, or insider computer abuse. Whether motivated by greed, disgruntlement, or other psychological processes, this act has the greatest potential for loss and damage to the employer. We argue the focus must include not only the act and its immediate antecedents of intention (to commit computer abuse) and deterrence (of the crime), but also phenomena which temporally precede these areas. Specifically, we assert the need to consider the thought processes of the potential offender and how these are influenced by the organizational context, prior to deterrence. We believe the interplay between thought processes and this context may significantly impact the efficacy of IS security controls, specifically deterrence safeguards. Through this focus, we extend the Straub and Welke (1998) security action cycle framework and propose three areas worthy of empirical investigation—techniques of neutralization (rationalization), expressive/instrumental criminal motivations, and disgruntlement as a result of perceptions of organizational injustice—and propose questions for future research in these areas.
In this paper we propose rhetoric as a valuable yet underdeveloped alternative paradigm for examining IT diffusion. Building on recent developments of computerization movements theory, our rhetorical approach proposes that two central elements of the theory, framing and ideology, rather than being treated as separate can be usefully integrated. We suggest that IT diffusion can be usefully explored through examining the interrelationship of the deep structures underlying ideology and the type and sequence of rhetorical claims underpinning actors’ framing strategies. Our theoretical developments also allow us to better understand competing discourses influencing the diffusion process. These discourses reflect the ideologies and shape the framing strategies of actors in the broader field context. We illuminate our theoretical approach by drawing on the history of the diffusion of free and open source software.
Post-analyses of major extreme events reveal that information sharing is critical for effective emergency response. The lack of consistent data standards for current emergency management practice, however, hinders efficient critical information flow among incident responders. In this paper, we adopt a third-generation activity theory guided approach to develop a data model that can be used in the response to fire-related extreme events. This data model prescribes the core data standards to reduce information interoperability barriers. The model is validated through a three-step approach including a request for comment (RFC) process, case application, and prototype system test. This study contributes to the literature in the area of interoperability and data modeling; it also informs practice in emergency response system design.10-26-2012
Because digital games are fun, engaging, and popular, organizations are attempting to integrate them within organizational activities as serious components, with the anticipation that they can improve employees’ motivation and performance. But in order to do so and to obtain the intended outcomes, it is necessary to first obtain an understanding of how different digital game designs impact players’ behaviors and emotional responses. Hence, in this study, we address one key element of popular game designs: competition. Using extant research on tournaments and intrinsic motivation, we model competitive games as a skill-based tournament and conduct an experimental study to understand player behaviors and emotional responses under different competition conditions. When players compete with players of similar skill levels, they apply more effort as indicated by more games played and longer duration of play. But when players compete with players of lower skill levels, they report higher levels of enjoyment and lower levels of arousal after game-playing. We discuss the implications for organizations seeking to introduce games premised on competition and provide a framework to guide information system researchers to embark on a study of games.
Despite extensive deliberations in contemporary literature, the design of citizen-centric e-government websites remains an unresolved theoretical and pragmatic conundrum. Operationalizing e-government service quality to investigate and improve the design of e-government websites has been a much sought-after objective. Yet, there is a lack of actionable guidance on how to develop e-government websites that exhibit high levels of service quality. Drawing from marketing literature, we undertake a goal approach to this problem by delineating e-government service quality into aspects of IT-mediated service content and service delivery. Whereas service content describes the functions available on an e-government website that assist citizens in completing their transactional goals, service delivery defines the manner by which these functions are made accessible via the web interface as a delivery channel. We construct and empirically test a research model that depicts a comprehensive collection of web-enabled service content functions and delivery dimensions desirable by citizens. Empirical findings from an online survey of 647 respondents attest to the value of distinguishing between service content functions and delivery dimensions in designing e-government websites. Both service content and delivery are found to be significant contributors to achieving e-government service quality. These IT-mediated service content functions and delivery dimensions represent core areas of e-government website design where the application of technology makes a difference, especially when considered in tandem with the type of transactional activity. A split sample analysis of the data further demonstrates our model’s robustness when applied to e-government transactions of varying frequency.
Theory suggests that coworkers may influence individuals’ technology use behaviors, but there is limited research in the technology diffusion literature that explicates how such social influence processes operate after initial adoption. We investigate how two key social influence mechanisms (identification and internalization) may explain the growth over time in individuals’ use of knowledge management systems (KMS)—a technology that because of its publicly visible use provides a rich context for investigating social influence. We test our hypotheses using longitudinal KMS usage data on over 80,000 employees of a management consulting firm. Our approach infers the presence of identification and internalization from associations between actual system use behaviors by a focal individual and prior system use by a range of reference groups. Evidence of these kinds of associations between system use behaviors helps construct a more complete picture of social influence mechanisms, and is to our knowledge novel to the technology diffusion literature. Our results confirm the utility of this approach for understanding social influence effects and reveal a fine-grained pattern of influence across different social groups: we found strong support for bottom-up social influence across hierarchical levels, limited support for peer-level influence within levels, and no support for top-down influence.
Internet privacy concerns (IPC) is an area of study that is receiving increased attention due to the huge amount of personal information being gathered, stored, transmitted, and published on the Internet. While there is an emerging literature on IPC, there is limited agreement about its conceptualization in terms of its key dimensions and its factor structure. Based on the multidimensional developmental theory and a review of the prior literature, we identify alternative conceptualizations of IPC. We examine the various conceptualizations of IPC with four online surveys involving nearly 4,000 Internet users. As a baseline, study 1 compares the integrated conceptualization of IPC to two existing conceptualizations in the literature. While the results provide support for the integrated conceptualization, the second-order factor model does not outperform the correlated first-order factor model. Study 2 replicates the study on a different sample and confirms the results of study 1. We also investigate whether the prior results are affected by the different perspectives adopted in the wording of items in the original instruments. In study 3, we find that focusing on one’s concern for website behavior (rather than one’s expectation of website behavior) and adopting a consistent perspective in the wording of the items help to improve the validity of the factor structure. We then examine the hypothesized third-order conceptualizations of IPC through a number of alternative higher-order models. The empirical results confirm that, in general, the third-order conceptualizations of IPC outperform their lower-order alternatives. In addition, the conceptualization of IPC that has the best fit with the data contains a third-order general IPC factor, two second-order factors of interaction management and information management, and six first-order factors (i.e., collection, secondary usage, errors, improper access, control, and awareness). Study 4 cross-validates the results with another data set and examines IPC within the context of a nomological network. The results confirm that the third-order conceptualization of IPC has nomological validity, and it is a significant determinant of both trusting beliefs and risk beliefs. Our research helps to resolve inconsistencies in the key underlying dimensions of IPC, the factor structure of IPC, and the wording of the original items in prior instruments of IPC. Finally, we discuss the implications of this research.
Does information technology outsourcing reduce non-IT operating costs? This study examines this question and also asks whether internal IT investments moderate the relationship between IT outsourcing and non-IT operating costs. Using a panel data set of approximately 300 U.S. firms from 1999 to 2003, we find that IT outsourcing has a significant negative association with firms’ non-IT operating costs. However, this finding does not imply that firms should completely outsource their entire IT function. Our results suggest that firms benefit more in terms of reduction in non-IT operating costs when they also have higher levels of complementary investments in internal IT, especially IT labor. Investments in internal IT systems can make business processes more amenable to outsourcing, and complementary investments in internal IT staff can facilitate monitoring of vendor performance and coordination with vendors. We discuss the implications of these findings for further research and for practice.
With China emerging as a new frontier of global IT outsourcing, many Chinese IT service suppliers are actively expanding in three major markets: Asia, especially Japan, the West, especially the United States, and the Chinese domestic market. Compared to multinational suppliers and established Indian suppliers, Chinese IT service firms are at a relatively early, but rapidly growing stage, which offers a unique opportunity to explore an understudied topic in the information systems literature: internationalization strategies of IT service suppliers from emerging economies. Through a three-part qualitative case study of 13 China-based IT service firms, including almost all of the Chinese suppliers recognized globally, this study elaborates the internationalization behavior and decision rationale of these suppliers. The findings show that these major Chinese suppliers include both firms that incrementally internationalize and firms that are “born global.” For both types of firms, the entry and growth in different markets is a highly dynamic activity combining a strategically planned, resource-seeking process and a flexible, opportunistic bricolage process based on existing operation capabilities and client relationships. The suppliers dynamically oscillate between these processes to exploit and create opportunities while expanding in multiple markets.10/29/12
Information Systems research has studied how buyers and suppliers can benefit from improved information visibility in supply chains characterized by uncertainty. However, the relation-specific information processing solutions that provide visibility can only be exploited if the two firms engage in sufficient coordination efforts. This work takes a nuanced look at how dyadic benefits are derived in the supply chain. Drawing on the information processing view, resource-based view, and transaction cost theory, this study explicates how buyer performance can result from buyer’s use of relation-specific information processing solutions and supplier’s relational responses. Two interfirm information processing solutions are proposed and examined: the use of IT-based systems for planning and control, and the use of relational (normative) contracts. Based on a sample of 144 manufacturing firms, eight of the nine proposed research hypotheses receive empirical support using PLS analysis. The findings suggest that as buyers and suppliers utilize the IT and relational solutions, they induce relation-specific responses represented as supplier’s business process investments and modification flexibility, which in turn lead to positive buyer outcomes. The results help us gain a more granular understanding on how relation-specific interfirm information processing solutions can lead to performance through enhanced interfirm governance capabilities.
Mixed methods research is an approach that combines quantitative and qualitative research methods in the same research inquiry. Such work can help develop rich insights into various phenomena of interest that cannot be fully understood using only a quantitative or a qualitative method. Notwithstanding the benefits and repeated calls for such work, there is a dearth of mixed methods research in information systems. Building on the literature on recent methodological advances in mixed methods research, we develop a set of guidelines for conducting mixed methods research in IS. We particularly elaborate on three important aspects of conducting mixed methods research: (1) appropriateness of a mixed methods approach; (2) development of meta-inferences (i.e., substantive theory) from mixed methods research; and (3) assessment of the quality of meta-inferences (i.e., validation of mixed methods research). The applicability of these guidelines is illustrated using two published IS papers that used mixed methods.
Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions—in which bidders can bid on combinations of goods—have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidders can submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free-flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism.10-05-12
Within changing value networks, the profits and competitive advantages of participation reside dynamically at control points that are the positions of greatest value and/or power. The enterprises that hold these positions have a great deal of control over how the network operates, how the benefits are redistributed, and how this influences the execution of a digital business strategy. This article is based on a field study that provides preliminary, yet promising, empirical evidence that sheds light on the dynamic cycle of value creation and value capture points in digitally enabled networks in response to triggers related to technology and business strategy. The context used is that of the European and U.S. broadcasting industry. Specifically, the paper illustrates how incremental innovations may shift value networks from static, vertically integrated networks to more loosely coupled networks, and how cross-boundary industry disruptions may then, in turn, shift those to two-sided markets. Based on the analysis, insights and implications for digital business strategy research and practice are then provided.
Adapting user interfaces to a user’s cultural background can increase satisfaction, revenue, and market share. Conventional approaches to catering for culture are restricted to adaptations for specific countries and modify only a limited number of interface components, such as the language or date and time formats. We argue that a more comprehensive personalization of interfaces to cultural background is needed to appeal to users in expanding markets. This paper introduces a low-cost, yet efficient method to achieve this goal: cultural adaptivity. Culturally adaptive interfaces are able to adapt their look and feel to suit visual preferences. In a design science approach, we have developed a number of artifacts that support cultural adaptivity, including a prototype web application. We evaluate the efficacy of the prototype’s automatically generated interfaces by comparing them with the preferred interfaces of 105 Rwandan, Swiss, Thai, and multicultural users. The findings demonstrate the feasibility of providing users with interfaces that correspond to their cultural preferences in a novel yet effective manner.NOTE: This is an open access article under the terms of the Creative Commons Attribution CC BY License. To download the article, go to the "Open Access" section of the "Online Supplements" page.This paper won the first-place award as the European Research Paper of the Year for 2014.
With the growing recognition of the customer’s role in service creation and delivery, there is an increased impetus on building customer-centric organizations. Digital technologies play a key role in such organizations. Prior research studying digital business strategies has largely focused on building production-side competencies and there has been little focus on customer-side digital business strategies to leverage these technologies. We propose a theory to understand the effectiveness of a customer-side digital business strategy focused on localized dynamics—here, a firm’s customer service units (CSUs). Specifically, we use a capabilities perspective to propose digital design as an antecedent to two customer service capabilities—namely, customer orientation capability and customer response capability—across a firm’s CSUs. These two capabilities will help a firm to locally sense and respond to customer needs, respectively. Information quality from the digital design of the CSU is proposed as the antecedent to the two capabilities. Proposed capability-building dynamics are tested using data collected from multiple respondents across 170 branches of a large bank. Findings suggest that the impacts of information quality in capability-building are contingent on the local process characteristics. We offer implications for a firm’s customer-side digital business strategy and present new areas for future examination of such strategies.
As information technology becomes integral to the products and services in a growing range of industries, there has been a corresponding surge of interest in understanding how firms can effectively formulate and execute digital business strategies. This fusion of IT within the business environment gives rise to a strategic tension between investing in digital artifacts for long-term value creation and exploiting them for short-term value appropriation. Further, relentless innovation and competitive pressures dictate that firms continually adapt these artifacts to changing market and technological conditions, but sustained profitability requires scalable architectures that can serve a large customer base and stable interfaces that support integration across a diverse ecosystem of complementary offerings. The study of digital business strategy needs new concepts and methods to examine how these forces are managed in pursuit of competitive advantage. We conceptualize the logic of digital business strategy in terms of two constructs: design capital (i.e., the cumulative stock of designs owned or controlled by a firm) and design moves (i.e., the discrete strategic actions that enlarge, reduce, or modify a firm’s stock of designs). We also identify two salient dimensions of design capital, namely, option value and technical debt. Using embedded case studies of four firms, we develop a rich conceptual model and testable propositions to lay out a design-based logic of digital business strategy. This logic highlights the interplay between design moves and design capital in the context of digital business strategy and contributes to a growing body of insights that link the design of digital artifacts to competitive strategy and firm-level performance.
Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications.
Digital artifacts are embedded in wider and constantly shifting ecosystems such that they become increasingly editable, interactive, reprogrammable, and distributable. This state of flux and constant transfiguration renders the value and utility of these artifacts contingent on shifting webs of functional relations with other artifacts across specific contexts and organizations. By the same token, it apportions control over the development and use of these artifacts over a range of dispersed stakeholders and makes their management a complex technical and social undertaking. These ideas are illustrated with reference to (1) provenance and authenticity of digital documents within the overall context of archiving and social memory and (2) the content dynamics occasioned by the findability of content mediated by Internet search engines. We conclude that the steady change and transfiguration of digital artifacts signal a shift of epochal dimensions that calls for rethinking some of the inherited wisdom in IS research and practice.
This section is a collection of shorter “Issue and Opinions” pieces that address some of the critical challenges around the evolution of digital business strategy. These voices and visions are from thought leaders who, in addition to their scholarship, have a keen sense of practice. They outline through their opinion pieces a series of issues that will need attention from both research and practice. These issues have been identified through their observation of practice with the eye of a scholar. They provide fertile opportunities for scholars in information systems, strategic management, and organizational theory. "Leadership in a Digital World: Embracing Transparency and Adaptive Capacity"Warren Bennis (pp. 635-636)"Transparency Strategy: Competing with Information in a Digital World"Nelson Granados and Alok Gupta (pp. 637-641)"Value Architectures for Digital Business: Beyond the Business Model"Peter Keen and Ronald Williams (pp. 642-647)"Commoditized Digital Processes and Business Community Platforms: New Opportunities and Challenges for Digital Business Strategies"M. Lynne Markus and Claudia Loebbecke (pp. 649-653)"Revealing Your Hand: Caveats in Implementing Digital Business Strategy"Varun Grover and Rajiv Kohli (pp. 655-662)
In this paper, we examine how the competitive industry environment shapes the way that digital strategic posture (defined as a focal firm’s degree of engagement in a particular class of digital business practices relative to the industry norm) influences firms’ realized digital business strategy. We focus on two forms of digital strategy: general IT investment and IT outsourcing investment. Drawing from prior literature on determinants of IT activity and competitive dynamics, we argue that three elements of the industry environment determine whether digital strategic posture has an increasingly convergent or divergent influence on digital business strategy. By divergent influence, we mean an influence that leads to spending substantially more or less on a particular strategic activity than industry norms. We predict that a digital strategic posture (difference from the industry mean) has an increasingly divergent effect on digital business strategy under higher industry turbulence, while having an increasingly convergent effect on digital business strategy under higher industry concentration and higher industry growth. The study uses archival data for 400 U.S.-based firms from 1999 to 2006. Our findings imply that digital business strategy is not solely a matter of optimizing firm operations internally or of responding to one or two focal competitors, but also arises strikingly from awareness and responsiveness to the digital business competitive environment. Collectively, the findings provide insights on how strategic posture and industry environment influence firms’ digital business strategy.
Information technology matters to business success because it directly affects the mechanisms through which they create and capture value to earn a profit: IT is thus integral to a firm’s business-level strategy. Much of the extant research on the IT/strategy relationship, however, inaccurately frames IT as only a functional-level strategy. This widespread under-appreciation of the business-level role of IT indicates a need for substantial retheorizing of its role in strategy and its complex and interdependent relationship with the mechanisms through which firms generate profit. Using a comprehensive framework of potential profit mechanisms, we argue that while IT activities remain integral to the functional-level strategies of the firm, they also play several significant roles in business strategy, with substantial performance implications. IT affects industry structure and the set of business-level strategic alternatives and value-creation opportunities that a firm may pursue. Along with complementary organizational changes, IT both enhances the firm’s current (ordinary) capabilities and enables new (dynamic) capabilities, including the flexibility to focus on rapidly changing opportunities or to abandon losing initiatives while salvaging substantial asset value. Such digitally attributable capabilities also determine how much of this value, once created, can be captured by the firm—and how much will be dissipated through competition or through the power of value chain partners, the governance of which itself depends on IT. We explore these business-level strategic roles of IT and discuss several provocative implications and future research directions in the converging information systems and strategy domains.
As one of the most commonly experienced problems on the Internet, download delay is a significant impediment to the success of e-commerce websites. While some research has examined how such delays can be reduced and how much delay online users will tolerate, little research has taken a theoretically grounded approach to managing perceptions of the wait. Based on time perception theories, we develop a research model of the effects of actual wait time, amount of information, and direction of attention on perceptions of the wait. Two empirical studies were conducted using an experimental travel website to test the proposed hypotheses. The results show that with shorter waits, providing additional visual content, such as a travel picture, may make the wait feel longer. However, with longer waits, additional visual content that distracts the user from the passage of time makes the wait feel shorter and reduces users’ negative affect toward the wait. Further, the benefits of providing visual content in longer waits are enhanced as more content is provided. Visual content should also be chosen to distract the user from time and temporal processing, as reminding users of the passage of time can encourage temporal processing and make the wait feel longer, especially in longer waits or when the amount of temporal visual content is high. Our findings extend time perception theories and contribute to the literature by identifying a potential paradigm shift, from the retrospective to the prospective paradigm, when waiting times are prolonged. Post hoc study results confirm the practical contribution of our research, demonstrating that several key findings are counter-intuitive to professional web designers.
The content industry has been undergoing a tremendous transformation in the last two decades. We focus in this paper on recent changes in the form of social computing. Although the content industry has implemented social computing to a large extent, it has done so from a techno-centric approach in which social features are viewed as complementary rather than integral to content. This approach does not capitalize on users’ social behavior in the website and does not answer the content industry’s need to elicit payment from consumers. We suggest that both of these objectives can be achieved by acknowledging the fusion between content and community, making the social experience central to the content website’s digital business strategy. We use data from Last.fm, a site offering both music consumption and online community features. The basic use of Last.fm is free, and premium services are provided for a fixed monthly subscription fee. Although the premium services on Last.fm are aimed primarily at improving the content consumption experience, we find that willingness to pay for premium services is strongly associated with the level of community participation of the user. Drawing from the literature on levels of participation in online communities, we show that consumers’ willingness to pay increases as they climb the so-called “ladder of participation” on the website. Moreover, we find that willingness to pay is more strongly linked to community participation than to the volume of content consumption. We control for self-selection bias by using propensity score matching. We extend our results by estimating a hazard model to study the effect of community activity on the time between joining the website and the subscription decision. Our results suggest that firms whose digital business models remain viable in a world of “freemium” will be those that take a strategic rather than techno-centric view of social media, that integrate social media into the consumption and purchase experience rather than use it merely as a substitute for offline soft marketing. We provide new evidence of the importance of fusing social computing with content delivery and, in the process, lay a foundation for a broader strategic path for the digital content industry in an age of growing user participation.
Information technology has arguably been one of the most important drivers of economic and social value in the last 50 years, enabling transformational change in virtually every aspect of society. Although the Information Systems community is engaged in significant research on IT, the reach of our findings may be limited. In this commentary, our objective is to focus the IS community’s attention on the striking transformations in economic and social systems spawned by IT and to encourage more research that offers useful implications for policy. We present examples of transformations occurring in four distinct sectors of the economy and propose policy-relevant questions that need to be addressed. We urge researchers to write papers based on their findings that inform policy makers, managers, and decision makers about the issues that transformational technologies raise. Finally, we suggest a new outlet to publish these essays on the implications of transformational informational technology.
Recent extreme events show that Twitter, a micro-blogging service, is emerging as the dominant social reporting tool to spread information on social crises. It is elevating the online public community to the status of first responders who can collectively cope with social crises. However, at the same time, many warnings have been raised about the reliability of community intelligence obtained through social reporting by the amateur online community. Using rumor theory, this paper studies citizen-driven information processing through Twitter services using data from three social crises: the Mumbai terrorist attacks in 2008, the Toyota recall in 2010, and the Seattle café shooting incident in 2012. We approach social crises as communal efforts for community intelligence gathering and collective information processing to cope with and adapt to uncertain external situations. We explore two issues: (1) collective social reporting as an information processing mechanism to address crisis problems and gather community intelligence, and (2) the degeneration of social reporting into collective rumor mills. Our analysis reveals that information with no clear source provided was the most important, personal involvement next in importance, and anxiety the least yet still important rumor causing factor on Twitter under social crisis situations.
In this study, we explore the Wiki affordance of enabling shaping behavior within organizational intranets supported by Wikis. Shaping is the continuous revision of one’s own and others’ contributions to a Wiki. Shaping promotes knowledge reuse through improved knowledge integration. Recognizing and clarifying the role of shaping allows us to theorize new ways in which knowledge resources affect knowledge reuse. We examine the role of three knowledge resources of a Wiki contributor: knowledge depth, knowledge breadth, and assessment of the level of development of the Wiki community’s transactive memory system. We offer preliminary evidence based on a sample of experienced organizational Wiki users that the three different knowledge resources have differential effects on shaping, that these effects differ from the effects on the more common user behavior of simply adding domain knowledge to a Wiki, and that shaping and adding each independently affect contributors’ perceptions that their knowledge in the Wiki has been reused for organizational improvement. By empirically distinguishing between the different knowledge antecedents and consequences of shaping and adding, we derive implications for theory and research on knowledge integration and reuse.10-12-2012
Convincing arguments for using critical realism as an underpinning for theories of IT-associated organizational change have appeared in the Information Systems literature. A central task in developing such theories is to uncover the generative mechanisms by which IT is implicated in organizational change processes, but to do so, we must explain how critical realism’s concept of generative mechanisms applies in an IS context. Similarly, convincing arguments have been made for using Gibson’s (1986) affordance theory from ecological psychology for developing theories of IT-associated organizational change, but this effort has been hampered due to insufficient attention to the ontological status of affordances. In this paper, we argue that affordances are the generative mechanisms we need to specify and explain how affordances are a specific type of generative mechanism. We use the core principles of critical realism to argue how affordances arise in the real domain from the relation between the complex assemblages of organizations and of IT artifacts, how affordances are actualized over time by organizational actors, and how these actualizations lead to the various effects we observe in the empirical domain. After presenting these arguments, we reanalyze two published cases in the literature, those of ACRO and Autoworks, to illustrate how affordance-based theories informed by critical realism enhance our ability to explain IT-associated organizational change. These examples show how researchers using this approach should proceed, and how managers can use these ideas to diagnose and address IT implementation problems.
In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within “activity systems” and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the “inscription” of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.04/05/13
Universal fast broadband is currently being implemented by the Australian government. It is the largest single project in Australia’s history. Represented as a nation-building exercise by the government and many public and private promoters, it is vilified by others as a massive waste of taxpayers’ money. Ultimately the target of successful universal availability will require that metropolitan installations subsidize rural adoption. The take-up of these facilities, particularly in regional and remote areas, constitutes a complex, multi-factorial scenario in which political, personal, and organizational decisions are shaped by physical, cultural, economic, and ideological elements. Critical realism is proposed here as an aid for examining the complex reality of rural adoption for communities and small businesses in the regions. This article highlights the importance of considering individual reflexivity in explaining the adoption decision and potential adoption barriers.
Large, multi-unit organizations are continually challenged to balance demands for centralization of information technology that lead to cost and service efficiencies through standardization while providing flexibility at the local unit level in order to meet unique business, customer, and service needs. This has led many organizations to adopt hybrid federated information technology governance (ITG) structures to find this balance. This approach to ITG establishes demand for various means to coordinate effectively across the organization to achieve the desired benefits. Past research has focused on the efficacy of various coordination mechanisms (e.g., steering committees, task forces) to coordinate activities related to information technology. However, we lack insights as to how and why these various coordination approaches help organizations achieve desired coordinated outcomes. This research specifically identifies coordinating as a process. Adopting the philosophy of critical realism, we conducted a longitudinal, comparative case study of two coordinating efforts in a federated ITG structure. Through a multifaceted approach to scientific logic employing deductive, inductive, and retroductive elements, we explicate two causal mechanisms, consensus making and unit aligning, which help to explain the coordinating process and the coordination outcomes observed in these efforts. We additionally elaborate the operation of the mechanisms through the typology of macro–micro–macro influences. Further, we demonstrate the value of the causal mechanisms to understanding the coordinating process by highlighting the complementarity in insights relative to the theories of power and politics and of rational choice. The study contributes to our understanding of coordinating as a process and of governance in federated IT organizations. Importantly, our study illustrates the value of applying critical realism to develop causal explanations and generate insights about a phenomenon.02/04/13
The current literature on digital infrastructure offers powerful lenses for conceptualizing the increasingly interconnected information system collectives found in contemporary organizations. However, little attention has been paid to the generative mechanisms of digital infrastructure, that is, the causal powers that explain how and why such infrastructure evolves over time. This is unfortunate, since more knowledge about what drives digital infrastructures would be highly valuable for managers and IT professionals confronted by the complexity of managing them. To this end, this paper adopts a critical realist view for developing a configurational perspective of infrastructure evolution. Our theorizing draws on a multimethod research design comprising an in-depth case study and a case survey. The in-depth case study, conducted at a Scandinavian airline, distinguishes three key mechanisms of digital infrastructure evolution: adoption, innovation, and scaling. The case survey research of 41 cases of digital infrastructure then identifies and analyzes causal paths through which configurations of these mechanisms lead to successful evolution outcomes. The study reported in this paper contributes to the infrastructure literature in two ways. First, we identify three generative mechanisms of digital infrastructure and how they contingently lead to evolution outcomes. Second, we use these mechanisms as a basis for developing a configurational perspective that advances current knowledge about why some digital infrastructures evolve successfully while others do not. In addition, the paper demonstrates and discusses the efficacy of critical realism as a philosophical tradition for developing substantive contributions in the field of information systems.12/03/12
Building on recent developments in mixed methods, we discuss the methodological implications of critical realism and explore how these can guide dynamic mixed-methods research design in information systems. Specifically, we examine the core ontological assumptions of CR in order to gain some perspective on key epistemological issues such as causation and validity, and illustrate how these shape our logic of inference in the research process through what is known as retroduction. We demonstrate the value of a CR-led mixed-methods research approach by drawing on a study that examines the impact of ICT adoption in the financial services sector. In doing so, we provide insight into the interplay between qualitative and quantitative methods and the particular value of applying mixed methods guided by CR methodological principles. Our positioning of demi-regularities within the process of retroduction contributes a distinctive development in this regard. We argue that such a research design enables us to better address issues of validity and the development of more robust meta-inferences.
The production of performance data in organizations is often described as a functional process that managers enforce on their employees to provide leaders with accurate information about employees’ work and their achievements. This study draws on a 15-month ethnography of a desk sales unit to build a dramaturgical model that explains how managers participate in the production of performance data to impress rather than inform leaders. Research on management information systems is reviewed to outline a protective specification of this model where managers participate in the production of performance data to suppress information that threatens the image they present to leaders. Ethnographic data about the production and use of performance records and performance reports in a desk sales unit is examined to induce an exploitive specification of this dramaturgical model. This specification explains how people can take advantage of the opportunities, rather than just avoid the threats that performance data presents for impression management. It also demonstrates how managers can participate in the production of performance data to create an idealized version of their accomplishments and that leaders reify these data by using them in their own attempts at impressing others. By doing so, leaders and managers turn information systems into store windows to show achievement upward instead of transparent windows to monitor compliance downward.12/03/12
This paper demonstrates the value of Archer’s morphogenetic approach (MA) in understanding and explaining the complexity of the broader context within which many developing country information and communication technology (ICT) projects are implemented. It does this by using MA’s analytical and explanatory apparatus to examine the evolution of the context of public sector ICT provision in Kenya over the period 1963–2006. In addition to demonstrating the practical value of MA, the paper contributes to the Information Systems literature on ICT for development (ICT4D). The analysis identifies (1) global normative pressures, polity, the national socio-economic base, disruptive technology, and the emergence of multistakeholder networks as key forces in shaping the evolutionary trajectory, (2) the explicit treatment of time and temporality as key for understanding mechanisms underpinning the evolutionary process, and (3) the difficulty of cleanly isolating the implementation of individual public sector ICT projects from the broader context and ICT4D agendas. The discussion elaborates on the features of MA found to be particularly valuable in this study. The paper concludes that explicitly attending to time and temporality, and to the broader context for ICT4D projects, would contribute to the development of more nuanced accounts of such projects and a more emancipatory outlook for ICT4D research.02/05/13
By distinguishing between employees’ online and offline workplace communication networks, this paper incorporates technology into social network theory to understand employees’ job performance. Specifically, we conceptualize network ties as direct and indirect ties in both online and offline workplace communication networks, thus resulting in four distinct types of ties. We theorize that employees’ ties in online and offline workplace communication networks are complementary resources that interact to influence their job performance. We found support for our model in a field study among 104 employees in a large telecommunication company. The paper concludes with theoretical and practical implications.
The transformational model of social activity (TMSA), in many ways the centerpiece of critical realism, has been widely used in areas of information systems research. However, little has been done so far to develop a systematic theory of the nature, position, and identity of technological objects within the context of the TMSA. Our aim in this paper is to fill this gap, paying particular attention to the important category of nonmaterial technological objects that lie at the heart of modern information systems.12/12/12
Wixom and Todd (2005) integrated the user satisfaction and the technology acceptance literatures to theorize about and account for the influence of the information technology artifact on usage. Based on Wixom and Todd’s integrated model of technology usage, we propose the 3Q model by investigating the role of service quality (SQ), in addition to system quality (SysQ) and information quality (IQ), in website adoption. Attention to SQ is critical, as consumer websites have increasingly become the target of SQ assessment made by consumers, not just traditional SysQ and IQ evaluations. As part of our study, we further theorize and empirically test the relationships among these three types of quality constructs and hypothesize that perceived SysQ influences perceived IQ and perceived SQ, and perceived IQ influences perceived SQ. Our study extends the Wixom and Todd model in the e-service context and is the first of its kind to empirically examine the combined impact of perceived SQ, perceived SysQ, and perceived IQ on usage intention. Our study advances the theoretical understanding of SQ and the relationships among perceptions of SysQ, IQ, and SQ in the e-service context. The results also inform practitioners that high IQ and SysQ can directly or indirectly improve SQ in the e-service context.4/9/13
The goal of this study is to augment explanations of how newly implemented technologies enable network change within organizations with an understanding of when such change is likely to happen. Drawing on the emerging literature on technology affordances, the paper suggests that informal network change within interdependent organizational groups is unlikely to occur until users converge on a shared appropriation of the new technology’s features such that the affordances the technology enables are jointly realized. In making the argument for the importance of shared affordances, this paper suggests that group-level network change has its most profound implications at the organization level when individuals use the same subset of a new information technology’s features. To explore this tentative theory, we turn to a comparative, multimethod, longitudinal study of computer-based simulation technology use in automotive engineering. The findings of this explanatory case study show that engineers used the new technology for more than three months, during which time neither group experienced changes to their advice networks. Initially, divergent uses of the technology’s features by engineers in both groups precluded them from being able to coordinate their work in ways that allowed them to structure their advice networks differently. Eventually, engineers in only one of the two groups converged on the use of a common set of the technology’s features to enact a shared affordance. This convergence was necessary to turn the technology into a resource that could collectively afford group members the ability to compare their simulation outputs with one another and, in so doing, alter the content and structure of the group’s advice network. The implications of these findings for the literatures on technology feature use, affordances, social networks, and post-adoption behaviors in organizations are discussed.
A large proportion of information systems research is concerned with developing and testing models pertaining to complex cognition, behaviors, and outcomes of individuals, teams, organizations, and other social systems that are involved in the development, implementation, and utilization of information technology. Given the complexity of these social and behavioral phenomena, heterogeneity is likely to exist in the samples used in IS studies. While researchers now routinely address observed heterogeneity by introducing moderators, a priori groupings, and contextual factors in their research models, they have not examined how unobserved heterogeneity may affect their findings. We describe why unobserved heterogeneity threatens different types of validity and use simulations to demonstrate that unobserved heterogeneity biases parameter estimates, thereby leading to Type I and Type II errors. We also review different methods that can be used to uncover unobserved heterogeneity in structural equation models. While methods to uncover unobserved heterogeneity in covariance-based structural equation models (CB-SEM) are relatively advanced, the methods for partial least squares (PLS) path models are limited and have relied on an extension of mixture regression—finite mixture partial least squares (FIMIX-PLS) and distance measure-based methods—that have mismatches with some characteristics of PLS path modeling. We propose a new method—prediction-oriented segmentation (PLS-POS)—to overcome the limitations of FIMIX-PLS and other distance measure-based methods and conduct extensive simulations to evaluate the ability of PLS-POS and FIMIX-PLS to discover unobserved heterogeneity in both structural and measurement models. Our results show that both PLS-POS and FIMIX-PLS perform well in discovering unobserved heterogeneity in structural paths when the measures are reflective and that PLS-POS also performs well in discovering unobserved heterogeneity in formative measures. We propose an unobserved heterogeneity discovery (UHD) process that researchers can apply to (1) avert validity threats by uncovering unobserved heterogeneity and (2) elaborate on theory by turning unobserved heterogeneity into observed heterogeneity, thereby expanding theory through the integration of new moderator or contextual variables.3/15/13
While much is known about selecting different types of control that can be exercised in information systems development projects, the control dynamics associated with ISD offshoring projects represent an important gap in our understanding. In this paper, we develop a substantive grounded theory of control balancing that addresses this theoretical gap. Based on a longitudinal case study of an ISD offshoring project in the financial services industry, we introduce a three-dimensional control configuration category that emerged from our data, suggesting that control type is only one dimension on which control configuration decisions need to be made. The other two dimensions that we identified are control degree (tight versus relaxed) and control style (unilateral versus bilateral). Furthermore, we illustrate that control execution during the life cycle of an ISD offshoring project is highly intertwined with the development of client–vendor shared understanding and that each influences the other. Based on these findings, we develop an integrative process model that explains how offshoring project managers make adjustments to the control configuration periodically to allow the ISD offshoring project and relationship to progress, yielding the iterative use of different three-dimensional control configurations that we conceptualize in the paper. Our process model of control balancing may trigger new ways of looking at control phenomena in temporary interfirm organizations such as client–vendor ISD offshoring projects. Implications for research on organizational control and ISD offshoring are discussed. In addition, guidelines for ISD offshoring practitioners are presented.5/15/13
While much is known about selecting different types of control that can be exercised in information systems development projects, the control dynamics associated with ISD offshoring projects represent an important gap in our understanding. In this paper, we develop a substantive grounded theory of control balancing that addresses this theoretical gap. Based on a longitudinal case study of an ISD offshoring project in the financial services industry, we introduce a three-dimensional control configuration category that emerged from our data, suggesting that control type is only one dimension on which control configuration decisions need to be made. The other two dimensions that we identified are control degree (tight versus relaxed) and control style (unilateral versus bilateral). Furthermore, we illustrate that control execution during the life cycle of an ISD offshoring project is highly intertwined with the development of client–vendor shared understanding and that each influences the other. Based on these findings, we develop an integrative process model that explains how offshoring project managers make adjustments to the control configuration periodically to allow the ISD offshoring project and relationship to progress, yielding the iterative use of different three-dimensional control configurations that we conceptualize in the paper. Our process model of control balancing may trigger new ways of looking at control phenomena in temporary interfirm organizations such as client–vendor ISD offshoring projects. Implications for research on organizational control and ISD offshoring are discussed. In addition, guidelines for ISD offshoring practitioners are presented.
Information systems use represents one of the core concepts defining the discipline. In this article, we develop a rich conceptualization of IS use patterns as individuals’ emotions, cognition, and behaviors while employing an information technology to accomplish a work-related task. By combining two novel perspectives—the affect–object paradigm and automaticity—with coping theory, we theorize how different patterns appear and disappear as a result of different IT events—expected and discrepant—as well as over time, and how these patterns influence short-term performance. In order to test our hypotheses, we conducted two studies, one qualitative and the other quantitative, that combined different methods (e.g., open-ended questions, physiological data, videos, protocol analysis) to study the influence of expected and discrepant events. The synergistic properties of the two studies demonstrate the existence of two IS use patterns, automatic and adjusting. Most interactions are automatic, and adjusting patterns, triggered by discrepant IT events, fade over time and transition into automatic ones. Further, automatic patterns result in enhanced short-term performance, while adjusting ones do not. Our conceptualization of IS use patterns is useful because it addresses important questions (such as why negative IT perceptions persist) and clarifies that it is how (rather than how much) people use IT that is pertinent for performance.5/23/13
Enterprise system implementations often create tension in organizations. On the one hand, these systems can provide significant operational and strategic benefits. On the other hand, implementation of these systems is risky and a source of major disruptions. In particular, employees experience significant changes in their work environment during an implementation. Although the relationship between ES implementations and employees’ jobs has been noted in prior research, there is limited research on the nature, extent, determinants, and outcomes of changes in employees’ job characteristics following an ES implementation. This paper develops and tests a model, termed the job characteristics change model (JCCM), that posits that employees will experience substantial changes in two job characteristics (i.e., job demands and job control) during the shakedown phase (i.e., immediately after the rollout) of an ES implementation. These changes are theorized to be predicted by work process characteristics, namely perceived process complexity, perceived process rigidity, and perceived process radicalness, that in turn will be influenced by technology characteristics (i.e., perceived technology complexity, perceived technology reconfigurability, and perceived technology customization). JCCM further posits that changes in job characteristics will influence employees’ job satisfaction. Longitudinal field studies conducted in two organizations (N = 281 and 141 respectively) provided support for the model. The scientific and practical implications of the findings are discussed.7/31/13
This study investigates the role of information systems in stimulating energy-efficient behavior in private households. We present the example of Velix, a web portal designed to motivate customers of a utility company to reduce their electricity consumption. In particular, we consider the effectiveness of goal setting functionality and defaults in influencing energy conservation behavior. For this purpose, we use the web portal as a test of the theoretical propositions underlying its design. Based on data collected from a field experiment with 1,791 electricity consumers, we test hypotheses regarding the structural relations between defaults and goals, the impact of defaults and goals on consumption behavior, and the moderating role of feedback on goal choice. Our results confirm the positive impact of goal setting on energy conservation. We show that default goals lead to statistically significant savings by affecting goal choice. However, if the default goals are set too low or too high with respect to a self-set goal, the defaults will detrimentally affect behavior. We also show that feedback on goal attainment moderates the effect of default goals on goal choice. The results extend the knowledge on goal setting and defaults and have implications for the design of effective energy feedback systems. The study’s approach, which combines hypothesis-driven work and design-oriented IS research, could serve as a blueprint for further research endeavors of this kind, particularly with regard to feedback systems based on future smart metering infrastructures.4/25/13
This paper explores how a world-wide operating software solutions provider implemented environmentally sustainable business practices in response to emerging environmental concerns. Through an interpretive case study, we develop a theoretical framework that identifies four important functional affordances originating in information systems, which are required in environmental sustainability transformations as they create an actionable context in which (1) organizations can engage in a sensemaking process related to understanding emerging environmental requirements, and (2) individuals can implement environmentally sustainable work practices. Through our work, we provide several contributions, including a better understanding of IS-enabled organizational change and the types of functional affordances of information systems that are required in sustainability transformations. We describe implications relating to (1) how information systems can contribute to the creation of environmentally sustainable organizations, (2) the design of information systems to create required functional affordances, (3) the management of sustainability transformations, and (4) the further development of the concept of functional affordances in IS research.4/17/13
Herd literature suggests that people tend to discount their own beliefs and imitate others when making adoption decisions and that the resulting adoption decisions are fragile and can be easily reversed during the post-adoptive stage. This helps explain why the adoption of a number of new technologies―from Amazon’s Kindle, to Apple’s iPod, iPhone, and iPad, to various types of Web 2. 0 technologies―appears to have adoption patterns similar to those of new fashion trends (i. e., an initial en masse acquisition followed by subsequent abandonment). It is important to understand these phenomena because they are strongly related to the staying power of technology. From a herd behavior perspective, this study proposes two new concepts, namely discounting one’s own information and imitating others, to describe herd behavior in technology adoption. A research model is developed to describe the conditions under which herd behavior in technology adoption occurs, how it impacts technology adoption decision making, and how it influences post-adoptive system use. A longitudinal study is conducted to examine the research model. Findings from this research suggest that the discounting of one’s own beliefs and the imitating of others when adopting a new technology are provoked primarily by the observation of prior adoptions and perceptions of uncertainty regarding the adoption of new technology. Herd behavior has a significant influence on user technology adoption; however, it does not necessarily lead to the collapse of the user base, as predicted in the herd literature. Instead, imitation can help reduce post-adoption regret and thus serve as a legitimate strategy for choosing a good enough technology, which may or may not be the best option to enhance job performance. People tend to adjust their beliefs when herding and also to revive their discounted initial beliefs to modify their beliefs about the technology at the post-adoptive stage. Findings from this study have significant research and practical implications.10/08/12
Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor™, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars’ “basket” of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals—with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.7/31/13
With an abundance of products available online, many online retailers provide sales rankings to make it easier for consumers to find the best-selling products. Successfully implementing product rankings online was done a decade ago by Amazon, and more recently by Apple’s App Store. However, neither market provides actual download data, a very useful statistic for both practitioners and researchers. In the past, researchers developed various strategies that allowed them to infer demand from rank data. Almost all of that work is based on an experiment that shifts sales or collaboration with a vendor to get actual sales data. In this research, we present an innovative method to use public data to infer the rank–demand relationship for the paid apps on Apple’s iTunes App Store. We find that the top-ranked paid app for iPhone generates 150 times more downloads compared to the paid app ranked at 200. Similarly, the top paid app on iPad generates 120 times more downloads compared to the paid app ranked at 200. We conclude with a discussion on an extension of this framework to the Android platform, in-app purchases, and free apps.7/5/13
Privacy has been an enduring concern associated with commercial information technology (IT) applications, in particular regarding the issue of personalization. IT-enabled personalization, while potentially making the user computing experience more gratifying, often relies heavily on the user’s personal information to deliver individualized services, which raises the user’s privacy concerns. We term the tension between personalization and privacy, which follows from marketers exploiting consumers’ data to offer personalized product information, the personalization–privacy paradox. To better understand this paradox, we build on the theoretical lenses of uses and gratification theory and information boundary theory to conceptualize the extent to which privacy impacts the process and content gratifications derived from personalization, and how an IT solution can be designed to alleviate privacy concerns.Set in the context of personalized advertising applications for smartphones, we propose and prototype an IT solution, referred to as a personalized, privacy-safe application, that retains users’ information locally on their smartphones while still providing them with personalized product messages. We validated this solution through a field experiment by benchmarking it against two more conventional applications: a base non-personalized application that broadcasts non-personalized product information to users, and a personalized, non-privacy safe application that transmits user information to a central marketer’s server. The results show that (compared to the non-personalized application), while personalized, privacy-safe or not increased application usage (reflecting process gratification), it was only when it was privacy-safe that users saved product messages (reflecting content gratification) more frequently. Follow-up surveys corroborated these nuanced findings and further revealed the users’ psychological states, which explained our field experiment results. We found that saving advertisements for content gratification led to a perceived intrusion of information boundary that made users reluctant to do so. Overall our proposed IT solution, which delivers a personalized service but avoids transmitting users’ personal information to third parties, reduces users’ perceptions that their information boundaries are being intruded upon, thus mitigating the personalization–privacy paradox and increasing both process and content gratification.
Protecting information from a variety of security threats is a daunting organizational activity. Organization managers must recognize the roles that organization insiders have in protecting information resources rather than solely relying upon technology to provide this protection. Unfortunately, compared to negative insider behaviors, the extant literature provides sparse coverage of beneficial insider activities. The few beneficial activities in the literature represent only a small portion of the diverse collection of insiders’ protective actions.This research focuses on protection-motivated behaviors (PMBs), which are volitional behaviors enacted by organization insiders to protect (1) organizationally relevant information and (2) the computer-based information systems in which the information is stored, collected, disseminated, and/or manipulated from information security threats. Based on systematics, we propose a six-step methodology of qualitative and quantitative approaches to develop a taxonomy and theory of diversity for PMBs. These approaches integrate the classification techniques of multidimensional scaling (MDS), property fitting (ProFit), and cluster analyses. We leverage these techniques to identify and display how insiders collectively classify 67 unique PMBs and their homogeneous classes. Our taxonomy provides researchers and practitioners a comprehensive guide and common nomenclature for PMBs. Our methodology can be similarly used to create other theories of diversity.4/22/13
In this paper, we seek to determine whether a typical social media platform, Wikipedia, improves the information environment for investors in the financial market. Our theoretical lens leads us to expect that information aggregation about public companies on Wikipedia may influence how management’s voluntary information disclosure reacts to market uncertainty with respect to investors’ information about these companies. Our empirical analysis is based on a unique data set collected from financial records, management disclosure records, news article coverage, and a Wikipedia modification history of public companies. On the supply side of information, we find that information aggregation on Wikipedia can moderate the timing of managers’ voluntary disclosure of companies’ earnings disappointments, or bad news. On the demand side of information, we find that Wikipedia’s information aggregation moderates investors’ negative reaction to bad news. Taken together, these findings support the view that Wikipedia improves the information environment in the financial market and underscore the value of information aggregation through the use of information technology.4/10/13
Commercial truck driving is an essential part of the national supply chain but one that adversely affects the environment. The purpose of this study is to determine the influence of the potential environmental benefits, among other factors, on continued use of bypass systems that can be discontinued at any time by a driver. The results from our study show that (1) economic benefits and industry pressures positively influence drivers’ use of bypass systems but (2) the environmental benefits of the technology do not, even though system vendors and state transportation agencies emphasize these benefits of the technology. Based on these findings, we conclude that sustainable information systems can be a viable option in a business context if usage leads to economic benefits. Our results and conclusions support the U.S. Environmental Protection Agency’s differentiation of public policy versus business perspectives on sustainable technology.4/9/13
In this paper, we study the differential influence of online user-generated content (UGC), specifically blogs, across the multiple stages of decision making of venture capitalists: screening stage, choice stage, and contract stage. We conjecture that, first, blogs are influential at the screening stage; second, after the screening stage, blogs are noninfluential since decision makers evaluate entities closely at later stages; third, blogs increase the interest from multiple decision makers which in turn increases the cost of the deal for a decision maker. This empirical investigation provides support for the hypotheses, which we tested for funding decisions by venture capitalists in information technology ventures. In particular, this study indicates that blogs can help managers in getting their products/services selected at the screening stage, but, beyond that, blogs do not help directly. However, since more decision makers screen products/services that receive blog coverage, the competition among decision makers helps managers in negotiating better contract terms. We advance the boundary of existing studies on the influence of UGC from single stage process to multiple stages.5/15/13
This paper examines how a new actor category may emerge in a field of discourse through the new media of the Internet. Existing literatures on professional and organizational identity have shown the importance of identity claims and of the tensions surrounding “optimal distinctiveness” for new actors in a field, but have not examined the roles of new media in these processes. The literature on information technology (IT) and identity has highlighted the identity-challenging and identity-enhancing aspects of new IT use for existing actor categories but has not examined the dynamics associated with the emergence of new actor categories. Here, we investigate how a new actor category may emerge through the use of new media as a dynamic interaction of discursive practices, identity claims, and new media use. Drawing on findings from a case study of technology bloggers, we identified discursive practices through which a group of technology bloggers enacted claims of a distinctive identity in the joint construction of their discourse and in response to continuous developments in new media. Emergence of this new category was characterized by ongoing, opposing yet coexisting tendencies toward coalescence, fragmentation, and dispersion. Socio-technical dynamics underlying bloggers’ use of new media and the actions of prominent (“A-list”) bloggers contributed to these tendencies. We untangle theoretically the identity-enabling and identity-unsettling effects of new media and conceptualize the emergence of a new actor category through new media as an ongoing process in which the category identity may remain fluid, rather than progress to an endpoint.02-13-13
The long-term viability of virtual communities depends critically on contribution behavior by their members. We deepen and extend prior research by conceptualizing contributions to virtual communities in terms of small friendship group–referent intentional actions. Specifically, we investigate cognitive, emotional, and social determinants of shared we-intentions and their consequences for member contribution behavior to the small friendship group to which they belong within a larger community. Using multiple measurement sources and a longitudinal quasi-experimental design, we show that group norms and social identity, as well as attitudes and anticipated emotions, contribute to the development of behavioral desires, which in turn influence we-intentions. In addition, subjective norms are less effective than either group norms or social identity in encouraging contribution behavior. Finally, members’ experience levels positively moderate the relationship between we-intentions and contribution behaviors, and differences between collectivistic versus individualistic orientations moderate the effects of social identity and anticipated emotions on the desire to contribute to one’s friendship group in the virtual community. Tests for methods biases were conducted, as well as rival hypotheses. These findings have significant research and managerial implications.4/25/13
The rise of social broadcasting technologies has greatly facilitated open access to information worldwide, not only by powering decentralized information production and consumption, but also by expediting information diffusion through social interactions like content sharing. Voluntary information sharing by users in the context of Twitter, the predominant social broadcasting site, is studied by modeling both the technology and user behavior. A detailed data set about the official content-sharing function on Twitter, called retweet, is collected and the statistical relationships between users’ social network characteristics and their retweeting acts are documented. A two-stage consumption-sharing model is then estimated using the conditional maximum likelihood estimatio (MLE) method. The empirical results convincingly support our hypothesis that weak ties (in the form of unidirectional links) are more likely to engage in the social exchange process of content sharing. Specifically, we find that after a median quality tweet (as defined in the sample) is consumed, the likelihood that a unidirectional follower will retweet is 3.1 percentage point higher than the likelihood that a bidirectional follower will do so.4/8/13
Many document classification applications require human understanding of the reasons for data-driven classification decisions by managers, client-facing employees, and the technical team. Predictive models treat documents as data to be classified, and document data are characterized by very high dimensionality, often with tens of thousands to millions of variables (words). Unfortunately, due to the high dimensionality, understanding the decisions made by document classifiers is very difficult. This paper begins by extending the most relevant prior theoretical model of explanations for intelligent systems to account for some missing elements. The main theoretical contribution is the definition of a new sort of explanation as a minimal set of words (terms, generally), such that removing all words within this set from the document changes the predicted class from the class of interest. We present an algorithm to find such explanations, as well as a framework to assess such an algorithm’s performance. We demonstrate the value of the new approach with a case study from a real-world document classification task: classifying web pages as containing objectionable content, with the goal of allowing advertisers to choose not to have their ads appear on those pages. A second empirical demonstration on news-story topic classification shows the explanations to be concise and document-specific, and to be capable of providing understanding of the exact reasons for the classification decisions, of the workings of the classification models, and of the business application itself. We also illustrate how explaining the classifications of documents can help to improve data quality and model performance.7/5/13
The concept of guanxi (i.e., a close and pervasive interpersonal relationship) has received little attention in the literature on online marketplaces, perhaps due to their impersonal nature. However, we propose that computer-mediated communication (CMC) technologies can mimic traditional interactive face-to-face communications, thus enabling a form of guanxi in online marketplaces. Extending the literature on traditional guanxi, we herein introduce the concept of swift guanxi, conceptualized as the buyer’s perception of a swiftly formed interpersonal relationship with a seller, which consists of mutual understanding, reciprocal favors, and relationship harmony.Integrating theories of CMC and guanxi, we develop a model that explains how a set of CMC tools (i.e., instant messaging, message box, feedback system) facilitate repeat transactions with sellers by building swift guanxi and trust through interactivity and presence (social presence and telepresence) with sellers. Longitudinal data from 338 buyers in TaoBao, China’s leading online marketplace, support our structural model, showing that the buyers’ effective use of CMC tools enable swift guanxi and trust by enhancing the buyers’ perceptions of interactivity and presence. In turn, swift guanxi and trust predict buyers’ repurchase intentions and their actual repurchases from sellers. We discuss the implications of swift guanxi in online marketplaces with the aid of CMC technologies.5/20/13
Several studies support the positive link between information technology capability and firm performance, including Bharadwaj (2000) and Santhanam and Hartono (2003), which appeared in MIS Quarterly. We conducted a study to see if this link is still statistically significant. It is now over a decade since the first study was published, during which several significant developments in the IT industry have taken place. Unlike the 1990s, when proprietary information systems prevailed, the 2000s are characterized by more standardized and homogeneous information systems and with the rapid adoption of ERP and web technologies. Thus, we attempted to reexamine the link between IT capability and firm performance with data from the 2000s. Surprisingly, the results of our current analysis showed no significant link between IT capability and firm performance. Contrary to earlier studies, IT leader firms in our study didn’t show better financial performance than control firms. We discuss several possible causes for the change in findings and present an in-depth comparison in business performance between the two groups—IT leader and control—over a period extending from 1991 to 2007.7/31/13
Although institutional theory has become a more dominant perspective in information systems research, studies have only paid scant attention to how field dynamics and organizational processes coevolve during information technology institutionalization. Against this backdrop, we present a new conceptualization based on the “traveling of ideas” metaphor that distinguishes between theorization of ideas about IT usage across an organizational field and translation of such ideas into practical use of IT within particular organizations. Drawing on these distinct analytical views, we posit that IT institutionalization is constituted through recursive intertwining of theorization and translation involving both linguistic and material objects. To illustrate the detailed workings of this conceptualization, we apply it to a longitudinal study of mobile IT institutionalization within Danish home care. We demonstrate how heterogeneous actors within the Danish home care field theorized ideas about mobile IT usage and how these ideas translated into different local arrangements. Further, our account reveals a complex institutionalization process in which mobile IT was first seen as a fashionable recipe for improvement but subsequently became the subject of controversy. The paper adds to the emerging process and discourse literature on IT institutionalization by shedding new light on how IT ideas travel across a field and within individual organizations, how they transform and become legitimized over time, and how they take on different linguistic and material forms across organizational settings.4/10/13
A growing number of firms are strategically utilizing information technology and the Internet to provide online services to consumers who buy their products. Online services differ from traditional services because they often promote interactivity among users and exhibit positive network effects. While the service increases the value obtained by consumers, network effects are known to intensify price competition and thus may reduce firms’ profits. In this paper, we model the competition between two firms that sell a differentiated product when each firm can offer a complementary online service to its customers. We derive the market equilibrium and determine how firms should adjust their strategies to account for network effects. We find that when the service exhibits network effects, a firm’s decision whether or not to offer the service depends on both the competitor’s decision and the competitor’s service quality. When the service does not exhibit network effects, this is not the case. In addition, we show that a firm can benefit from the technological ability to offer the service, and from an increase in the strength of network effects or in the market size of the service, only when the value customers derive from the direct functionalities (those that do not rely on the network) of the service are sufficiently high. As a result, a firm’s investment in the direct functionalities of its service increases with the strength of network effects of the service as long as the marginal development cost is not too high. Finally, we show that inefficiencies in terms of the number of firms offering the service as well as the total number of service users may prevail..9/10/13
This paper develops and illustrates the theory of collaboration through open superposition: the process of depositing motivationally independent layers of work on top of each other over time. The theory is developed in a study of community-based free and open source software (FLOSS) development, through a research arc of discovery (participant observation), replication (two archival case studies), and theorization. The theory explains two key findings: (1) the overwhelming majority of work is accomplished with only a single programmer working on any one task, and (2) tasks that appear too large for any one individual are more likely to be deferred until they are easier rather than being undertaken through structured team work. Moreover, the theory explains how working through open superposition can lead to the discovery of a work breakdown that results in complex, functionally interdependent, work being accomplished without crippling search costs. We identify a set of socio-technical contingencies under which collaboration through open superposition is likely to be effective, including characteristics of artifacts made from information as the objects being worked on. We demonstrate the usefulness of the theory by using it to analyze difficulties in learning from FLOSS in other domains of work and in the IS function of for-profit organizations.
A reliability generalization study (a meta-analysis of reliability coefficients) was conducted on three widely studied information systems constructs from the technology acceptance model (TAM): perceived ease of use, perceived usefulness, and behavioral intentions. This form of meta-analysis summarizes the reliability coefficients of the scores on a specified scale across studies and identifies the study characteristics that influence the reliability of these scores. Reliability is a critical issue in conducting empirical research as the reliability of the scores on well-established scales can vary with study characteristics, attenuating effect sizes. In conducting this study, an extensive literature search was conducted, with 380 articles reviewed and coded to perform reliability generalization. Study characteristics, including technology, sample, and measurement characteristics, for these articles were recorded along with effect size data for the relationships among these variables. After controlling for number of items, sample size, and sampling error, differences in reliability coefficients were found with several study characteristics for the three technology acceptance constructs. The reliability coefficients of PEOU and PU were lower in hedonic contexts than in utilitarian contexts, and were higher when the originally validated scales were used as compared to when other items were substituted. Only 27 percent of the studies that provided the measurement items used the original PEOU items, while 39 percent used the original PU items. Scales that were administered in English had higher reliability coefficients for PU and BI, with a marginal effect for PEOU. Reliability differences were also found for other study characteristics, including reliability type, subject experience, and gender composition. While average reliability coefficients were high, the results show that, on average, relationships among these constructs are attenuated by 12 percent with maximum attenuation in the range of 35 to 43 percent. Implications for technology acceptance research are discussed and suggestions for addressing variation in reliability coefficients across studies are provided.7/8/13
In recent years, we have witnessed the rapid proliferation and widespread adoption of a new class of information technologies, commonly known as social media. Researchers often rely on social network analysis (SNA) when attempting to understand these technologies, often without considering how the novel capabilities of social media platforms might affect the underlying theories of SNA, which were developed primarily through studies of offline social networks. This article outlines several key differences between traditional offline social networks and online social media networks by juxtaposing an established typology of social network research with a well-regarded definition of social media platforms that articulates four key features. The results show that at four major points of intersection, social media has considerable theoretical implications for SNA. In exploring these points of intersection, this study outlines a series of theoretically distinct research questions for SNA in social media contexts. These points of intersection offer considerable opportunities for researchers to investigate the theoretical implications introduced by social media and lay the groundwork for a robust social media agenda potentially spanning multiple disciplines.
Motivated by the growing importance of social media, this paper examines the relationship between new media, old media, and sales in the context of the music industry. In particular, we study the interplay between blog buzz, radio play, and music sales at both the album and song levels of analysis. We employ the panel vector autoregression (PVAR) methodology, an extension of vector autoregression to panel data. We find that radio play is consistently and positively related to future sales at both the song and album levels. Blog buzz, however, is not related to album sales and negatively related to song sales, suggesting that sales displacement due to free online sampling dominates any positive word-of-mouth effects of song buzz on sales. Further, the negative relationship between song buzz and sales is stronger for niche music relative to mainstream music, and for less popular songs within albums. We discuss the implications of these results for both research and practice regarding the role of new media in the music industry.7/18/13
The implementation of enterprise systems, such as enterprise resource planning (ERP) systems, alters business processes and associated workflows, and introduces new software applications that employees must use. Employees frequently find such technology-enabled organizational change to be a major challenge. Although many challenges related to such changes have been discussed in prior work, little research has focused on post-implementation job outcomes of employees affected by such change. We draw from social network theory— specifically, advice networks—to understand a key post-implementation job outcome (i.e., job performance). We conducted a study among 87 employees, with data gathered before and after the implementation of an ERP system module in a business unit of a large organization. We found support for our hypotheses that workflow advice and software advice are associated with job performance. Further, as predicted, we found that the interactions of workflow and software get-advice, workflow and software give-advice, and software get- and give-advice were associated with job performance. This nuanced treatment of advice networks advances our understanding of post-implementation success of enterprise systems.7/1213
Software design is a knowledge intensive task that constitutes a critical part of the software development process. Using a controlled experiment involving software practitioners, this research examines two potentially useful mechanisms for improving the software design process. Specifically, this study examines the impact of structural distribution of cognition through design patterns and social distribution of cognition through collaborating pairs on design outcomes. The results indicate that the use of design patterns as external cognitive artifacts improves design quality, reduces time taken to solve a design problem, and leads to higher participant satisfaction. Collaborating pairs of software designers were compared to participants working alone but whose efforts were conjointly considered as the best and second-best members of nominal pairs. It was found that paired designers produced higher quality designs compared with the second-best members of nominal pairs, did not differ from the best member of a nominal pair, but took more time to complete a design task than either member of a nominal pair. The results also indicate that the availability of design patterns raises the performance level of the second-best member of a nominal pair, in terms of quality, and reduces task completion time when compared with a pair not using design patterns. Finally, paired designers were found to experience higher levels of task satisfaction when compared with individuals. Implications for research and practice are discussed.7/26/13
Online user forums for technical support are being widely adopted by IT firms to supplement traditional customer support channels. Customers benefit from having an additional means of product support, while firms benefit by lowering the costs of supporting a large customer base. Typically these forums are populated with content generated by users, consisting of questioners (solution seekers) and solvers (solution providers). While questioners can be expected to keep returning as long as they can find answers, firms must employ different means in order to recognize and encourage the contributions of solvers. We identify and compare the impact of two widely adopted recognition mechanisms on the philanthropic behavior of solvers. In the first mechanism, feedback-based recognition, solver contribution is evaluated by questioners. In the second mechanism, quantity-based recognition, all contributions are weighted equally regardless of questioner feedback. We draw on the pro-social behavior literature to identify four drivers of solver contribution: (1) peer recognition, (2) image motivation, (3) social comparison, and (4) social exposure. We show that the choice of recognition mechanism strongly influences a solver’s problem-solving behavior, highlighting the importance of the firm’s decision in this regard. We address issues of solvers self-selecting a type of recognition mechanism by using propensity score analysis in order to show that solver behavior is a result of forum conditioning. We also study the impact of the recognition mechanism on forum quality and the effectiveness of support to draw comparative analytics. 5/15/13
Online publishers and advertisers have recently shown increasing interest in using targeted advertising online. Such targeting allows them to present users with advertisements that are a better match, based on their past browsing and search behavior and other available information (e.g., hobbies registered on a web site). This technique, known as behavioral targeting, has been hailed as the new “Holy Grail” in online advertising because of its potential effectiveness. In this paper, we study the economic implications when an online publisher engages in behavioral targeting. The publisher auctions off an advertising slot and is paid on a cost-per-click basis. Using a horizontal differentiation model to capture the fit between a user and an advertisement being displayed, we identify the factors that affect the publisher’s revenue, the advertisers’ payoffs, and social welfare. We show that revenue for the online publisher in some circumstances can double when behavioral targeting is used. However, increased revenue for the publisher is not guaranteed: in some cases, the prices of advertising and hence the publisher’s revenue can be lower, depending on the degree of competition and the advertisers’ valuations. We identify two effects associated with behavioral targeting: a competitive effect and a propensity effect. The relative strength of the two effects determines whether the publisher’s revenue is positively or negatively affected. We also demonstrate that, although social welfare is increased and small advertisers are better off under behavioral targeting, the dominant advertiser might be worse off and reluctant to switch from traditional advertising.8/16/13
Much prior research on virtual teams has examined the impact of the features and capabilities of different communication tools (the nature of communication) on team performance. In this paper, we examine how the social structures (i.e., genre rules) that emerge around different communication tools (the nurture of communication) can be as important in influencing performance. During habitual use situations, team members enact genre rules associated with communication tools without conscious thought via automaticity. These genre rules influence how teams interact and ultimately how well they perform. We conducted an experimental study to examine the impact of different genre rules that have developed for two communication tools: instant messenger and discussion forum. Our results show that in habitual use situations, these tools triggered different genre rules with different behaviors, which in turn resulted in significantly different decision quality. We used heightened time pressure as a discrepant event to interrupt the automatic enactment of habitual genre rules and found that users adopted similar behaviors for both tools, which resulted in no significant differences in decision quality. These findings suggest that the automatic enactment of genre rules for a communication tool may have as powerful an effect on behavior and performance as the actual features of the tool itself. We believe that our results, taken together with past research showing the effects of social structures on communication, call for the expansion of task–technology fit theories to include the role of social structures in explaining the use of and performance from communication tools.10/31/13
Online crowdsourcing markets (OCM) are becoming more popular as a source for data collection. In this paper, we examine the consistency of survey results across student samples, consumer panels, and online crowdsourcing markets (specifically Amazon’s Mechanical Turk) both within the United States and outside. We conduct two studies examining the technology acceptance model (TAM) and the expectation– disconfirmation theory (EDT) to explore potential differences in demographics, psychometrics, structural model estimates, and measurement invariances. Our findings indicate that (1) U.S.-based OCM samples provide demographics much more similar to our student and consumer panel samples than the non-U.S.-based OCM samples; (2) both U.S. and non-U.S. OCM samples provide initial psychometric properties (reliability, convergent, and divergent validity) that are similar to those of both student and consumer panels; (3) non-U.S. OCM samples generally provide differences in scale means compared to those of our students, consumer panels, and U.S. OCM samples; and (4) one of the non-U.S. OCM samples refuted the highly replicated and validated TAM model in the relationship of perceived usefulness to behavioral intentions. Although our post hoc analyses isolated some cultural and demographic effects with regard to the non-U.S. samples in Study 1, they did not address the model differences found in Study 2. Specifically, the inclusion of non-U.S. OCM respondents led to statistically significant differences in parameter estimates, and hence to different statistical conclusions. Due to these unexplained differences that exist within the non-U.S. OCM samples, we caution that the inclusion of non-U.S. OCM participants may lead to different conclusions than studies with only U.S. OCM participants. We are unable to conclude whether this is due to of cultural differences, differences in the demographic profiles of non-U.S. OCM participants, or some unexplored factors within the models. Therefore, until further research is conducted to explore these differences in detail, we urge researchers utilizing OCMs with the intention to generalize to U.S. populations focus on U.S.-based participants and exercise caution in using non-U.S. participants. We further recommend that researchers should clearly describe their OCM usage and design (e.g., demographics, participant filters, etc.) procedures. Overall, we find that U.S. OCM samples produced models that lead to similar statistical conclusions as both U.S. students and U.S. consumer panels at a considerably reduced cost.9/27/13
This paper explores the effects of emotions embedded in a seller review on its perceived helpfulness to readers. Drawing on frameworks in literature on emotion and cognitive processing, we propose that over and above a well-known negativity bias, the impact of discrete emotions in a review will vary, and that one source of this variance is reader perceptions of reviewers’ cognitive effort. We focus on the roles of two distinct, negative emotions common to seller reviews: anxiety and anger. In the first two studies, experimental methods were utilized to identify and explain the differential impact of anxiety and anger in terms of perceived reviewer effort. In the third study, seller reviews from Yahoo! Shopping web sites were collected to examine the relationship between emotional review content and helpfulness ratings. Our findings demonstrate the importance of examining discrete emotions in online word-of-mouth, and they carry important practical implications for consumers and online retailers.8/9/13
The effects of e-commerce institutional mechanisms on trust and online purchase have traditionally been understood in the initial online purchase context. This study extends this literature by exploring the role of e-commerce institutional mechanisms in the online repurchase context. In doing so, it responds to the emerging call for understanding the institutional context under which customer trust operates in an e-commerce environment. Specifically, this study introduces a key moderator, perceived effectiveness of e-commerce institutional mechanisms (PEEIM), to the relationships between trust, satisfaction, and repurchase intention. Drawing on the theory of organizational trust, and based on a survey of 362 returning online customers, we find that PEEIM negatively moderates the relationship between trust in an online vendor and online customer repurchase intention, as it decreases the importance of trust to promoting repurchase behavior. We also find that PEEIM positively moderates the relationship between customer satisfaction and trust as it enhances the customer’s reliance on past transaction experience with the vendor to reevaluate trust in the vendor. Consistent with the predictions made in the literature, PEEIM does not directly affect trust or repurchase intention. Academic and practical implications and future research directions are discussed.
Web personalization can achieve two business goals: increased advertising revenue and increased sales revenue. The realization of the two goals is related to two kinds of user behavior: item sampling and item selection. Prior research does not provide a model of attitude formation toward a personalization agent nor of how attitudes relate to these two behaviors. This limits our understanding of how web personalization can be managed to increase advertising revenues and/or sales revenues. To fill this gap, the current research develops and tests a theoretical model of user attitudes and behaviors toward a personalization agent. The model is based on an integration of two theories: the elaboration likelihood model (ELM) and consumer search theory (CST). In the integrated model, a user’s attitude toward a personalization agent is influenced by both the number of items he/she has sampled so far (from CST) and the degree to which he/she cognitively processes each one (from ELM). In turn, attitude is modeled to influence both behaviors—that is, item selection and any further item sampling. We conducted a lab study and a field study to test six hypotheses. This research extends the theory on web personalization by providing a more complete picture of how sampling and processing of personalized recommendations influence a user’s attitude and behavior toward the personalization agent. For online merchants, this research highlights the trade-off between item sampling and item selection and provides practical guidance on how to steer users toward the attitudes and behaviors that will realize their business goals.9/20/13
The 50-year march of Moore’s Law has led to the creation of a relatively cheap and increasingly easy-to-use world-wide digital infrastructure of computers, mobile devices, broadband network connections, and advanced application platforms. This digital infrastructure has, in turn, accelerated the emergence of new technologies that enable transformations in how we live and work, how companies organize, and the structure of entire industries.As a result, it has become important for all business students to have a strong grounding in IT and digital innovation in order to manage, lead, and transform organizations that are increasingly dependent on digital innovation. Yet, at many schools, students do not get such grounding because the required information systems core class is stuck in the past. We present a vision for a redesigned IS core class that adopts digital innovation as a fundamental and powerful concept (FPC). A good FPC serves as both a foundational concept and an organizing principle for a course.We espouse a particularly broad conceptualization of digital innovation that allows for a variety of teaching styles and topical emphases for the IS core class. This conceptualization includes three types of innovation (i.e., process, product, and business model innovation), and four stages for the overall innovation process (i.e., discovery, development, diffusion, and impact). Based on this conceptualization, we examine the implications of adopting digital innovation as an FPC. We also briefly discuss broader implications relating to (1) the IS curriculum beyond the core class, (2) the research agenda for the IS field, and (3) the identity and legitimacy of IS in business schools.09/25/13
That recommendation agents (RAs) can substantially improve consumers’ decision making is well understood. Far less understood is the influence of specific design attributes of the RA interface on decision making and other outcome measures. We investigate a novel design for an RA interface that enables it to interactively demonstrate trade-offs among product attribute values (i.e., trade-off transparency feature) to improve consumers’ perceived product diagnosticity and perceived enjoyment. We also examine the extent to which the trade-offs among product attribute values should be revealed to the user. Further, based on the stimulus– organism–response model, we develop a theoretical model that extends the effort–accuracy framework by proposing perceived enjoyment and perceived product diagnosticity as two antecedents for perceived decision quality and perceived decision effort, respectively. In an experimental study, we find that (1) the trade-off transparency feature significantly affects perceived enjoyment and perceived product diagnosticity, (2) perceived enjoyment and perceived product diagnosticity follow an inverted U-shaped curve as the level of trade-off transparency increases, (3) although users spend more time understanding attribute trade-offs with the trade-off transparency feature, they are more efficient in selecting a product, (4) perceived enjoyment simultaneously leads to better perceived decision quality and lower perceived decision effort, and (5) perceived product diagnosticity leads to better perceived decision quality without compromising perceptions of decision effort. Theoretically, this study increases our understanding of how the design of an RA interface can improve consumers’ product diagnosticity and enjoyment, and proposes two antecedents to improve perceived decision quality and reduce perceived decision effort. For design practitioners, our results indicate the importance of providing the trade-off transparency design feature to potential consumers.7/26/13
The paper questions common assumptions in the dominant representational framings of information systems success and failure and proposes a performative perspective that conceives IS success and failure as relational effects performed by sociomaterial practices of IS project actor-networks of developers, managers, technologies, project documents, methodologies, and other actors. Drawing from a controversial case of a highly innovative information system in an insurance company—considered a success and failure at the same time— the paper reveals the inherent indeterminacy of IS success and failure and describes the mechanisms by which success and failure become performed and thus determined by sociomaterial practices. This is explained by exposing ontological politics in the reconfiguration and decomposition of the IS project actor-network and the emergence of different agencies of assessment that performed both different IS realities and competing IS assessments. The analysis shows that the IS project and the implemented system as objects of assessment are not given and fixed, but are performed by the agencies of assessment together with the assessment outcomes of success and failure. The paper demonstrates that by reframing IS success and failure, the performative perspective provides some novel and surprising insights that have a potential to change conversations on IS assessments in both the IS literature and IS practice.12/18/13
The coordination of effort within and among different expert groups is a central feature of contemporary organizations. Within the existing literature, however, a dichotomy has emerged in our understanding of the role played by codification in coordinating expert groups. One strand of literature emphasizes codification as a process that supports coordination by enabling the storage and ready transfer of knowledge. In contrast, another strand highlights the persistent differences between expert groups that create boundaries to the transfer of knowledge, seeing coordination as dependent on the quality of the reciprocal interactions between groups and individuals. Our research helps to resolve such contested understandings of the coordinative role played by codification. By focusing on the offshore-outsourcing of knowledge-intensive services, we examine the role played by codification when expertise was coordinated between client staff and onsite and offshore vendor personnel in a large-scale outsourcing contract between TATA Consultancy Services (TCS) and ABN AMRO bank. A number of theoretical contributions flow from our analysis of the case study, helping to move our understanding beyond the dichotomized views of codification outlined above. First, our study adds to previous work where codification has been seen as a static concept by demonstrating the multiple, coexisting, and complementary roles that codification may play. We examine the dynamic nature of codification and show changes in the relative importance of these different roles in coordinating distributed expertise over time. Second, we reconceptualize the commonly accepted view of codification as focusing on the replication and diffusion of knowledge by developing the notion of the codification of the “knower” as complementary to the codification of knowledge. Unlike previous studies of expertise directories, codification of the knower does not involve representing expertise in terms of occupational skills or competences but enables the reciprocal interrelating of expertise required by more unstructured tasks.THIS IS AN OPEN-ACCESS ARTICLE. Go to the "Online Supplements" page to download this article.11/12/13
This study identifies the effects of security investments that arise from previous failures or external regulatory pressure. Building on organizational learning theory, the study focuses on the healthcare sector where legislation mandates breach disclosure and detailed data on security investments are available. Using a Cox proportional hazard model, we demonstrate that proactive security investments are associated with lower security failure rates. Coupling that result with the economics of breach disclosure, we also show that proactive investments are more cost effective in healthcare security than reactive investments. Our results further indicate that this effect is amplified at the state level, supporting the argument that security investments create positive externalities. We also find that external pressure decreases the effect of proactive investments on security performance. This implies that proactive investments, voluntarily made, have more impact than those involuntarily made. Our findings suggest that security managers and policy makers should pay attention to the strategic and regulatory factors influencing security investment decisions.6/27/13
In recent years, we have witnessed an unprecedented growth in the security software market. This market is now fiercely competitive with hundreds of nearly identical products; yet, the price is high and coverage low. Although recent research has examined such idiosyncrasies and found the existence of a negative network effect as a possible explanation, several important questions still remain: (1) What possibly discourages product differentiation in such a competitive market? (2) Why is versioning absent here? (3) How does the presence of free alternatives in this market impact its structure? We develop a comprehensive oligopoly model, with endogenous quality and versioning decisions, to address these issues. Our analyses reveal that, although the presence of numerous competitors leads to a greater need to differentiate, the network effect in this market works as a counterweight, incentivizing vendors to sacrifice differentiation in favor of collocating in the top end of the quality spectrum. We explain the reasons and implications of this important finding. We further show that this result is robust and applicable even when versioning by competing vendors or the presence of free software is taken into consideration. Furthermore, given that the presence of free software actually intensifies competitive pressure and heightens the need to differentiate, the role of the network effect in abating differentiation becomes even more discernible.9/9/13
The paper extends the concept of “user” to account for a new, more formalized role that some client organizations play in the diffusion of packaged enterprise systems. Package vendors are attempting to draw parts of their user base into activities related to the promotion, selling, and commodification of systems. Users, in turn, appear willing to help construct these systems as objects of consumption for others. This can appear to be rather idiosyncratic behavior. Information Systems scholars have argued that relations between packaged enterprise system vendors and users are attenuated. Why might the user help the vendor market its systems in this way? What benefits accrue from it? And what role are users performing in carrying out this work? To show how this is becoming a general facet of the work of some packaged enterprise system users, we develop the notion of “reference actor,” which is an extension of the earlier Information Systems concept of “social actor.” In combining insights from the social shaping of technology and the biography of artifacts, and drawing on long-term qualitative fieldwork, we analyze this new actor role in relation to expectations and commitments coming from the wider packaged enterprise system community. In return for the help provided to prospective adopters, reference actors are also able to gather various kinds of benefits for themselves and others. In particular, they build closer relations with vendors such that they can influence product development strategies.7/9/13
Information systems researchers have shown an increasing interest in the notion of sociomateriality. In this paper, we continue this exploration by focusing specifically on entanglement: the inseparability of meaning and matter. Our particular approach is differentiated by its grounding in a relational and performative ontology, and its use of agential realism. We explore some of the key ideas of entanglement through a comparison of two phenomena in the travel sector: an institutionalized accreditation scheme offered by the AA and an online social media website hosted by TripAdvisor. Our analysis centers on the production of anonymity in these two practices of hotel evaluation. By examining how anonymity is constituted through an entanglement of matter and meaning, we challenge the predominantly social treatments of anonymity to date and draw attention to the uncertainties and outcomes generated by specific performances of anonymity in practice. In closing, we consider what the particular agential realist concept of entanglement entails for understanding anonymity, and discuss its implications for research practice.10/27/13
Expectation confirmation research in general, and in information systems (IS) in particular, has produced conflicting results. In this paper, we discuss six different models of expectation confirmation: assimilation, contrast, generalized negativity, assimilation-contrast, experiences only, and expectations only. Relying on key constructs from the technology acceptance model (TAM), we test each of these six models that suggests different roles for expectations and experiences of the key predictor—here, perceived usefulness—and their impacts on key outcomes—here, behavioral intention, use, and satisfaction. Data were collected in a field study from 1,113 participants at two points in time. Using polynomial modeling and response surface analysis, we provide the analytical representations for each of the six models and empirically test them to demonstrate that the assimilation-contrast is the best existing model in terms of its ability to explain the relationships between expectations and experiences of perceived usefulness and important dependent variables—namely, behavioral intention, use, and satisfaction—in individual-level research on IS implementations.2/10/14
This paper develops a sociomaterial perspective on digital coordination. It extends Pickering’s mangle of practice by using a trichordal approach to temporal emergence. We provide new understanding as to how the nonhuman and human agencies involved in coordination are embedded in the past, present, and future. We draw on an in-depth field study conducted between 2006 and 2010 of the development, introduction, and use of a computing grid infrastructure by the CERN particle physics community. Three coordination tensions are identified at different temporal dimensions, namelyobtaining adequate transparency in the present, modeling a future infrastructure, and the historical disciplining of social and material inertias. We propose and develop the concept of digital coordination, and contribute a trichordal temporal approach to understanding the development and use of digital infrastructure as being orientated to the past and future while emerging in the present.THIS IS AN OPEN ACCESS ARTICLE. Go to the "Online Supplements" page to download this article.
Despite growing interest in the economic and policy aspects of information security, little academic research has used field data to examine the development process of a security countermeasure provider. In this paper, we empirically examine the learning process a security software developer undergoes in resolving a malware problem. Using the data collected from a leading antivirus software company in Asia, we study the differential effects of experience on the malware resolution process. Our findings reveal that general knowledge from cross-family experience has greater impact than specific knowledge from within-family experience on performance in the malware resolution process. We also examine the factors that drive the differential effects of prior experience. Interestingly, our data show that cross-family experience is more effective than within-family experience in malware resolution when malware targets the general public than when a specific victim is targeted. Similar results—for example, the higher (lower) effect of cross-family (within-family) experience— were observed in the presence of information sharing among software vendors or during a disruption caused by a catastrophe. Our study contributes to a better understanding of the specific expertise required for security countermeasure providers to be able to respond under varying conditions to fast-evolving malware.12/9/13
The ongoing digitization of multiple industries has drastically reduced the half-life of skills and capabilities acquired by knowledge workers through formal education. Thus, firms are forced to make significant ongoing investments in training their employees to remain competitive. Existing research has not examined the role of training in improving firm-level productivity of knowledge firms. This paper provides an innovative econometric framework to estimate returns to such employee training investments made by firms. We use a panel dataset of small- to medium-sized Indian IT services firms and assess how training enhances human capital, a critical input for such firms, thereby improving firm revenues. We use econometric approaches based on optimization of the firm’s profit function to eliminate the endogenous choice of inputs common in production function estimations. We find that an increase in training investments is significantly linked to an increase in revenue per employee. Further, marginal returns to training are increasing firm size. Therefore, relatively speaking, large firms benefit more from training. For the median company in our data, we find that a dollar invested in training yields a return of $4.67, and this effect approximately grows 2.5 times for the 75th percentile-sized firm. A variety of robustness checks, including the use of data envelopment analysis, are used to establish the veracity of our results.4/14/14
Online communities bring together individuals with shared interest in joint action or sustained interaction. Power law distributions of user popularity appear ubiquitous in online communities but their formation mechanisms are not well understood. This study tests for the emergence of power law distributions via the mechanisms of preferential attachment, least efforts, direct reciprocity, and indirect reciprocity. Preferential attachment, where new entrants favor connections with already popular participants, is the predominant explanation suggested by prior literature. Yet, the attribution of preferential attachment or any other mechanism as a single unitary reason for the emergence of power law distributions runs contrary to the social nature of online communities and does not account for diversity of participants’ motivation. Agent-based modeling is used to test if a single social mechanism alone or multiple mechanisms together can generate power law distributions observed in online communities. Data from 28 online communities is used to calibrate, validate, and analyze the simulation. Simulated communication networks are randomly generated according to parameters for each hypothesis. The fit of the power law distribution in the model testing subset is then compared against the fit for these simulated networks. The major finding is that, in contrast to research in more general network settings, neither preferential attachment nor any other single mechanism alone generates a power law distribution. Instead, a blended model of preferential attachment with other social network formation mechanisms was most consistent with power law distributions seen in online communities. This suggests the need to move away from stylized explanations of network emergence that rely on single theories toward more highly socialized and multitheoretic explanations of community development.3/11/14
Online discussion communities play an important role in the development of relationships and the transfer of knowledge within and across organizations. Their underlying technologies enhance these processes by providing infrastructures through which group-based communication can occur. Community administrators often make decisions about technologies with the goal of enhancing the user experience, but the impact of such decisions on how a community develops must also be considered. To shed light on this complex and under-researched phenomenon, we offer a model of key latent constructs influenced by technology choices and possible causal paths by which they have dynamic effects on communities. Two important community characteristics that can be impacted are community size (number of members) and community resilience (membership that is willing to remain involved with the community in spite of variability and change in the topics discussed). To model community development, we build on attraction–selection–attrition (ASA) theory, introducing two new concepts: participation costs (how much time and effort are required to engage with content provided in a community) and topic consistency cues (how strongly a community signals that topics that may appear in the future will be consistent with what it has hosted in the past). We use the proposed ASA theory of online communities (OCASA) to develop a simulation model of community size and resilience that affirms some conventional wisdom and also has novel and counterintuitive implications. Analysis of the model leads to testable new propositions about the causal paths by which technology choices affect the emergence of community size and community resilience, and associated implications for community sustainability.1-22-14
Reviews and product recommendations at online stores have enabled customers to readily evaluate alternative products prior to any purchase. In this context, firms generate recommendations to refer customers to a wider variety of products. They also display customer-generated online reviews to facilitate evaluation of those recommended products. This study integrates these two IT artifacts to investigate consumer choice vis-à-vis competing products. We use a dataset we collected from Amazon.com consisting of books, sales ranks, recommendations, reviews, and reviewers. We derive the granular impact of reviews, product referrals, and reviewer opinions on the dynamics of product sales within a competitive market using comprehensive econometric analyses.10/1/13
In this paper, we analyze patterns of transaction between individuals using data drawn from Kiva.org, a global online crowdfunding platform that facilitates prosocial, peer-to-peer lending. Our analysis, which employs an aggregate dataset of country-to-country lending volumes based on more than three million individual lending transactions that took place between 2005 and 2010, considers the dual roles of geographic distance and cultural differences on lenders’ decisions about which borrowers to support. While cultural differences have seen extensive study in the Information Systems literature as sources of friction in extended interactions, here, we argue and demonstrate their role in individuals’ selection of a transaction partner. We present evid ence that lenders do prefer culturally similar and geographically proximate borrowers. An analysis of the marginal effects indicates that an increase of one standard deviation in the cultural differences between lender and borrower countries is associated with 30 fewer lending actions, while an increase of one standard deviation in physical distance is associated with 0.23 fewer lending actions. We also identify a substitution effect between cultural differences and physical distance, such that a 50 percent increase in physical distance is associated with an approximate 30 percent decline in the effect of cultural differences. Considering approaches to overcoming the observed cultural effect, we offer some empirical evidence of the potential of IT-based trust mechanisms, focusing on Kiva’s reputation rating system for microfinance intermediaries. We discuss the implications of our findings for prosocial lending, online crowdfunding, and electronic markets more broadly.2/11/14
In this paper, a computational, mixed methods approach that combines qualitative analysis with a novel approach to sequence analysis for studying the entanglement of human activities and digital capabilities in organizational routines is described. The approach is scalable across multiple contexts and complements the dominant idiographic modes of sociomaterial inquiry. The approach is rooted in the epistemology of a “rational reconstruction” consistent with the interpretive stance underlying the sociomaterial position. It arms researchers with the means to seek and uncover regularities in the ways human activities and digital capabilities become entangled across contexts by enabling the identification and articulation of generalizable patterns of sociomaterial activity. The computational approach is founded on sequence-analytic techniques that originated from the field of computational biology (genetics), but are now gaining popularity in the study of temporally ordered social phenomena such as organizational routines. These techniques are extended by drawing upon theoretical insights gained within sociomaterial scholarship on how the digital and the social become entangled. By detecting the variation in activities, actors, artifacts, and affordances that comprise what we denote a sociomaterial routine, the approach directly attends to ways in which human actors and the material features of technology become entangled in patterns of practice. Beyond motivating and describing the approach, the different insights that researchers can generate through its application in the study of the digitalization of organizational routines are illustrated. We conclude by suggesting several lines of inquiry that can enrich sociomaterial research.09/26/13
In taking into account the ways in which material and social realms are constitutively entangled within organizations, it is rhetorically tempting to say that technologies and social structures reconfigure each other. But what does it mean to reconfigure? How does one “figure” the other and how do we fully embrace a mutually constitutive relationship when examining fluid relations? This paper delves into these questions by exploring how physical, social, material, technological, and organizational arrangements dynamically reconfigure each other in the duration of organizational practice. Using the venue of space exploration, we present three empirical examples from an ethnographic engagement with a NASA mission orbiting an outer planet in the solar system to examine various configurations and sociomaterial relations. In this endeavor, we suggest that theoretical and empirical traction can be gained by focusing attention on the dynamic reconfigurations between social and material realms. In so doing, we call attention to the ways in which current sociomaterial perspectives have difficulty articulating the shifting, figural, asymmetric and dynamic negotiations between people, social structures, information technologies, and representational objects. This paper contributes to current discussions of sociomaterial relations in information systems research by presenting an empirical treatment of entangled and shifting reconfigurations and providing language for engaging with this perspective.10/23/13
Sociomateriality has been attracting growing attention in the Organization Studies and Information Systems literatures since 2007, with more than 140 journal articles now referring to the concept. Over 80 percent of these articles have been published since January 2011 and almost all cite the work of Orlikowski (2007, 2010; Orlikowski and Scott 2008) as the source of the concept. Only a few, however, address all of the notions that Orlikowski suggests are entailed in sociomateriality, namely materiality, inseparability, relationality, performativity, and practices, with many employing the concept quite selectively. The contribution of sociomateriality to these literatures is, therefore, still unclear. Drawing on evidence from an ongoing study of the adoption of a computer-based clinical information system in a hospital critical care unit, this paper explores whether the notions, individually and collectively, offer a distinctive and coherent account of the relationship between the social and the material that may be useful in Information Systems research. It is argued that if sociomateriality is to be more than simply a label for research employing a number of loosely related existing theoretical approaches, then studies employing the concept need to pay greater attention to the notions entailed in it and to differences in their interpretation.9/27/13
Research examining the relationship between IT–business strategic alignment (hereafter referred to as alignment) and firm performance (hereafter referred to as performance) has produced apparently conflicting findings (i.e., an alignment paradox). To examine the alignment paradox, we conducted a meta-analysis that probed the interrelationships between alignment, performance, and context constructs. We found the alignment dimensions (intellectual, operational, and cross-domain) demonstrate unique relationships with the different performance types (financial performance, productivity, and customer benefit) and with many of the other constructs in alignment’s nomological network. All mean corrected correlations between dimensions of alignment and dependent variables were positive and most of the credibility interval values in these analyses were also positive. Overall, the evidence gathered from the extant literature suggests there is not much of an alignment paradox. This study contributes to the literature by clarifying the relationships between alignment and performance outcomes and offering insight into sources of inconsistencies in alignment research. By doing so, this paper lays a foundation for more consistent treatment of alignment in future IT research.07/16/14
Online platforms offer access to a larger social group than is generally available through offline contacts, making the Internet an emerging venue for seeking casual sex partners. The ease of seeking sex partners through classified ad sites may promote risky behaviors that increase the transmission of STDs. In this paper, using a natural experiment setup, we investigate whether the entry of a major online personals ad site, Craigslist, increases the prevalence of HIV over a 10 year period from 1999 to 2008 across 33 states in the United States. After controlling for extraneous factors, our results suggest that the entry of Craigslist is related to a 15.9 percent increase in HIV cases. Our analysis suggests that the site entry produces an average of 6,130 to 6,455 cases of HIV infection in the United States each year, mapping out to between $62 million and $65.3 million in annual treatment costs. In addition, the analyses reveal that nonmarket-related casual sex is the primary driver of the increase in HIV cases, in contrast to paid transactions solicited on the site (e.g., escort services and prostitution), which has a negative relationship with HIV trends. These findings are essential to understanding the social routes through which HIV transmission takes place and the extent to which site entry can influence HIV trends. Implications for healthcare practitioners and policy makers are discussed..12/9/13
In this paper, we develop a game theoretic model to study the pricing of e-books and e-readers under two pricing models: wholesale and agency. We analyze pricing strategies for a publisher and a retailer. We identify the complementary relationship between e-books and e-readers as the main reason for the retailer to set a low e-book price in the wholesale model. Comparing the wholesale and the agency models, we find, in a wide range of market conditions, the price for e-book readers is lower in the agency model, leading to a higher e-book market share. However, a higher e-book price in the agency model lowers e-book consumption. Overall social welfare is lower in the agency model than in the wholesale model. While total consumer surplus is slightly higher in the agency model, largely because of a lower e-reader price, business profit is lower. The publisher, surprisingly, is worse off under the agency model.5/13/14
Recent papers have shown that, in contrast to the long tail theory, movie sales remain concentrated in a small number of hits. These papers have argued that concentrated sales can be explained, in part, by heterogeneity in quality and increasing returns from social effects. Our research analyzes an additional explanation: how incomplete information may skew sales patterns. We use the movie broadcast on pay-cable channels as an exogenous shock to the availability of information, and analyze how this shock changes the resulting sales distribution.Our data show that the pay-cable broadcast shifts the distribution of DVD sales toward long tail movies, suggesting an information spillover from the broadcast. We develop a learning-based movie discovery model to precisely quantify the two mechanisms of movie discovery: word-of-mouth from previous sales and information spillover from broadcast. We use this model to estimate the lost DVD sales due to incomplete information. Our study contributes to the literature by analyzing how information provided in one channel can change the assortment of the same products demanded in another channel.07/02/14
This paper explores value creation from government use of information technologies (IT). While the majority of studies in the information systems (IS) discipline have focused on discovering IT business value in for-profit organizations, the performance effects of IT in the public sector have not been extensively studied in either the IS or the public administration literature. We examine whether IT improves administrative efficiency in U.S. state governments. Utilizing IT budget data in state governments, the census data on state government expenditures, and a variety of information on public services that states provide, we measure technical efficiency with a stochastic frontier analysis and a translog cost function and estimate the effect of IT spending on efficiency. Our analyses provide evidence for a positive relationship between IT spending and cost efficiency and indicate that, on average, a $1 increase in per capita IT budget is associated with $1.13 in efficiency gains. This study contributes to the IS literature by expanding the scope of IT value research to public sector organizations and provides meaningful implications for elected officials and public sector managers.5/15/14
Organizational knowledge is one of the most important assets of an enterprise. Therefore, many organizations invest in enterprise social media (ESM) to establish electronic networks of practice and to foster knowledge exchange among employees. ESM improves interaction transparency and can be regarded as a sociotechnical system that provides a language for communication and symbolic action as well as a better sense of others’ social identity. Accordingly, the individual characteristics of knowledge seekers and contributors determine why and how interactions occur. However, existing studies tend to focus only on knowledge contributors’ characteristics and to treat knowledge as an object that needs to be transferred. To address this gap, this study conceptualizes and empirically tests a multilevel model of knowledge exchange in electronic networks of practice (ENoP) that includes the characteristics of knowledge seekers and knowledge contributors as well as their dyadic relationship from an activity-centered language/action point of view. A dataset of 15,505 enterprise microblogging messages reveals that knowledge seekers’ characteristics and relational factors drive knowledge exchanges in social media-enabled ENoP. Focusing on organizations with knowledge exchanges supported by information technology, our research extends prior findings by providing the first evidence that the communicative act expressed by question–answer pairs impacts the quality of knowledge exchanged.2/14/14
User communities are increasingly becoming an essential element of companies’ business processes. However, reaping the benefits of such social systems does not always prove effective, often because companies fail to stimulate members’ collaboration continuously or neglect their social integration. Following communication accommodation theory, the authors posit that members’ communication style alignment symbolically reflects their community identification and affects subsequent participation behavior. This research uses text mining to extract the linguistic style properties of 74,246 members’ posts across 37 user communities. Two mixed multilevel Poisson regression models show that when members’ linguistic style matches with the conventional community style, it signals their community identification and affects their participation quantity and quality. Drawing on an expanded view of organizational identification, the authors consider dynamics in members’ social identification by examining trends and reversals in linguistic style match developments. Whereas a stronger trend of alignment leads to greater participation quantity and quality, frequent reversals suggest lower participation quantity. At a community level, greater synchronicity in the linguistic style across all community members fosters individual members’ participation behavior.6/25/14
Online business-to-business (B2B) exchanges are proliferating, giving firms numerous platforms from which to choose. Many firms are also multihoming, using competing platforms concurrently. In this study, we examine how selling and buying activities on B2B exchanges affect multihoming buyers’ preferences for exchanges. We posit that these activities influence buyers’ perceived returns and risks of using the exchanges, and impact buyers’ preferences. Using a unique dataset of 118 buyers’ participation in two B2B exchanges over seven months, we find that buyers prefer exchanges with more selling activities. However, buyers’ preferences and buying levels on the exchanges are non-monotonically related. At low buying levels, an increase in buying by others positively affects buyers’ preferences. This effect may result from observational learning, where individual buyers learn from other buyers’ behaviors. On the other hand, as buying level increases further on the exchange, competition among buyers also increases. Consequently, buyers lower their preferences for the exchange. In addition, we find that the effects of selling and buying activities on buyers’ preferences change over time. Our results highlight the need to correctly model buyers’ homing behavior; failing to do so could bias the picture of competitive dynamics between platforms and lead to suboptimal strategies by exchanges.6/19/14
In recent years, the world has witnessed a number of severe natural disasters, causing heavy losses to families, communities, and even nations. Natural disaster management (NDM) websites play an important role in assisting people through various disaster stages. However, such websites are complex and there is little research on standards and guidelines for their development and evaluation. In this paper, we develop an ontology-based evaluation tool to assess the utility of NDM websites. Two main groups of stakeholders— experts who are in charge of NDM websites and potential users of such websites—contributed to the process. A total of 73 experts validated the ontology developed for NDM web elements through a Delphi study. These experts also provided importance ratings for web elements in the ontology. In a survey of the second major group of stakeholders—potential users—818 participants provided another set of importance ratings for web elements in the ontology. The design theory in this work is based on utility theory. The metrics for the evaluation of websites are relative utility and absolute utility. Using the evaluation tool, we evaluated the NDM websites of the 50 U.S. states from the perspectives of the two groups of stakeholders. The results indicate a lack of readiness in most of these websites.6/19/14
Organizational crisis management has traditionally favored a centralized plan-and-control approach. This study explores the possibility for an orderly crisis management process to arise unintentionally from decentralized and spontaneous actions in an online community (i.e., self-organization). Based on complex adaptive systems theory, a multilevel model is developed to account for the logical relation between individual-level actions and interactions in an online community and an organizational-level orderly and rational crisis management process, as described by the organizational crisis management literature. We apply this multilevel model to an analysis of 89,596 posts from an online community that was deeply embedded in an earthquake-induced organizational crisis. Results indicate that fluctuation of message content themes in this online community served to energize continuous input from ordinary organization members. These input actualized new possibilities offered by the technology platform for crisis management actions (i.e., actualized IT affordances). Concatenation of immediate impacts of message content themes and actualized IT affordances formed feedback loops that moderated the crisis management activities toward an efficient trajectory. Our findings challenge the traditional assumption that macro-level order requires micro-level order-seeking behaviors. They suggest the viability of self-organization as a new source of organizational order that complements the traditional centralized plan-and-control approach. Theoretical and empirical implications for harnessing the power of ordinary organization members connected by today’s technology platforms are discussed.08/08/14
In this paper, we study the effect of peer influence in the diffusion of the iPhone 3G across a number of communities sampled from a large dataset provided by a major European Mobile carrier in one country. We identify tight communities of users in which peer influence may play a role and use instrumental variables to control for potential correlation between unobserved subscriber heterogeneity and friends’ adoption. We provide evidence that the propensity of a subscriber to adopt increases with the percentage of friends who have already adopted. During a period of 11 months, we estimate that 14 percent of iPhone 3Gs sold by this carrier were due to peer influence. This result is obtained after controlling for social clustering, gender, previous adoption of mobile Internet data plans, ownership of technologically advanced handsets, and heterogeneity in the regions where subscribers move during the day and spend most of their evenings. This result remains qualitatively unchanged when we control for changes over time in the structure of the social network. We provide results from several policy experiments showing that, with this level of effect of peer influence, the carrier would have hardly benefitted from using traditional marketing strategies to seed the iPhone 3G to benefit from viral marketing.9/8/14
Using a mixed-methods approach, we develop the concept of perceived proximity, which is created through communication, shared identity, and the symbolic aspects thereof. Building on previous theoretical work, we create and validate measures of perceived proximity. Then, we compare how perceived proximity and objective distance relate to relationship quality for collocated and geographically dispersed work colleagues. Our results show that perceived proximity (i.e., a cognitive and affective sense of relational closeness) and not physical proximity (i.e., geographic closeness measured in miles or kilometers) affects relationship quality in an international survey of more than 600 people and 1,300 dyadic work relationships. We also find that people’s perceptions of proximity mediate the effects of communication and identification on relationship quality. Using qualitative data (2,289 comments from 1,188 respondents coded into 9 themes), we explore the symbolic meaning of perceived proximity. We show how people can form strong bonds despite being separated by large distances and continue to shift the emphasis from information systems as “pipes” or channels to information systems as vehicles for conveying shared meaning and symbolic value. Our findings have important implications for scholars, managers, systems designers, and members of virtual teams, teleworkers, and other geographically dispersed contexts.1/29/14
This study examines how sellers respond to changes in the design of reputation systems on eBay. Specifically, we focus on one particular strategic behavior on eBay’s reputation system: sellers’ explicit retaliation against negative feedback provided by buyers to coerce buyers into revoking their negative feedback. We examine how these strategic sellers respond to removal of their ability to retaliate against buyers. We utilize one key policy change of eBay’s reputation system, which provides a natural experimental setting that allows us to infer the causal impact of the reputation system on seller behavior. Our results show that coercing buyers to revoke their negative feedback through retaliation enables low-quality sellers to manipulate their reputations and masquerade as high-quality sellers. We find that these sellers reacted strongly to eBay’s announcement of a proposed ban on revoking. Interestingly, after the power of these strategic sellers is curtailed, we find evidence that they exert more efforts to improve their reputation scores. This study provides valuable insights about the relationship between reputation system and seller behavior, which have important implications for the design of online reputation mechanisms..5/13/14
Regression techniques can be used not only for legitimate data analysis, but also to infer private information about individuals. In this paper, we demonstrate that regression trees, a popular data-analysis and data-mining technique, can be used to effectively reveal individuals’ sensitive data. This problem, which we call a regression attack, has not been addressed in the data privacy literature, and existing privacy-preserving techniques are not appropriate in coping with this problem. We propose a new approach to counter regression attacks. To protect against privacy disclosure, our approach introduces a novel measure, called digression, which assesses the sensitive value disclosure risk in the process of building a regression tree model. Specifically, we develop an algorithm that uses the measure for pruning the tree to limit disclosure of sensitive data. We also propose a dynamic value-concatenation method for anonymizing data, which better preserves data utility than a user-defined generalization scheme commonly used in existing approaches. Our approach can be used for anonymizing both numeric and categorical data. An experimental study is conducted using real-world financial, economic, and healthcare data. The results of the experiments demonstrate that the proposed approach is very effective in protecting data privacy while preserving data quality for research and analysis.12/11/13
Globalization has triggered a rapid increase in cross-border mergers and acquisitions (M&As). However, research shows that only 17 percent of cross-border M&As create shareholder value. One of the main reasons for this poor track record is top management’s lack of attention to nonfinancial aspects (e.g., sociocultural aspects) of M&As. With the rapid growth of Web 2.0 applications, online environmental scanning provides top executives with unprecedented opportunities to tap into collective web intelligence to develop better insights about the sociocultural and political–economic factors that cross-border M&As face. Grounded in Porter’s five forces model, one major contribution of our research is the design of a novel due diligence scorecard model that leverages collective web intelligence to enhance M&A decision making. Another important contribution of our work is the design and development of an adaptive business intelligence (BI) 2.0 system underpinned by an evolutionary learning approach, domain-specific sentiment analysis, and business relation mining to operationalize the aforementioned scorecard model for adaptive M&A decision support. With Chinese companies’ cross-border M&As as the business context, our experimental results confirm that the proposed adaptive BI 2.0 system can significantly aid decision makers under different M&A scenarios. The managerial implication of our findings is that firms can apply the proposed BI 2.0 technology to enhance their strategic decision making, particularly when making cross-border investments in targeted markets for which private information may not be readily available.
